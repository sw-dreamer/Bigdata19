# 시카고 샌드위치 맛집 분석

## 학습목표
- 웹 데이터를 가져오는 Beautiful Soup 익히기
- 크롬 개발자 도구를 이용해서 원하는 태그 찾기
- 시카고 샌드위치 맛집 소개 사이트에 접근하기
- 접근한 웹 페이지에서 원하는 데이터 추출하고 정리하기
- 다수의 웹페이지에서 자동으로 접근해서 원하는 정보 가져오기 => a태그 반복 사용 
- Jupyter notebook 상태 진행바 생성
- 상태 진행바 적용 페이지 접근하기
- 50개 웹페이지 정보 가져오기

---

## 웹 데이터를 가져오는 Beautiful Soup 익히기

```
# BeautifulSoup 라이브러리 사용
from bs4 import BeautifulSoup
```
![image](https://github.com/user-attachments/assets/3472e2c6-2d54-497d-8cb1-7418be46ac69)

```
pip install bs4
```
![image](https://github.com/user-attachments/assets/202fda29-1d25-49c8-8a08-860379d907e3)

```
conda install beautifulsoup4
```
![image](https://github.com/user-attachments/assets/11abc0f3-7cc0-41c8-bee8-d3a9c18ada0b)


```
# BeautifulSoup 라이브러리 사용
from bs4 import BeautifulSoup

page=open('./data/03. test_first.html','r') # file object 생성
page = page.read() # 전채를 읽어서 문자열로 반환
page
```

![image](https://github.com/user-attachments/assets/5db4e29e-54f0-493a-b177-64eb924c5105)

```
# BeautifulSoup 라이브러리 사용
from bs4 import BeautifulSoup

page=open('./data/03. test_first.html','r').read()
page
```
![image](https://github.com/user-attachments/assets/6edfe870-02f8-4279-9965-6b93b6d170c3)

```
# BeautifulSoup 라이브러리 사용
from bs4 import BeautifulSoup

page=open('./data/03. test_first.html','r').read()
print(page)
```
![image](https://github.com/user-attachments/assets/6f938409-fdbc-4e82-baa9-ab0eb02ae27a)

```
# BeautifulSoup 라이브러리 사용
from bs4 import BeautifulSoup

page=open('./data/03. test_first.html','r').read()
# print(page) # 문자열일뿐이다.
# print(page,'html.parser') 해야지 dom tree로 나온다
soup=BeautifulSoup(page, 'html.parser')  # dom tree 생성되고 selector 가능해진다
soup
```
![image](https://github.com/user-attachments/assets/50edbb28-86ad-49e1-a170-4ff1c2164199)

데이터 추출시 자주 사용하는 메소드
1. find() : 주어진 태그와 속성을 가진 첫번째 요소를 검색할 때 사용
2. find_all() : 주어진 태그와 속성을 가진 모든 요소를 검색할 때 사용
3. select_one() : CSS selector 사용해서 첫번째 요소를 검색할 때 사용
4. select() :CSS selector 사용해서 모든 요소를 검색할 때 사용

```
from bs4 import BeautifulSoup
# 데이터 추출시 자주 사용하는 메소드
# 1. find() : 주어진 태그와 속성을 가진 첫번째 요소를 검색할 때 사용
# 2. find_all() : 주어진 태그와 속성을 가진 모든 요소를 검색할 때 사용
# 3. select_one() : CSS selector 사용해서 첫번째 요소를 검색할 때 사용
# 4. select() :CSS selector 사용해서 모든 요소를 검색할 때 사용
soup=BeautifulSoup(page, 'html.parser') 
list(soup.children)[2]
```
![image](https://github.com/user-attachments/assets/86296139-5507-489f-b41e-de9319038917)

```
from bs4 import BeautifulSoup
soup=BeautifulSoup(page, 'html.parser') 
html=list(soup.children)[2]
list(html.children)
```
![image](https://github.com/user-attachments/assets/e10850b0-4d5b-4a5f-b386-40819c5282b9)

```
from bs4 import BeautifulSoup
soup=BeautifulSoup(page, 'html.parser') 
html=list(soup.children)[2]

body=list(html.children)[3]

list(body.children)
```
![image](https://github.com/user-attachments/assets/18b02225-4a72-4f03-9977-601b5b23076a)

```
from bs4 import BeautifulSoup
soup=BeautifulSoup(page, 'html.parser') 

# p 태그 전체 검색
soup.find_all('p')
```
![image](https://github.com/user-attachments/assets/cf4ddbfb-8bfd-47f7-9d8c-1b5c0ea76f08)

```
from bs4 import BeautifulSoup
soup=BeautifulSoup(page, 'html.parser') 

# p 태그 검색
soup.find('p')
```
![image](https://github.com/user-attachments/assets/78a8719e-e1bb-4fc9-b1c1-95e84e5c9456)

p tag 전체 검색
- find_all('p')는 검색 결과 여러개를 리스트로 반환
- find('p')는 첫번째 검색 결과를 반환

```
from bs4 import BeautifulSoup
soup=BeautifulSoup(page, 'html.parser') 

# p태그이면서 outer-text인 class만 출력
soup.find_all('p',class_='outer-text')
```
![image](https://github.com/user-attachments/assets/3b0f0e66-2b25-4129-abf3-5919e20bcc00)

```
from bs4 import BeautifulSoup
soup=BeautifulSoup(page, 'html.parser') 

# p태그이면서 id가 first인 것을 출력
soup.find_all('p',id='first')
```
![image](https://github.com/user-attachments/assets/129238c6-fa8b-4db8-a529-85829a975584)

```
from bs4 import BeautifulSoup
soup=BeautifulSoup(page, 'html.parser') 

# 데이터들만 추출
soup.find_all('p')[0].get_text() #get_text()는 문자열(데이터) 추출
```
![image](https://github.com/user-attachments/assets/0b21a015-d5ab-4362-97a6-3bf9ce756dc0)

```
from bs4 import BeautifulSoup
soup=BeautifulSoup(page, 'html.parser') 

soup.find_all('a')
```
![image](https://github.com/user-attachments/assets/6cd4eb8e-a617-4c7b-b689-f8960a48a049)

```
from bs4 import BeautifulSoup
soup=BeautifulSoup(page, 'html.parser') 

# 링크만 갖고오기
links=soup.find_all('a')
link =links[0]
link.get_attribute_list('href')
```
![image](https://github.com/user-attachments/assets/1569ff3f-5b1f-49ca-b07a-ea58dc2be33b)

```
from bs4 import BeautifulSoup
soup=BeautifulSoup(page, 'html.parser') 

# 링크만 갖고오기
links=soup.find_all('a')
link =links[0]
link['href']
```
![image](https://github.com/user-attachments/assets/226fb59d-ad5c-4bbf-baf0-c748adb5f935)

---
## 네이버 증권 사이트에서 환율 정보 추출

![image](https://github.com/user-attachments/assets/9f2b18aa-8d71-4a03-99b0-d6cbf59ce500)

위 화면에서 파이썬 코드로 환율 정보 가져오기(https://finance.naver.com/marketindex/)

```
from bs4 import BeautifulSoup
from urllib.request import urlopen

url='https://finance.naver.com/marketindex/'
page=urlopen(url)
page
```
![image](https://github.com/user-attachments/assets/0582b826-a610-4e69-81cc-f327c46ac93a)

```
from bs4 import BeautifulSoup
from urllib.request import urlopen

url='https://finance.naver.com/marketindex/'
page=urlopen(url)

soup=BeautifulSoup(page,'html.parser')
soup
```

![image](https://github.com/user-attachments/assets/792e152b-c739-4717-ace7-3a7ed6b030ff)

```
from bs4 import BeautifulSoup
from urllib.request import urlopen

url='https://finance.naver.com/marketindex/'
page=urlopen(url)

soup=BeautifulSoup(page,'html.parser')

soup.find_all('span',class_='value')
```
![image](https://github.com/user-attachments/assets/b92f9a4d-73c3-4bd6-ab7b-87b8a0b0efb1)

```
from bs4 import BeautifulSoup
from urllib.request import urlopen

url='https://finance.naver.com/marketindex/'
page=urlopen(url)

soup=BeautifulSoup(page,'html.parser')

soup.find_all('span',class_='value')[0]
```
![image](https://github.com/user-attachments/assets/e860870e-4b1b-4700-822c-e0c47b10d280)

```
from bs4 import BeautifulSoup
from urllib.request import urlopen

url='https://finance.naver.com/marketindex/'
page=urlopen(url)

soup=BeautifulSoup(page,'html.parser')

soup.find_all('span',class_='value')[0].get_text()
```
![image](https://github.com/user-attachments/assets/f1d2f78e-dc37-47f3-94d0-dfa8f8b96205)

```
from bs4 import BeautifulSoup
from urllib.request import urlopen

url='https://finance.naver.com/marketindex/'
page=urlopen(url)

soup=BeautifulSoup(page,'html.parser')

soup.find_all('span',class_='value')[0].string
```
![image](https://github.com/user-attachments/assets/56da9a8e-b355-4843-879e-2fe274a70be3)

---
## 시카고 샌드위치 맛집 소개 사이트에 접근하기

https://www.chicagomag.com/Chicago-Magazine/November-2012/Best-Sandwiches-Chicago

![image](https://github.com/user-attachments/assets/b1b927ed-9430-4dce-bede-b469a1262896)

```
from bs4 import BeautifulSoup
from urllib.request import urlopen
url_base = 'http://www.chicagomag.com'
url_sub = '/Chicago-Magazine/November-2012/Best-Sandwiches-Chicago/'
url = url_base + url_sub
html = urlopen(url)
soup = BeautifulSoup(html, "html.parser")
soup
```
![image](https://github.com/user-attachments/assets/7eeca62c-9c2b-4cf0-a4b9-e3cd2cc1ea38)


```
from bs4 import BeautifulSoup
from urllib.request import urlopen

import numpy as np
import pandas as pd

import requests

url_base = 'https://www.chicagomag.com'
url_sub = '/Chicago-Magazine/November-2012/Best-Sandwiches-Chicago/'
url = url_base + url_sub

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

response = requests.get(url=url, headers=headers)
html = response.text
print(html)

```
![image](https://github.com/user-attachments/assets/123bf8d8-e184-420c-a8c7-1bc101be1dcb)

```
from bs4 import BeautifulSoup
from urllib.request import urlopen

import numpy as np
import pandas as pd

import requests

url_base = 'https://www.chicagomag.com'
url_sub = '/Chicago-Magazine/November-2012/Best-Sandwiches-Chicago/'
url = url_base + url_sub

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

response = requests.get(url=url, headers=headers)
html = response.text

soup  = BeautifulSoup(html,'html.parser')
print(soup)
```
![image](https://github.com/user-attachments/assets/37273c81-5b95-427b-89a7-1fa108616149)

```
from bs4 import BeautifulSoup
from urllib.request import urlopen

import numpy as np
import pandas as pd

import requests

url_base = 'https://www.chicagomag.com'
url_sub = '/Chicago-Magazine/November-2012/Best-Sandwiches-Chicago/'
url = url_base + url_sub

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

response = requests.get(url=url, headers=headers)
html = response.text

soup  = BeautifulSoup(html,'html.parser')

soup.find_all('div',class_='sammy')
```
![image](https://github.com/user-attachments/assets/73425d59-39fe-4301-817d-cabab5399c9f)

```
from bs4 import BeautifulSoup
from urllib.request import urlopen

import numpy as np
import pandas as pd

import requests

url_base = 'https://www.chicagomag.com'
url_sub = '/Chicago-Magazine/November-2012/Best-Sandwiches-Chicago/'
url = url_base + url_sub

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

response = requests.get(url=url, headers=headers)
html = response.text

soup  = BeautifulSoup(html,'html.parser')

len(soup.find_all('div',class_='sammy'))
```
![image](https://github.com/user-attachments/assets/2c86e8b6-214e-4e03-972a-7e3f0c3c1a35)

```
from bs4 import BeautifulSoup
from urllib.request import urlopen

import numpy as np
import pandas as pd

import requests

url_base = 'https://www.chicagomag.com'
url_sub = '/Chicago-Magazine/November-2012/Best-Sandwiches-Chicago/'
url = url_base + url_sub

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

response = requests.get(url=url, headers=headers)
html = response.text

soup  = BeautifulSoup(html,'html.parser')

soup.find_all('div',class_='sammy')[0]
```
![image](https://github.com/user-attachments/assets/0b0a0cc8-35f5-4b15-9459-c566c73d81d3)

```
from bs4 import BeautifulSoup
from urllib.request import urlopen

import numpy as np
import pandas as pd

import requests

url_base = 'https://www.chicagomag.com'
url_sub = '/Chicago-Magazine/November-2012/Best-Sandwiches-Chicago/'
url = url_base + url_sub

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

response = requests.get(url=url, headers=headers)
html = response.text

soup  = BeautifulSoup(html,'html.parser')

tmp = soup.find_all('div',class_='sammy')[0]

tmp.find(class_='sammyRank').get_text()
```
![image](https://github.com/user-attachments/assets/42e2435d-f9e3-4cda-8710-1881ab765bd0)

```
from bs4 import BeautifulSoup
from urllib.request import urlopen

import numpy as np
import pandas as pd

import requests

url_base = 'https://www.chicagomag.com'
url_sub = '/Chicago-Magazine/November-2012/Best-Sandwiches-Chicago/'
url = url_base + url_sub

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

response = requests.get(url=url, headers=headers)
html = response.text

soup  = BeautifulSoup(html,'html.parser')

tmp = soup.find_all('div',class_='sammy')[0]

tmp.find(class_='sammyListing').get_text().split('\n')
```
![image](https://github.com/user-attachments/assets/3a9519bb-0f7e-4e5d-8bda-c703a7f89027)

```
from bs4 import BeautifulSoup
from urllib.request import urlopen

import numpy as np
import pandas as pd

import requests

url_base = 'https://www.chicagomag.com'
url_sub = '/Chicago-Magazine/November-2012/Best-Sandwiches-Chicago/'
url = url_base + url_sub

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

response = requests.get(url=url, headers=headers)
html = response.text

soup  = BeautifulSoup(html,'html.parser')

tmp = soup.find_all('div',class_='sammy')[0]

temp=tmp.find(class_='sammyListing').get_text().split('\n')

temp[0],temp[1]
```
![image](https://github.com/user-attachments/assets/53e963d7-881d-4e1c-9dbc-6152897b58a7)

```
from bs4 import BeautifulSoup
from urllib.request import urlopen

import numpy as np
import pandas as pd

import requests

url_base = 'https://www.chicagomag.com'
url_sub = '/Chicago-Magazine/November-2012/Best-Sandwiches-Chicago/'
url = url_base + url_sub

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

response = requests.get(url=url, headers=headers)
html = response.text

soup  = BeautifulSoup(html,'html.parser')

tmp = soup.find_all('div',class_='sammy')[0]

temp=tmp.find(class_='sammyListing').get_text().split('\n')

tmp.find_all('a')[0]['href']
```
![image](https://github.com/user-attachments/assets/869c3073-7358-4676-9ed5-42ab65d8f894)

```
from bs4 import BeautifulSoup
from urllib.request import urlopen

import numpy as np
import pandas as pd

import requests

url_base = 'https://www.chicagomag.com'
url_sub = '/Chicago-Magazine/November-2012/Best-Sandwiches-Chicago/'
url = url_base + url_sub

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

response = requests.get(url=url, headers=headers)
html = response.text

soup  = BeautifulSoup(html,'html.parser')

# 샌드위치 메인 페이지 데이터 추출
# 순위, 메인메뉴, 카페명, 세부페이지 이동 url
ranks=[]
main_menus=[]
cafe_names=[]
url_address=[] # 상세로 들어가는 url

# 50개 순위와 정보가 들어 잇는 리스트
list_soup=soup.find_all('div',class_='sammy')

# 반복 처리해서 데이터를 추출하고 각 리스트에 저장
for item in list_soup: # 50번만 반복
    # 순위 추출해서 저장
    ranks.append(item.find(class_='sammyRank').text)
    # 메인메뉴와 카페명 추출해서 저장
    tmp_str=item.find(class_='sammyListing').text
    main_menus.append(tmp_str.split('\n')[0])
    cafe_names.append(tmp_str.split('\n')[1])
    
    # url
    tmp_url=item.find('a')['href'] # sub의 url을 추출
    # tmp_url 추출 결과 sub가 아닌 경우도 있음 : https:// 시작하거나 /Chicago 으로 시작할수 잇다
    # https://으로 시작하면 전체를 가지고오고 /Chicago로 시작하면 base를 더해줘야한다.
    if tmp_url.startswith('https://'): #전체 url이 다 있는 경우
        url_address.append(tmp_url)
    else:
        url_address.append(url_base+tmp_url)

print('ranks :',ranks)
print('main_menus : ',main_menus)
print('cafe_names : ',cafe_names)
print('url_address : ',url_address)
```
![image](https://github.com/user-attachments/assets/ff15d5e1-0f2a-42ef-b5d4-d5246ecd0331)

```
from bs4 import BeautifulSoup
from urllib.request import urlopen

import numpy as np
import pandas as pd

import requests

url_base = 'https://www.chicagomag.com'
url_sub = '/Chicago-Magazine/November-2012/Best-Sandwiches-Chicago/'
url = url_base + url_sub

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

response = requests.get(url=url, headers=headers)
html = response.text

soup  = BeautifulSoup(html,'html.parser')

# 샌드위치 메인 페이지 데이터 추출
# 순위, 메인메뉴, 카페명, 세부페이지 이동 url
ranks=[]
main_menus=[]
cafe_names=[]
url_address=[] # 상세로 들어가는 url

# 50개 순위와 정보가 들어 잇는 리스트
list_soup=soup.find_all('div',class_='sammy')

# 반복 처리해서 데이터를 추출하고 각 리스트에 저장
for item in list_soup: # 50번만 반복
    # 순위 추출해서 저장
    ranks.append(item.find(class_='sammyRank').text)
    # 메인메뉴와 카페명 추출해서 저장
    tmp_str=item.find(class_='sammyListing').text
    main_menus.append(tmp_str.split('\n')[0])
    cafe_names.append(tmp_str.split('\n')[1])
    
    # url
    tmp_url=item.find('a')['href'] # sub의 url을 추출
    # tmp_url 추출 결과 sub가 아닌 경우도 있음 : https:// 시작하거나 /Chicago 으로 시작할수 잇다
    # https://으로 시작하면 전체를 가지고오고 /Chicago로 시작하면 base를 더해줘야한다.
    if tmp_url.startswith('https://'): #전체 url이 다 있는 경우
        url_address.append(tmp_url)
    else:
        url_address.append(url_base+tmp_url)

data_df=pd.DataFrame(
    {
        'Rank': ranks
        ,'Menu': main_menus
        ,'Cafe': cafe_names
        ,'Url': url_address
    }
)
data_df.head()
```
![image](https://github.com/user-attachments/assets/d7c984ba-8280-4fcc-9e0b-23cd669fd537)

```
from bs4 import BeautifulSoup
from urllib.request import urlopen

import numpy as np
import pandas as pd

import requests

url_base = 'https://www.chicagomag.com'
url_sub = '/Chicago-Magazine/November-2012/Best-Sandwiches-Chicago/'
url = url_base + url_sub

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

response = requests.get(url=url, headers=headers)
html = response.text

soup  = BeautifulSoup(html,'html.parser')

# 샌드위치 메인 페이지 데이터 추출
# 순위, 메인메뉴, 카페명, 세부페이지 이동 url
ranks=[]
main_menus=[]
cafe_names=[]
url_address=[] # 상세로 들어가는 url

# 50개 순위와 정보가 들어 잇는 리스트
list_soup=soup.find_all('div',class_='sammy')

# 반복 처리해서 데이터를 추출하고 각 리스트에 저장
for item in list_soup: # 50번만 반복
    # 순위 추출해서 저장
    ranks.append(item.find(class_='sammyRank').text)
    # 메인메뉴와 카페명 추출해서 저장
    tmp_str=item.find(class_='sammyListing').text
    main_menus.append(tmp_str.split('\n')[0])
    cafe_names.append(tmp_str.split('\n')[1])
    
    # url
    tmp_url=item.find('a')['href'] # sub의 url을 추출
    # tmp_url 추출 결과 sub가 아닌 경우도 있음 : https:// 시작하거나 /Chicago 으로 시작할수 잇다
    # https://으로 시작하면 전체를 가지고오고 /Chicago로 시작하면 base를 더해줘야한다.
    if tmp_url.startswith('https://'): #전체 url이 다 있는 경우
        url_address.append(tmp_url)
    else:
        url_address.append(url_base+tmp_url)

data_df=pd.DataFrame(
    {
        'Rank': ranks
        ,'Menu': main_menus
        ,'Cafe': cafe_names
        ,'Url': url_address
    }
)
data_df.to_csv(
    './data/03. 시카고 샌드위치 메인 페이지 데이터.csv'
    ,encoding='utf-8'
)
```
![image](https://github.com/user-attachments/assets/d7b70e1a-23f4-45b5-90fc-b00ba22bf5f1)

```
# 서브 페이지 이동해서 가격, 주소 추출
from bs4 import BeautifulSoup
from urllib.request import urlopen

import numpy as np
import pandas as pd

import requests

data_df=pd.read_csv(
    './data/03. 시카고 샌드위치 메인 페이지 데이터.csv'
    ,encoding='utf-8'
)

# 서브 페이지 이동해서 가격, 주소 추출
# 1등 서브 페이지로 이동해서 가격, 주소

url_str=data_df['Url'][0]
url_str
```
![image](https://github.com/user-attachments/assets/4df8c286-6947-4826-8388-dd3e20857fef)

```
# 서브 페이지 이동해서 가격, 주소 추출
from bs4 import BeautifulSoup
from urllib.request import urlopen

import numpy as np
import pandas as pd

import requests

data_df=pd.read_csv(
    './data/03. 시카고 샌드위치 메인 페이지 데이터.csv'
    ,encoding='utf-8'
)

url_str=data_df['Url'][0]

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

response=requests.get(
    url=url
    , headers=headers
).text

print(response)
```
![image](https://github.com/user-attachments/assets/2b0ab61e-aadd-431b-90e3-754e371853a1)

```
# 서브 페이지 이동해서 가격, 주소 추출
from bs4 import BeautifulSoup
from urllib.request import urlopen

import numpy as np
import pandas as pd

import requests

data_df=pd.read_csv(
    './data/03. 시카고 샌드위치 메인 페이지 데이터.csv'
    ,encoding='utf-8'
)

url_str=data_df['Url'][0]

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

response=requests.get(
    url=url_str
    , headers=headers
).text

# text를 Dom 파싱
tmp_soup = BeautifulSoup(response,'html.parser')

# 가격, 주소가 있는 영역 추출
print_address= tmp_soup.find('p',class_='addy').text
print_address
```
![image](https://github.com/user-attachments/assets/0c976015-0e89-4849-9d7a-c65c7cbe285d)

```
# 서브 페이지 이동해서 가격, 주소 추출
from bs4 import BeautifulSoup
from urllib.request import urlopen

import numpy as np
import pandas as pd

import requests

data_df=pd.read_csv(
    './data/03. 시카고 샌드위치 메인 페이지 데이터.csv'
    ,encoding='utf-8'
)

url_str=data_df['Url'][0]

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

response=requests.get(
    url=url_str
    , headers=headers
).text

# text를 Dom 파싱
tmp_soup = BeautifulSoup(response,'html.parser')

# 가격, 주소가 있는 영역 추출
print_address= tmp_soup.find('p',class_='addy').text

# 가격 추출
print_address.split()[0].replace('.','')
```
![image](https://github.com/user-attachments/assets/fc2a3eff-a624-43d9-80ca-ddd351815206)

```
# 서브 페이지 이동해서 가격, 주소 추출
from bs4 import BeautifulSoup
from urllib.request import urlopen

import numpy as np
import pandas as pd

import requests

data_df=pd.read_csv(
    './data/03. 시카고 샌드위치 메인 페이지 데이터.csv'
    ,encoding='utf-8'
)

url_str=data_df['Url'][0]

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

response=requests.get(
    url=url_str
    , headers=headers
).text

# text를 Dom 파싱
tmp_soup = BeautifulSoup(response,'html.parser')

# 가격, 주소가 있는 영역 추출
print_address= tmp_soup.find('p',class_='addy').text

# 가격 추출
print_address.split()[0][:-1]
```
![image](https://github.com/user-attachments/assets/ea76b9a9-1e39-43b3-af51-1a555d0b85ab)

```
# 서브 페이지 이동해서 가격, 주소 추출
from bs4 import BeautifulSoup
from urllib.request import urlopen

import numpy as np
import pandas as pd

import requests

data_df=pd.read_csv(
    './data/03. 시카고 샌드위치 메인 페이지 데이터.csv'
    ,encoding='utf-8'
)

url_str=data_df['Url'][0]

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

response=requests.get(
    url=url_str
    , headers=headers
).text

# text를 Dom 파싱
tmp_soup = BeautifulSoup(response,'html.parser')

# 가격, 주소가 있는 영역 추출
print_address= tmp_soup.find('p',class_='addy').text

# 가격 추출
print_address.split()[0][:-1]

# 주소 추출(길이가 가변 1부터 )
temp_addr=print_address.split()[1:-2]

print(' '.join(temp_addr)[:-2])
```
![image](https://github.com/user-attachments/assets/1dccb2b9-df15-4418-ae57-5e434201cabd)

---

## Jupyter Notebook에서 상태 진행바를 쉽게 만들어주는 tqdm 모듈
```
pip install tqdm
```
![image](https://github.com/user-attachments/assets/92663583-bb59-496b-9ce9-6cd46b0aebff)

```
# url이 50개다. 50번 반복 처리
# 반복시 반복 상태를 화면에 표시 : tqdm 

from tqdm import tqdm


from bs4 import BeautifulSoup
from urllib.request import urlopen

import numpy as np
import pandas as pd

import requests

data_df=pd.read_csv(
    './data/03. 시카고 샌드위치 메인 페이지 데이터.csv'
    ,encoding='utf-8'
)




tmp_soup = BeautifulSoup(response,'html.parser')

# 가격 주소 저장
price=[]
address=[]

# list(data_df['Url'].index) : [0,1,2,.....,49,50]
for idx in tqdm(data_df['Url'].index):
    url = data_df['Url'][idx]  # url에 한 개씩 추출
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    response = requests.get(url=url, headers=headers).text
    
    soup = BeautifulSoup(response, 'html.parser')
    
    get_str = soup.find('p', class_='addy').text
    
    price.append(get_str.split()[0][-1])
    address.append(' '.join(get_str.split()[1:-2])[:-1])
```
![image](https://github.com/user-attachments/assets/21a0cc24-78c4-4a6c-9ee5-92f888b9f69c)

```

# url이 50개다. 50번 반복 처리
# 반복시 반복 상태를 화면에 표시 : tqdm 

from tqdm import tqdm


from bs4 import BeautifulSoup
from urllib.request import urlopen

import numpy as np
import pandas as pd

import requests

data_df=pd.read_csv(
    './data/03. 시카고 샌드위치 메인 페이지 데이터.csv'
    ,encoding='utf-8'
)

tmp_soup = BeautifulSoup(response,'html.parser')

# 가격 주소 저장
price=[]
address=[]

# list(data_df['Url'].index) : [0,1,2,.....,49,50]
for idx in tqdm(data_df['Url'].index):
    url = data_df['Url'][idx]  # url에 한 개씩 추출
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    response = requests.get(url=url, headers=headers).text
    
    soup = BeautifulSoup(response, 'html.parser')
    
    get_str = soup.find('p', class_='addy').text
    
    price.append(get_str.split()[0][:-1])
    address.append(' '.join(get_str.split()[1:-2])[:-1])


data_df['Price']=price
data_df['Address']=address

data_df.head()

```
![image](https://github.com/user-attachments/assets/898d5a6f-b3b4-4eb0-bc67-253f14f9bb83)

```
# Rank Menu Cage Price Address
# data_result_df

# url이 50개다. 50번 반복 처리
# 반복시 반복 상태를 화면에 표시 : tqdm 

from tqdm import tqdm


from bs4 import BeautifulSoup
from urllib.request import urlopen

import numpy as np
import pandas as pd

import requests

data_df=pd.read_csv(
    './data/03. 시카고 샌드위치 메인 페이지 데이터.csv'
    ,encoding='utf-8'
)

tmp_soup = BeautifulSoup(response,'html.parser')

# 가격 주소 저장
price=[]
address=[]

# list(data_df['Url'].index) : [0,1,2,.....,49,50]
for idx in tqdm(data_df['Url'].index):
    url = data_df['Url'][idx]  # url에 한 개씩 추출
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    response = requests.get(url=url, headers=headers).text
    
    soup = BeautifulSoup(response, 'html.parser')
    
    get_str = soup.find('p', class_='addy').text
    
    price.append(get_str.split()[0][:-1])
    address.append(' '.join(get_str.split()[1:-2])[:-1])


data_df['Price']=price
data_df['Address']=address

data_result_df = data_df[
    [
        'Rank'
        ,'Menu'
        ,'Cafe'
        ,'Price'
        ,'Address'
    ]
]

data_result_df.head()
```
![image](https://github.com/user-attachments/assets/26058d87-f45b-482e-8360-fb892e1ff7b5)

```
# Rank Menu Cage Price Address
# data_result_df

# url이 50개다. 50번 반복 처리
# 반복시 반복 상태를 화면에 표시 : tqdm 

from tqdm import tqdm


from bs4 import BeautifulSoup
from urllib.request import urlopen

import numpy as np
import pandas as pd

import requests

data_df=pd.read_csv(
    './data/03. 시카고 샌드위치 메인 페이지 데이터.csv'
    ,encoding='utf-8'
)

tmp_soup = BeautifulSoup(response,'html.parser')

# 가격 주소 저장
price=[]
address=[]

# list(data_df['Url'].index) : [0,1,2,.....,49,50]
for idx in tqdm(data_df['Url'].index):
    url = data_df['Url'][idx]  # url에 한 개씩 추출
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    response = requests.get(url=url, headers=headers).text
    
    soup = BeautifulSoup(response, 'html.parser')
    
    get_str = soup.find('p', class_='addy').text
    
    price.append(get_str.split()[0][:-1])
    address.append(' '.join(get_str.split()[1:-2])[:-1])


data_df['Price']=price
data_df['Address']=address

data_result_df = data_df[
    [
        'Rank'
        ,'Menu'
        ,'Cafe'
        ,'Price'
        ,'Address'
    ]
]

data_result_df.set_index(
    'Rank' #인덱스로 보낼 컬럼들 지정정
    ,inplace=True
)
data_result_df.head()
```
![image](https://github.com/user-attachments/assets/684bda0a-b664-4b0b-bfa4-34bc631bb62a)

```
# Rank Menu Cage Price Address
# data_result_df

# url이 50개다. 50번 반복 처리
# 반복시 반복 상태를 화면에 표시 : tqdm 

from tqdm import tqdm


from bs4 import BeautifulSoup
from urllib.request import urlopen

import numpy as np
import pandas as pd

import requests

data_df=pd.read_csv(
    './data/03. 시카고 샌드위치 메인 페이지 데이터.csv'
    ,encoding='utf-8'
)

tmp_soup = BeautifulSoup(response,'html.parser')

# 가격 주소 저장
price=[]
address=[]

# list(data_df['Url'].index) : [0,1,2,.....,49,50]
for idx in tqdm(data_df['Url'].index):
    url = data_df['Url'][idx]  # url에 한 개씩 추출
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    response = requests.get(url=url, headers=headers).text
    
    soup = BeautifulSoup(response, 'html.parser')
    
    get_str = soup.find('p', class_='addy').text
    
    price.append(get_str.split()[0][:-1])
    address.append(' '.join(get_str.split()[1:-2])[:-1])


data_df['Price']=price
data_df['Address']=address

data_result_df = data_df[
    [
        'Rank'
        ,'Menu'
        ,'Cafe'
        ,'Price'
        ,'Address'
    ]
]

data_result_df.set_index(
    'Rank' #인덱스로 보낼 컬럼들 지정정
    ,inplace=True
)

data_result_df.to_csv(
    './data/03. best_sandwiches_list_chicago3.csv'
    ,encoding='utf-8'
)

```
![image](https://github.com/user-attachments/assets/1db5d42d-5cfd-454c-85fd-2935b054882b)

```
# 맛집 위치를 지도에 표시

from tqdm import tqdm


from bs4 import BeautifulSoup
from urllib.request import urlopen

import numpy as np
import pandas as pd

import requests

import folium
import googlemaps

data_result_df=pd.read_csv(
    './data/03. best_sandwiches_list_chicago3.csv'
    ,encoding='utf-8'
)

# data_result_df['Address'][0] : 경고

# loc[행,열] : named index 사용
# iloc[행, 열]: integer index 사용

gmas_key=개인키

# Client 생성
gmps=googlemaps.Client(key=gmas_key)
target_name=data_result_df.loc[1,'Address']+'Chicago'
gmaps_output=gmps.geocode(target_name)[0].get('geometry')
print(gmaps_output)

```
![image](https://github.com/user-attachments/assets/54b6fc68-1308-41e9-9e98-ab6c8369dfba)

```
# 맛집 위치를 지도에 표시

from tqdm import tqdm


from bs4 import BeautifulSoup
from urllib.request import urlopen

import numpy as np
import pandas as pd

import requests

import folium
import googlemaps

data_result_df=pd.read_csv(
    './data/03. best_sandwiches_list_chicago3.csv'
    ,encoding='utf-8'
)


# data_result_df['Address'][0] : 경고

# loc[행,열] : named index 사용
# iloc[행, 열]: integer index 사용

gmas_key=개인키

# Client 생성
gmps=googlemaps.Client(key=gmas_key)
target_name=data_result_df.loc[1,'Address']+'Chicago'
gmaps_output=gmps.geocode(target_name)[0].get('geometry')
print(gmaps_output['location']['lat'])
print(gmaps_output['location']['lng'])
```
![image](https://github.com/user-attachments/assets/8538146e-4bd5-4df1-93f0-4013fd2448f9)

```
# 맛집 위치를 지도에 표시

from tqdm import tqdm


from bs4 import BeautifulSoup
from urllib.request import urlopen

import numpy as np
import pandas as pd

import requests

import folium
import googlemaps

data_result_df=pd.read_csv(
    './data/03. best_sandwiches_list_chicago3.csv'
    ,encoding='utf-8'
    ,index_col=0
)

data_result_df

```
![image](https://github.com/user-attachments/assets/a60032fa-9f60-496e-b632-8a668015a288)

주소가 다수인 곳은 `Multipl`로 표시

```
# 맛집 위치를 지도에 표시

from tqdm import tqdm


from bs4 import BeautifulSoup
from urllib.request import urlopen

import numpy as np
import pandas as pd

import requests

import folium
import googlemaps

data_result_df=pd.read_csv(
    './data/03. best_sandwiches_list_chicago3.csv'
    ,encoding='utf-8'
    ,index_col=0
)


# data_result_df['Address'][0] : 경고

# loc[행,열] : named index 사용
# iloc[행, 열]: integer index 사용



# Client 생성
gmps=googlemaps.Client(key=gmas_key)
target_name=data_result_df.loc[1,'Address']+'Chicago'
gmaps_output=gmps.geocode(target_name)[0].get('geometry')

# lat,lang 구한다
# 주소에 Multiple => 주소가 없는 경우로 취급

lat =[]
lng=[]
for idx in tqdm(data_result_df.index):
    if data_result_df['Address'][idx] != 'Multipl':
        # 주소가 있는 경우 : 구글로 보내서 lat, lng 구한다
        target_name=data_result_df.loc[idx,'Address']+', Chicago'
        gmaps_output=gmps.geocode(target_name)
        location_output = gmaps_output[0].get('geometry')
        lat.append(location_output['location']['lat'])
        lng.append(location_output['location']['lng'])
        
    else :
        # 주소가 없는 경우 : lat랑 lng에 NaN 처리
        lat.append(np.NAN)
        lng.append(np.NAN)

print('lat--------------------->',lat)
print('lng--------------------->',lng)
print(len(lat))
print(len(lng))
        
```
![image](https://github.com/user-attachments/assets/3805736f-51e4-4c94-8cb3-a65878f7d138)

```
# 맛집 위치를 지도에 표시

from tqdm import tqdm


from bs4 import BeautifulSoup
from urllib.request import urlopen

import numpy as np
import pandas as pd

import requests

import folium
import googlemaps

data_result_df=pd.read_csv(
    './data/03. best_sandwiches_list_chicago3.csv'
    ,encoding='utf-8'
    ,index_col=0
)


# data_result_df['Address'][0] : 경고

# loc[행,열] : named index 사용
# iloc[행, 열]: integer index 사용



# Client 생성
gmps=googlemaps.Client(key=gmas_key)
target_name=data_result_df.loc[1,'Address']+'Chicago'
gmaps_output=gmps.geocode(target_name)[0].get('geometry')

# lat,lang 구한다
# 주소에 Multiple => 주소가 없는 경우로 취급

lat =[]
lng=[]
for idx in tqdm(data_result_df.index):
    if data_result_df['Address'][idx] != 'Multipl':
        # 주소가 있는 경우 : 구글로 보내서 lat, lng 구한다
        target_name=data_result_df.loc[idx,'Address']+', Chicago'
        gmaps_output=gmps.geocode(target_name)
        location_output = gmaps_output[0].get('geometry')
        lat.append(location_output['location']['lat'])
        lng.append(location_output['location']['lng'])
        
    else :
        # 주소가 없는 경우 : lat랑 lng에 NaN 처리
        lat.append(np.NAN)
        lng.append(np.NAN)


data_result_df['lat']=lat
data_result_df['lng']=lng
data_result_df.head()
```
![image](https://github.com/user-attachments/assets/e9d105c3-912a-4e1d-b9ae-acfec939610c)

```
# 맛집 위치를 지도에 표시

from tqdm import tqdm


from bs4 import BeautifulSoup
from urllib.request import urlopen

import numpy as np
import pandas as pd

import requests

import folium
import googlemaps

data_result_df=pd.read_csv(
    './data/03. best_sandwiches_list_chicago3.csv'
    ,encoding='utf-8'
    ,index_col=0
)


# data_result_df['Address'][0] : 경고

# loc[행,열] : named index 사용
# iloc[행, 열]: integer index 사용



# Client 생성
gmps=googlemaps.Client(key=gmas_key)
target_name=data_result_df.loc[1,'Address']+'Chicago'
gmaps_output=gmps.geocode(target_name)[0].get('geometry')

# lat,lang 구한다
# 주소에 Multiple => 주소가 없는 경우로 취급

lat =[]
lng=[]
for idx in tqdm(data_result_df.index):
    if data_result_df['Address'][idx] != 'Multipl':
        # 주소가 있는 경우 : 구글로 보내서 lat, lng 구한다
        target_name=data_result_df.loc[idx,'Address']+', Chicago'
        gmaps_output=gmps.geocode(target_name)
        location_output = gmaps_output[0].get('geometry')
        lat.append(location_output['location']['lat'])
        lng.append(location_output['location']['lng'])
        
    else :
        # 주소가 없는 경우 : lat랑 lng에 NaN 처리
        lat.append(np.NAN)
        lng.append(np.NAN)


data_result_df['lat']=lat
data_result_df['lng']=lng

# 지도 표시

#지도 생성

map = folium.Map(
    location=[
        data_result_df['lat'].mean()
        ,data_result_df['lng'].mean()
    ]
    ,zoom_start=11
)

map

```
![image](https://github.com/user-attachments/assets/bfa23836-be50-4499-8a24-cd52c71ec881)

```

# 맛집 위치를 지도에 표시

from tqdm import tqdm


from bs4 import BeautifulSoup
from urllib.request import urlopen

import numpy as np
import pandas as pd

import requests

import folium
import googlemaps

data_result_df=pd.read_csv(
    './data/03. best_sandwiches_list_chicago3.csv'
    ,encoding='utf-8'
    ,index_col=0
)


# data_result_df['Address'][0] : 경고

# loc[행,열] : named index 사용
# iloc[행, 열]: integer index 사용



# Client 생성
gmps=googlemaps.Client(key=gmas_key)
target_name=data_result_df.loc[1,'Address']+'Chicago'
gmaps_output=gmps.geocode(target_name)[0].get('geometry')

# lat,lang 구한다
# 주소에 Multiple => 주소가 없는 경우로 취급

lat =[]
lng=[]
for idx in tqdm(data_result_df.index):
    if data_result_df['Address'][idx] != 'Multipl':
        # 주소가 있는 경우 : 구글로 보내서 lat, lng 구한다
        target_name=data_result_df.loc[idx,'Address']+', Chicago'
        gmaps_output=gmps.geocode(target_name)
        location_output = gmaps_output[0].get('geometry')
        lat.append(location_output['location']['lat'])
        lng.append(location_output['location']['lng'])
        
    else :
        # 주소가 없는 경우 : lat랑 lng에 NaN 처리
        lat.append(np.NAN)
        lng.append(np.NAN)


data_result_df['lat']=lat
data_result_df['lng']=lng

# 지도 표시

#지도 생성

map = folium.Map(
    location=[
        data_result_df['lat'].mean()
        ,data_result_df['lng'].mean()
    ]
    ,zoom_start=11
)

# 50개 맛집 마커로 표시
for idx in data_result_df.index:
    if data_result_df['Address'][idx] != 'Multipl':
        folium.Marker(
            [data_result_df['lat'][idx]
             ,data_result_df['lng'][idx]
             ]
            , popup=data_result_df['Cafe'][idx]
        ).add_to(map)
        
map
```
![image](https://github.com/user-attachments/assets/1e31a7a7-d8af-45fb-bf8f-0f473697f570)

![image](https://github.com/user-attachments/assets/631c2a9f-b58c-471e-b835-da9273437a9b)

