# 시카고 샌드위치 맛집 분석

## 학습목표
- 웹 데이터를 가져오는 Beautiful Soup 익히기
- 크롬 개발자 도구를 이용해서 원하는 태그 찾기
- 시카고 샌드위치 맛집 소개 사이트에 접근하기
- 접근한 웹 페이지에서 원하는 데이터 추출하고 정리하기
- 다수의 웹페이지에서 자동으로 접근해서 원하는 정보 가져오기 => a태그 반복 사용 
- Jupyter notebook 상태 진행바 생성
- 상태 진행바 적용 페이지 접근하기
- 50개 웹페이지 정보 가져오기

---

## 웹 데이터를 가져오는 Beautiful Soup 익히기

```
# BeautifulSoup 라이브러리 사용
from bs4 import BeautifulSoup
```
![image](https://github.com/user-attachments/assets/3472e2c6-2d54-497d-8cb1-7418be46ac69)

```
pip install bs4
```
![image](https://github.com/user-attachments/assets/202fda29-1d25-49c8-8a08-860379d907e3)

```
conda install beautifulsoup4
```
![image](https://github.com/user-attachments/assets/11abc0f3-7cc0-41c8-bee8-d3a9c18ada0b)


```
# BeautifulSoup 라이브러리 사용
from bs4 import BeautifulSoup

page=open('./data/03. test_first.html','r') # file object 생성
page = page.read() # 전채를 읽어서 문자열로 반환
page
```

![image](https://github.com/user-attachments/assets/5db4e29e-54f0-493a-b177-64eb924c5105)

```
# BeautifulSoup 라이브러리 사용
from bs4 import BeautifulSoup

page=open('./data/03. test_first.html','r').read()
page
```
![image](https://github.com/user-attachments/assets/6edfe870-02f8-4279-9965-6b93b6d170c3)

```
# BeautifulSoup 라이브러리 사용
from bs4 import BeautifulSoup

page=open('./data/03. test_first.html','r').read()
print(page)
```
![image](https://github.com/user-attachments/assets/6f938409-fdbc-4e82-baa9-ab0eb02ae27a)

```
# BeautifulSoup 라이브러리 사용
from bs4 import BeautifulSoup

page=open('./data/03. test_first.html','r').read()
# print(page) # 문자열일뿐이다.
# print(page,'html.parser') 해야지 dom tree로 나온다
soup=BeautifulSoup(page, 'html.parser')  # dom tree 생성되고 selector 가능해진다
soup
```
![image](https://github.com/user-attachments/assets/50edbb28-86ad-49e1-a170-4ff1c2164199)

데이터 추출시 자주 사용하는 메소드
1. find() : 주어진 태그와 속성을 가진 첫번째 요소를 검색할 때 사용
2. find_all() : 주어진 태그와 속성을 가진 모든 요소를 검색할 때 사용
3. select_one() : CSS selector 사용해서 첫번째 요소를 검색할 때 사용
4. select() :CSS selector 사용해서 모든 요소를 검색할 때 사용

```
from bs4 import BeautifulSoup
# 데이터 추출시 자주 사용하는 메소드
# 1. find() : 주어진 태그와 속성을 가진 첫번째 요소를 검색할 때 사용
# 2. find_all() : 주어진 태그와 속성을 가진 모든 요소를 검색할 때 사용
# 3. select_one() : CSS selector 사용해서 첫번째 요소를 검색할 때 사용
# 4. select() :CSS selector 사용해서 모든 요소를 검색할 때 사용
soup=BeautifulSoup(page, 'html.parser') 
list(soup.children)[2]
```
![image](https://github.com/user-attachments/assets/86296139-5507-489f-b41e-de9319038917)

```
from bs4 import BeautifulSoup
soup=BeautifulSoup(page, 'html.parser') 
html=list(soup.children)[2]
list(html.children)
```
![image](https://github.com/user-attachments/assets/e10850b0-4d5b-4a5f-b386-40819c5282b9)

```
from bs4 import BeautifulSoup
soup=BeautifulSoup(page, 'html.parser') 
html=list(soup.children)[2]

body=list(html.children)[3]

list(body.children)
```
![image](https://github.com/user-attachments/assets/18b02225-4a72-4f03-9977-601b5b23076a)

```
from bs4 import BeautifulSoup
soup=BeautifulSoup(page, 'html.parser') 

# p 태그 전체 검색
soup.find_all('p')
```
![image](https://github.com/user-attachments/assets/cf4ddbfb-8bfd-47f7-9d8c-1b5c0ea76f08)

```
from bs4 import BeautifulSoup
soup=BeautifulSoup(page, 'html.parser') 

# p 태그 검색
soup.find('p')
```
![image](https://github.com/user-attachments/assets/78a8719e-e1bb-4fc9-b1c1-95e84e5c9456)

p tag 전체 검색
- find_all('p')는 검색 결과 여러개를 리스트로 반환
- find('p')는 첫번째 검색 결과를 반환

```
from bs4 import BeautifulSoup
soup=BeautifulSoup(page, 'html.parser') 

# p태그이면서 outer-text인 class만 출력
soup.find_all('p',class_='outer-text')
```
![image](https://github.com/user-attachments/assets/3b0f0e66-2b25-4129-abf3-5919e20bcc00)

```
from bs4 import BeautifulSoup
soup=BeautifulSoup(page, 'html.parser') 

# p태그이면서 id가 first인 것을 출력
soup.find_all('p',id='first')
```
![image](https://github.com/user-attachments/assets/129238c6-fa8b-4db8-a529-85829a975584)

```
from bs4 import BeautifulSoup
soup=BeautifulSoup(page, 'html.parser') 

# 데이터들만 추출
soup.find_all('p')[0].get_text() #get_text()는 문자열(데이터) 추출
```
![image](https://github.com/user-attachments/assets/0b21a015-d5ab-4362-97a6-3bf9ce756dc0)

```
from bs4 import BeautifulSoup
soup=BeautifulSoup(page, 'html.parser') 

soup.find_all('a')
```
![image](https://github.com/user-attachments/assets/6cd4eb8e-a617-4c7b-b689-f8960a48a049)

```
from bs4 import BeautifulSoup
soup=BeautifulSoup(page, 'html.parser') 

# 링크만 갖고오기
links=soup.find_all('a')
link =links[0]
link.get_attribute_list('href')
```
![image](https://github.com/user-attachments/assets/1569ff3f-5b1f-49ca-b07a-ea58dc2be33b)

```
from bs4 import BeautifulSoup
soup=BeautifulSoup(page, 'html.parser') 

# 링크만 갖고오기
links=soup.find_all('a')
link =links[0]
link['href']
```
![image](https://github.com/user-attachments/assets/226fb59d-ad5c-4bbf-baf0-c748adb5f935)

---
## 네이버 증권 사이트에서 환율 정보 추출

![image](https://github.com/user-attachments/assets/9f2b18aa-8d71-4a03-99b0-d6cbf59ce500)

위 화면에서 파이썬 코드로 환율 정보 가져오기(https://finance.naver.com/marketindex/)

```
from bs4 import BeautifulSoup
from urllib.request import urlopen

url='https://finance.naver.com/marketindex/'
page=urlopen(url)
page
```
![image](https://github.com/user-attachments/assets/0582b826-a610-4e69-81cc-f327c46ac93a)

```
from bs4 import BeautifulSoup
from urllib.request import urlopen

url='https://finance.naver.com/marketindex/'
page=urlopen(url)

soup=BeautifulSoup(page,'html.parser')
soup
```

![image](https://github.com/user-attachments/assets/792e152b-c739-4717-ace7-3a7ed6b030ff)

```
from bs4 import BeautifulSoup
from urllib.request import urlopen

url='https://finance.naver.com/marketindex/'
page=urlopen(url)

soup=BeautifulSoup(page,'html.parser')

soup.find_all('span',class_='value')
```
![image](https://github.com/user-attachments/assets/b92f9a4d-73c3-4bd6-ab7b-87b8a0b0efb1)

```
from bs4 import BeautifulSoup
from urllib.request import urlopen

url='https://finance.naver.com/marketindex/'
page=urlopen(url)

soup=BeautifulSoup(page,'html.parser')

soup.find_all('span',class_='value')[0]
```
![image](https://github.com/user-attachments/assets/e860870e-4b1b-4700-822c-e0c47b10d280)

```
from bs4 import BeautifulSoup
from urllib.request import urlopen

url='https://finance.naver.com/marketindex/'
page=urlopen(url)

soup=BeautifulSoup(page,'html.parser')

soup.find_all('span',class_='value')[0].get_text()
```
![image](https://github.com/user-attachments/assets/f1d2f78e-dc37-47f3-94d0-dfa8f8b96205)

```
from bs4 import BeautifulSoup
from urllib.request import urlopen

url='https://finance.naver.com/marketindex/'
page=urlopen(url)

soup=BeautifulSoup(page,'html.parser')

soup.find_all('span',class_='value')[0].string
```
![image](https://github.com/user-attachments/assets/56da9a8e-b355-4843-879e-2fe274a70be3)

---
## 시카고 샌드위치 맛집 소개 사이트에 접근하기

https://www.chicagomag.com/Chicago-Magazine/November-2012/Best-Sandwiches-Chicago

![image](https://github.com/user-attachments/assets/b1b927ed-9430-4dce-bede-b469a1262896)

```
from bs4 import BeautifulSoup
from urllib.request import urlopen
url_base = 'http://www.chicagomag.com'
url_sub = '/Chicago-Magazine/November-2012/Best-Sandwiches-Chicago/'
url = url_base + url_sub
html = urlopen(url)
soup = BeautifulSoup(html, "html.parser")
soup
```
![image](https://github.com/user-attachments/assets/7eeca62c-9c2b-4cf0-a4b9-e3cd2cc1ea38)


```
from bs4 import BeautifulSoup
from urllib.request import urlopen

import numpy as np
import pandas as pd

import requests

url_base = 'https://www.chicagomag.com'
url_sub = '/Chicago-Magazine/November-2012/Best-Sandwiches-Chicago/'
url = url_base + url_sub

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

response = requests.get(url=url, headers=headers)
html = response.text
print(html)

```
![image](https://github.com/user-attachments/assets/123bf8d8-e184-420c-a8c7-1bc101be1dcb)

```
from bs4 import BeautifulSoup
from urllib.request import urlopen

import numpy as np
import pandas as pd

import requests

url_base = 'https://www.chicagomag.com'
url_sub = '/Chicago-Magazine/November-2012/Best-Sandwiches-Chicago/'
url = url_base + url_sub

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

response = requests.get(url=url, headers=headers)
html = response.text

soup  = BeautifulSoup(html,'html.parser')
print(soup)
```
![image](https://github.com/user-attachments/assets/37273c81-5b95-427b-89a7-1fa108616149)

```
from bs4 import BeautifulSoup
from urllib.request import urlopen

import numpy as np
import pandas as pd

import requests

url_base = 'https://www.chicagomag.com'
url_sub = '/Chicago-Magazine/November-2012/Best-Sandwiches-Chicago/'
url = url_base + url_sub

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

response = requests.get(url=url, headers=headers)
html = response.text

soup  = BeautifulSoup(html,'html.parser')

soup.find_all('div',class_='sammy')
```
![image](https://github.com/user-attachments/assets/73425d59-39fe-4301-817d-cabab5399c9f)

```
from bs4 import BeautifulSoup
from urllib.request import urlopen

import numpy as np
import pandas as pd

import requests

url_base = 'https://www.chicagomag.com'
url_sub = '/Chicago-Magazine/November-2012/Best-Sandwiches-Chicago/'
url = url_base + url_sub

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

response = requests.get(url=url, headers=headers)
html = response.text

soup  = BeautifulSoup(html,'html.parser')

len(soup.find_all('div',class_='sammy'))
```
![image](https://github.com/user-attachments/assets/2c86e8b6-214e-4e03-972a-7e3f0c3c1a35)

```
from bs4 import BeautifulSoup
from urllib.request import urlopen

import numpy as np
import pandas as pd

import requests

url_base = 'https://www.chicagomag.com'
url_sub = '/Chicago-Magazine/November-2012/Best-Sandwiches-Chicago/'
url = url_base + url_sub

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

response = requests.get(url=url, headers=headers)
html = response.text

soup  = BeautifulSoup(html,'html.parser')

soup.find_all('div',class_='sammy')[0]
```
![image](https://github.com/user-attachments/assets/0b0a0cc8-35f5-4b15-9459-c566c73d81d3)

```
from bs4 import BeautifulSoup
from urllib.request import urlopen

import numpy as np
import pandas as pd

import requests

url_base = 'https://www.chicagomag.com'
url_sub = '/Chicago-Magazine/November-2012/Best-Sandwiches-Chicago/'
url = url_base + url_sub

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

response = requests.get(url=url, headers=headers)
html = response.text

soup  = BeautifulSoup(html,'html.parser')

tmp = soup.find_all('div',class_='sammy')[0]

tmp.find(class_='sammyRank').get_text()
```
![image](https://github.com/user-attachments/assets/42e2435d-f9e3-4cda-8710-1881ab765bd0)

```
from bs4 import BeautifulSoup
from urllib.request import urlopen

import numpy as np
import pandas as pd

import requests

url_base = 'https://www.chicagomag.com'
url_sub = '/Chicago-Magazine/November-2012/Best-Sandwiches-Chicago/'
url = url_base + url_sub

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

response = requests.get(url=url, headers=headers)
html = response.text

soup  = BeautifulSoup(html,'html.parser')

tmp = soup.find_all('div',class_='sammy')[0]

tmp.find(class_='sammyListing').get_text().split('\n')
```
![image](https://github.com/user-attachments/assets/3a9519bb-0f7e-4e5d-8bda-c703a7f89027)

```
from bs4 import BeautifulSoup
from urllib.request import urlopen

import numpy as np
import pandas as pd

import requests

url_base = 'https://www.chicagomag.com'
url_sub = '/Chicago-Magazine/November-2012/Best-Sandwiches-Chicago/'
url = url_base + url_sub

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

response = requests.get(url=url, headers=headers)
html = response.text

soup  = BeautifulSoup(html,'html.parser')

tmp = soup.find_all('div',class_='sammy')[0]

temp=tmp.find(class_='sammyListing').get_text().split('\n')

temp[0],temp[1]
```
![image](https://github.com/user-attachments/assets/53e963d7-881d-4e1c-9dbc-6152897b58a7)

```
from bs4 import BeautifulSoup
from urllib.request import urlopen

import numpy as np
import pandas as pd

import requests

url_base = 'https://www.chicagomag.com'
url_sub = '/Chicago-Magazine/November-2012/Best-Sandwiches-Chicago/'
url = url_base + url_sub

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

response = requests.get(url=url, headers=headers)
html = response.text

soup  = BeautifulSoup(html,'html.parser')

tmp = soup.find_all('div',class_='sammy')[0]

temp=tmp.find(class_='sammyListing').get_text().split('\n')

tmp.find_all('a')[0]['href']
```
![image](https://github.com/user-attachments/assets/869c3073-7358-4676-9ed5-42ab65d8f894)

```
from bs4 import BeautifulSoup
from urllib.request import urlopen

import numpy as np
import pandas as pd

import requests

url_base = 'https://www.chicagomag.com'
url_sub = '/Chicago-Magazine/November-2012/Best-Sandwiches-Chicago/'
url = url_base + url_sub

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

response = requests.get(url=url, headers=headers)
html = response.text

soup  = BeautifulSoup(html,'html.parser')

# 샌드위치 메인 페이지 데이터 추출
# 순위, 메인메뉴, 카페명, 세부페이지 이동 url
ranks=[]
main_menus=[]
cafe_names=[]
url_address=[] # 상세로 들어가는 url

# 50개 순위와 정보가 들어 잇는 리스트
list_soup=soup.find_all('div',class_='sammy')

# 반복 처리해서 데이터를 추출하고 각 리스트에 저장
for item in list_soup: # 50번만 반복
    # 순위 추출해서 저장
    ranks.append(item.find(class_='sammyRank').text)
    # 메인메뉴와 카페명 추출해서 저장
    tmp_str=item.find(class_='sammyListing').text
    main_menus.append(tmp_str.split('\n')[0])
    cafe_names.append(tmp_str.split('\n')[1])
    
    # url
    tmp_url=item.find('a')['href'] # sub의 url을 추출
    # tmp_url 추출 결과 sub가 아닌 경우도 있음 : https:// 시작하거나 /Chicago 으로 시작할수 잇다
    # https://으로 시작하면 전체를 가지고오고 /Chicago로 시작하면 base를 더해줘야한다.
    if tmp_url.startswith('https://'): #전체 url이 다 있는 경우
        url_address.append(tmp_url)
    else:
        url_address.append(url_base+tmp_url)

print('ranks :',ranks)
print('main_menus : ',main_menus)
print('cafe_names : ',cafe_names)
print('url_address : ',url_address)
```
![image](https://github.com/user-attachments/assets/ff15d5e1-0f2a-42ef-b5d4-d5246ecd0331)

```
from bs4 import BeautifulSoup
from urllib.request import urlopen

import numpy as np
import pandas as pd

import requests

url_base = 'https://www.chicagomag.com'
url_sub = '/Chicago-Magazine/November-2012/Best-Sandwiches-Chicago/'
url = url_base + url_sub

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

response = requests.get(url=url, headers=headers)
html = response.text

soup  = BeautifulSoup(html,'html.parser')

# 샌드위치 메인 페이지 데이터 추출
# 순위, 메인메뉴, 카페명, 세부페이지 이동 url
ranks=[]
main_menus=[]
cafe_names=[]
url_address=[] # 상세로 들어가는 url

# 50개 순위와 정보가 들어 잇는 리스트
list_soup=soup.find_all('div',class_='sammy')

# 반복 처리해서 데이터를 추출하고 각 리스트에 저장
for item in list_soup: # 50번만 반복
    # 순위 추출해서 저장
    ranks.append(item.find(class_='sammyRank').text)
    # 메인메뉴와 카페명 추출해서 저장
    tmp_str=item.find(class_='sammyListing').text
    main_menus.append(tmp_str.split('\n')[0])
    cafe_names.append(tmp_str.split('\n')[1])
    
    # url
    tmp_url=item.find('a')['href'] # sub의 url을 추출
    # tmp_url 추출 결과 sub가 아닌 경우도 있음 : https:// 시작하거나 /Chicago 으로 시작할수 잇다
    # https://으로 시작하면 전체를 가지고오고 /Chicago로 시작하면 base를 더해줘야한다.
    if tmp_url.startswith('https://'): #전체 url이 다 있는 경우
        url_address.append(tmp_url)
    else:
        url_address.append(url_base+tmp_url)

data_df=pd.DataFrame(
    {
        'Rank': ranks
        ,'Menu': main_menus
        ,'Cafe': cafe_names
        ,'Url': url_address
    }
)
data_df.head()
```
![image](https://github.com/user-attachments/assets/d7c984ba-8280-4fcc-9e0b-23cd669fd537)

```
from bs4 import BeautifulSoup
from urllib.request import urlopen

import numpy as np
import pandas as pd

import requests

url_base = 'https://www.chicagomag.com'
url_sub = '/Chicago-Magazine/November-2012/Best-Sandwiches-Chicago/'
url = url_base + url_sub

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

response = requests.get(url=url, headers=headers)
html = response.text

soup  = BeautifulSoup(html,'html.parser')

# 샌드위치 메인 페이지 데이터 추출
# 순위, 메인메뉴, 카페명, 세부페이지 이동 url
ranks=[]
main_menus=[]
cafe_names=[]
url_address=[] # 상세로 들어가는 url

# 50개 순위와 정보가 들어 잇는 리스트
list_soup=soup.find_all('div',class_='sammy')

# 반복 처리해서 데이터를 추출하고 각 리스트에 저장
for item in list_soup: # 50번만 반복
    # 순위 추출해서 저장
    ranks.append(item.find(class_='sammyRank').text)
    # 메인메뉴와 카페명 추출해서 저장
    tmp_str=item.find(class_='sammyListing').text
    main_menus.append(tmp_str.split('\n')[0])
    cafe_names.append(tmp_str.split('\n')[1])
    
    # url
    tmp_url=item.find('a')['href'] # sub의 url을 추출
    # tmp_url 추출 결과 sub가 아닌 경우도 있음 : https:// 시작하거나 /Chicago 으로 시작할수 잇다
    # https://으로 시작하면 전체를 가지고오고 /Chicago로 시작하면 base를 더해줘야한다.
    if tmp_url.startswith('https://'): #전체 url이 다 있는 경우
        url_address.append(tmp_url)
    else:
        url_address.append(url_base+tmp_url)

data_df=pd.DataFrame(
    {
        'Rank': ranks
        ,'Menu': main_menus
        ,'Cafe': cafe_names
        ,'Url': url_address
    }
)
data_df.to_csv(
    './data/03. 시카고 샌드위치 메인 페이지 데이터.csv'
    ,encoding='utf-8'
)
```
![image](https://github.com/user-attachments/assets/d7b70e1a-23f4-45b5-90fc-b00ba22bf5f1)

```
# 서브 페이지 이동해서 가격, 주소 추출
from bs4 import BeautifulSoup
from urllib.request import urlopen

import numpy as np
import pandas as pd

import requests

data_df=pd.read_csv(
    './data/03. 시카고 샌드위치 메인 페이지 데이터.csv'
    ,encoding='utf-8'
)

# 서브 페이지 이동해서 가격, 주소 추출
# 1등 서브 페이지로 이동해서 가격, 주소

url_str=data_df['Url'][0]
url_str
```
![image](https://github.com/user-attachments/assets/4df8c286-6947-4826-8388-dd3e20857fef)

```
# 서브 페이지 이동해서 가격, 주소 추출
from bs4 import BeautifulSoup
from urllib.request import urlopen

import numpy as np
import pandas as pd

import requests

data_df=pd.read_csv(
    './data/03. 시카고 샌드위치 메인 페이지 데이터.csv'
    ,encoding='utf-8'
)

url_str=data_df['Url'][0]

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

response=requests.get(
    url=url
    , headers=headers
).text

print(response)
```
![image](https://github.com/user-attachments/assets/2b0ab61e-aadd-431b-90e3-754e371853a1)
