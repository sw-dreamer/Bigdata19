# 시카고 샌드위치 맛집 분석

## 학습목표
- 웹 데이터를 가져오는 Beautiful Soup 익히기
- 크롬 개발자 도구를 이용해서 원하는 태그 찾기
- 시카고 샌드위치 맛집 소개 사이트에 접근하기
- 접근한 웹 페이지에서 원하는 데이터 추출하고 정리하기
- 다수의 웹페이지에서 자동으로 접근해서 원하는 정보 가져오기 => a태그 반복 사용 
- Jupyter notebook 상태 진행바 생성
- 상태 진행바 적용 페이지 접근하기
- 50개 웹페이지 정보 가져오기

---

## 웹 데이터를 가져오는 Beautiful Soup 익히기

```
# BeautifulSoup 라이브러리 사용
from bs4 import BeautifulSoup
```
![image](https://github.com/user-attachments/assets/3472e2c6-2d54-497d-8cb1-7418be46ac69)

```
pip install bs4
```
![image](https://github.com/user-attachments/assets/202fda29-1d25-49c8-8a08-860379d907e3)

```
conda install beautifulsoup4
```
![image](https://github.com/user-attachments/assets/11abc0f3-7cc0-41c8-bee8-d3a9c18ada0b)


```
# BeautifulSoup 라이브러리 사용
from bs4 import BeautifulSoup

page=open('./data/03. test_first.html','r') # file object 생성
page = page.read() # 전채를 읽어서 문자열로 반환
page
```

![image](https://github.com/user-attachments/assets/5db4e29e-54f0-493a-b177-64eb924c5105)

```
# BeautifulSoup 라이브러리 사용
from bs4 import BeautifulSoup

page=open('./data/03. test_first.html','r').read()
page
```
![image](https://github.com/user-attachments/assets/6edfe870-02f8-4279-9965-6b93b6d170c3)

```
# BeautifulSoup 라이브러리 사용
from bs4 import BeautifulSoup

page=open('./data/03. test_first.html','r').read()
print(page)
```
![image](https://github.com/user-attachments/assets/6f938409-fdbc-4e82-baa9-ab0eb02ae27a)

```
# BeautifulSoup 라이브러리 사용
from bs4 import BeautifulSoup

page=open('./data/03. test_first.html','r').read()
# print(page) # 문자열일뿐이다.
# print(page,'html.parser') 해야지 dom tree로 나온다
soup=BeautifulSoup(page, 'html.parser')  # dom tree 생성되고 selector 가능해진다
soup
```
![image](https://github.com/user-attachments/assets/50edbb28-86ad-49e1-a170-4ff1c2164199)

데이터 추출시 자주 사용하는 메소드
1. find() : 주어진 태그와 속성을 가진 첫번째 요소를 검색할 때 사용
2. find_all() : 주어진 태그와 속성을 가진 모든 요소를 검색할 때 사용
3. select_one() : CSS selector 사용해서 첫번째 요소를 검색할 때 사용
4. select() :CSS selector 사용해서 모든 요소를 검색할 때 사용

```
from bs4 import BeautifulSoup
# 데이터 추출시 자주 사용하는 메소드
# 1. find() : 주어진 태그와 속성을 가진 첫번째 요소를 검색할 때 사용
# 2. find_all() : 주어진 태그와 속성을 가진 모든 요소를 검색할 때 사용
# 3. select_one() : CSS selector 사용해서 첫번째 요소를 검색할 때 사용
# 4. select() :CSS selector 사용해서 모든 요소를 검색할 때 사용
soup=BeautifulSoup(page, 'html.parser') 
list(soup.children)[2]
```
![image](https://github.com/user-attachments/assets/86296139-5507-489f-b41e-de9319038917)

```
from bs4 import BeautifulSoup
soup=BeautifulSoup(page, 'html.parser') 
html=list(soup.children)[2]
list(html.children)
```
![image](https://github.com/user-attachments/assets/e10850b0-4d5b-4a5f-b386-40819c5282b9)

```
from bs4 import BeautifulSoup
soup=BeautifulSoup(page, 'html.parser') 
html=list(soup.children)[2]

body=list(html.children)[3]

list(body.children)
```
![image](https://github.com/user-attachments/assets/18b02225-4a72-4f03-9977-601b5b23076a)

```
from bs4 import BeautifulSoup
soup=BeautifulSoup(page, 'html.parser') 

# p 태그 전체 검색
soup.find_all('p')
```
![image](https://github.com/user-attachments/assets/cf4ddbfb-8bfd-47f7-9d8c-1b5c0ea76f08)

```
from bs4 import BeautifulSoup
soup=BeautifulSoup(page, 'html.parser') 

# p 태그 검색
soup.find('p')
```
![image](https://github.com/user-attachments/assets/78a8719e-e1bb-4fc9-b1c1-95e84e5c9456)

p tag 전체 검색
- find_all('p')는 검색 결과 여러개를 리스트로 반환
- find('p')는 첫번째 검색 결과를 반환

```
from bs4 import BeautifulSoup
soup=BeautifulSoup(page, 'html.parser') 

# p태그이면서 outer-text인 class만 출력
soup.find_all('p',class_='outer-text')
```
![image](https://github.com/user-attachments/assets/3b0f0e66-2b25-4129-abf3-5919e20bcc00)

```
from bs4 import BeautifulSoup
soup=BeautifulSoup(page, 'html.parser') 

# p태그이면서 id가 first인 것을 출력
soup.find_all('p',id='first')
```
![image](https://github.com/user-attachments/assets/129238c6-fa8b-4db8-a529-85829a975584)

```
from bs4 import BeautifulSoup
soup=BeautifulSoup(page, 'html.parser') 

# 데이터들만 추출
soup.find_all('p')[0].get_text() #get_text()는 문자열(데이터) 추출
```
![image](https://github.com/user-attachments/assets/0b21a015-d5ab-4362-97a6-3bf9ce756dc0)

```
from bs4 import BeautifulSoup
soup=BeautifulSoup(page, 'html.parser') 

soup.find_all('a')
```
![image](https://github.com/user-attachments/assets/6cd4eb8e-a617-4c7b-b689-f8960a48a049)

```
from bs4 import BeautifulSoup
soup=BeautifulSoup(page, 'html.parser') 

# 링크만 갖고오기
links=soup.find_all('a')
link =links[0]
link.get_attribute_list('href')
```
![image](https://github.com/user-attachments/assets/1569ff3f-5b1f-49ca-b07a-ea58dc2be33b)

```
from bs4 import BeautifulSoup
soup=BeautifulSoup(page, 'html.parser') 

# 링크만 갖고오기
links=soup.find_all('a')
link =links[0]
link['href']
```
![image](https://github.com/user-attachments/assets/226fb59d-ad5c-4bbf-baf0-c748adb5f935)

---
## 네이버 증권 사이트에서 환율 정보 추출

![image](https://github.com/user-attachments/assets/9f2b18aa-8d71-4a03-99b0-d6cbf59ce500)

위 화면에서 파이썬 코드로 환율 정보 가져오기(https://finance.naver.com/marketindex/)

```
from bs4 import BeautifulSoup
from urllib.request import urlopen

url='https://finance.naver.com/marketindex/'
page=urlopen(url)
page
```
![image](https://github.com/user-attachments/assets/0582b826-a610-4e69-81cc-f327c46ac93a)

```
from bs4 import BeautifulSoup
from urllib.request import urlopen

url='https://finance.naver.com/marketindex/'
page=urlopen(url)

soup=BeautifulSoup(page,'html.parser')
soup
```

![image](https://github.com/user-attachments/assets/792e152b-c739-4717-ace7-3a7ed6b030ff)

```
from bs4 import BeautifulSoup
from urllib.request import urlopen

url='https://finance.naver.com/marketindex/'
page=urlopen(url)

soup=BeautifulSoup(page,'html.parser')

soup.find_all('span',class_='value')
```
![image](https://github.com/user-attachments/assets/b92f9a4d-73c3-4bd6-ab7b-87b8a0b0efb1)

```
from bs4 import BeautifulSoup
from urllib.request import urlopen

url='https://finance.naver.com/marketindex/'
page=urlopen(url)

soup=BeautifulSoup(page,'html.parser')

soup.find_all('span',class_='value')[0]
```
![image](https://github.com/user-attachments/assets/e860870e-4b1b-4700-822c-e0c47b10d280)

```
from bs4 import BeautifulSoup
from urllib.request import urlopen

url='https://finance.naver.com/marketindex/'
page=urlopen(url)

soup=BeautifulSoup(page,'html.parser')

soup.find_all('span',class_='value')[0].get_text()
```
![image](https://github.com/user-attachments/assets/f1d2f78e-dc37-47f3-94d0-dfa8f8b96205)

```
from bs4 import BeautifulSoup
from urllib.request import urlopen

url='https://finance.naver.com/marketindex/'
page=urlopen(url)

soup=BeautifulSoup(page,'html.parser')

soup.find_all('span',class_='value')[0].string
```
![image](https://github.com/user-attachments/assets/56da9a8e-b355-4843-879e-2fe274a70be3)

---
## 시카고 샌드위치 맛집 소개 사이트에 접근하기

https://www.chicagomag.com/Chicago-Magazine/November-2012/Best-Sandwiches-Chicago

![image](https://github.com/user-attachments/assets/b1b927ed-9430-4dce-bede-b469a1262896)

```
from bs4 import BeautifulSoup
from urllib.request import urlopen
url_base = 'http://www.chicagomag.com'
url_sub = '/Chicago-Magazine/November-2012/Best-Sandwiches-Chicago/'
url = url_base + url_sub
html = urlopen(url)
soup = BeautifulSoup(html, "html.parser")
soup
```
![image](https://github.com/user-attachments/assets/7eeca62c-9c2b-4cf0-a4b9-e3cd2cc1ea38)


```
from bs4 import BeautifulSoup
from urllib.request import urlopen

import numpy as np
import pandas as pd

import requests

url_base = 'https://www.chicagomag.com'
url_sub = '/Chicago-Magazine/November-2012/Best-Sandwiches-Chicago/'
url = url_base + url_sub

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

response = requests.get(url=url, headers=headers)
html = response.text
print(html)

```
![image](https://github.com/user-attachments/assets/123bf8d8-e184-420c-a8c7-1bc101be1dcb)

```
from bs4 import BeautifulSoup
from urllib.request import urlopen

import numpy as np
import pandas as pd

import requests

url_base = 'https://www.chicagomag.com'
url_sub = '/Chicago-Magazine/November-2012/Best-Sandwiches-Chicago/'
url = url_base + url_sub

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

response = requests.get(url=url, headers=headers)
html = response.text

soup  = BeautifulSoup(html,'html.parser')
print(soup)
```
![image](https://github.com/user-attachments/assets/37273c81-5b95-427b-89a7-1fa108616149)

```
from bs4 import BeautifulSoup
from urllib.request import urlopen

import numpy as np
import pandas as pd

import requests

url_base = 'https://www.chicagomag.com'
url_sub = '/Chicago-Magazine/November-2012/Best-Sandwiches-Chicago/'
url = url_base + url_sub

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

response = requests.get(url=url, headers=headers)
html = response.text

soup  = BeautifulSoup(html,'html.parser')

soup.find_all('div',class_='sammy')
```
![image](https://github.com/user-attachments/assets/73425d59-39fe-4301-817d-cabab5399c9f)

```
from bs4 import BeautifulSoup
from urllib.request import urlopen

import numpy as np
import pandas as pd

import requests

url_base = 'https://www.chicagomag.com'
url_sub = '/Chicago-Magazine/November-2012/Best-Sandwiches-Chicago/'
url = url_base + url_sub

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

response = requests.get(url=url, headers=headers)
html = response.text

soup  = BeautifulSoup(html,'html.parser')

len(soup.find_all('div',class_='sammy'))
```
![image](https://github.com/user-attachments/assets/2c86e8b6-214e-4e03-972a-7e3f0c3c1a35)

```
from bs4 import BeautifulSoup
from urllib.request import urlopen

import numpy as np
import pandas as pd

import requests

url_base = 'https://www.chicagomag.com'
url_sub = '/Chicago-Magazine/November-2012/Best-Sandwiches-Chicago/'
url = url_base + url_sub

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

response = requests.get(url=url, headers=headers)
html = response.text

soup  = BeautifulSoup(html,'html.parser')

soup.find_all('div',class_='sammy')[0]
```
![image](https://github.com/user-attachments/assets/0b0a0cc8-35f5-4b15-9459-c566c73d81d3)
