## 결정트리시각화

```
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

# DecisionTree Classifier 생성
dt_clf = DecisionTreeClassifier(random_state=156, max_depth=3)

# 붓꽃 데이터를 로딩하고, 학습과 테스트 데이터 셋으로 분리
iris_data = load_iris()
X_train , X_test , y_train , y_test = train_test_split(
    iris_data.data
    , iris_data.target
    , test_size=0.2
    , random_state=11)

# DecisionTreeClassifer 학습. 
dt_clf.fit(X_train , y_train)
print(f'DecisionTreeClassifer 학습\n {dt_clf.fit(X_train , y_train)}')
```
![image](https://github.com/user-attachments/assets/9fe0d92b-e50c-4be0-aa38-98a5fc7b565f)

```
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

# DecisionTree Classifier 생성
dt_clf = DecisionTreeClassifier(random_state=156, max_depth=3)

# 붓꽃 데이터를 로딩하고, 학습과 테스트 데이터 셋으로 분리
iris_data = load_iris()
X_train , X_test , y_train , y_test = train_test_split(
    iris_data.data
    , iris_data.target
    , test_size=0.2
    , random_state=11)

# DecisionTreeClassifer 학습. 
dt_clf.fit(X_train , y_train)
# print(f'DecisionTreeClassifer 학습\n{dt_clf.fit(X_train , y_train)}')

from sklearn.tree import export_graphviz

# export_graphviz()의 호출 결과로 out_file로 지정된 tree.dot 파일을 생성함.
export_graphviz(
    dt_clf, out_file="./tree.dot"
    , class_names=iris_data.target_names
    ,feature_names = iris_data.feature_names
    , impurity=True
    , filled=True
)
```
![image](https://github.com/user-attachments/assets/134a596c-bbcb-4531-bb6c-8fdbda03608f)

```
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

# DecisionTree Classifier 생성
dt_clf = DecisionTreeClassifier(random_state=156, max_depth=3)

# 붓꽃 데이터를 로딩하고, 학습과 테스트 데이터 셋으로 분리
iris_data = load_iris()
X_train , X_test , y_train , y_test = train_test_split(
    iris_data.data
    , iris_data.target
    , test_size=0.2
    , random_state=11)

# DecisionTreeClassifer 학습. 
dt_clf.fit(X_train , y_train)
# print(f'DecisionTreeClassifer 학습\n{dt_clf.fit(X_train , y_train)}')

from sklearn.tree import export_graphviz

# export_graphviz()의 호출 결과로 out_file로 지정된 tree.dot 파일을 생성함.
export_graphviz(
    dt_clf, out_file="./tree.dot"
    , class_names=iris_data.target_names
    ,feature_names = iris_data.feature_names
    , impurity=True
    , filled=True
)

# 피처 중요도 시각화
# 피처 중요도 파악
import seaborn as sns
import numpy as np
# %matplotlib inline


# feature importance 추출
print("Feature importances:\n{0}".format(np.round(dt_clf.feature_importances_, 3)))


# feature별 importance 매핑
for name, value in zip(iris_data.feature_names , dt_clf.feature_importances_):
    print('{0} : {1:.3f}'.format(name, value))


# feature importance를 column 별로 시각화 하기
sns.barplot(x=dt_clf.feature_importances_ , y=iris_data.feature_names)
```
![image](https://github.com/user-attachments/assets/f030ae6a-c5f1-4233-a0cf-6c1f61306135)

![image](https://github.com/user-attachments/assets/e7e43d0a-5cf5-4323-8002-6104a5ac4f66)

```
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

# DecisionTree Classifier 생성
dt_clf = DecisionTreeClassifier(random_state=156, max_depth=3)

# 붓꽃 데이터를 로딩하고, 학습과 테스트 데이터 셋으로 분리
iris_data = load_iris()
X_train , X_test , y_train , y_test = train_test_split(
    iris_data.data
    , iris_data.target
    , test_size=0.2
    , random_state=11)

# DecisionTreeClassifer 학습. 
dt_clf.fit(X_train , y_train)
# print(f'DecisionTreeClassifer 학습\n{dt_clf.fit(X_train , y_train)}')

from sklearn.tree import export_graphviz

# export_graphviz()의 호출 결과로 out_file로 지정된 tree.dot 파일을 생성함.
export_graphviz(
    dt_clf, out_file="./tree.dot"
    , class_names=iris_data.target_names
    ,feature_names = iris_data.feature_names
    , impurity=True
    , filled=True
)

# 피처 중요도 시각화
# 피처 중요도 파악
import seaborn as sns
import numpy as np
# %matplotlib inline


# feature importance 추출
print("Feature importances:\n{0}".format(np.round(dt_clf.feature_importances_, 3)))


# feature별 importance 매핑
for name, value in zip(iris_data.feature_names , dt_clf.feature_importances_):
    print('{0} : {1:.3f}'.format(name, value))


# feature importance를 column 별로 시각화 하기
sns.barplot(x=dt_clf.feature_importances_ , y=iris_data.feature_names)

# 과적합 처리
# 데이터 생성
from sklearn.datasets import make_classification
import matplotlib.pyplot as plt
%matplotlib inline


plt.title("3 Class values with 2 Features Sample data creation")


# 2차원 시각화를 위해서 feature는 2개, 결정값 클래스는 3가지 유형의 classification 샘플 데이터 생성.
X_features, y_labels = make_classification(n_features=2, n_redundant=0, n_informative=2,
                             n_classes=3, n_clusters_per_class=1,random_state=0)


# plot 형태로 2개의 feature로 2차원 좌표 시각화, 각 클래스값은 다른 색깔로 표시됨.
plt.scatter(X_features[:, 0], X_features[:, 1], marker='o', c=y_labels, s=25, cmap='rainbow', edgecolor='k')
plt.show()
```
![image](https://github.com/user-attachments/assets/35f72173-6a25-4ed6-bdd7-eeb5f8b05677)

![image](https://github.com/user-attachments/assets/975b5d7c-b2eb-4c02-a064-4a519fbd9bff)

```
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

# DecisionTree Classifier 생성
dt_clf = DecisionTreeClassifier(random_state=156, max_depth=3)

# 붓꽃 데이터를 로딩하고, 학습과 테스트 데이터 셋으로 분리
iris_data = load_iris()
X_train , X_test , y_train , y_test = train_test_split(
    iris_data.data
    , iris_data.target
    , test_size=0.2
    , random_state=11)

# DecisionTreeClassifer 학습. 
dt_clf.fit(X_train , y_train)
# print(f'DecisionTreeClassifer 학습\n{dt_clf.fit(X_train , y_train)}')

from sklearn.tree import export_graphviz

# export_graphviz()의 호출 결과로 out_file로 지정된 tree.dot 파일을 생성함.
export_graphviz(
    dt_clf, out_file="./tree.dot"
    , class_names=iris_data.target_names
    ,feature_names = iris_data.feature_names
    , impurity=True
    , filled=True
)

# 피처 중요도 시각화
# 피처 중요도 파악
import seaborn as sns
import numpy as np
# %matplotlib inline


# feature importance 추출
# print("Feature importances:\n{0}".format(np.round(dt_clf.feature_importances_, 3)))


# feature별 importance 매핑
# for name, value in zip(iris_data.feature_names , dt_clf.feature_importances_):
    # print('{0} : {1:.3f}'.format(name, value))


# feature importance를 column 별로 시각화 하기
# sns.barplot(x=dt_clf.feature_importances_ , y=iris_data.feature_names)

# # 과적합 처리
# # 데이터 생성
# from sklearn.datasets import make_classification
# import matplotlib.pyplot as plt
# %matplotlib inline

# plt.title("3 Class values with 2 Features Sample data creation")


# # 2차원 시각화를 위해서 feature는 2개, 결정값 클래스는 3가지 유형의 classification 샘플 데이터 생성.
# X_features, y_labels = make_classification(n_features=2, n_redundant=0, n_informative=2,
#                              n_classes=3, n_clusters_per_class=1,random_state=0)


# # plot 형태로 2개의 feature로 2차원 좌표 시각화, 각 클래스값은 다른 색깔로 표시됨.
# plt.scatter(X_features[:, 0], X_features[:, 1], marker='o', c=y_labels, s=25, cmap='rainbow', edgecolor='k')

# Classifier의 Decision Boundary를 시각화 하는 함수
def visualize_boundary(model, X, y):
    fig,ax = plt.subplots()
   
    # 학습 데이타 scatter plot으로 나타내기
    ax.scatter(X[:, 0], X[:, 1], c=y, s=25, cmap='rainbow', edgecolor='k',
               clim=(y.min(), y.max()), zorder=3)
    ax.axis('tight')
    ax.axis('off')
    xlim_start , xlim_end = ax.get_xlim()
    ylim_start , ylim_end = ax.get_ylim()
   
    # 호출 파라미터로 들어온 training 데이타로 model 학습 .
    model.fit(X, y)
    # meshgrid 형태인 모든 좌표값으로 예측 수행.
    xx, yy = np.meshgrid(np.linspace(xlim_start,xlim_end, num=200),np.linspace(ylim_start,ylim_end, num=200))
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)
   
    # contourf() 를 이용하여 class boundary 를 visualization 수행.
    n_classes = len(np.unique(y))
    contours = ax.contourf(xx, yy, Z, alpha=0.3,
                           levels=np.arange(n_classes + 1) - 0.5,
                           cmap='rainbow', clim=(y.min(), y.max()),
                           zorder=1)


from sklearn.tree import DecisionTreeClassifier

# 학습이 종료된 모델 나왔다
dt_clf=DecisionTreeClassifier(random_state=156).fit(X_features, y_labels)
visualize_boundary(dt_clf, X_features, y_labels)
plt.show()
```
![image](https://github.com/user-attachments/assets/5f27907b-7c36-411a-b235-e8bad86ba63f)

![image](https://github.com/user-attachments/assets/ab6ee2b0-24ef-4c14-971a-628dfdf5bb21)

```
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

# DecisionTree Classifier 생성
dt_clf = DecisionTreeClassifier(random_state=156, max_depth=3)

# 붓꽃 데이터를 로딩하고, 학습과 테스트 데이터 셋으로 분리
iris_data = load_iris()
X_train , X_test , y_train , y_test = train_test_split(
    iris_data.data
    , iris_data.target
    , test_size=0.2
    , random_state=11)

# DecisionTreeClassifer 학습. 
dt_clf.fit(X_train , y_train)
# print(f'DecisionTreeClassifer 학습\n{dt_clf.fit(X_train , y_train)}')

from sklearn.tree import export_graphviz

# export_graphviz()의 호출 결과로 out_file로 지정된 tree.dot 파일을 생성함.
export_graphviz(
    dt_clf, out_file="./tree.dot"
    , class_names=iris_data.target_names
    ,feature_names = iris_data.feature_names
    , impurity=True
    , filled=True
)

# 피처 중요도 시각화
# 피처 중요도 파악
import seaborn as sns
import numpy as np
# %matplotlib inline


# feature importance 추출
# print("Feature importances:\n{0}".format(np.round(dt_clf.feature_importances_, 3)))


# feature별 importance 매핑
# for name, value in zip(iris_data.feature_names , dt_clf.feature_importances_):
    # print('{0} : {1:.3f}'.format(name, value))


# feature importance를 column 별로 시각화 하기
# sns.barplot(x=dt_clf.feature_importances_ , y=iris_data.feature_names)

# # 과적합 처리
# # 데이터 생성
# from sklearn.datasets import make_classification
# import matplotlib.pyplot as plt
# %matplotlib inline

# plt.title("3 Class values with 2 Features Sample data creation")


# # 2차원 시각화를 위해서 feature는 2개, 결정값 클래스는 3가지 유형의 classification 샘플 데이터 생성.
# X_features, y_labels = make_classification(n_features=2, n_redundant=0, n_informative=2,
#                              n_classes=3, n_clusters_per_class=1,random_state=0)


# # plot 형태로 2개의 feature로 2차원 좌표 시각화, 각 클래스값은 다른 색깔로 표시됨.
# plt.scatter(X_features[:, 0], X_features[:, 1], marker='o', c=y_labels, s=25, cmap='rainbow', edgecolor='k')

# Classifier의 Decision Boundary를 시각화 하는 함수
def visualize_boundary(model, X, y):
    fig,ax = plt.subplots()
   
    # 학습 데이타 scatter plot으로 나타내기
    ax.scatter(X[:, 0], X[:, 1], c=y, s=25, cmap='rainbow', edgecolor='k',
               clim=(y.min(), y.max()), zorder=3)
    ax.axis('tight')
    ax.axis('off')
    xlim_start , xlim_end = ax.get_xlim()
    ylim_start , ylim_end = ax.get_ylim()
   
    # 호출 파라미터로 들어온 training 데이타로 model 학습 .
    model.fit(X, y)
    # meshgrid 형태인 모든 좌표값으로 예측 수행.
    xx, yy = np.meshgrid(np.linspace(xlim_start,xlim_end, num=200),np.linspace(ylim_start,ylim_end, num=200))
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)
   
    # contourf() 를 이용하여 class boundary 를 visualization 수행.
    n_classes = len(np.unique(y))
    contours = ax.contourf(xx, yy, Z, alpha=0.3,
                           levels=np.arange(n_classes + 1) - 0.5,
                           cmap='rainbow', clim=(y.min(), y.max()),
                           zorder=1)


from sklearn.tree import DecisionTreeClassifier

# 학습이 종료된 모델 나왔다
# dt_clf=DecisionTreeClassifier(random_state=156).fit(X_features, y_labels)
# visualize_boundary(dt_clf, X_features, y_labels)

# min_samples_leaf=6 으로 트리 생성 조건을 제약한 Decision Boundary 시각화
dt_clf = DecisionTreeClassifier(min_samples_leaf=6, random_state=156).fit(X_features, y_labels)
visualize_boundary(dt_clf, X_features, y_labels)


plt.show()
```
![image](https://github.com/user-attachments/assets/d7e64871-f4e8-4a90-b95f-4f12a567aa7d)

---
## 결정 트리 실습

https://archive.ics.uci.edu/dataset/240/human+activity+recognition+using+smartphones 에서 데이터 다운로드

```
import pandas as pd
import numpy as np
import matplotlib as plt

# 피처명 읽기
# features.txt 파일에는 피쳐 이름 index와 피쳐명이 공백으로 분리되어 있음
# 이름 DataFrame 롣,
# \s+ => \s : white space (공백,\n,\r,\) 한자
# + : 한개 이상
feature_name_dir=pd.read_csv(
    './UCI HAR Dataset/data/features.txt'
    ,sep='\s+'
    ,header=None
    ,names=['column_index','column_name']
)
feature_name_dir.head()
```
![image](https://github.com/user-attachments/assets/704f531b-2e37-4cb4-b0f8-b25654039f71)

```
import pandas as pd
import numpy as np
import matplotlib as plt

# 피처명 읽기
# features.txt 파일에는 피쳐 이름 index와 피쳐명이 공백으로 분리되어 있음
# 이름 DataFrame 롣,
# \s+ => \s : white space (공백,\n,\r,\) 한자
# + : 한개 이상
feature_name_dir=pd.read_csv(
    './UCI HAR Dataset/data/features.txt'
    ,sep='\s+'
    ,header=None
    ,names=['column_index','column_name']
)

# 중복된 피쳐명 확인

feature_dup_df = feature_name_dir.groupby(
    'column_name'
).count()

print(f"중복된 개수 확인 : {feature_dup_df[feature_dup_df['column_index']>1].count()}")
```
![image](https://github.com/user-attachments/assets/c49c2387-52b7-4208-843e-f052e7cf6bef)

```
import pandas as pd
import numpy as np
import matplotlib as plt

# 피처명 읽기
# features.txt 파일에는 피쳐 이름 index와 피쳐명이 공백으로 분리되어 있음
# 이름 DataFrame 롣,
# \s+ => \s : white space (공백,\n,\r,\) 한자
# + : 한개 이상
feature_name_dir=pd.read_csv(
    './UCI HAR Dataset/data/features.txt'
    ,sep='\s+'
    ,header=None
    ,names=['column_index','column_name']
)

# 중복된 피쳐명 확인

feature_dup_df = feature_name_dir.groupby(
    'column_name'
).count()

def get_new_feature_name_df(old_feature_name_df):
    feature_dup_df = pd.DataFrame(
        data=old_feature_name_df.groupby('column_name').cumcount()
        ,columns=['dup_cnt']
    )
    feature_dup_df = feature_dup_df.reset_index()
    new_feature_name_df = pd.merge(old_feature_name_df.reset_index(), feature_dup_df, how='outer')
    new_feature_name_df['column_name'] = new_feature_name_df[['column_name', 'dup_cnt']].apply(
        lambda x : x[0]+'_'+str(x[1]) if x[1] >0 else x[0]
        ,axis=1
    )
    new_feature_name_df = new_feature_name_df.drop(['index'], axis=1)
    return new_feature_name_df

def get_human_dataset( ):
   
    # 각 데이터 파일들은 공백으로 분리되어 있으므로 read_csv에서 공백 문자를 sep으로 할당.
    feature_name_df = pd.read_csv('./UCI HAR Dataset/data/features.txt',sep='\s+',
                        header=None,names=['column_index','column_name'])
   
    # 중복된 피처명을 수정하는 get_new_feature_name_df()를 이용, 신규 피처명 DataFrame생성.
    new_feature_name_df = get_new_feature_name_df(feature_name_df)
   
    # DataFrame에 피처명을 컬럼으로 부여하기 위해 리스트 객체로 다시 변환
    feature_name = new_feature_name_df.iloc[:, 1].values.tolist()
   
    # 학습 피처 데이터 셋과 테스트 피처 데이터을 DataFrame으로 로딩. 컬럼명은 feature_name 적용
    X_train = pd.read_csv('./UCI HAR Dataset/data/train/X_train.txt',sep='\s+', names=feature_name )
    X_test = pd.read_csv('./UCI HAR Dataset/data//test/X_test.txt',sep='\s+', names=feature_name)
   
    # 학습 레이블과 테스트 레이블 데이터을 DataFrame으로 로딩하고 컬럼명은 action으로 부여
    y_train = pd.read_csv('./UCI HAR Dataset/data/train/y_train.txt',sep='\s+',header=None,names=['action'])
    y_test = pd.read_csv('./UCI HAR Dataset/data/test/y_test.txt',sep='\s+',header=None,names=['action'])
   
    # 로드된 학습/테스트용 DataFrame을 모두 반환
    return X_train, X_test, y_train, y_test

X_train, X_test, y_train, y_test = get_human_dataset()

print('학습 데이터 구조')
print(f'X_train shape : {X_train.shape}')
print(f'X_test shape : {X_test.shape}')
print(f'y_train shape : {y_train.shape}')
print(f'y_test shape : {y_test.shape}')

# 행동-답 확인
print(f"행동-답 확인\n{y_train['action']}")
```
![image](https://github.com/user-attachments/assets/4a87c1ab-b431-4455-8ac4-a528ab0aaab5)

```
import pandas as pd
import numpy as np
import matplotlib as plt

# 피처명 읽기
# features.txt 파일에는 피쳐 이름 index와 피쳐명이 공백으로 분리되어 있음
# 이름 DataFrame 롣,
# \s+ => \s : white space (공백,\n,\r,\) 한자
# + : 한개 이상
feature_name_dir=pd.read_csv(
    './UCI HAR Dataset/data/features.txt'
    ,sep='\s+'
    ,header=None
    ,names=['column_index','column_name']
)

# 중복된 피쳐명 확인

feature_dup_df = feature_name_dir.groupby(
    'column_name'
).count()

def get_new_feature_name_df(old_feature_name_df):
    feature_dup_df = pd.DataFrame(
        data=old_feature_name_df.groupby('column_name').cumcount()
        ,columns=['dup_cnt']
    )
    feature_dup_df = feature_dup_df.reset_index()
    new_feature_name_df = pd.merge(old_feature_name_df.reset_index(), feature_dup_df, how='outer')
    new_feature_name_df['column_name'] = new_feature_name_df[['column_name', 'dup_cnt']].apply(
        lambda x : x[0]+'_'+str(x[1]) if x[1] >0 else x[0]
        ,axis=1
    )
    new_feature_name_df = new_feature_name_df.drop(['index'], axis=1)
    return new_feature_name_df

def get_human_dataset( ):
   
    # 각 데이터 파일들은 공백으로 분리되어 있으므로 read_csv에서 공백 문자를 sep으로 할당.
    feature_name_df = pd.read_csv('./UCI HAR Dataset/data/features.txt',sep='\s+',
                        header=None,names=['column_index','column_name'])
   
    # 중복된 피처명을 수정하는 get_new_feature_name_df()를 이용, 신규 피처명 DataFrame생성.
    new_feature_name_df = get_new_feature_name_df(feature_name_df)
   
    # DataFrame에 피처명을 컬럼으로 부여하기 위해 리스트 객체로 다시 변환
    feature_name = new_feature_name_df.iloc[:, 1].values.tolist()
   
    # 학습 피처 데이터 셋과 테스트 피처 데이터을 DataFrame으로 로딩. 컬럼명은 feature_name 적용
    X_train = pd.read_csv('./UCI HAR Dataset/data/train/X_train.txt',sep='\s+', names=feature_name )
    X_test = pd.read_csv('./UCI HAR Dataset/data//test/X_test.txt',sep='\s+', names=feature_name)
   
    # 학습 레이블과 테스트 레이블 데이터을 DataFrame으로 로딩하고 컬럼명은 action으로 부여
    y_train = pd.read_csv('./UCI HAR Dataset/data/train/y_train.txt',sep='\s+',header=None,names=['action'])
    y_test = pd.read_csv('./UCI HAR Dataset/data/test/y_test.txt',sep='\s+',header=None,names=['action'])
   
    # 로드된 학습/테스트용 DataFrame을 모두 반환
    return X_train, X_test, y_train, y_test

X_train, X_test, y_train, y_test = get_human_dataset()

# 데이터 불균형 확인
# Series -> value_counts()
y_train['action'].value_counts() # 확인결과 불균형 하지 않음
```
![image](https://github.com/user-attachments/assets/8fd39b02-c7a3-433e-93dc-b5720b596e77)

```
import pandas as pd
import numpy as np
import matplotlib as plt

# 피처명 읽기
# features.txt 파일에는 피쳐 이름 index와 피쳐명이 공백으로 분리되어 있음
# 이름 DataFrame 롣,
# \s+ => \s : white space (공백,\n,\r,\) 한자
# + : 한개 이상
feature_name_dir=pd.read_csv(
    './UCI HAR Dataset/data/features.txt'
    ,sep='\s+'
    ,header=None
    ,names=['column_index','column_name']
)

# 중복된 피쳐명 확인

feature_dup_df = feature_name_dir.groupby(
    'column_name'
).count()

def get_new_feature_name_df(old_feature_name_df):
    feature_dup_df = pd.DataFrame(
        data=old_feature_name_df.groupby('column_name').cumcount()
        ,columns=['dup_cnt']
    )
    feature_dup_df = feature_dup_df.reset_index()
    new_feature_name_df = pd.merge(old_feature_name_df.reset_index(), feature_dup_df, how='outer')
    new_feature_name_df['column_name'] = new_feature_name_df[['column_name', 'dup_cnt']].apply(
        lambda x : x[0]+'_'+str(x[1]) if x[1] >0 else x[0]
        ,axis=1
    )
    new_feature_name_df = new_feature_name_df.drop(['index'], axis=1)
    return new_feature_name_df

def get_human_dataset( ):
   
    # 각 데이터 파일들은 공백으로 분리되어 있으므로 read_csv에서 공백 문자를 sep으로 할당.
    feature_name_df = pd.read_csv('./UCI HAR Dataset/data/features.txt',sep='\s+',
                        header=None,names=['column_index','column_name'])
   
    # 중복된 피처명을 수정하는 get_new_feature_name_df()를 이용, 신규 피처명 DataFrame생성.
    new_feature_name_df = get_new_feature_name_df(feature_name_df)
   
    # DataFrame에 피처명을 컬럼으로 부여하기 위해 리스트 객체로 다시 변환
    feature_name = new_feature_name_df.iloc[:, 1].values.tolist()
   
    # 학습 피처 데이터 셋과 테스트 피처 데이터을 DataFrame으로 로딩. 컬럼명은 feature_name 적용
    X_train = pd.read_csv('./UCI HAR Dataset/data/train/X_train.txt',sep='\s+', names=feature_name )
    X_test = pd.read_csv('./UCI HAR Dataset/data//test/X_test.txt',sep='\s+', names=feature_name)
   
    # 학습 레이블과 테스트 레이블 데이터을 DataFrame으로 로딩하고 컬럼명은 action으로 부여
    y_train = pd.read_csv('./UCI HAR Dataset/data/train/y_train.txt',sep='\s+',header=None,names=['action'])
    y_test = pd.read_csv('./UCI HAR Dataset/data/test/y_test.txt',sep='\s+',header=None,names=['action'])
   
    # 로드된 학습/테스트용 DataFrame을 모두 반환
    return X_train, X_test, y_train, y_test

X_train, X_test, y_train, y_test = get_human_dataset()

# Decision Tree
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score # 정확도 점수

# 아래 코드를 반복 실행할 때마다 예측 결과가 동일하게 나오게 해야한다. : random_state
dt_clf=DecisionTreeClassifier(random_state=156)
dt_clf.fit(X_train,y_train)
pred=dt_clf.predict(X_test)
accuracy=accuracy_score(pred,y_test)
print(f'결정 트리 예측 정확도 : {accuracy}')
```
![image](https://github.com/user-attachments/assets/4fc1526f-bc3d-408d-bd46-49030669a5a2)

```
import pandas as pd
import numpy as np
import matplotlib as plt

# 피처명 읽기
# features.txt 파일에는 피쳐 이름 index와 피쳐명이 공백으로 분리되어 있음
# 이름 DataFrame 롣,
# \s+ => \s : white space (공백,\n,\r,\) 한자
# + : 한개 이상
feature_name_dir=pd.read_csv(
    './UCI HAR Dataset/data/features.txt'
    ,sep='\s+'
    ,header=None
    ,names=['column_index','column_name']
)

# 중복된 피쳐명 확인

feature_dup_df = feature_name_dir.groupby(
    'column_name'
).count()

def get_new_feature_name_df(old_feature_name_df):
    feature_dup_df = pd.DataFrame(
        data=old_feature_name_df.groupby('column_name').cumcount()
        ,columns=['dup_cnt']
    )
    feature_dup_df = feature_dup_df.reset_index()
    new_feature_name_df = pd.merge(old_feature_name_df.reset_index(), feature_dup_df, how='outer')
    new_feature_name_df['column_name'] = new_feature_name_df[['column_name', 'dup_cnt']].apply(
        lambda x : x[0]+'_'+str(x[1]) if x[1] >0 else x[0]
        ,axis=1
    )
    new_feature_name_df = new_feature_name_df.drop(['index'], axis=1)
    return new_feature_name_df

def get_human_dataset( ):
   
    # 각 데이터 파일들은 공백으로 분리되어 있으므로 read_csv에서 공백 문자를 sep으로 할당.
    feature_name_df = pd.read_csv('./UCI HAR Dataset/data/features.txt',sep='\s+',
                        header=None,names=['column_index','column_name'])
   
    # 중복된 피처명을 수정하는 get_new_feature_name_df()를 이용, 신규 피처명 DataFrame생성.
    new_feature_name_df = get_new_feature_name_df(feature_name_df)
   
    # DataFrame에 피처명을 컬럼으로 부여하기 위해 리스트 객체로 다시 변환
    feature_name = new_feature_name_df.iloc[:, 1].values.tolist()
   
    # 학습 피처 데이터 셋과 테스트 피처 데이터을 DataFrame으로 로딩. 컬럼명은 feature_name 적용
    X_train = pd.read_csv('./UCI HAR Dataset/data/train/X_train.txt',sep='\s+', names=feature_name )
    X_test = pd.read_csv('./UCI HAR Dataset/data//test/X_test.txt',sep='\s+', names=feature_name)
   
    # 학습 레이블과 테스트 레이블 데이터을 DataFrame으로 로딩하고 컬럼명은 action으로 부여
    y_train = pd.read_csv('./UCI HAR Dataset/data/train/y_train.txt',sep='\s+',header=None,names=['action'])
    y_test = pd.read_csv('./UCI HAR Dataset/data/test/y_test.txt',sep='\s+',header=None,names=['action'])
   
    # 로드된 학습/테스트용 DataFrame을 모두 반환
    return X_train, X_test, y_train, y_test

X_train, X_test, y_train, y_test = get_human_dataset()

# Decision Tree
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score # 정확도 점수

# 아래 코드를 반복 실행할 때마다 예측 결과가 동일하게 나오게 해야한다. : random_state
dt_clf=DecisionTreeClassifier(random_state=156)
dt_clf.fit(X_train,y_train)
pred=dt_clf.predict(X_test)
accuracy=accuracy_score(pred,y_test)
# print(f'결정 트리 예측 정확도 : {accuracy}')
# y_test['action'].value_counts()

# 튜닝 : 하이퍼파라미티
print('Decision Tree 기본 하이퍼 파라미터 :\n',dt_clf.get_params())
```
![image](https://github.com/user-attachments/assets/41ae42c2-0944-4285-921a-31e2085a32df)

=> {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 156, 'splitter': 'best'}

```
import pandas as pd
import numpy as np
import matplotlib as plt

# 피처명 읽기
# features.txt 파일에는 피쳐 이름 index와 피쳐명이 공백으로 분리되어 있음
# 이름 DataFrame 롣,
# \s+ => \s : white space (공백,\n,\r,\) 한자
# + : 한개 이상
feature_name_dir=pd.read_csv(
    './UCI HAR Dataset/data/features.txt'
    ,sep='\s+'
    ,header=None
    ,names=['column_index','column_name']
)

# 중복된 피쳐명 확인

feature_dup_df = feature_name_dir.groupby(
    'column_name'
).count()

def get_new_feature_name_df(old_feature_name_df):
    feature_dup_df = pd.DataFrame(
        data=old_feature_name_df.groupby('column_name').cumcount()
        ,columns=['dup_cnt']
    )
    feature_dup_df = feature_dup_df.reset_index()
    new_feature_name_df = pd.merge(old_feature_name_df.reset_index(), feature_dup_df, how='outer')
    new_feature_name_df['column_name'] = new_feature_name_df[['column_name', 'dup_cnt']].apply(
        lambda x : x[0]+'_'+str(x[1]) if x[1] >0 else x[0]
        ,axis=1
    )
    new_feature_name_df = new_feature_name_df.drop(['index'], axis=1)
    return new_feature_name_df

def get_human_dataset( ):
   
    # 각 데이터 파일들은 공백으로 분리되어 있으므로 read_csv에서 공백 문자를 sep으로 할당.
    feature_name_df = pd.read_csv('./UCI HAR Dataset/data/features.txt',sep='\s+',
                        header=None,names=['column_index','column_name'])
   
    # 중복된 피처명을 수정하는 get_new_feature_name_df()를 이용, 신규 피처명 DataFrame생성.
    new_feature_name_df = get_new_feature_name_df(feature_name_df)
   
    # DataFrame에 피처명을 컬럼으로 부여하기 위해 리스트 객체로 다시 변환
    feature_name = new_feature_name_df.iloc[:, 1].values.tolist()
   
    # 학습 피처 데이터 셋과 테스트 피처 데이터을 DataFrame으로 로딩. 컬럼명은 feature_name 적용
    X_train = pd.read_csv('./UCI HAR Dataset/data/train/X_train.txt',sep='\s+', names=feature_name )
    X_test = pd.read_csv('./UCI HAR Dataset/data//test/X_test.txt',sep='\s+', names=feature_name)
   
    # 학습 레이블과 테스트 레이블 데이터을 DataFrame으로 로딩하고 컬럼명은 action으로 부여
    y_train = pd.read_csv('./UCI HAR Dataset/data/train/y_train.txt',sep='\s+',header=None,names=['action'])
    y_test = pd.read_csv('./UCI HAR Dataset/data/test/y_test.txt',sep='\s+',header=None,names=['action'])
   
    # 로드된 학습/테스트용 DataFrame을 모두 반환
    return X_train, X_test, y_train, y_test

X_train, X_test, y_train, y_test = get_human_dataset()

# Decision Tree
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score # 정확도 점수

# 아래 코드를 반복 실행할 때마다 예측 결과가 동일하게 나오게 해야한다. : random_state
dt_clf=DecisionTreeClassifier(random_state=156)
dt_clf.fit(X_train,y_train)
pred=dt_clf.predict(X_test)
accuracy=accuracy_score(pred,y_test)
# print(f'결정 트리 예측 정확도 : {accuracy}')
# y_test['action'].value_counts()

# 튜닝 : 하이퍼파라미티
# print('Decision Tree 기본 하이퍼 파라미터 :\n',dt_clf.get_params())

# 하이퍼파라미티 튜닝 : GridSearchCV(), max_depth로 BEST 찾기
from sklearn.model_selection import GridSearchCV

grid_params={
    'max_depth':[6,8,10,12,16,20,24]
}

grid_cv=GridSearchCV(
    dt_clf # 모델 : 학습할 알고리즘
    ,param_grid=grid_params
    ,scoring='accuracy' #정확도로 평가
    ,cv=5
    ,verbose=1
)

grid_cv.fit(X_train,y_train)

```
![image](https://github.com/user-attachments/assets/939e60cc-9f83-4615-ba78-fd7aa11be096)

![image](https://github.com/user-attachments/assets/451bbc1b-4742-44ee-ba43-b4f414058317)

```
import pandas as pd
import numpy as np
import matplotlib as plt

# 피처명 읽기
# features.txt 파일에는 피쳐 이름 index와 피쳐명이 공백으로 분리되어 있음
# 이름 DataFrame 롣,
# \s+ => \s : white space (공백,\n,\r,\) 한자
# + : 한개 이상
feature_name_dir=pd.read_csv(
    './UCI HAR Dataset/data/features.txt'
    ,sep='\s+'
    ,header=None
    ,names=['column_index','column_name']
)

# 중복된 피쳐명 확인

feature_dup_df = feature_name_dir.groupby(
    'column_name'
).count()

def get_new_feature_name_df(old_feature_name_df):
    feature_dup_df = pd.DataFrame(
        data=old_feature_name_df.groupby('column_name').cumcount()
        ,columns=['dup_cnt']
    )
    feature_dup_df = feature_dup_df.reset_index()
    new_feature_name_df = pd.merge(old_feature_name_df.reset_index(), feature_dup_df, how='outer')
    new_feature_name_df['column_name'] = new_feature_name_df[['column_name', 'dup_cnt']].apply(
        lambda x : x[0]+'_'+str(x[1]) if x[1] >0 else x[0]
        ,axis=1
    )
    new_feature_name_df = new_feature_name_df.drop(['index'], axis=1)
    return new_feature_name_df

def get_human_dataset( ):
   
    # 각 데이터 파일들은 공백으로 분리되어 있으므로 read_csv에서 공백 문자를 sep으로 할당.
    feature_name_df = pd.read_csv('./UCI HAR Dataset/data/features.txt',sep='\s+',
                        header=None,names=['column_index','column_name'])
   
    # 중복된 피처명을 수정하는 get_new_feature_name_df()를 이용, 신규 피처명 DataFrame생성.
    new_feature_name_df = get_new_feature_name_df(feature_name_df)
   
    # DataFrame에 피처명을 컬럼으로 부여하기 위해 리스트 객체로 다시 변환
    feature_name = new_feature_name_df.iloc[:, 1].values.tolist()
   
    # 학습 피처 데이터 셋과 테스트 피처 데이터을 DataFrame으로 로딩. 컬럼명은 feature_name 적용
    X_train = pd.read_csv('./UCI HAR Dataset/data/train/X_train.txt',sep='\s+', names=feature_name )
    X_test = pd.read_csv('./UCI HAR Dataset/data//test/X_test.txt',sep='\s+', names=feature_name)
   
    # 학습 레이블과 테스트 레이블 데이터을 DataFrame으로 로딩하고 컬럼명은 action으로 부여
    y_train = pd.read_csv('./UCI HAR Dataset/data/train/y_train.txt',sep='\s+',header=None,names=['action'])
    y_test = pd.read_csv('./UCI HAR Dataset/data/test/y_test.txt',sep='\s+',header=None,names=['action'])
   
    # 로드된 학습/테스트용 DataFrame을 모두 반환
    return X_train, X_test, y_train, y_test

X_train, X_test, y_train, y_test = get_human_dataset()

# Decision Tree
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score # 정확도 점수

# 아래 코드를 반복 실행할 때마다 예측 결과가 동일하게 나오게 해야한다. : random_state
dt_clf=DecisionTreeClassifier(random_state=156)
dt_clf.fit(X_train,y_train)
pred=dt_clf.predict(X_test)
accuracy=accuracy_score(pred,y_test)
# print(f'결정 트리 예측 정확도 : {accuracy}')
# y_test['action'].value_counts()

# 튜닝 : 하이퍼파라미티
# print('Decision Tree 기본 하이퍼 파라미터 :\n',dt_clf.get_params())

# 하이퍼파라미티 튜닝 : GridSearchCV(), max_depth로 BEST 찾기
from sklearn.model_selection import GridSearchCV

grid_params={
    'max_depth':[6,8,10,12,16,20,24]
}

grid_cv=GridSearchCV(
    dt_clf # 모델 : 학습할 알고리즘
    ,param_grid=grid_params
    ,scoring='accuracy' #정확도로 평가
    ,cv=5
    ,verbose=1
    ,n_jobs=-1
)

grid_cv.fit(X_train,y_train)

# 평균 정확도, 베스트 파라미터 확인
print(f'최고 평균 정확도 : {grid_cv.best_score_}')
print(f'베세트 파라미터 : {grid_cv.best_params_}')
```
![image](https://github.com/user-attachments/assets/47190c15-e508-4d6e-ba97-d88b722b4647)

```
import pandas as pd
import numpy as np
import matplotlib as plt

# 피처명 읽기
# features.txt 파일에는 피쳐 이름 index와 피쳐명이 공백으로 분리되어 있음
# 이름 DataFrame 롣,
# \s+ => \s : white space (공백,\n,\r,\) 한자
# + : 한개 이상
feature_name_dir=pd.read_csv(
    './UCI HAR Dataset/data/features.txt'
    ,sep='\s+'
    ,header=None
    ,names=['column_index','column_name']
)

# 중복된 피쳐명 확인

feature_dup_df = feature_name_dir.groupby(
    'column_name'
).count()

def get_new_feature_name_df(old_feature_name_df):
    feature_dup_df = pd.DataFrame(
        data=old_feature_name_df.groupby('column_name').cumcount()
        ,columns=['dup_cnt']
    )
    feature_dup_df = feature_dup_df.reset_index()
    new_feature_name_df = pd.merge(old_feature_name_df.reset_index(), feature_dup_df, how='outer')
    new_feature_name_df['column_name'] = new_feature_name_df[['column_name', 'dup_cnt']].apply(
        lambda x : x[0]+'_'+str(x[1]) if x[1] >0 else x[0]
        ,axis=1
    )
    new_feature_name_df = new_feature_name_df.drop(['index'], axis=1)
    return new_feature_name_df

def get_human_dataset( ):
   
    # 각 데이터 파일들은 공백으로 분리되어 있으므로 read_csv에서 공백 문자를 sep으로 할당.
    feature_name_df = pd.read_csv('./UCI HAR Dataset/data/features.txt',sep='\s+',
                        header=None,names=['column_index','column_name'])
   
    # 중복된 피처명을 수정하는 get_new_feature_name_df()를 이용, 신규 피처명 DataFrame생성.
    new_feature_name_df = get_new_feature_name_df(feature_name_df)
   
    # DataFrame에 피처명을 컬럼으로 부여하기 위해 리스트 객체로 다시 변환
    feature_name = new_feature_name_df.iloc[:, 1].values.tolist()
   
    # 학습 피처 데이터 셋과 테스트 피처 데이터을 DataFrame으로 로딩. 컬럼명은 feature_name 적용
    X_train = pd.read_csv('./UCI HAR Dataset/data/train/X_train.txt',sep='\s+', names=feature_name )
    X_test = pd.read_csv('./UCI HAR Dataset/data//test/X_test.txt',sep='\s+', names=feature_name)
   
    # 학습 레이블과 테스트 레이블 데이터을 DataFrame으로 로딩하고 컬럼명은 action으로 부여
    y_train = pd.read_csv('./UCI HAR Dataset/data/train/y_train.txt',sep='\s+',header=None,names=['action'])
    y_test = pd.read_csv('./UCI HAR Dataset/data/test/y_test.txt',sep='\s+',header=None,names=['action'])
   
    # 로드된 학습/테스트용 DataFrame을 모두 반환
    return X_train, X_test, y_train, y_test

X_train, X_test, y_train, y_test = get_human_dataset()

# Decision Tree
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score # 정확도 점수

# 아래 코드를 반복 실행할 때마다 예측 결과가 동일하게 나오게 해야한다. : random_state
dt_clf=DecisionTreeClassifier(random_state=156)
dt_clf.fit(X_train,y_train)
pred=dt_clf.predict(X_test)
accuracy=accuracy_score(pred,y_test)
# print(f'결정 트리 예측 정확도 : {accuracy}')
# y_test['action'].value_counts()

# 튜닝 : 하이퍼파라미티
# print('Decision Tree 기본 하이퍼 파라미터 :\n',dt_clf.get_params())

# 하이퍼파라미티 튜닝 : GridSearchCV(), max_depth로 BEST 찾기
from sklearn.model_selection import GridSearchCV

grid_params={
    'max_depth':[6,8,10,12,16,20,24]
}

grid_cv=GridSearchCV(
    dt_clf # 모델 : 학습할 알고리즘
    ,param_grid=grid_params
    ,scoring='accuracy' #정확도로 평가
    ,cv=5
    ,n_jobs=-1
)

grid_cv.fit(X_train,y_train)

# 평균 정확도, 베스트 파라미터 확인
print(f'최고 평균 정확도 : {grid_cv.best_score_}')
print(f'베스트 파라미터 : {grid_cv.best_params_}')

params={
    'max_depth':[6,8,10,12,16,20,24]
    ,'min_samples_split':[16,24]
}

grid_cv_params=GridSearchCV(
    dt_clf
    ,param_grid=params
    ,scoring='accuracy'
    ,cv=5
    ,n_jobs=-1
)

grid_cv_params.fit(X_train,y_train)

# 평균 정확도, 베스트 파라미터 확인
print(f'min_samples_split가 포함된 최고 평균 정확도 : {grid_cv_params.best_score_}')
print(f'min_samples_split가 포함된 최적 하이퍼 파라미터 : {grid_cv_params.best_params_}')
```
![image](https://github.com/user-attachments/assets/022e647c-5b95-4507-bd0e-cde9b63675c7)

```
import pandas as pd
import numpy as np
import matplotlib as plt

# 피처명 읽기
# features.txt 파일에는 피쳐 이름 index와 피쳐명이 공백으로 분리되어 있음
# 이름 DataFrame 롣,
# \s+ => \s : white space (공백,\n,\r,\) 한자
# + : 한개 이상
feature_name_dir=pd.read_csv(
    './UCI HAR Dataset/data/features.txt'
    ,sep='\s+'
    ,header=None
    ,names=['column_index','column_name']
)

# 중복된 피쳐명 확인

feature_dup_df = feature_name_dir.groupby(
    'column_name'
).count()

def get_new_feature_name_df(old_feature_name_df):
    feature_dup_df = pd.DataFrame(
        data=old_feature_name_df.groupby('column_name').cumcount()
        ,columns=['dup_cnt']
    )
    feature_dup_df = feature_dup_df.reset_index()
    new_feature_name_df = pd.merge(old_feature_name_df.reset_index(), feature_dup_df, how='outer')
    new_feature_name_df['column_name'] = new_feature_name_df[['column_name', 'dup_cnt']].apply(
        lambda x : x[0]+'_'+str(x[1]) if x[1] >0 else x[0]
        ,axis=1
    )
    new_feature_name_df = new_feature_name_df.drop(['index'], axis=1)
    return new_feature_name_df

def get_human_dataset( ):
   
    # 각 데이터 파일들은 공백으로 분리되어 있으므로 read_csv에서 공백 문자를 sep으로 할당.
    feature_name_df = pd.read_csv('./UCI HAR Dataset/data/features.txt',sep='\s+',
                        header=None,names=['column_index','column_name'])
   
    # 중복된 피처명을 수정하는 get_new_feature_name_df()를 이용, 신규 피처명 DataFrame생성.
    new_feature_name_df = get_new_feature_name_df(feature_name_df)
   
    # DataFrame에 피처명을 컬럼으로 부여하기 위해 리스트 객체로 다시 변환
    feature_name = new_feature_name_df.iloc[:, 1].values.tolist()
   
    # 학습 피처 데이터 셋과 테스트 피처 데이터을 DataFrame으로 로딩. 컬럼명은 feature_name 적용
    X_train = pd.read_csv('./UCI HAR Dataset/data/train/X_train.txt',sep='\s+', names=feature_name )
    X_test = pd.read_csv('./UCI HAR Dataset/data//test/X_test.txt',sep='\s+', names=feature_name)
   
    # 학습 레이블과 테스트 레이블 데이터을 DataFrame으로 로딩하고 컬럼명은 action으로 부여
    y_train = pd.read_csv('./UCI HAR Dataset/data/train/y_train.txt',sep='\s+',header=None,names=['action'])
    y_test = pd.read_csv('./UCI HAR Dataset/data/test/y_test.txt',sep='\s+',header=None,names=['action'])
   
    # 로드된 학습/테스트용 DataFrame을 모두 반환
    return X_train, X_test, y_train, y_test

X_train, X_test, y_train, y_test = get_human_dataset()

# Decision Tree
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score # 정확도 점수

# 아래 코드를 반복 실행할 때마다 예측 결과가 동일하게 나오게 해야한다. : random_state
dt_clf=DecisionTreeClassifier(random_state=156)
dt_clf.fit(X_train,y_train)
pred=dt_clf.predict(X_test)
accuracy=accuracy_score(pred,y_test)
# print(f'결정 트리 예측 정확도 : {accuracy}')
# y_test['action'].value_counts()

# 튜닝 : 하이퍼파라미티
# print('Decision Tree 기본 하이퍼 파라미터 :\n',dt_clf.get_params())

# 하이퍼파라미티 튜닝 : GridSearchCV(), max_depth로 BEST 찾기
from sklearn.model_selection import GridSearchCV

grid_params={
    'max_depth':[2,4,6,8,10,12,16,20,24]
}

grid_cv=GridSearchCV(
    dt_clf # 모델 : 학습할 알고리즘
    ,param_grid=grid_params
    ,scoring='accuracy' #정확도로 평가
    ,cv=5
    ,n_jobs=-1
)

grid_cv.fit(X_train,y_train)

# 평균 정확도, 베스트 파라미터 확인
print(f'최고 평균 정확도 : {grid_cv.best_score_}')
print(f'베스트 파라미터 : {grid_cv.best_params_}')

params={
    'max_depth':[8,10,12,16,20,24]
    ,'min_samples_split':[12,14,16,18,20,22,24]
}

grid_cv_params=GridSearchCV(
    dt_clf
    ,param_grid=params
    ,scoring='accuracy'
    ,cv=5
    ,n_jobs=-1
)

grid_cv_params.fit(X_train,y_train)

# 평균 정확도, 베스트 파라미터 확인
print(f'min_samples_split가 포함된 최고 평균 정확도 : {grid_cv_params.best_score_}')
print(f'min_samples_split가 포함된 최적 하이퍼 파라미터 : {grid_cv_params.best_params_}')
```
![image](https://github.com/user-attachments/assets/a077682f-557f-4ec8-9abf-f9993fc1ec27)

```
import pandas as pd
import numpy as np
import matplotlib as plt

# 피처명 읽기
# features.txt 파일에는 피쳐 이름 index와 피쳐명이 공백으로 분리되어 있음
# 이름 DataFrame 롣,
# \s+ => \s : white space (공백,\n,\r,\) 한자
# + : 한개 이상
feature_name_dir=pd.read_csv(
    './UCI HAR Dataset/data/features.txt'
    ,sep='\s+'
    ,header=None
    ,names=['column_index','column_name']
)

# 중복된 피쳐명 확인

feature_dup_df = feature_name_dir.groupby(
    'column_name'
).count()

def get_new_feature_name_df(old_feature_name_df):
    feature_dup_df = pd.DataFrame(
        data=old_feature_name_df.groupby('column_name').cumcount()
        ,columns=['dup_cnt']
    )
    feature_dup_df = feature_dup_df.reset_index()
    new_feature_name_df = pd.merge(old_feature_name_df.reset_index(), feature_dup_df, how='outer')
    new_feature_name_df['column_name'] = new_feature_name_df[['column_name', 'dup_cnt']].apply(
        lambda x : x[0]+'_'+str(x[1]) if x[1] >0 else x[0]
        ,axis=1
    )
    new_feature_name_df = new_feature_name_df.drop(['index'], axis=1)
    return new_feature_name_df

def get_human_dataset( ):
   
    # 각 데이터 파일들은 공백으로 분리되어 있으므로 read_csv에서 공백 문자를 sep으로 할당.
    feature_name_df = pd.read_csv('./UCI HAR Dataset/data/features.txt',sep='\s+',
                        header=None,names=['column_index','column_name'])
   
    # 중복된 피처명을 수정하는 get_new_feature_name_df()를 이용, 신규 피처명 DataFrame생성.
    new_feature_name_df = get_new_feature_name_df(feature_name_df)
   
    # DataFrame에 피처명을 컬럼으로 부여하기 위해 리스트 객체로 다시 변환
    feature_name = new_feature_name_df.iloc[:, 1].values.tolist()
   
    # 학습 피처 데이터 셋과 테스트 피처 데이터을 DataFrame으로 로딩. 컬럼명은 feature_name 적용
    X_train = pd.read_csv('./UCI HAR Dataset/data/train/X_train.txt',sep='\s+', names=feature_name )
    X_test = pd.read_csv('./UCI HAR Dataset/data//test/X_test.txt',sep='\s+', names=feature_name)
   
    # 학습 레이블과 테스트 레이블 데이터을 DataFrame으로 로딩하고 컬럼명은 action으로 부여
    y_train = pd.read_csv('./UCI HAR Dataset/data/train/y_train.txt',sep='\s+',header=None,names=['action'])
    y_test = pd.read_csv('./UCI HAR Dataset/data/test/y_test.txt',sep='\s+',header=None,names=['action'])
   
    # 로드된 학습/테스트용 DataFrame을 모두 반환
    return X_train, X_test, y_train, y_test

X_train, X_test, y_train, y_test = get_human_dataset()

# Decision Tree
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score # 정확도 점수

# 아래 코드를 반복 실행할 때마다 예측 결과가 동일하게 나오게 해야한다. : random_state
dt_clf=DecisionTreeClassifier(random_state=156)
dt_clf.fit(X_train,y_train)
pred=dt_clf.predict(X_test)
accuracy=accuracy_score(pred,y_test)
# print(f'결정 트리 예측 정확도 : {accuracy}')
# y_test['action'].value_counts()

# 튜닝 : 하이퍼파라미티
# print('Decision Tree 기본 하이퍼 파라미터 :\n',dt_clf.get_params())

# 하이퍼파라미티 튜닝 : GridSearchCV(), max_depth로 BEST 찾기
from sklearn.model_selection import GridSearchCV

grid_params={
    'max_depth':[2,4,6,8,10,12,16,20,24]
}

grid_cv=GridSearchCV(
    dt_clf # 모델 : 학습할 알고리즘
    ,param_grid=grid_params
    ,scoring='accuracy' #정확도로 평가
    ,cv=5
    ,n_jobs=-1
)

grid_cv.fit(X_train,y_train)

# 평균 정확도, 베스트 파라미터 확인
# print(f'최고 평균 정확도 : {grid_cv.best_score_}')
# print(f'베스트 파라미터 : {grid_cv.best_params_}')

params={
    'max_depth':[8,10,12,16,20,24]
    ,'min_samples_split':[12,14,16,18,20,22,24]
}

grid_cv_params=GridSearchCV(
    dt_clf
    ,param_grid=params
    ,scoring='accuracy'
    ,cv=5
    ,n_jobs=-1
)

grid_cv_params.fit(X_train,y_train)

# 평균 정확도, 베스트 파라미터 확인
# print(f'min_samples_split가 포함된 최고 평균 정확도 : {grid_cv_params.best_score_}')
# print(f'min_samples_split가 포함된 최적 하이퍼 파라미터 : {grid_cv_params.best_params_}')

# 일반화 성능 확인
best_df_clf=grid_cv_params.best_estimator_
pred1=best_df_clf.predict(X_test)
acc=accuracy_score(pred1,y_test)
print(f'일반화 성능 확인 : {acc}')
```
![image](https://github.com/user-attachments/assets/41588970-aac5-4284-aa76-cf6f02b74c26)

```
import pandas as pd
import numpy as np
import matplotlib as plt

# 피처명 읽기
# features.txt 파일에는 피쳐 이름 index와 피쳐명이 공백으로 분리되어 있음
# 이름 DataFrame 롣,
# \s+ => \s : white space (공백,\n,\r,\) 한자
# + : 한개 이상
feature_name_dir=pd.read_csv(
    './UCI HAR Dataset/data/features.txt'
    ,sep='\s+'
    ,header=None
    ,names=['column_index','column_name']
)

# 중복된 피쳐명 확인

feature_dup_df = feature_name_dir.groupby(
    'column_name'
).count()

def get_new_feature_name_df(old_feature_name_df):
    feature_dup_df = pd.DataFrame(
        data=old_feature_name_df.groupby('column_name').cumcount()
        ,columns=['dup_cnt']
    )
    feature_dup_df = feature_dup_df.reset_index()
    new_feature_name_df = pd.merge(old_feature_name_df.reset_index(), feature_dup_df, how='outer')
    new_feature_name_df['column_name'] = new_feature_name_df[['column_name', 'dup_cnt']].apply(
        lambda x : x[0]+'_'+str(x[1]) if x[1] >0 else x[0]
        ,axis=1
    )
    new_feature_name_df = new_feature_name_df.drop(['index'], axis=1)
    return new_feature_name_df

def get_human_dataset( ):
   
    # 각 데이터 파일들은 공백으로 분리되어 있으므로 read_csv에서 공백 문자를 sep으로 할당.
    feature_name_df = pd.read_csv('./UCI HAR Dataset/data/features.txt',sep='\s+',
                        header=None,names=['column_index','column_name'])
   
    # 중복된 피처명을 수정하는 get_new_feature_name_df()를 이용, 신규 피처명 DataFrame생성.
    new_feature_name_df = get_new_feature_name_df(feature_name_df)
   
    # DataFrame에 피처명을 컬럼으로 부여하기 위해 리스트 객체로 다시 변환
    feature_name = new_feature_name_df.iloc[:, 1].values.tolist()
   
    # 학습 피처 데이터 셋과 테스트 피처 데이터을 DataFrame으로 로딩. 컬럼명은 feature_name 적용
    X_train = pd.read_csv('./UCI HAR Dataset/data/train/X_train.txt',sep='\s+', names=feature_name )
    X_test = pd.read_csv('./UCI HAR Dataset/data//test/X_test.txt',sep='\s+', names=feature_name)
   
    # 학습 레이블과 테스트 레이블 데이터을 DataFrame으로 로딩하고 컬럼명은 action으로 부여
    y_train = pd.read_csv('./UCI HAR Dataset/data/train/y_train.txt',sep='\s+',header=None,names=['action'])
    y_test = pd.read_csv('./UCI HAR Dataset/data/test/y_test.txt',sep='\s+',header=None,names=['action'])
   
    # 로드된 학습/테스트용 DataFrame을 모두 반환
    return X_train, X_test, y_train, y_test

X_train, X_test, y_train, y_test = get_human_dataset()

# Decision Tree
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score # 정확도 점수

# 아래 코드를 반복 실행할 때마다 예측 결과가 동일하게 나오게 해야한다. : random_state
dt_clf=DecisionTreeClassifier(random_state=156)
dt_clf.fit(X_train,y_train)
pred=dt_clf.predict(X_test)
accuracy=accuracy_score(pred,y_test)
# print(f'결정 트리 예측 정확도 : {accuracy}')
# y_test['action'].value_counts()

# 튜닝 : 하이퍼파라미티
# print('Decision Tree 기본 하이퍼 파라미터 :\n',dt_clf.get_params())

# 하이퍼파라미티 튜닝 : GridSearchCV(), max_depth로 BEST 찾기
from sklearn.model_selection import GridSearchCV

grid_params={
    'max_depth':[2,4,6,8,10,12,16,20,24]
}

grid_cv=GridSearchCV(
    dt_clf # 모델 : 학습할 알고리즘
    ,param_grid=grid_params
    ,scoring='accuracy' #정확도로 평가
    ,cv=5
    ,n_jobs=-1
)

grid_cv.fit(X_train,y_train)

# 평균 정확도, 베스트 파라미터 확인
# print(f'최고 평균 정확도 : {grid_cv.best_score_}')
# print(f'베스트 파라미터 : {grid_cv.best_params_}')

params={
    'max_depth':[8,10,12,16,20,24]
    ,'min_samples_split':[12,14,16,18,20,22,24]
}

grid_cv_params=GridSearchCV(
    dt_clf
    ,param_grid=params
    ,scoring='accuracy'
    ,cv=5
    ,n_jobs=-1
)

grid_cv_params.fit(X_train,y_train)

# 평균 정확도, 베스트 파라미터 확인
# print(f'min_samples_split가 포함된 최고 평균 정확도 : {grid_cv_params.best_score_}')
# print(f'min_samples_split가 포함된 최적 하이퍼 파라미터 : {grid_cv_params.best_params_}')

# 일반화 성능 확인
best_df_clf=grid_cv_params.best_estimator_
pred1=best_df_clf.predict(X_test)
acc=accuracy_score(pred1,y_test)
# print(f'일반화 성능 확인 : {acc}')

# 일반화 성능 확인
best_dt_clf=grid_cv_params.best_estimator_
# print(best_dt_clf)
pred1=best_dt_clf.predict(X_test)
acc=accuracy_score(pred1,y_test)
# print(f'일반화 성능 확인 : {acc}')

# 피쳐 : 561개
# 피쳐 중요도 시각화
import seaborn as sns
import matplotlib.pyplot as plt

ftr_importance_values= best_dt_clf.feature_importances_
ftr_important=pd.Series(
    ftr_importance_values
    ,index=X_train.columns
)
ftr_important
```
![image](https://github.com/user-attachments/assets/edb49369-3b34-4217-8c24-79d07a02b0c7)

```
import pandas as pd
import numpy as np
import matplotlib as plt

# 피처명 읽기
# features.txt 파일에는 피쳐 이름 index와 피쳐명이 공백으로 분리되어 있음
# 이름 DataFrame 롣,
# \s+ => \s : white space (공백,\n,\r,\) 한자
# + : 한개 이상
feature_name_dir=pd.read_csv(
    './UCI HAR Dataset/data/features.txt'
    ,sep='\s+'
    ,header=None
    ,names=['column_index','column_name']
)

# 중복된 피쳐명 확인

feature_dup_df = feature_name_dir.groupby(
    'column_name'
).count()

def get_new_feature_name_df(old_feature_name_df):
    feature_dup_df = pd.DataFrame(
        data=old_feature_name_df.groupby('column_name').cumcount()
        ,columns=['dup_cnt']
    )
    feature_dup_df = feature_dup_df.reset_index()
    new_feature_name_df = pd.merge(old_feature_name_df.reset_index(), feature_dup_df, how='outer')
    new_feature_name_df['column_name'] = new_feature_name_df[['column_name', 'dup_cnt']].apply(
        lambda x : x[0]+'_'+str(x[1]) if x[1] >0 else x[0]
        ,axis=1
    )
    new_feature_name_df = new_feature_name_df.drop(['index'], axis=1)
    return new_feature_name_df

def get_human_dataset( ):
   
    # 각 데이터 파일들은 공백으로 분리되어 있으므로 read_csv에서 공백 문자를 sep으로 할당.
    feature_name_df = pd.read_csv('./UCI HAR Dataset/data/features.txt',sep='\s+',
                        header=None,names=['column_index','column_name'])
   
    # 중복된 피처명을 수정하는 get_new_feature_name_df()를 이용, 신규 피처명 DataFrame생성.
    new_feature_name_df = get_new_feature_name_df(feature_name_df)
   
    # DataFrame에 피처명을 컬럼으로 부여하기 위해 리스트 객체로 다시 변환
    feature_name = new_feature_name_df.iloc[:, 1].values.tolist()
   
    # 학습 피처 데이터 셋과 테스트 피처 데이터을 DataFrame으로 로딩. 컬럼명은 feature_name 적용
    X_train = pd.read_csv('./UCI HAR Dataset/data/train/X_train.txt',sep='\s+', names=feature_name )
    X_test = pd.read_csv('./UCI HAR Dataset/data//test/X_test.txt',sep='\s+', names=feature_name)
   
    # 학습 레이블과 테스트 레이블 데이터을 DataFrame으로 로딩하고 컬럼명은 action으로 부여
    y_train = pd.read_csv('./UCI HAR Dataset/data/train/y_train.txt',sep='\s+',header=None,names=['action'])
    y_test = pd.read_csv('./UCI HAR Dataset/data/test/y_test.txt',sep='\s+',header=None,names=['action'])
   
    # 로드된 학습/테스트용 DataFrame을 모두 반환
    return X_train, X_test, y_train, y_test

X_train, X_test, y_train, y_test = get_human_dataset()

# Decision Tree
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score # 정확도 점수

# 아래 코드를 반복 실행할 때마다 예측 결과가 동일하게 나오게 해야한다. : random_state
dt_clf=DecisionTreeClassifier(random_state=156)
dt_clf.fit(X_train,y_train)
pred=dt_clf.predict(X_test)
accuracy=accuracy_score(pred,y_test)
# print(f'결정 트리 예측 정확도 : {accuracy}')
# y_test['action'].value_counts()

# 튜닝 : 하이퍼파라미티
# print('Decision Tree 기본 하이퍼 파라미터 :\n',dt_clf.get_params())

# 하이퍼파라미티 튜닝 : GridSearchCV(), max_depth로 BEST 찾기
from sklearn.model_selection import GridSearchCV

grid_params={
    'max_depth':[2,4,6,8,10,12,16,20,24]
}

grid_cv=GridSearchCV(
    dt_clf # 모델 : 학습할 알고리즘
    ,param_grid=grid_params
    ,scoring='accuracy' #정확도로 평가
    ,cv=5
    ,n_jobs=-1
)

grid_cv.fit(X_train,y_train)

# 평균 정확도, 베스트 파라미터 확인
# print(f'최고 평균 정확도 : {grid_cv.best_score_}')
# print(f'베스트 파라미터 : {grid_cv.best_params_}')

params={
    'max_depth':[8,10,12,16,20,24]
    ,'min_samples_split':[12,14,16,18,20,22,24]
}

grid_cv_params=GridSearchCV(
    dt_clf
    ,param_grid=params
    ,scoring='accuracy'
    ,cv=5
    ,n_jobs=-1
)

grid_cv_params.fit(X_train,y_train)

# 평균 정확도, 베스트 파라미터 확인
# print(f'min_samples_split가 포함된 최고 평균 정확도 : {grid_cv_params.best_score_}')
# print(f'min_samples_split가 포함된 최적 하이퍼 파라미터 : {grid_cv_params.best_params_}')

# 일반화 성능 확인
best_df_clf=grid_cv_params.best_estimator_
pred1=best_df_clf.predict(X_test)
acc=accuracy_score(pred1,y_test)
# print(f'일반화 성능 확인 : {acc}')

# 일반화 성능 확인
best_dt_clf=grid_cv_params.best_estimator_
# print(best_dt_clf)
pred1=best_dt_clf.predict(X_test)
acc=accuracy_score(pred1,y_test)
# print(f'일반화 성능 확인 : {acc}')

# 피쳐 : 561개
# 피쳐 중요도 시각화
import seaborn as sns
import matplotlib.pyplot as plt

ftr_importance_values= best_dt_clf.feature_importances_
ftr_important=pd.Series(
    ftr_importance_values
    ,index=X_train.columns
)
ftr_important.sort_values(
    ascending=False
)
```
![image](https://github.com/user-attachments/assets/ec37da9d-7e32-455c-9815-c57438d93971)

```
import pandas as pd
import numpy as np
import matplotlib as plt

# 피처명 읽기
# features.txt 파일에는 피쳐 이름 index와 피쳐명이 공백으로 분리되어 있음
# 이름 DataFrame 롣,
# \s+ => \s : white space (공백,\n,\r,\) 한자
# + : 한개 이상
feature_name_dir=pd.read_csv(
    './UCI HAR Dataset/data/features.txt'
    ,sep='\s+'
    ,header=None
    ,names=['column_index','column_name']
)

# 중복된 피쳐명 확인

feature_dup_df = feature_name_dir.groupby(
    'column_name'
).count()

def get_new_feature_name_df(old_feature_name_df):
    feature_dup_df = pd.DataFrame(
        data=old_feature_name_df.groupby('column_name').cumcount()
        ,columns=['dup_cnt']
    )
    feature_dup_df = feature_dup_df.reset_index()
    new_feature_name_df = pd.merge(old_feature_name_df.reset_index(), feature_dup_df, how='outer')
    new_feature_name_df['column_name'] = new_feature_name_df[['column_name', 'dup_cnt']].apply(
        lambda x : x[0]+'_'+str(x[1]) if x[1] >0 else x[0]
        ,axis=1
    )
    new_feature_name_df = new_feature_name_df.drop(['index'], axis=1)
    return new_feature_name_df

def get_human_dataset( ):
   
    # 각 데이터 파일들은 공백으로 분리되어 있으므로 read_csv에서 공백 문자를 sep으로 할당.
    feature_name_df = pd.read_csv('./UCI HAR Dataset/data/features.txt',sep='\s+',
                        header=None,names=['column_index','column_name'])
   
    # 중복된 피처명을 수정하는 get_new_feature_name_df()를 이용, 신규 피처명 DataFrame생성.
    new_feature_name_df = get_new_feature_name_df(feature_name_df)
   
    # DataFrame에 피처명을 컬럼으로 부여하기 위해 리스트 객체로 다시 변환
    feature_name = new_feature_name_df.iloc[:, 1].values.tolist()
   
    # 학습 피처 데이터 셋과 테스트 피처 데이터을 DataFrame으로 로딩. 컬럼명은 feature_name 적용
    X_train = pd.read_csv('./UCI HAR Dataset/data/train/X_train.txt',sep='\s+', names=feature_name )
    X_test = pd.read_csv('./UCI HAR Dataset/data//test/X_test.txt',sep='\s+', names=feature_name)
   
    # 학습 레이블과 테스트 레이블 데이터을 DataFrame으로 로딩하고 컬럼명은 action으로 부여
    y_train = pd.read_csv('./UCI HAR Dataset/data/train/y_train.txt',sep='\s+',header=None,names=['action'])
    y_test = pd.read_csv('./UCI HAR Dataset/data/test/y_test.txt',sep='\s+',header=None,names=['action'])
   
    # 로드된 학습/테스트용 DataFrame을 모두 반환
    return X_train, X_test, y_train, y_test

X_train, X_test, y_train, y_test = get_human_dataset()

# Decision Tree
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score # 정확도 점수

# 아래 코드를 반복 실행할 때마다 예측 결과가 동일하게 나오게 해야한다. : random_state
dt_clf=DecisionTreeClassifier(random_state=156)
dt_clf.fit(X_train,y_train)
pred=dt_clf.predict(X_test)
accuracy=accuracy_score(pred,y_test)
# print(f'결정 트리 예측 정확도 : {accuracy}')
# y_test['action'].value_counts()

# 튜닝 : 하이퍼파라미티
# print('Decision Tree 기본 하이퍼 파라미터 :\n',dt_clf.get_params())

# 하이퍼파라미티 튜닝 : GridSearchCV(), max_depth로 BEST 찾기
from sklearn.model_selection import GridSearchCV

grid_params={
    'max_depth':[2,4,6,8,10,12,16,20,24]
}

grid_cv=GridSearchCV(
    dt_clf # 모델 : 학습할 알고리즘
    ,param_grid=grid_params
    ,scoring='accuracy' #정확도로 평가
    ,cv=5
    ,n_jobs=-1
)

grid_cv.fit(X_train,y_train)

# 평균 정확도, 베스트 파라미터 확인
# print(f'최고 평균 정확도 : {grid_cv.best_score_}')
# print(f'베스트 파라미터 : {grid_cv.best_params_}')

params={
    'max_depth':[8,10,12,16,20,24]
    ,'min_samples_split':[12,14,16,18,20,22,24]
}

grid_cv_params=GridSearchCV(
    dt_clf
    ,param_grid=params
    ,scoring='accuracy'
    ,cv=5
    ,n_jobs=-1
)

grid_cv_params.fit(X_train,y_train)

# 평균 정확도, 베스트 파라미터 확인
# print(f'min_samples_split가 포함된 최고 평균 정확도 : {grid_cv_params.best_score_}')
# print(f'min_samples_split가 포함된 최적 하이퍼 파라미터 : {grid_cv_params.best_params_}')

# 일반화 성능 확인
best_df_clf=grid_cv_params.best_estimator_
pred1=best_df_clf.predict(X_test)
acc=accuracy_score(pred1,y_test)
# print(f'일반화 성능 확인 : {acc}')

# 일반화 성능 확인
best_dt_clf=grid_cv_params.best_estimator_
# print(best_dt_clf)
pred1=best_dt_clf.predict(X_test)
acc=accuracy_score(pred1,y_test)
# print(f'일반화 성능 확인 : {acc}')

# 피쳐 : 561개
# 피쳐 중요도 시각화
import seaborn as sns
import matplotlib.pyplot as plt

ftr_importance_values= best_dt_clf.feature_importances_
ftr_important=pd.Series(
    ftr_importance_values
    ,index=X_train.columns
)
ftr_top20=ftr_important.sort_values(
    ascending=False
)[:20]

sns.barplot(x=ftr_top20,y=ftr_top20.index)
plt.show()
```
![image](https://github.com/user-attachments/assets/3c06b20b-7323-46ab-8e2b-8ecc15165838)

```
import pandas as pd
import numpy as np
import matplotlib as plt

# 피처명 읽기
# features.txt 파일에는 피쳐 이름 index와 피쳐명이 공백으로 분리되어 있음
# 이름 DataFrame 롣,
# \s+ => \s : white space (공백,\n,\r,\) 한자
# + : 한개 이상
feature_name_dir=pd.read_csv(
    './UCI HAR Dataset/data/features.txt'
    ,sep='\s+'
    ,header=None
    ,names=['column_index','column_name']
)

# 중복된 피쳐명 확인

feature_dup_df = feature_name_dir.groupby(
    'column_name'
).count()

def get_new_feature_name_df(old_feature_name_df):
    feature_dup_df = pd.DataFrame(
        data=old_feature_name_df.groupby('column_name').cumcount()
        ,columns=['dup_cnt']
    )
    feature_dup_df = feature_dup_df.reset_index()
    new_feature_name_df = pd.merge(old_feature_name_df.reset_index(), feature_dup_df, how='outer')
    new_feature_name_df['column_name'] = new_feature_name_df[['column_name', 'dup_cnt']].apply(
        lambda x : x[0]+'_'+str(x[1]) if x[1] >0 else x[0]
        ,axis=1
    )
    new_feature_name_df = new_feature_name_df.drop(['index'], axis=1)
    return new_feature_name_df

def get_human_dataset( ):
   
    # 각 데이터 파일들은 공백으로 분리되어 있으므로 read_csv에서 공백 문자를 sep으로 할당.
    feature_name_df = pd.read_csv('./UCI HAR Dataset/data/features.txt',sep='\s+',
                        header=None,names=['column_index','column_name'])
   
    # 중복된 피처명을 수정하는 get_new_feature_name_df()를 이용, 신규 피처명 DataFrame생성.
    new_feature_name_df = get_new_feature_name_df(feature_name_df)
   
    # DataFrame에 피처명을 컬럼으로 부여하기 위해 리스트 객체로 다시 변환
    feature_name = new_feature_name_df.iloc[:, 1].values.tolist()
   
    # 학습 피처 데이터 셋과 테스트 피처 데이터을 DataFrame으로 로딩. 컬럼명은 feature_name 적용
    X_train = pd.read_csv('./UCI HAR Dataset/data/train/X_train.txt',sep='\s+', names=feature_name )
    X_test = pd.read_csv('./UCI HAR Dataset/data//test/X_test.txt',sep='\s+', names=feature_name)
   
    # 학습 레이블과 테스트 레이블 데이터을 DataFrame으로 로딩하고 컬럼명은 action으로 부여
    y_train = pd.read_csv('./UCI HAR Dataset/data/train/y_train.txt',sep='\s+',header=None,names=['action'])
    y_test = pd.read_csv('./UCI HAR Dataset/data/test/y_test.txt',sep='\s+',header=None,names=['action'])
   
    # 로드된 학습/테스트용 DataFrame을 모두 반환
    return X_train, X_test, y_train, y_test

X_train, X_test, y_train, y_test = get_human_dataset()

# Decision Tree
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score # 정확도 점수

# 아래 코드를 반복 실행할 때마다 예측 결과가 동일하게 나오게 해야한다. : random_state
dt_clf=DecisionTreeClassifier(random_state=156)
dt_clf.fit(X_train,y_train)
pred=dt_clf.predict(X_test)
accuracy=accuracy_score(pred,y_test)
# print(f'결정 트리 예측 정확도 : {accuracy}')
# y_test['action'].value_counts()

# 튜닝 : 하이퍼파라미티
# print('Decision Tree 기본 하이퍼 파라미터 :\n',dt_clf.get_params())

# 하이퍼파라미티 튜닝 : GridSearchCV(), max_depth로 BEST 찾기
from sklearn.model_selection import GridSearchCV

grid_params={
    'max_depth':[2,4,6,8,10,12,16,20,24]
}

grid_cv=GridSearchCV(
    dt_clf # 모델 : 학습할 알고리즘
    ,param_grid=grid_params
    ,scoring='accuracy' #정확도로 평가
    ,cv=5
    ,n_jobs=-1
)

grid_cv.fit(X_train,y_train)

# 평균 정확도, 베스트 파라미터 확인
# print(f'최고 평균 정확도 : {grid_cv.best_score_}')
# print(f'베스트 파라미터 : {grid_cv.best_params_}')

params={
    'max_depth':[8,10,12,16,20,24]
    ,'min_samples_split':[12,14,16,18,20,22,24]
}

grid_cv_params=GridSearchCV(
    dt_clf
    ,param_grid=params
    ,scoring='accuracy'
    ,cv=5
    ,n_jobs=-1
)

grid_cv_params.fit(X_train,y_train)

# 평균 정확도, 베스트 파라미터 확인
# print(f'min_samples_split가 포함된 최고 평균 정확도 : {grid_cv_params.best_score_}')
# print(f'min_samples_split가 포함된 최적 하이퍼 파라미터 : {grid_cv_params.best_params_}')

# 일반화 성능 확인
best_df_clf=grid_cv_params.best_estimator_
pred1=best_df_clf.predict(X_test)
acc=accuracy_score(pred1,y_test)
# print(f'일반화 성능 확인 : {acc}')

# 일반화 성능 확인
best_dt_clf=grid_cv_params.best_estimator_
# print(best_dt_clf)
pred1=best_dt_clf.predict(X_test)
acc=accuracy_score(pred1,y_test)
# print(f'일반화 성능 확인 : {acc}')

# 피쳐 : 561개
# 피쳐 중요도 시각화
import seaborn as sns
import matplotlib.pyplot as plt

ftr_importance_values= best_dt_clf.feature_importances_
ftr_important=pd.Series(
    ftr_importance_values
    ,index=X_train.columns
)
ftr_top20=ftr_important.sort_values(
    ascending=False
)[:20]

# sns.barplot(x=ftr_top20,y=ftr_top20.index)
# plt.show()
top20_col_names=ftr_top20.index.tolist()
# X_train[top20_col_names].head()
# X_test[top20_col_names].head()

X_train_top20=X_train[top20_col_names]
X_test_top20=X_test[top20_col_names]


print(f'X_train의 shape : {X_train.shape}')
print(f'X_train_top20의 shape : {X_train_top20.shape}')

print(f'X_test의 shape : {X_test.shape}')
print(f'X_test_top20의 shape : {X_test_top20.shape}')
```
![image](https://github.com/user-attachments/assets/cc46b490-74a4-4456-b30c-f5f19180994c)

```
import pandas as pd
import numpy as np
import matplotlib as plt

# 피처명 읽기
# features.txt 파일에는 피쳐 이름 index와 피쳐명이 공백으로 분리되어 있음
# 이름 DataFrame 롣,
# \s+ => \s : white space (공백,\n,\r,\) 한자
# + : 한개 이상
feature_name_dir=pd.read_csv(
    './UCI HAR Dataset/data/features.txt'
    ,sep='\s+'
    ,header=None
    ,names=['column_index','column_name']
)

# 중복된 피쳐명 확인

feature_dup_df = feature_name_dir.groupby(
    'column_name'
).count()

def get_new_feature_name_df(old_feature_name_df):
    feature_dup_df = pd.DataFrame(
        data=old_feature_name_df.groupby('column_name').cumcount()
        ,columns=['dup_cnt']
    )
    feature_dup_df = feature_dup_df.reset_index()
    new_feature_name_df = pd.merge(old_feature_name_df.reset_index(), feature_dup_df, how='outer')
    new_feature_name_df['column_name'] = new_feature_name_df[['column_name', 'dup_cnt']].apply(
        lambda x : x[0]+'_'+str(x[1]) if x[1] >0 else x[0]
        ,axis=1
    )
    new_feature_name_df = new_feature_name_df.drop(['index'], axis=1)
    return new_feature_name_df

def get_human_dataset( ):
   
    # 각 데이터 파일들은 공백으로 분리되어 있으므로 read_csv에서 공백 문자를 sep으로 할당.
    feature_name_df = pd.read_csv('./UCI HAR Dataset/data/features.txt',sep='\s+',
                        header=None,names=['column_index','column_name'])
   
    # 중복된 피처명을 수정하는 get_new_feature_name_df()를 이용, 신규 피처명 DataFrame생성.
    new_feature_name_df = get_new_feature_name_df(feature_name_df)
   
    # DataFrame에 피처명을 컬럼으로 부여하기 위해 리스트 객체로 다시 변환
    feature_name = new_feature_name_df.iloc[:, 1].values.tolist()
   
    # 학습 피처 데이터 셋과 테스트 피처 데이터을 DataFrame으로 로딩. 컬럼명은 feature_name 적용
    X_train = pd.read_csv('./UCI HAR Dataset/data/train/X_train.txt',sep='\s+', names=feature_name )
    X_test = pd.read_csv('./UCI HAR Dataset/data//test/X_test.txt',sep='\s+', names=feature_name)
   
    # 학습 레이블과 테스트 레이블 데이터을 DataFrame으로 로딩하고 컬럼명은 action으로 부여
    y_train = pd.read_csv('./UCI HAR Dataset/data/train/y_train.txt',sep='\s+',header=None,names=['action'])
    y_test = pd.read_csv('./UCI HAR Dataset/data/test/y_test.txt',sep='\s+',header=None,names=['action'])
   
    # 로드된 학습/테스트용 DataFrame을 모두 반환
    return X_train, X_test, y_train, y_test

X_train, X_test, y_train, y_test = get_human_dataset()

# Decision Tree
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score # 정확도 점수

# 아래 코드를 반복 실행할 때마다 예측 결과가 동일하게 나오게 해야한다. : random_state
dt_clf=DecisionTreeClassifier(random_state=156)
dt_clf.fit(X_train,y_train)
pred=dt_clf.predict(X_test)
accuracy=accuracy_score(pred,y_test)
# print(f'결정 트리 예측 정확도 : {accuracy}')
# y_test['action'].value_counts()

# 튜닝 : 하이퍼파라미티
# print('Decision Tree 기본 하이퍼 파라미터 :\n',dt_clf.get_params())

# 하이퍼파라미티 튜닝 : GridSearchCV(), max_depth로 BEST 찾기
from sklearn.model_selection import GridSearchCV

grid_params={
    'max_depth':[2,4,6,8,10,12,16,20,24]
}

grid_cv=GridSearchCV(
    dt_clf # 모델 : 학습할 알고리즘
    ,param_grid=grid_params
    ,scoring='accuracy' #정확도로 평가
    ,cv=5
    ,n_jobs=-1
)

grid_cv.fit(X_train,y_train)

# 평균 정확도, 베스트 파라미터 확인
# print(f'최고 평균 정확도 : {grid_cv.best_score_}')
# print(f'베스트 파라미터 : {grid_cv.best_params_}')

params={
    'max_depth':[8,10,12,16,20,24]
    ,'min_samples_split':[12,14,16,18,20,22,24]
}

grid_cv_params=GridSearchCV(
    dt_clf
    ,param_grid=params
    ,scoring='accuracy'
    ,cv=5
    ,n_jobs=-1
)

grid_cv_params.fit(X_train,y_train)

# 평균 정확도, 베스트 파라미터 확인
# print(f'min_samples_split가 포함된 최고 평균 정확도 : {grid_cv_params.best_score_}')
# print(f'min_samples_split가 포함된 최적 하이퍼 파라미터 : {grid_cv_params.best_params_}')

# 일반화 성능 확인
best_df_clf=grid_cv_params.best_estimator_
pred1=best_df_clf.predict(X_test)
acc=accuracy_score(pred1,y_test)
# print(f'일반화 성능 확인 : {acc}')

# 일반화 성능 확인
best_dt_clf=grid_cv_params.best_estimator_
# print(best_dt_clf)
pred1=best_dt_clf.predict(X_test)
acc=accuracy_score(pred1,y_test)
# print(f'일반화 성능 확인 : {acc}')

# 피쳐 : 561개
# 피쳐 중요도 시각화
import seaborn as sns
import matplotlib.pyplot as plt

ftr_importance_values= best_dt_clf.feature_importances_
ftr_important=pd.Series(
    ftr_importance_values
    ,index=X_train.columns
)
ftr_top20=ftr_important.sort_values(
    ascending=False
)[:20]

#sns.barplot(x=ftr_top20,y=ftr_top20.index)
#plt.show()

params_top_20={
    'max_depth':[8,10,12,16,20,24]
    ,'min_samples_split':[12,14,16,18,20,22,24]
}

grid_cv_params_top_20=GridSearchCV(
    dt_clf
    ,param_grid=params_top_20
    ,scoring='accuracy'
    ,cv=5
    ,n_jobs=-1
)

grid_cv_params_top_20.fit(X_train,y_train)

# 평균 정확도, 베스트 파라미터 확인
print(f'min_samples_split가 포함된 최고 평균 정확도 : {grid_cv_params_top_20.best_score_}')
print(f'min_samples_split가 포함된 최적 하이퍼 파라미터 : {grid_cv_params_top_20.best_params_}')

```
![image](https://github.com/user-attachments/assets/94dd877d-4986-4bbf-89ec-94abd0a66660)

---
## 배깅

```
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

import warnings
warnings.filterwarnings('ignore')

def get_new_feature_name_df(old_feature_name_df):
    feature_dup_df = pd.DataFrame(data=old_feature_name_df.groupby('column_name').cumcount(),
                                  columns=['dup_cnt'])
    feature_dup_df = feature_dup_df.reset_index()
    new_feature_name_df = pd.merge(old_feature_name_df.reset_index(), feature_dup_df, how='outer')
    new_feature_name_df['column_name'] = new_feature_name_df[['column_name', 'dup_cnt']].apply(lambda x : x[0]+'_'+str(x[1])
                                                                                         if x[1] >0 else x[0] ,  axis=1)
    new_feature_name_df = new_feature_name_df.drop(['index'], axis=1)
    return new_feature_name_df

def get_human_dataset( ):
   
    # 각 데이터 파일들은 공백으로 분리되어 있으므로 read_csv에서 공백 문자를 sep으로 할당.
    feature_name_df = pd.read_csv('./UCI HAR Dataset/data/features.txt',sep='\s+',
                        header=None,names=['column_index','column_name'])
   
    # 중복된 피처명을 수정하는 get_new_feature_name_df()를 이용, 신규 피처명 DataFrame생성.
    new_feature_name_df = get_new_feature_name_df(feature_name_df)
   
    # DataFrame에 피처명을 컬럼으로 부여하기 위해 리스트 객체로 다시 변환
    feature_name = new_feature_name_df.iloc[:, 1].values.tolist()
   
    # 학습 피처 데이터 셋과 테스트 피처 데이터을 DataFrame으로 로딩. 컬럼명은 feature_name 적용
    X_train = pd.read_csv('./UCI HAR Dataset/data/train/X_train.txt',sep='\s+', names=feature_name )
    X_test = pd.read_csv('./UCI HAR Dataset/data//test/X_test.txt',sep='\s+', names=feature_name)
   
    # 학습 레이블과 테스트 레이블 데이터을 DataFrame으로 로딩하고 컬럼명은 action으로 부여
    y_train = pd.read_csv('./UCI HAR Dataset/data/train/y_train.txt',sep='\s+',header=None,names=['action'])
    y_test = pd.read_csv('./UCI HAR Dataset/data/test/y_test.txt',sep='\s+',header=None,names=['action'])
   
    # 로드된 학습/테스트용 DataFrame을 모두 반환
    return X_train, X_test, y_train, y_test

X_train, X_test, y_train, y_test = get_human_dataset()

# Randomforest 일반화 성능 확인
from sklearn.ensemble import RandomForestClassifier

rf_clf=RandomForestClassifier(random_state=0,max_depth=8)

# 학습
rf_clf.fit(X_train,y_train)

# 테스트데이터(new data)에 대한 예측값 추출
pred=rf_clf.predict(X_test)

# 테스트데이터에 대한 예측값의 정확도 추출
acc=accuracy_score(pred,y_test)

print(f'랜덤 포레스트의 정확도 : {acc}')
```
![image](https://github.com/user-attachments/assets/d09faa77-32b6-4146-9f1f-f8e7bc136a38)

```
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

import warnings
warnings.filterwarnings('ignore')

def get_new_feature_name_df(old_feature_name_df):
    feature_dup_df = pd.DataFrame(data=old_feature_name_df.groupby('column_name').cumcount(),
                                  columns=['dup_cnt'])
    feature_dup_df = feature_dup_df.reset_index()
    new_feature_name_df = pd.merge(old_feature_name_df.reset_index(), feature_dup_df, how='outer')
    new_feature_name_df['column_name'] = new_feature_name_df[['column_name', 'dup_cnt']].apply(lambda x : x[0]+'_'+str(x[1])
                                                                                         if x[1] >0 else x[0] ,  axis=1)
    new_feature_name_df = new_feature_name_df.drop(['index'], axis=1)
    return new_feature_name_df

def get_human_dataset( ):
   
    # 각 데이터 파일들은 공백으로 분리되어 있으므로 read_csv에서 공백 문자를 sep으로 할당.
    feature_name_df = pd.read_csv('./UCI HAR Dataset/data/features.txt',sep='\s+',
                        header=None,names=['column_index','column_name'])
   
    # 중복된 피처명을 수정하는 get_new_feature_name_df()를 이용, 신규 피처명 DataFrame생성.
    new_feature_name_df = get_new_feature_name_df(feature_name_df)
   
    # DataFrame에 피처명을 컬럼으로 부여하기 위해 리스트 객체로 다시 변환
    feature_name = new_feature_name_df.iloc[:, 1].values.tolist()
   
    # 학습 피처 데이터 셋과 테스트 피처 데이터을 DataFrame으로 로딩. 컬럼명은 feature_name 적용
    X_train = pd.read_csv('./UCI HAR Dataset/data/train/X_train.txt',sep='\s+', names=feature_name )
    X_test = pd.read_csv('./UCI HAR Dataset/data//test/X_test.txt',sep='\s+', names=feature_name)
   
    # 학습 레이블과 테스트 레이블 데이터을 DataFrame으로 로딩하고 컬럼명은 action으로 부여
    y_train = pd.read_csv('./UCI HAR Dataset/data/train/y_train.txt',sep='\s+',header=None,names=['action'])
    y_test = pd.read_csv('./UCI HAR Dataset/data/test/y_test.txt',sep='\s+',header=None,names=['action'])
   
    # 로드된 학습/테스트용 DataFrame을 모두 반환
    return X_train, X_test, y_train, y_test

X_train, X_test, y_train, y_test = get_human_dataset()

# Randomforest 일반화 성능 확인
from sklearn.ensemble import RandomForestClassifier

rf_clf=RandomForestClassifier(random_state=0,max_depth=8)

# 학습
rf_clf.fit(X_train,y_train)

# 테스트데이터(new data)에 대한 예측값 추출
pred=rf_clf.predict(X_test)

# 테스트데이터에 대한 예측값의 정확도 추출
acc=accuracy_score(pred,y_test)

# print(f'랜덤 포레스트의 정확도 : {acc:.4f}')

# GridSearchCV
from sklearn.model_selection import GridSearchCV
params={
    'n_estimators':[100,500]
    ,'max_depth':[6,8,10,12]
    ,'min_samples_leaf':[8,12,16,18]
    ,'min_samples_split':[8,16,20]
}

rf_clf_par=RandomForestClassifier(
    random_state=0
    ,n_jobs=-1
)

grid_cv=GridSearchCV(
    rf_clf_par
    ,param_grid=params
    ,cv=10
    ,n_jobs=-1
)

grid_cv.fit(X_train,y_train)

print(f'베스트 정확도 : {grid_cv.best_score_}')
print(f'최적의 파라미터 :\n{grid_cv.best_params_}')
print(f'grid_cv.best_estimator_ :\n{grid_cv.best_estimator_}')
```
![image](https://github.com/user-attachments/assets/26f8e4c1-53f5-4fee-932f-e75f07c416d4)

```
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

import warnings
warnings.filterwarnings('ignore')

def get_new_feature_name_df(old_feature_name_df):
    feature_dup_df = pd.DataFrame(data=old_feature_name_df.groupby('column_name').cumcount(),
                                  columns=['dup_cnt'])
    feature_dup_df = feature_dup_df.reset_index()
    new_feature_name_df = pd.merge(old_feature_name_df.reset_index(), feature_dup_df, how='outer')
    new_feature_name_df['column_name'] = new_feature_name_df[['column_name', 'dup_cnt']].apply(lambda x : x[0]+'_'+str(x[1])
                                                                                         if x[1] >0 else x[0] ,  axis=1)
    new_feature_name_df = new_feature_name_df.drop(['index'], axis=1)
    return new_feature_name_df

def get_human_dataset( ):
   
    # 각 데이터 파일들은 공백으로 분리되어 있으므로 read_csv에서 공백 문자를 sep으로 할당.
    feature_name_df = pd.read_csv('./UCI HAR Dataset/data/features.txt',sep='\s+',
                        header=None,names=['column_index','column_name'])
   
    # 중복된 피처명을 수정하는 get_new_feature_name_df()를 이용, 신규 피처명 DataFrame생성.
    new_feature_name_df = get_new_feature_name_df(feature_name_df)
   
    # DataFrame에 피처명을 컬럼으로 부여하기 위해 리스트 객체로 다시 변환
    feature_name = new_feature_name_df.iloc[:, 1].values.tolist()
   
    # 학습 피처 데이터 셋과 테스트 피처 데이터을 DataFrame으로 로딩. 컬럼명은 feature_name 적용
    X_train = pd.read_csv('./UCI HAR Dataset/data/train/X_train.txt',sep='\s+', names=feature_name )
    X_test = pd.read_csv('./UCI HAR Dataset/data//test/X_test.txt',sep='\s+', names=feature_name)
   
    # 학습 레이블과 테스트 레이블 데이터을 DataFrame으로 로딩하고 컬럼명은 action으로 부여
    y_train = pd.read_csv('./UCI HAR Dataset/data/train/y_train.txt',sep='\s+',header=None,names=['action'])
    y_test = pd.read_csv('./UCI HAR Dataset/data/test/y_test.txt',sep='\s+',header=None,names=['action'])
   
    # 로드된 학습/테스트용 DataFrame을 모두 반환
    return X_train, X_test, y_train, y_test

X_train, X_test, y_train, y_test = get_human_dataset()

# Randomforest 일반화 성능 확인
from sklearn.ensemble import RandomForestClassifier

rf_clf=RandomForestClassifier(random_state=0,max_depth=8)

# 학습
rf_clf.fit(X_train,y_train)

# 테스트데이터(new data)에 대한 예측값 추출
pred=rf_clf.predict(X_test)

# 테스트데이터에 대한 예측값의 정확도 추출
acc=accuracy_score(pred,y_test)

# print(f'랜덤 포레스트의 정확도 : {acc:.4f}')

print('='*100)
print('GridSearchCV 시작')
# GridSearchCV
from sklearn.model_selection import GridSearchCV
params={
    'n_estimators':[100,500]
    ,'max_depth':[6,8,10,12]
    ,'min_samples_leaf':[8,12,18]
    ,'min_samples_split':[8,16,20]
}

rf_clf_par=RandomForestClassifier(
    random_state=0
    ,n_jobs=-1
)

grid_cv=GridSearchCV(
    rf_clf_par
    ,param_grid=params
    ,cv=5
    ,n_jobs=-1
)

grid_cv.fit(X_train,y_train)

# print(f'베스트 정확도 : {grid_cv.best_score_}')
# print(f'최적의 파라미터 :\n{grid_cv.best_params_}')
# print(f'grid_cv.best_estimator_ :\n{grid_cv.best_estimator_}')

from sklearn.ensemble import RandomForestClassifier

rf_clf1=RandomForestClassifier(
    n_estimators=500
    ,min_samples_leaf=12
    ,min_samples_split=8
    ,max_depth=12
    ,random_state=0
    ,n_jobs=-1
)
rf_clf1.fit(X_train,y_train)

pred_rf=rf_clf1.predict(X_test)

accuracy_score(y_test,pred_rf)
```
![image](https://github.com/user-attachments/assets/97331d60-e5b0-4bd6-93f7-e50dcb0b370a)

---
## GBM

```
import numpy as np
import pandas as pd
import time

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

import warnings
warnings.filterwarnings('ignore')

def get_new_feature_name_df(old_feature_name_df):
    feature_dup_df = pd.DataFrame(data=old_feature_name_df.groupby('column_name').cumcount(),
                                  columns=['dup_cnt'])
    feature_dup_df = feature_dup_df.reset_index()
    new_feature_name_df = pd.merge(old_feature_name_df.reset_index(), feature_dup_df, how='outer')
    new_feature_name_df['column_name'] = new_feature_name_df[['column_name', 'dup_cnt']].apply(lambda x : x[0]+'_'+str(x[1])
                                                                                         if x[1] >0 else x[0] ,  axis=1)
    new_feature_name_df = new_feature_name_df.drop(['index'], axis=1)
    return new_feature_name_df

def get_human_dataset( ):
   
    # 각 데이터 파일들은 공백으로 분리되어 있으므로 read_csv에서 공백 문자를 sep으로 할당.
    feature_name_df = pd.read_csv('./UCI HAR Dataset/data/features.txt',sep='\s+',
                        header=None,names=['column_index','column_name'])
   
    # 중복된 피처명을 수정하는 get_new_feature_name_df()를 이용, 신규 피처명 DataFrame생성.
    new_feature_name_df = get_new_feature_name_df(feature_name_df)
   
    # DataFrame에 피처명을 컬럼으로 부여하기 위해 리스트 객체로 다시 변환
    feature_name = new_feature_name_df.iloc[:, 1].values.tolist()
   
    # 학습 피처 데이터 셋과 테스트 피처 데이터을 DataFrame으로 로딩. 컬럼명은 feature_name 적용
    X_train = pd.read_csv('./UCI HAR Dataset/data/train/X_train.txt',sep='\s+', names=feature_name )
    X_test = pd.read_csv('./UCI HAR Dataset/data//test/X_test.txt',sep='\s+', names=feature_name)
   
    # 학습 레이블과 테스트 레이블 데이터을 DataFrame으로 로딩하고 컬럼명은 action으로 부여
    y_train = pd.read_csv('./UCI HAR Dataset/data/train/y_train.txt',sep='\s+',header=None,names=['action'])
    y_test = pd.read_csv('./UCI HAR Dataset/data/test/y_test.txt',sep='\s+',header=None,names=['action'])
   
    # 로드된 학습/테스트용 DataFrame을 모두 반환
    return X_train, X_test, y_train, y_test

X_train, X_test, y_train, y_test = get_human_dataset()

from sklearn.ensemble import GradientBoostingClassifier

# GBM 수행 시간 측정을 위한 시작 시간 설정
start_time=time.time()

# 나머지 파라미터들은 기본값을 사용
gb_clf=GradientBoostingClassifier(random_state=0)
gb_clf.fit(X_train,y_train)
gb_pred=gb_clf.predict(X_test)
gb_accuracy=accuracy_score(y_test,gb_pred)

print(f'GBM 정확도 : {gb_accuracy}')
print(f'GBM 수행시간 : {time.time()-start_time}')
```
![image](https://github.com/user-attachments/assets/95d0a1d9-e212-4a90-b0b4-435600f65301)


