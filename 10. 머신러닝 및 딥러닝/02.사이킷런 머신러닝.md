# 사이킷런 머신러닝

## 1. 트리 계열 (Tree-based Models)

트리 계열 알고리즘은 데이터의 특성에 따라 의사결정 트리(Decision Tree)를 구성하여 예측을 수행합니다. 주로 비선형 관계를 잘 처리할 수 있습니다.

- **Decision Tree Classifier (의사결정 트리 분류기)**  
  의사결정 트리는 데이터를 분할하여 분류를 하는 모델입니다. 각 분기점에서 데이터의 특성을 기준으로 두 가지 이상의 범주로 분할합니다. 직관적이고 해석이 쉬운 장점이 있지만, 과적합에 취약할 수 있습니다.

- **Random Forest Classifier (랜덤 포레스트)**  
  여러 개의 의사결정 트리를 생성하고, 그 예측 결과를 종합하여 최종 예측을 도출합니다. 과적합을 줄이고 예측 성능을 향상시킬 수 있습니다.

- **Gradient Boosting Classifier (그라디언트 부스팅)**  
  약한 예측기들을 결합하여 강한 예측기를 만드는 방식입니다. 각 트리가 이전 트리의 오차를 보정하면서 학습합니다.

## 2. 서포트 벡터 머신 (Support Vector Machines, SVM)

서포트 벡터 머신은 데이터의 분포를 고려하여 최적의 결정 경계를 찾는 방식으로 분류를 수행합니다. 주로 고차원 데이터에서 뛰어난 성능을 발휘합니다.

- **SVC (Support Vector Classifier)**  
  커널 트릭(kernel trick)을 사용하여 비선형 데이터를 고차원 공간으로 변환한 후 최적의 분리 경계를 찾습니다.

- **SVR (Support Vector Regression)**  
  분류가 아닌 회귀 문제를 다룰 때 사용됩니다. 데이터를 고차원 공간으로 변환한 후 회귀를 수행하여 예측값을 도출합니다.

## 3. 로지스틱 회귀 (Logistic Regression)

로지스틱 회귀는 선형 회귀 모델을 기반으로 하며, 결과가 특정 범주에 속할 확률을 예측합니다. 주로 이진 분류 문제에 많이 사용됩니다.

- **Logistic Regression (로지스틱 회귀)**  
  입력값에 대해 로지스틱 함수(sigmoid function)를 사용하여 확률값을 출력하고, 이를 기준으로 분류합니다.

- **다중 클래스 분류 (Multinomial Logistic Regression)**  
  로지스틱 회귀는 기본적으로 이진 분류지만, `softmax` 함수를 사용하여 다수의 클래스를 처리할 수 있습니다. `multi_class='ovr'` (One-vs-Rest) 또는 `multi_class='multinomial'`을 설정하여 다중 클래스 분류를 수행합니다.

## 4. K-최근접 이웃 (K-Nearest Neighbors, KNN)

KNN은 분류할 데이터의 가장 가까운 K개의 이웃을 찾아서, 그 이웃들이 속한 클래스에 따라 예측을 수행하는 모델입니다.

- **K-Nearest Neighbors (KNN)**  
  주어진 데이터와 가장 가까운 K개의 데이터를 찾고, 그들의 클래스에서 다수결을 통해 분류합니다.

## 5. 나이브 베이즈 (Naive Bayes)

나이브 베이즈는 베이즈 이론을 기반으로 한 분류 모델입니다. 각 특징이 독립적이라고 가정하고, 조건부 확률을 계산하여 분류합니다.

- **GaussianNB (가우시안 나이브 베이즈)**  
  데이터가 연속형일 때 사용되며, 각 클래스에 대해 특성 값이 가우시안 분포를 따른다고 가정합니다.

- **MultinomialNB (다항 나이브 베이즈)**  
  주로 텍스트 분류에서 사용되며, 각 클래스에 대해 특정 특성이 다항 분포를 따른다고 가정합니다.

## 6. K-평균 군집화 (K-Means Clustering)

K-Means는 분류가 아닌 비지도 학습 알고리즘입니다. 데이터들을 군집화하여 군집의 중심을 찾고, 그 중심을 기준으로 데이터를 그룹화합니다.

- **KMeans (K-평균 군집화)**  
  주어진 K개의 군집에 데이터를 분할합니다. 각 군집은 군집의 중심(centroid)을 기준으로 데이터를 그룹화합니다.

---

## 혼동 행렬 (Confusion Matrix)

혼동 행렬(Confusion Matrix)은 분류 모델의 성능을 평가하는 데 사용되는 도구로, 예측한 결과와 실제 값을 비교하여 모델의 정확도를 평가합니다. 혼동 행렬은 주로 이진 분류 문제에서 사용되지만, 다중 클래스 분류에서도 적용할 수 있습니다.

### 혼동 행렬의 구성 요소

이진 분류 문제를 예로 들어 혼동 행렬의 구성 요소를 설명하겠습니다. 이진 분류에서는 두 클래스(예: 긍정 클래스와 부정 클래스)가 있습니다.

혼동 행렬은 2x2 형태로 나타내며, 다음과 같이 구성됩니다:

|                | **예측: 긍정** | **예측: 부정** |
|----------------|----------------|----------------|
| **실제: 긍정** | True Positive (TP)  | False Negative (FN) |
| **실제: 부정** | False Positive (FP) | True Negative (TN)  |

#### 각 항목의 의미
- **True Positive (TP)**: 모델이 긍정 클래스를 긍정 클래스로 정확하게 예측한 경우.
- **False Positive (FP)**: 모델이 부정 클래스를 긍정 클래스로 잘못 예측한 경우 (종종 'Type I error'라고 불림).
- **False Negative (FN)**: 모델이 긍정 클래스를 부정 클래스로 잘못 예측한 경우 (종종 'Type II error'라고 불림).
- **True Negative (TN)**: 모델이 부정 클래스를 부정 클래스로 정확하게 예측한 경우.

### 혼동 행렬을 활용한 평가 지표

혼동 행렬을 통해 여러 가지 성능 지표를 계산할 수 있습니다:

1. **정확도 (Accuracy)**
   
전체 예측 중에서 맞게 예측한 비율입니다.

Accuracy = (TP + TN) / (TP + TN + FP + FN)

2. **정밀도 (Precision)**  

모델이 긍정 클래스라고 예측한 것 중에서 실제로 긍정 클래스인 비율입니다.

Precision = TP / (TP + FP)


3. **재현율 (Recall)**  

실제 긍정 클래스 중에서 모델이 긍정 클래스라고 정확히 예측한 비율입니다.

Recall = TP / (TP + FN)

4. **F1-Score**  

정밀도와 재현율의 조화 평균으로, 불균형 데이터셋에서 중요한 지표입니다.

F1-Score = 2 * (Precision * Recall) / (Precision + Recall)

5. **특이도 (Specificity)**  

실제 부정 클래스 중에서 모델이 부정 클래스라고 정확히 예측한 비율입니다.

Specificity = TN / (TN + FP)

### ROC (Receiver Operating Characteristic) Curve
ROC 곡선은 모델의 True Positive Rate (재현율, Recall)과 False Positive Rate (1 - 특이도, Specificity)을 비교하여 성능을 평가하는 곡선입니다.

- **True Positive Rate (TPR)**: 재현율 (Recall)

  TPR = TP / (TP + FN)

- **False Positive Rate (FPR)**: 1 - 특이도 (1 - Specificity)

  FPR = FP / (FP + TN)


### AUC (Area Under the Curve)
AUC는 ROC 곡선 아래의 면적을 의미합니다. AUC 값은 모델이 얼마나 잘 분류하는지에 대한 성능 지표로, 1에 가까울수록 모델의 성능이 뛰어납니다.

- **AUC**는 ROC 곡선 아래의 면적을 나타내며, 다음과 같은 기준으로 해석됩니다:
  - **AUC = 0.5**: 모델이 랜덤 추측을 하는 것과 동일
  - **AUC > 0.5**: 모델이 데이터의 패턴을 일부 인식하고 있다는 의미
  - **AUC = 1**: 모델이 완벽하게 예측하는 경우

---
## 분류 알고리즘
```
# 데이터 준비 : 분꽃 데이터
from sklearn import datasets
import numpy as np
import matplotlib.pyplot as plt

iris=datasets.load_iris() # 딕셔너리

iris
```
![image](https://github.com/user-attachments/assets/cb6db826-4f38-4809-8053-5a2d98b0a1ab)

```
# 데이터 준비 : 분꽃 데이터
from sklearn import datasets
import numpy as np
import matplotlib.pyplot as plt

iris=datasets.load_iris() # 딕셔너리

print(iris.keys())
print(np.unique(iris['target']))
```
![image](https://github.com/user-attachments/assets/daaab6a5-cfe2-49d1-8c20-25da454833cc)

```
# 데이터 준비 : 분꽃 데이터
from sklearn import datasets
import numpy as np
import matplotlib.pyplot as plt

iris=datasets.load_iris() # 딕셔너리

print(iris.keys())
print(np.unique(iris['target'])) # 원래 답 [0,1,2] < = 품종이 3개
print(np.unique(iris['target_names'])) # 품종 이름
print(np.unique(iris['feature_names'])) # 컬럼명
```
![image](https://github.com/user-attachments/assets/13a33669-2fe0-462e-9850-30a456603b1e)

```
# 데이터 준비 : 분꽃 데이터
from sklearn import datasets
import numpy as np
import matplotlib.pyplot as plt

iris=datasets.load_iris() # 딕셔너리
X=iris['data'][:,[2,3]] # feature 두개만 가지고 오겟다. (꽃잎 길이와 꽃잎 너비)
print(X)
y=iris['target']
print(y)

print(f'클래스 레이블 : {np.unique(y)}')
print(f'X 행렬(데이터)의 구조 : {X.shape}')
print(f'y 벡터(원래 답)의 구조 : {y.shape}')
```
![image](https://github.com/user-attachments/assets/0d6ebddb-d355-4ac5-9b34-f4b6f4a8286f)

```
from sklearn import datasets
import numpy as np
import matplotlib.pyplot as plt

iris=datasets.load_iris() 
X=iris['data'][:,[2,3]]
y=iris['target']

# 학습데이터와 답, 테스트 데이터와 답 분리
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test =train_test_split(
    X # 분리할 데이터
    ,y # 분리할 원래 답
    ,train_size=0.7 # 학습에 사용할 데이터를 전체의 70%로 만들어라
#    ,test_size=0.3 # 테스트에 사용할 데이터를 전체의 30%로 만들어라
    ,random_state=1 # 시드 고정
    ,stratify=y # 계층화 : y는 True와 같음, random으로 분리 할 시 특정 레이블은 조금만 갖고 오고 어느 레이블은 많이 갖고 올 수 있는 것을 방지하는 방법 예를 들어 0,1,2번 레이블이 있다면 0번에서 (train_size*100)%를 1번에서 (train_size*100)%를 2번에서 (train_size*100)% 훈련 데이터로 가지고 오게 하는 방법
)
print('====================================================================')
print(f'X는 {len(X)}개를 가지고 있고 shape은 {X.shape}이다')
print(f'y는 {len(y)}개를 가지고 있고 shape은 {y.shape}이다')
print('==============================모델 학습==============================')
print(f'X_train는 {len(X_train)}개를 가지고 있고 shape은 {X_train.shape}이다')
print(f'X_test는 {len(X_test)}개를 가지고 있고 shape은 {X_test.shape}이다')
print('==============================모델 테스트==============================')
print(f'y_train는 {len(y_train)}개를 가지고 있고 shape은 {y_train.shape}이다')
print(f'y_test는 {len(y_test)}개를 가지고 있고 shape은 {y_test.shape}이다')

```
![image](https://github.com/user-attachments/assets/e9d8d0ca-212a-41b0-85f1-c90595aa27e2)

```
from sklearn import datasets
import numpy as np
import matplotlib.pyplot as plt

iris=datasets.load_iris() 
X=iris['data'][:,[2,3]]
y=iris['target']

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test =train_test_split(
    X 
    ,y
    ,train_size=0.7
    ,random_state=1 
    ,stratify=y 
)

print(f'y의 레이블 카운트 : {np.bincount(y)}') # 0이 50개, 1이 50개, 2가 50개 이렇게 y에 들어 가 있다
print(f'y_train의 레이블 카운트 : {np.bincount(y_train)}') 
print(f'y_test의 레이블 카운트 : {np.bincount(y_test)}') 
```
![image](https://github.com/user-attachments/assets/812d01b2-d3fd-4c16-82e7-e7596990227e)

```
from sklearn import datasets
import numpy as np
import matplotlib.pyplot as plt

# 사이킷런의 iris 데이터셋 로드
iris = datasets.load_iris()

# 꽃잎의 길이(3번째 열)와 너비(4번째 열)만 선택하여 특징 행렬 X 생성
X = iris['data'][:, [2, 3]]

# 클래스 라벨 (타겟 값) 저장
y = iris['target']

from sklearn.model_selection import train_test_split

# 데이터를 학습용(70%)과 테스트용(30%)으로 분할
# stratify=y를 설정하여 각 클래스가 훈련 세트와 테스트 세트에서 동일한 비율을 유지하도록 함
X_train, X_test, y_train, y_test = train_test_split(
    X,  # 입력 데이터
    y,  # 출력 데이터 (레이블)
    train_size=0.7,  # 훈련 데이터 비율
    random_state=1,  # 랜덤 시드 고정 (재현 가능성 확보)
    stratify=y  # 클래스 비율 유지
)

# X는 시계얼 데이터가 아니다.
# X 랜덤하게 가지고와서 train과 test를 가지고 온다 (만약에 X의 데이터가 시계얼 데이터 였으면 문제가 생긴다)
# 시계열 데이터(Time Series Data)는 시간의 흐름에 따라 수집된 데이터를 의미한다. 즉, 일정한 시간 간격(예: 초, 분, 시간, 일, 월, 년)으로 측정된 데이터로, 시점(time stamp)과 함께 기록되기 때문에
# 시계열 데이터는 Random으로 처리하면 안된다. 

# 시계열 데이터의 랜덤 샘플링 문제
# 시간 의존성 - 시계열 데이터는 과거 값이 미래 값에 영향을 미칠 수 있으므로, 데이터 포인트 간의 순서가 매우 중요합니다.
# 추세(Trend)와 계절성(Seasonality) - 데이터가 특정 방향으로 변화하거나, 주기적인 패턴을 가질 수 있음.
# 일반적인 머신러닝 모델에서는 데이터를 랜덤하게 섞어서 학습(train)과 테스트(test) 데이터를 나누는 것이 일반적이지만, 시계열 데이터에서는 랜덤 샘플링을 하면 데이터의 시간적 관계가 깨지므로 문제가 발생합니다.

# 시계열 데이터 처리 시 주의할 점
# 시간 순서 유지 - 데이터를 랜덤하게 섞지 않고, 시간 순서대로 학습 및 테스트 데이터를 분리해야 함
# Train/Test Split 방식 - 일반적인 데이터는 무작위로 나누지만, 시계열 데이터는 과거 데이터를 학습용(train), 이후 데이터를 테스트용(test)으로 사용해야 함.


from sklearn.preprocessing import StandardScaler

# 표준화 전처리 객체 생성 (StandardScaler는 평균 0, 분산 1로 데이터를 조정)
sc = StandardScaler()

# 훈련 데이터의 평균과 표준편차를 계산 (테스트 데이터 변환 시 같은 기준 사용)
sc.fit(X_train)

# 훈련 데이터 표준화 변환 (각 특징을 평균 0, 분산 1로 조정)
X_train_std = sc.transform(X_train)

# 테스트 데이터도 같은 변환 적용
# X_train으로 학습하고 X_test로 테스트하므로, X_train의 변환 기준에 X_test도 맞춰야 함.
X_test_std = sc.transform(X_test)

from sklearn.linear_model import Perceptron

# 퍼셉트론(Perceptron) 모델 생성
# eta0: 학습률 (learning rate) 설정 (0.1)
# random_state=1: 동일한 초기 가중치를 보장하여 실행할 때마다 같은 결과가 나오도록 함
# random_state=None으로 설정하면 실행할 때마다 학습 결과가 달라질 수 있음
ppn = Perceptron(eta0=0.1, random_state=1)  # ppn은 Perceptron 모델을 의미

# 퍼셉트론 모델을 훈련 데이터에 맞춰 학습 (가중치 업데이트)
ppn.fit(X_train_std, y_train) # ppn.fit(X_train_std, y_train)을 통해 학습을 진행

# 학습된 모델을 사용하여 테스트 데이터 예측
y_pred = ppn.predict(X_test_std) # 예측을 수행

# y_test 갯수
print(f'y_test의 개수 : {len(y_test)}')

# y_pred 갯수
print(f'y_pred의 개수 : {len(y_pred)}')

# 잘 분류된 샘플 개수 출력
print(f'잘 분류된 샘플 개수 : {(y_test == y_pred).sum()}')

# 잘못 분류된 샘플 개수 출력
print(f'잘못 분류된 샘플 개수 : {(y_test != y_pred).sum()}')
```
![image](https://github.com/user-attachments/assets/1e0efced-f842-4699-8b29-97c78cc530b7)

```
from sklearn import datasets
import numpy as np
import matplotlib.pyplot as plt

iris = datasets.load_iris()

X = iris['data'][:, [2, 3]]

y = iris['target']

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, 
    y, 
    train_size=0.7, 
    random_state=1, 
    stratify=y 
)


from sklearn.preprocessing import StandardScaler

sc = StandardScaler()

sc.fit(X_train)

X_train_std = sc.transform(X_train)

X_test_std = sc.transform(X_test)

from sklearn.linear_model import Perceptron

ppn = Perceptron(eta0=0.1, random_state=42)  

ppn.fit(X_train_std, y_train) 

y_pred = ppn.predict(X_test_std)

print(f'y_test의 개수 : {len(y_test)}')

print(f'y_pred의 개수 : {len(y_pred)}')

print(f'잘 분류된 샘플 개수 : {(y_test == y_pred).sum()}')

print(f'잘못 분류된 샘플 개수 : {(y_test != y_pred).sum()}')
```
![image](https://github.com/user-attachments/assets/1a6439a5-8c70-49b8-b3a0-2cf17969a690)

```
from sklearn import datasets
import numpy as np
import matplotlib.pyplot as plt

iris = datasets.load_iris()

X = iris['data'][:, [2, 3]]

y = iris['target']

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, 
    y, 
    train_size=0.7, 
    random_state=1, 
    stratify=y 
)


from sklearn.preprocessing import StandardScaler

sc = StandardScaler()

sc.fit(X_train)

X_train_std = sc.transform(X_train)

X_test_std = sc.transform(X_test)

from sklearn.linear_model import Perceptron

ppn = Perceptron(eta0=0.1, random_state=1)  

ppn.fit(X_train_std, y_train) 

y_pred = ppn.predict(X_test_std)

from sklearn.metrics import accuracy_score

# 정확도 함수 : 분류 모델에서만 사용된다.
# 회귀모델에서는 얼마나 맞는지를 평가하는 방식으로 결정계수를 사용한다.
# accuracy_score(원래답,예측값)
print(f'정확도(accuracy_score) : {round(accuracy_score(y_test, y_pred), 3)}')

# score(테스트 데이터, 테스트데이터의 원래답)
 
print(f'정확도(score) : {round(ppn.score(X_test_std, y_test), 3)}')
```
![image](https://github.com/user-attachments/assets/30eb42a9-b1af-4c74-8679-01691d89d818)

---
## Overfitting과 Underfitting

**Overfitting**과 **Underfitting**은 머신러닝 모델의 성능에 중요한 영향을 미칩니다. 이 두 가지는 모델의 학습 및 일반화 능력과 관련이 있으며, 적절한 균형을 맞추는 것이 중요합니다.

### 1. Overfitting (과적합)
- **정의**: 모델이 훈련 데이터에 너무 잘 맞춰져서, 훈련 데이터에 있는 노이즈나 특이한 패턴까지 학습하는 현상입니다. 이로 인해 훈련 데이터에서는 성능이 매우 좋지만, 새로운 테스트 데이터에서는 성능이 떨어집니다.
- **원인**: 모델이 너무 복잡하거나 학습이 너무 오래되어 훈련 데이터에 과도하게 적합할 때 발생합니다.
- **예시**: 선형 회귀 모델이 아닌, 지나치게 복잡한 다항 회귀 모델이 훈련 데이터의 미세한 변화까지 학습하여, 새로운 데이터에서는 예측이 부정확해지는 경우.
- **해결 방법**:
  - 모델 단순화 (파라미터 수 줄이기)
  - 교차 검증 사용
  - 규제 (regularization) 기법 사용 (예: L1, L2 정규화)
  - 더 많은 데이터를 수집하거나 데이터 증강 (data augmentation) 방법 사용

### 2. Underfitting (과소적합)
- **정의**: 모델이 훈련 데이터의 주요 패턴을 충분히 학습하지 못하는 현상입니다. 모델이 너무 간단하거나 학습이 부족하여, 훈련 데이터와 테스트 데이터 모두에서 성능이 낮은 경우입니다.
- **원인**: 모델이 너무 단순하거나 훈련 데이터를 충분히 학습하지 않았을 때 발생합니다.
- **예시**: 너무 간단한 선형 회귀 모델이 복잡한 비선형 관계를 제대로 학습하지 못해, 예측 성능이 낮아지는 경우.
- **해결 방법**:
  - 모델을 복잡하게 만들어 더 많은 패턴을 학습할 수 있게 함
  - 더 긴 학습 시간이나 더 많은 학습 데이터를 제공
  - 더 적합한 모델 선택 (예: 비선형 모델을 사용)

### 학습 성능과 일반화 성능에 따른 Overfitting과 Underfitting

| **상태**          | **학습 성능 (Training Performance)** | **일반화 성능 (Generalization Performance)** | **설명**                                                      |
|-------------------|-----------------------------------|--------------------------------------------|-------------------------------------------------------------|
| **적정 모델**      | 높음                              | 높음                                       | 모델이 훈련 데이터와 새로운 데이터 모두에서 잘 일반화됨.       |
| **Overfitting**   | 높음                              | 낮음                                       | 모델이 훈련 데이터에 과도하게 적합되어 새로운 데이터에서 성능이 떨어짐. |
| **Underfitting**  | 낮음                              | 낮음                                       | 모델이 너무 단순하여 훈련 데이터의 패턴을 충분히 학습하지 못함. |
|    | 낮음                              | 높음                                       | 존재가 할 수 없음 |

### 방지 방법
- 더 많은 데이터 확보
- 규제 (Regulation) : L1,L2규제로 w 조정
- 모델의 복잡성 제한 
- 조기 종료
- 드롭 아웃
- 앙상블 기법 사용 : Random Forest, XGBoost

  
#### 추가 설명:
- **Underfitting**: 모델이 너무 간단하거나 학습이 부족하여 훈련 데이터와 테스트 데이터 모두에서 성능이 낮습니다.
- **적정 모델 (Good Fit)**: 모델이 훈련 데이터와 테스트 데이터 모두에서 잘 동작하여 일반화 성능이 높습니다.
- **Overfitting**: 모델이 훈련 데이터에 너무 적합하여 훈련 데이터에서 성능은 높지만, 새로운 데이터에서 성능이 떨어집니다.

## 하이퍼파라미터 튜닝

**하이퍼파라미터**는 머신러닝 알고리즘의 성능에 중요한 영향을 미칩니다. 모델 학습 전에 설정해야 하는 **하이퍼파라미터**들은 모델의 성능을 최적화하는데 중요한 역할을 합니다.

### 하이퍼파라미터 튜닝 (Hyperparameter Tuning)
- **정의**: 하이퍼파라미터 튜닝은 모델 학습 전에 설정할 수 있는 파라미터 값을 조정하여 모델 성능을 최적화하는 과정입니다.
- **목표**: 모델이 overfitting이나 underfitting되지 않도록 하이퍼파라미터를 적절히 조정하여 최상의 성능을 끌어내는 것.
- **주요 하이퍼파라미터**:
  - **학습률 (Learning Rate)**: 모델이 학습하는 속도를 결정합니다. 너무 높으면 학습이 불안정해지고, 너무 낮으면 학습이 너무 느려집니다.
  - **정규화 파라미터 (Regularization Parameter)**: 모델이 과적합을 방지하도록 도와주는 파라미터입니다.
  - **배치 크기 (Batch Size)**: 한 번에 네트워크에 입력되는 샘플의 수입니다. 작은 배치 크기는 더 자주 업데이트되며, 큰 배치 크기는 더 안정적인 업데이트를 합니다.
  - **에포크 수 (Epochs)**: 전체 훈련 데이터가 모델을 통해 몇 번 반복되는지 설정합니다. 에포크 수가 너무 많으면 과적합이 일어날 수 있습니다.
  - **모델의 복잡성**: 예를 들어, 결정 트리 모델의 최대 깊이(max depth)나 신경망 모델의 은닉층 수(hidden layers) 등을 조정할 수 있습니다.

### 하이퍼파라미터 튜닝 방법
- **그리드 서치 (Grid Search)**: 지정된 하이퍼파라미터 값의 조합을 모두 시도하여 최적의 성능을 찾아내는 방법입니다.
- **랜덤 서치 (Random Search)**: 하이퍼파라미터 값을 랜덤하게 선택하여 모델을 학습시키고, 가장 좋은 성능을 가진 값을 선택하는 방법입니다.
- **베이지안 최적화 (Bayesian Optimization)**: 확률론적인 모델을 사용하여 하이퍼파라미터 값을 탐색합니다. 그리드 서치나 랜덤 서치보다 효율적인 경우가 많습니다.
- **교차 검증 (Cross-Validation)**: 하이퍼파라미터를 선택할 때, 데이터셋을 여러 부분으로 나누어 모델을 평가하는 방법으로, 모델의 일반화 성능을 더 정확하게 측정할 수 있습니다.

### 하이퍼파라미터 튜닝 시 고려 사항
- **과적합 방지**: 하이퍼파라미터를 조정할 때, 과적합을 방지할 수 있도록 주의합니다.
- **컴퓨팅 자원**: 그리드 서치나 랜덤 서치처럼 많은 하이퍼파라미터 조합을 시도할 경우, 컴퓨팅 자원이 많이 소모될 수 있습니다. 베이지안 최적화는 자원을 절약할 수 있는 방법 중 하나입니다.
- **모델의 해석성**: 모델이 너무 복잡해지면 해석하기 어려워질 수 있으므로, 적절한 복잡성을 유지하는 것이 중요합니다.

```
from matplotlib.colors import ListedColormap
import matplotlib.pyplot as plt

# 맷플롯립 호환성을 체크합니다
import matplotlib
from distutils.version import LooseVersion


def plot_decision_regions(X, y, classifier, test_idx=None, resolution=0.02):
    # 마커와 컬러맵을 설정합니다.
    markers = ('o', 's', '^', 'v', '<')  # 각 클래스별로 사용할 마커 기호
    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')  # 각 클래스별로 사용할 색상
    cmap = ListedColormap(colors[:len(np.unique(y))])  # y의 고유 클래스 수에 맞게 색상을 선택하여 컬러맵 생성

    # 결정 경계를 그립니다.
    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1  # x1축의 최소값과 최대값 계산
    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1  # x2축의 최소값과 최대값 계산
    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
                           np.arange(x2_min, x2_max, resolution))  # x1, x2에 대한 그리드 생성
    
    # 각 그리드 지점에 대해 예측된 클래스 라벨을 계산
    lab = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
    lab = lab.reshape(xx1.shape)  # 예측된 클래스 라벨을 그리드 형상에 맞게 재구성
    
    # 결정 경계를 색칠합니다.
    plt.contourf(xx1, xx2, lab, alpha=0.3, cmap=cmap)  # 각 클래스의 영역을 구분하여 색칠
    plt.xlim(xx1.min(), xx1.max())  # x축 범위 설정
    plt.ylim(xx2.min(), xx2.max())  # y축 범위 설정

    # 클래스 샘플을 그립니다.
    for idx, cl in enumerate(np.unique(y)):  # y의 고유 클래스에 대해 반복
        plt.scatter(x=X[y == cl, 0],  # x값
                    y=X[y == cl, 1],  # y값
                    alpha=0.8,  # 투명도
                    c=colors[idx],  # 각 클래스에 맞는 색상
                    marker=markers[idx],  # 각 클래스에 맞는 마커
                    label=f'Class {cl}',  # 범례에 클래스 이름 추가
                    edgecolor='black')  # 마커 테두리 색상 설정

    # 테스트 샘플을 부각하여 그립니다.
    if test_idx:
        # 테스트 샘플을 선택합니다.
        X_test, y_test = X[test_idx, :], y[test_idx]

        # 테스트 샘플을 크고 검정색 테두리로 표시하여 구별합니다.
        plt.scatter(X_test[:, 0], 
                    X_test[:, 1],
                    c='none',  # 내부 색상 없음
                    edgecolor='black',  # 테두리 색상 검정
                    alpha=1.0,  # 완전 불투명
                    linewidth=1,  # 테두리 두께
                    marker='o',  # 마커 모양 원
                    s=100,  # 마커 크기
                    label='Test set')  # 테스트 집합 레이블 추가

X_combined_std = np.vstack((X_train_std, X_test_std))
y_combined = np.hstack((y_train, y_test))

plot_decision_regions(X=X_combined_std, y=y_combined,
                      classifier=ppn, test_idx=range(105, 150))
plt.xlabel('Petal length [standardized]')
plt.ylabel('Petal width [standardized]')
plt.legend(loc='upper left')

plt.tight_layout()
#plt.savefig('figures/03_01.png', dpi=300)
plt.show()
```
![image](https://github.com/user-attachments/assets/abd91d87-4970-48cd-9b7f-962087bb826c)

---
## 로지스틱 회귀를 사용한 클래스 확률 모델링

로지스틱 회귀(Logistic Regression)는 **분류 문제**를 해결하기 위한 알고리즘으로, 클래스 확률을 예측하는 모델링을 제공합니다. 이 모델은 특히 이진 분류(binary classification) 문제에 많이 사용되며, 주어진 입력에 대해 **각 클래스에 속할 확률**을 출력합니다.

### 1. 로지스틱 회귀의 기본 개념

로지스틱 회귀는 선형 회귀(Linear Regression)와 비슷하지만, 예측값을 **확률**로 변환하기 위해 **시그모이드 함수**를 사용합니다. 시그모이드 함수는 입력값을 0과 1 사이의 값으로 매핑시킵니다. 이는 분류 문제에서 "확률"로 해석할 수 있습니다.

### 2. 모델 수식

로지스틱 회귀의 핵심은 **시그모이드 함수**(Logistic Function)를 사용하는 것입니다. 이 함수는 다음과 같이 표현됩니다:

$$
\[
\sigma(z) = \frac{1}{1 + e^{-z}}
\]
$$

$$
z = w_0 + w_1 x_1 + w_2 x_2 + \dots + w_n x_n \quad \text{는 선형 방정식입니다.}
$$

$$
w_0, w_1, \dots, w_n \quad \text{은 모델의 파라미터(weights)입니다.}
$$

$$
x_1, x_2, \dots, x_n \quad \text{은 입력 변수(feature)입니다.}
$$

$$
\sigma(z) \quad \text{는 시그모이드 함수의 출력으로, 이 값은 0과 1 사이의 확률 값을 가집니다.}
$$

## 3. 클래스 확률 예측

로지스틱 회귀는 클래스의 확률을 예측합니다. 예를 들어, 이진 분류에서 두 클래스 \( y = 0 \) 또는 \( y = 1 \)의 확률을 예측하는 방식입니다.

- 예측된 확률이 0.5보다 크면 클래스 1로 분류하고, 0.5보다 작으면 클래스 0으로 분류합니다.
- 실제로 로지스틱 회귀는 **클래스 1**에 속할 확률을 예측하며, 이 값을 **p**라고 할 때:
  
$$
\[
p = \sigma(z) = \frac{1}{1 + e^{-(w_0 + w_1 x_1 + w_2 x_2 + \dots + w_n x_n)}}
\]
$$

## 3. 로지스틱 회귀의 Cost Function (L(W,b|x))
로지스틱 회귀의 Cost Function은 다음과 같습니다:

$$
\[
L(W, b | x) = - \left( y \log(\hat{y}) + (1 - y) \log(1 - \hat{y}) \right)
\]
$$

$$
\( y \)는  실제 값(타겟 값),
$$
$$
\( \hat{y} \)는  모델이 예측한 확률 값
$$

또는

![image](https://github.com/user-attachments/assets/d460400e-b262-4d64-a4b7-9c471f92a562)


### 4. 전체 데이터에 대한 Cost Function
전체 학습 데이터셋 \( D \)에 대해 평균 손실을 구하려면, 각 데이터 포인트에 대한 손실의 평균을 취합니다. 즉, 모든 데이터에 대해 위의 비용 함수의 평균을 취합니다:

$$
\[
J(W, b) = - \frac{1}{m} \sum_{i=1}^{m} \left[ y^{(i)} \log(\hat{y}^{(i)}) + (1 - y^{(i)}) \log(1 - \hat{y}^{(i)}) \right]
\]
$$

$$
\( m \)은   전체 데이터 포인트의 수
$$

$$
\( y^{(i)} \)는   \( i \)-번째 데이터의 실제 값
$$

$$
\( \hat{y}^{(i)} \)는   \( i \)-번째 데이터의 예측된 확률 값
$$


### 5. 모델 학습 (학습 알고리즘)

로지스틱 회귀 모델은 **경사 하강법(Gradient Descent)**을 사용하여 파라미터 \( w_0, w_1, \dots, w_n \)를 업데이트합니다. 경사 하강법은 비용 함수의 기울기를 계산하여, 비용 함수 값을 최소화하는 방향으로 파라미터를 조정하는 방법입니다.

### 6. 예시: 이진 분류에서 로지스틱 회귀

- **문제 설정**: 예를 들어, 주어진 입력 특성(예: 나이, 소득 등)을 바탕으로 사람의 **구매 여부**를 예측하는 문제입니다. 구매 여부는 이진 값(구매: 1, 미구매: 0)으로 표현됩니다.
- **목표**: 이 입력을 바탕으로 사람의 구매 확률을 예측합니다. 예를 들어, 0.7의 확률을 예측하면, 이 사람은 70% 확률로 구매할 가능성이 있습니다.
- **시그모이드 함수 적용**: 로지스틱 회귀는 시그모이드 함수를 통해 확률값을 출력합니다. 예를 들어, 나이와 소득을 바탕으로 시그모이드 함수에서 나온 확률 값은 특정 사람이 구매할 확률을 의미합니다.

### 7. 다중 클래스 분류 (One-vs-Rest 방식)

로지스틱 회귀는 기본적으로 **이진 분류** 모델입니다. 하지만 다중 클래스 분류 문제에서 **One-vs-Rest (OvR)** 방식 또는 **Softmax 회귀**를 사용할 수 있습니다.

- **One-vs-Rest**: 각 클래스에 대해 개별적으로 로지스틱 회귀 모델을 학습하여 다중 클래스를 분류합니다.
- **Softmax 회귀**: 여러 클래스에 대해 확률을 동시에 예측하며, 클래스 간 확률을 비교하여 가장 높은 확률을 가지는 클래스를 예측합니다.

### 8. 로지스틱 회귀의 활용

- **이진 분류**: 이메일 스팸 여부 분류, 질병 유무 예측 등
- **다중 클래스 분류**: 이미지 분류, 텍스트 분류 등
- **확률적 예측**: 특정 이벤트의 발생 확률 예측

### 결론

로지스틱 회귀는 주어진 입력에 대해 각 클래스에 속할 확률을 예측하는 **확률적 분류 모델**입니다. 이 모델은 간단하면서도 해석이 용이하고, 이진 분류뿐만 아니라 다중 클래스 분류 문제에도 활용될 수 있습니다. 로지스틱 회귀의 핵심은 **시그모이드 함수**를 사용하여 예측값을 확률로 변환하는 것입니다.

---
## Odds

**Odds**는 주어진 사건이 발생할 확률과 발생하지 않을 확률의 비율을 나타냅니다. 확률(probability)과는 다르게, odds는 발생하지 않는 경우를 고려하여 사건의 상대적인 가능성을 설명합니다.

### 정의
**Odds**는 다음과 같이 정의됩니다:

$$
\text{Odds} = \frac{\text{사건이 발생한 확률}}{\text{사건이 발생하지 않은 확률}}
$$

이때, 사건이 발생한 확률을 \( P(A) \)라고 하고, 사건이 발생하지 않은 확률을 \( 1 - P(A) \)로 나타낼 수 있습니다. 그래서 odds는 다음과 같이 나타낼 수 있습니다:

$$
\text{Odds} = \frac{P(A)}{1 - P(A)}
$$

### 예시 1:

1. 사건이 발생할 확률 (P(A) = 0.75)일 때  
   사건이 발생하지 않을 확률은 (1 - 0.75 = 0.25)  
   이때 **odds**는 0.75/0.25 =3
   즉, 사건이 발생할 확률이 발생하지 않을 확률에 비해 3배 더 클 수 있다는 의미입니다.

3. 사건이 발생할 확률 (P(A) = 0.2)일 때  
   사건이 발생하지 않을 확률은 (1 - 0.2 = 0.8)  
   이때 **odds**는 0.2/0.8=0.25
   
   즉, 사건이 발생할 확률이 발생하지 않을 확률에 비해 4배 낮다는 의미입니다.

### 예시 2: 주사위 던지기
주사위를 던졌을 때 숫자 6이 나올 확률:

$$
\frac{1}{6}
$$

6이 나오지 않을 확률:

$$
1 - \frac{1}{6} = \frac{5}{6}
$$

이때, **odds**는 다음과 같이 계산됩니다:

$$
\text{Odds} = \frac{\frac{1}{6}}{1 - \frac{1}{6}} = \frac{1}{5} = 0.2
$$


즉, 6이 나올 확률 대비, 나오지 않을 확률이 5배 크다.


### 확률과 Odds의 관계
- **확률**은 사건이 발생할 가능성을 나타내며, 0과 1 사이의 값을 가집니다.
- **Odds**는 사건이 발생할 확률에 비해 얼마나 상대적으로 발생하지 않을 가능성이 높은지를 나타내며, 0 이상의 값을 가질 수 있습니다. 확률이 50%인 경우, odds는 1:1로 발생할 확률과 발생하지 않을 확률이 같음을 의미합니다.
  
| 확률 p    | odds(p / (1 - p)) | 의미                          |
|-----------|------------------|-------------------------------|
| 0.1 (10%) | 0.111            | 사건이 발생하지 않을 확률이 9배 크다 |
| 0.2 (20%) | 0.25             | 사건이 발생하지 않을 확률이 4배 크다 |
| 0.5 (50%) | 1                | 사건이 발생하거나 발생하지 않을 확률이 동일 |
| 0.8 (80%) | 4                | 발생 확률이 4배 더 크다         |
| 0.9 (90%) | 9                | 발생 확률이 9배 더 크다         |

확률 p가 100에 가까워지면 odds는 무한대로 증가합니다. 이를 해결하기 위해 Logit 함수는 다음과 같이 정의됩니다:

$$
\text{Logit}(p) = \log\left(\frac{p}{1 - p}\right)
$$

이렇게 Logit(p)는 odds의 로그 값을 취한 형태로, 확률 p가 100에 가까울 때 발생할 수 있는 무한대 값을 피할 수 있습니다.

Logit(p) 값이 0.5보다 크면 양수를 반환하고 Logit(p) 값이 0.5보다 작으면 음수를 반환한다.

![image](https://github.com/user-attachments/assets/3c6f57a3-7502-4932-b6ef-9507448d33e4)

**Logit(p)** 에서는 확률을 선형적인 형태로 변환하여 모델링에 사용 가능

### 요약
- 확률은 "사건이 일어날 가능성"을 의미합니다.
- Odds는 "사건이 일어날 확률에 비해 일어나지 않을 확률"의 비율을 나타냅니다.

---

시그모이드 함수는 확률 \( p \)를 출력하는 함수입니다. 시그모이드 함수는 다음과 같습니다:

$$
\sigma(z) = \frac{1}{1 + e^{-z}} = \frac{1}{1 + e^{-(W^T x + b)}}
$$

이제, Logit 함수는 시그모이드 함수의 역함수입니다. Logit 함수는 시그모이드 함수의 출력을 입력으로 받아, 그 값을 로짓 값으로 변환합니다. 즉, Logit은 다음과 같습니다:

$$
\text{Logit}(p) = \log \left( \frac{p}{1 - p} \right)
$$

시그모이드 함수의 결과를 Logit에 넣으면:

$$
\text{Logit}(\sigma(z)) = \log \left( \frac{\sigma(z)}{1 - \sigma(z)} \right)
$$

시그모이드 함수의 정의를 넣으면:

$$
\text{Logit}(\sigma(z)) = \log \left( \frac{\frac{1}{1 + e^{-z}}}{1 - \frac{1}{1 + e^{-z}}} \right)
$$

이 식은 단순화하면 다음과 같습니다:

$$
\text{Logit}(\sigma(z)) = \log \left( \frac{1}{e^{-z}} \right) = \log(e^{z}) = z
$$

따라서, 시그모이드 함수에 대한 Logit 값은 결국 원래의 \( z \) 로 돌아옵니다.

따라서 Logit을 이용하여 확률 계산을 선형연산으로 할 수 있게 한다.

```
import matplotlib.pyplot as plt
import numpy as np


def sigmoid(z):
    return 1.0 / (1.0 + np.exp(-z))

z = np.arange(-7, 7, 0.1)
sigma_z = sigmoid(z)

plt.plot(z, sigma_z)
plt.axvline(0.0, color='k')
plt.ylim(-0.1, 1.1)
plt.xlabel('z')
plt.ylabel('$\sigma (z)$')

# y 축의 눈금과 격자선
plt.yticks([0.0, 0.5, 1.0])
ax = plt.gca()
ax.yaxis.grid(True)

plt.tight_layout()
#plt.savefig('figures/03_02.png', dpi=300)
plt.show()
```
![image](https://github.com/user-attachments/assets/83b9915d-0f5b-4a6b-84a1-0539c753bda1)

---
## 아달린과 로지스틱 회귀 비교
![image](https://github.com/user-attachments/assets/a5f7ca8a-c72c-4c92-99f3-03467ed5196e)

### 1. 모델의 기본 원리
- **Adaline (Adaptive Linear Neuron)**:
  - Adaline은 **선형 모델**입니다. 예측을 위해 선형 방정식을 사용합니다.
  - 출력은 실수 값이며, 이를 **단계 함수**나 **시그모이드 함수** 등을 통해 0 또는 1로 변환하여 이진 분류를 합니다.
  - Adaline은 **선형 회귀**와 유사하며, 오차 제곱합(MSE)을 최소화하는 방식으로 학습합니다.

- **Logistic Regression**:
  - Logistic Regression은 **선형 모델**이지만, 그 출력은 **시그모이드 함수**를 통과하여 확률 값으로 변환됩니다.
  - 출력 값은 0과 1 사이의 값이며, 이를 바탕으로 이진 분류를 수행합니다.
  - **로지스틱 함수**는 확률 값을 제공하므로, 분류 문제에서 더 직관적인 해석을 할 수 있습니다.

### 2. 학습 방식
- **Adaline**:
  - **MSE (Mean Squared Error)** 를 손실 함수로 사용하여, 경사 하강법(Gradient Descent)을 통해 가중치를 업데이트합니다.
  - 예측 값과 실제 값의 차이를 최소화하는 방식으로 학습합니다.

- **Logistic Regression**:
  - **로그 손실 함수 (Log-Loss)** 를 사용하여 학습합니다. 이는 교차 엔트로피를 최소화하려는 방식입니다.
  - 경사 하강법을 사용하여 가중치를 업데이트하며, 예측 값이 확률로 나옵니다.

### 3. 출력 값
- **Adaline**:
  - 출력 값은 **연속적인 실수 값**입니다. 이 값은 예측된 클래스에 대한 "활성화 정도"를 나타냅니다.
  
- **Logistic Regression**:
  - 출력 값은 **확률 값**입니다. 0과 1 사이의 값으로, 특정 클래스에 속할 확률을 나타냅니다.

### 4. 활성화 함수
- **Adaline**:
  - **선형 함수**를 활성화 함수로 사용합니다. 즉, 입력 값의 가중 합으로 출력이 결정됩니다.
  
- **Logistic Regression**:
  - **시그모이드 함수**를 활성화 함수로 사용하여, 예측 값을 확률로 변환합니다.

### 5. 주요 차이점

| 특징             | Adaline                        | Logistic Regression              |
|------------------|------------------------------|----------------------------------|
| 모델 형태        | 선형 회귀와 유사               | 선형 모델, 출력은 확률 값         |
| 손실 함수        | MSE (Mean Squared Error)      | 로그 손실 함수 (Log-Loss)         |
| 출력 값          | 실수 값                       | 확률 값 (0과 1 사이)              |
| 활성화 함수      | 선형 함수                     | 시그모이드 함수                   |
| 사용 목적        | 회귀 및 분류                  | 분류 문제 (이진 분류)             |

### 결론
- **Adaline**은 주로 선형 회귀 문제에 가까우며, **Logistic Regression**은 이진 분류 문제에 적합합니다.
- **Logistic Regression**은 확률을 출력하므로 분류 문제에서 더 직관적이고 해석 가능한 방식으로 활용됩니다.
---
## Logistic Regression

```
# 필요한 라이브러리 임포트
from sklearn import datasets
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression

# 붓꽃(iris) 데이터셋 로드
iris = datasets.load_iris()

# 꽃잎의 길이와 너비(2,3번째 특성)만 사용
X = iris['data'][:, [2, 3]]

# 타겟값(품종 정보) 로드
y = iris['target']

# 데이터셋을 학습용(train)과 테스트용(test)으로 분할
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X,        # 입력 데이터 (꽃잎 길이, 너비)
    y,        # 타겟값 (품종)
    train_size=0.7,   # 70%를 학습 데이터로 사용
    random_state=1,   # 재현성을 위한 난수 고정
    stratify=y        # 계층적 샘플링 (각 클래스 비율 유지)
)

# 데이터 표준화 (Standardization)
from sklearn.preprocessing import StandardScaler

sc = StandardScaler()  # 표준화 스케일러 객체 생성

sc.fit(X_train)  # 학습 데이터의 평균과 표준편차 계산

# 학습 데이터와 테스트 데이터 표준화 적용
X_train_std = sc.transform(X_train)
X_test_std = sc.transform(X_test)

# 퍼셉트론(Perceptron) 모델 적용
from sklearn.linear_model import Perceptron

ppn = Perceptron(eta0=0.1, random_state=1)  # 학습률(eta0)을 0.1로 설정

ppn.fit(X_train_std, y_train)  # 모델 학습 수행

# Logistic Regression(로지스틱 회귀) 적용한 붓꽃 품종 예측
lr = LogisticRegression(
    C=100,            # 규제 강도 (C가 클수록 규제가 약해짐)
    solver='lbfgs',   # 최적화 알고리즘 선택
    multi_class='ovr' # 다중 클래스 분류 방식 (One-vs-Rest)
)

# 로지스틱 회귀 모델 학습
lr.fit(X_train_std, y_train)

# 테스트 데이터에 대한 예측값 추출
y_pred = lr.predict(X_test_std)

# 정확도 평가
from sklearn.metrics import accuracy_score

# 정확도(Accuracy) 계산
acc = accuracy_score(y_pred, y_test)

# 정확도 출력
print(f'정확도(accuracy_score) : {acc:.3f}')
```
![image](https://github.com/user-attachments/assets/8c4aba3a-e71b-42d8-bbe9-1db8098df809)

```
# 필요한 라이브러리 임포트
from sklearn import datasets
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression

# 붓꽃(iris) 데이터셋 로드
iris = datasets.load_iris()

# 꽃잎의 길이와 너비(2,3번째 특성)만 사용
X = iris['data'][:, [2, 3]]

# 타겟값(품종 정보) 로드
y = iris['target']

# 데이터셋을 학습용(train)과 테스트용(test)으로 분할
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X,        # 입력 데이터 (꽃잎 길이, 너비)
    y,        # 타겟값 (품종)
    train_size=0.7,   # 70%를 학습 데이터로 사용
    random_state=1,   # 재현성을 위한 난수 고정
    stratify=y        # 계층적 샘플링 (각 클래스 비율 유지)
)

# 데이터 표준화 (Standardization)
from sklearn.preprocessing import StandardScaler

sc = StandardScaler()  # 표준화 스케일러 객체 생성

sc.fit(X_train)  # 학습 데이터의 평균과 표준편차 계산

# 학습 데이터와 테스트 데이터 표준화 적용
X_train_std = sc.transform(X_train)
X_test_std = sc.transform(X_test)

# 퍼셉트론(Perceptron) 모델 적용
from sklearn.linear_model import Perceptron

ppn = Perceptron(eta0=0.1, random_state=1)  # 학습률(eta0)을 0.1로 설정

ppn.fit(X_train_std, y_train)  # 모델 학습 수행

# Logistic Regression(로지스틱 회귀) 적용한 붓꽃 품종 예측
lr = LogisticRegression(
    C=100,            # 규제 강도 (C가 클수록 규제가 약해짐)
    solver='lbfgs',   # 최적화 알고리즘 선택
    multi_class='ovr' # 다중 클래스 분류 방식 (One-vs-Rest)
)

# 로지스틱 회귀 모델 학습
lr.fit(X_train_std, y_train)

# 테스트 데이터에 대한 예측값 추출
y_pred = lr.predict(X_test_std)

# 정확도 평가
from sklearn.metrics import accuracy_score

# 정확도(Accuracy) 계산
acc = accuracy_score(y_pred, y_test)


# 좋은 기능 : 예측 확률을 확인할 수 있다.
print('='*50)
print(X_test_std[:3,:]) # 테스트 데이터 3개 출력
print('='*50)
print(y_pred[:3]) 
print('='*50)
lr.predict_proba(
    X_test_std[:3,:]
)

```
![image](https://github.com/user-attachments/assets/ce767213-2445-4fa4-a470-b91f43d6e1a4)

```
# 필요한 라이브러리 임포트
from sklearn import datasets
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression

# 붓꽃(iris) 데이터셋 로드
iris = datasets.load_iris()

# 꽃잎의 길이와 너비(2,3번째 특성)만 사용
X = iris['data'][:, [2, 3]]

# 타겟값(품종 정보) 로드
y = iris['target']

# 데이터셋을 학습용(train)과 테스트용(test)으로 분할
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X,        # 입력 데이터 (꽃잎 길이, 너비)
    y,        # 타겟값 (품종)
    train_size=0.7,   # 70%를 학습 데이터로 사용
    random_state=1,   # 재현성을 위한 난수 고정
    stratify=y        # 계층적 샘플링 (각 클래스 비율 유지)
)

# 데이터 표준화 (Standardization)
from sklearn.preprocessing import StandardScaler

sc = StandardScaler()  # 표준화 스케일러 객체 생성

sc.fit(X_train)  # 학습 데이터의 평균과 표준편차 계산

# 학습 데이터와 테스트 데이터 표준화 적용
X_train_std = sc.transform(X_train)
X_test_std = sc.transform(X_test)

# 퍼셉트론(Perceptron) 모델 적용
from sklearn.linear_model import Perceptron

ppn = Perceptron(eta0=0.1, random_state=1)  # 학습률(eta0)을 0.1로 설정

ppn.fit(X_train_std, y_train)  # 모델 학습 수행

# Logistic Regression(로지스틱 회귀) 적용한 붓꽃 품종 예측
lr = LogisticRegression(
    C=100,            # 규제 강도 (C가 클수록 규제가 약해짐)
    solver='lbfgs',   # 최적화 알고리즘 선택
    multi_class='ovr' # 다중 클래스 분류 방식 (One-vs-Rest)
)

# 로지스틱 회귀 모델 학습
lr.fit(X_train_std, y_train)

# 테스트 데이터에 대한 예측값 추출
y_pred = lr.predict(X_test_std)

# 정확도 평가
from sklearn.metrics import accuracy_score

# 정확도(Accuracy) 계산
acc = accuracy_score(y_pred, y_test)


# 좋은 기능 : 예측 확률을 확인할 수 있다.
print('='*50)
print(X_test_std[:3,:]) # 테스트 데이터 3개 출력
print('='*50)
print(y_pred[:3]) 
print('='*50)
# 샘플의 예측 확률중에 제일 큰값을 가지고 있는 인덱스 반환
lr.predict_proba(
    X_test_std[:3,:]
).argmax(axis=1)
```
![image](https://github.com/user-attachments/assets/2dc08c8d-8d4f-44c3-8099-71ccbbed035b)

---
## 규제(Regulation)

### 개념과 목적

개념 : 파라미터(w)의 크기를 제한하거나 페널티 부과 => Overfitting 방지

## 규제 (Regulation)

### 개념과 목적
- **개념**: 파라미터 \( w \)의 크기를 제한하거나 페널티를 부과하여 **Overfitting 방지**
- **목적**: 모델의 복잡도를 줄이고, 일반화 성능을 향상



### L1 규제와 L2 규제

#### 1. L1 규제 (Lasso, Least Absolute Shrinkage and Selection Operator)
- 손실 함수에 **가중치 절댓값의 합**을 추가하는 방식
- **정규화 항**:
  
  $$
  \[
  \lambda \sum |w|
  \]
  $$
  
- **특징**:
  - 일부 가중치를 0으로 만들어 **특성 선택(feature selection)** 효과 제공
  - 희소 모델(Sparse Model)을 생성하여 중요하지 않은 특성을 제거 가능
  - 절댓값을 사용하기 때문에 미분 불가능한 점이 있어 최적화가 어려울 수 있음



#### 2. L2 규제 (Ridge Regression)
- 손실 함수에 **가중치의 제곱합**을 추가하는 방식
- **정규화 항**:
  
  $$
  \[
  \lambda \sum w^2
  \]
  $$
  
- **특징**:
  - 가중치를 0으로 만들지 않고, 모든 특성을 작게 조정하여 **모델의 복잡도를 낮춤**
  - 높은 다중공선성을 가진 데이터에서 성능을 향상 가능
  - 미분 가능하여 최적화가 L1 규제보다 용이함



#### L1 vs L2 규제 비교

|  | **L1 규제 (Lasso)** | **L2 규제 (Ridge)** |
|---|---|---|
| **정규화 항** | \( \lambda \sum |w| \) | \( \lambda \sum w^2 \) |
| **효과** | 특성 선택 가능 (희소성 유지) | 가중치를 작게 유지 (모든 변수 사용) |
| **0이 되는 가중치** | 일부 가중치 0 가능 | 0이 되는 가중치 없음 |
| **계산 효율성** | 최적화 어려움 (비미분점 존재) | 최적화 쉬움 (미분 가능) |
| **적용 분야** | 특성 선택이 중요한 경우 | 다중공선성이 높은 경우 |

#### 엘라스틱넷 (Elastic Net)

엘라스틱넷(Elastic Net)은 **릿지 회귀 (L2 정규화)** 와 **라소 회귀 (L1 정규화)** 를 결합한 선형 회귀 모델의 정규화 기법입니다. L1과 L2 정규화를 모두 적용하여 다중공선성을 완화하고, 특성 선택(feature selection)을 할 수 있습니다.

##### 비용 함수 (Loss Function)
엘라스틱넷의 비용 함수는 다음과 같이 표현할 수 있습니다:

$$
\[
J(w) = \text{Loss}(w) + \lambda_1 \sum |\mathbf{w}| + \lambda_2 \sum \mathbf{w}^2
\]
$$

$$
\text{Loss}(w) : \text{원래의 손실 함수 (예: 평균 제곱 오차, MSE)}
$$

$$
\lambda_1 \sum |\mathbf{w}| : \text{L1 정규화(라소) → 일부 가중치를 0으로 만들어 변수 선택}
$$

$$
\lambda_2 \sum \mathbf{w}^2 : \text{L2 정규화(릿지) → 모든 가중치를 줄여 다중공선성 완화}
$$


## 엘라스틱넷의 일반적인 비용 함수
엘라스틱넷의 비용 함수는 L1과 L2 정규화 비율을 조정할 수 있는 형태로 작성됩니다:

$$
\[
J(w) = \text{Loss}(w) + \gamma \left( \alpha \sum |\mathbf{w}| + (1 - \alpha) \sum \mathbf{w}^2 \right)
\]
$$

---
## SVM (Support Vector Machine) 설명

SVM(Support Vector Machine)은 지도 학습 알고리즘 중 하나로, 주로 분류와 회귀 문제를 해결하는 데 사용됩니다. 기본적으로 SVM은 데이터를 분류하기 위해 두 클래스 간의 경계(결정 경계)를 찾는 방법을 제공합니다. 이 경계는 데이터가 가장 잘 구분될 수 있도록 하며, SVM은 이 경계를 **최대 마진**으로 설정하여 최적화합니다. 

### SVM의 기본 아이디어
- SVM은 데이터를 n차원 공간에 맵핑하고, 각 클래스의 데이터를 **하이퍼플레인(hyperplane)**이라는 결정 경계로 나누려고 합니다.
- 최적의 하이퍼플레인은 두 클래스 사이의 간격을 최대화하는 방향으로 선택됩니다. 이 간격을 **마진(margin)**이라고 하며, SVM은 이 마진을 최대화하려고 합니다.
- 마진을 최대화하는 하이퍼플레인은 **서포트 벡터**에 의해 정의되며, 서포트 벡터는 경계와 가장 가까운 데이터 포인트입니다.

### 커널(Kernel) 기법
SVM은 선형 분리가 불가능한 데이터에 대해서도 잘 동작하도록 커널 함수를 사용합니다.

기본적으로 SVM은 데이터를 선형적으로 분리하려고 하지만, 데이터가 비선형적으로 분리되어 있을 경우, 이를 해결하기 위해 **커널 기법**을 사용하여 데이터를 더 높은 차원으로 변환한 후, 그 차원에서 선형 분리가 가능하도록 만듭니다.



#### 주요 커널 함수
1. **선형 커널 (Linear Kernel)**: 데이터가 원래 차원에서 선형적으로 분리될 때 사용합니다.
2. **다항 커널 (Polynomial Kernel)**: 데이터가 다항식 형태로 분리될 수 있을 때 사용합니다.
3. **가우시안 커널 (RBF 커널 (Radial Basis Function Kernel))**: 가장 많이 사용되는 커널로, 비선형 데이터를 고차원으로 매핑하여 선형 분리가 가능하도록 만듭니다. 사용을 많이 하고 성능이 좋다.
4. **시그모이드 커널 (Sigmoid Kernel)**: 신경망의 활성화 함수처럼 동작하는 커널입니다.

#### 커널 트릭
커널 트릭은 데이터를 명시적으로 고차원으로 변환하지 않고, 커널 함수를 사용하여 내적을 계산함으로써 고차원에서의 선형 분리를 가능하게 합니다. 이렇게 하면 계산 효율성을 높일 수 있습니다.

### SVM의 장점
- **높은 정확도**: 특히 고차원 데이터에서 뛰어난 성능을 보입니다.
- **일반화 능력**: 마진을 최대화하여 과적합을 방지합니다.

### SVM의 단점
- **훈련 시간**: 데이터가 많을 경우 학습 시간이 길어질 수 있습니다.
- **커널 선택**: 커널 함수와 하이퍼파라미터 튜닝이 중요합니다.

```
# 필요한 라이브러리 임포트
from sklearn import datasets
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression

# 붓꽃(iris) 데이터셋 로드
iris = datasets.load_iris()

# 꽃잎의 길이와 너비(2,3번째 특성)만 사용
X = iris['data'][:, [2, 3]]

# 타겟값(품종 정보) 로드
y = iris['target']

# 데이터셋을 학습용(train)과 테스트용(test)으로 분할
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X,        # 입력 데이터 (꽃잎 길이, 너비)
    y,        # 타겟값 (품종)
    train_size=0.7,   # 70%를 학습 데이터로 사용
    random_state=1,   # 재현성을 위한 난수 고정
    stratify=y        # 계층적 샘플링 (각 클래스 비율 유지)
)

# 데이터 표준화 (Standardization)
from sklearn.preprocessing import StandardScaler

sc = StandardScaler()  # 표준화 스케일러 객체 생성

sc.fit(X_train)  # 학습 데이터의 평균과 표준편차 계산

# 학습 데이터와 테스트 데이터 표준화 적용
X_train_std = sc.transform(X_train)
X_test_std = sc.transform(X_test)

# SVC 적용해서 분류

from sklearn.svm import SVC

# 데이터 확인
print(f'X_train_std의 shape : {X_train_std.shape}') # 피쳐는 2개
print(f'X_test_std의 shape : {X_test_std.shape}')
print(f'y_train의 bincount : {np.bincount(y_train)}') #다중 분류, 3개 품종 예측 모델

# SVC 오브젝트 생성
svm= SVC(
    kernel='linear' # 데이터가 원래 차원에서 선형적으로 분리될 때 사용합니다.
    ,C=0.1 # 규제
    ,random_state=1 # 시드 고정 => 랜덤 값들이 고정된다
)
print(f'svm : {svm}')

# 학습 : 마진이 최대인 결정경계를 찾는다 => kernel='linear'이기 때문에 직선으로
svm.fit(X_train_std,y_train)

# 예측값 추출 : 테스트 데이터로 처리
y_pred=svm.predict(
    X_test_std
)

# 정확도 출력
from sklearn.metrics import accuracy_score
print(f'정확도 :{accuracy_score(y_pred,y_test):.3f}')
```
![image](https://github.com/user-attachments/assets/323570ac-b918-4b0d-95d3-fb066dbf1538)

```
# 필요한 라이브러리 임포트
from sklearn import datasets
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression

# 붓꽃(iris) 데이터셋 로드
iris = datasets.load_iris()

# 꽃잎의 길이와 너비(2,3번째 특성)만 사용
X = iris['data'][:, [2, 3]]

# 타겟값(품종 정보) 로드
y = iris['target']

# 데이터셋을 학습용(train)과 테스트용(test)으로 분할
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X,        # 입력 데이터 (꽃잎 길이, 너비)
    y,        # 타겟값 (품종)
    train_size=0.7,   # 70%를 학습 데이터로 사용
    random_state=1,   # 재현성을 위한 난수 고정
    stratify=y        # 계층적 샘플링 (각 클래스 비율 유지)
)

# 데이터 표준화 (Standardization)
from sklearn.preprocessing import StandardScaler

sc = StandardScaler()  # 표준화 스케일러 객체 생성

sc.fit(X_train)  # 학습 데이터의 평균과 표준편차 계산

# 학습 데이터와 테스트 데이터 표준화 적용
X_train_std = sc.transform(X_train)
X_test_std = sc.transform(X_test)

# SVC 적용해서 분류

from sklearn.svm import SVC

# 데이터 확인
print(f'X_train_std의 shape : {X_train_std.shape}') # 피쳐는 2개
print(f'X_test_std의 shape : {X_test_std.shape}')
print(f'y_train의 bincount : {np.bincount(y_train)}') #다중 분류, 3개 품종 예측 모델

# SVC 오브젝트 생성
svm= SVC(
    kernel='rbf' 
    ,C=0.1 # 규제
    ,random_state=1 # 시드 고정 => 랜덤 값들이 고정된다
    , gamma=0.2 # rbf일때만 쓸수 있다
)
print(f'svm : {svm}')


svm.fit(X_train_std,y_train)

# 예측값 추출 : 테스트 데이터로 처리
y_pred=svm.predict(
    X_test_std
)

# 정확도 출력
from sklearn.metrics import accuracy_score
print(f'정확도 :{accuracy_score(y_pred,y_test):.3f}')
```
![image](https://github.com/user-attachments/assets/0d1432e3-c8e9-4464-964e-a38360cb89e5)

```
# 필요한 라이브러리 임포트
from sklearn import datasets
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression

# 붓꽃(iris) 데이터셋 로드
iris = datasets.load_iris()

# 꽃잎의 길이와 너비(2,3번째 특성)만 사용
X = iris['data'][:, [2, 3]]

# 타겟값(품종 정보) 로드
y = iris['target']

# 데이터셋을 학습용(train)과 테스트용(test)으로 분할
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X,        # 입력 데이터 (꽃잎 길이, 너비)
    y,        # 타겟값 (품종)
    train_size=0.7,   # 70%를 학습 데이터로 사용
    random_state=1,   # 재현성을 위한 난수 고정
    stratify=y        # 계층적 샘플링 (각 클래스 비율 유지)
)

# 데이터 표준화 (Standardization)
from sklearn.preprocessing import StandardScaler

sc = StandardScaler()  # 표준화 스케일러 객체 생성

sc.fit(X_train)  # 학습 데이터의 평균과 표준편차 계산

# 학습 데이터와 테스트 데이터 표준화 적용
X_train_std = sc.transform(X_train)
X_test_std = sc.transform(X_test)

# SVC 적용해서 분류

from sklearn.svm import SVC

# 데이터 확인
print(f'X_train_std의 shape : {X_train_std.shape}') # 피쳐는 2개
print(f'X_test_std의 shape : {X_test_std.shape}')
print(f'y_train의 bincount : {np.bincount(y_train)}') #다중 분류, 3개 품종 예측 모델

# SVC 오브젝트 생성
svm= SVC(
    kernel='rbf' 
    ,C=1 # 규제
    ,random_state=1 # 시드 고정 => 랜덤 값들이 고정된다
    , gamma=100 # rbf일때만 쓸수 있다
)
print(f'svm : {svm}')


svm.fit(X_train_std,y_train)

# 예측값 추출 : 테스트 데이터로 처리
y_pred=svm.predict(
    X_test_std
)

# 정확도 출력
from sklearn.metrics import accuracy_score
print(f'정확도 :{accuracy_score(y_pred,y_test):.3f}')
```
![image](https://github.com/user-attachments/assets/93bf48a7-782c-4d08-bbe1-a8bae4d84daa)

분꽃데이터에서 오버피팅이 발생하여 정확도가 줄어들었다.
 - 훈련 데이터에서는 잘 맞지만 본 적 없는 데이터에서는 일반화 오차가 높다
 - 즉, gamma가 커지면 오버피팅이 발생 할 수 있다.

### RGB (Radial Basis Function) 장단점

#### 장점
- 데이터 차원이 높은 경우에도 잘 작동한다.
- Gamma로 overfitting을 제어할 수 있다.
- 모델이 간결하다: 소수의 벡터만으로 결정 경계를 정의할 수 있다.

#### 단점
- 하이퍼파라미터(C, gamma) 설정이 민감하다: 모델의 성능은 주로 C와 gamma 값에 따라 달라지며, 잘못 설정하면 과적합(overfitting)이나 과소적합(underfitting) 문제가 발생할 수 있다.
- 큰 데이터셋에서 계산 복잡도가 증가한다: 훈련 데이터가 많거나 차원이 높으면 계산 시간이 길어질 수 있다.
- 해석의 어려움: RGB 커널은 비선형 변환을 사용하므로 모델의 결정 경계를 해석하기 어렵다.
- 메모리 사용량이 많을 수 있다: 큰 데이터셋을 처리할 때 메모리 요구 사항이 높아질 수 있다.
- noise 데이터에 상당히 민감하다

---
## 결정 트리 학습

```
from sklearn import datasets
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression

# 붓꽃(iris) 데이터셋 로드
iris = datasets.load_iris()

X = iris['data'][:, [2, 3]]
y = iris['target']

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X,        # 입력 데이터 (꽃잎 길이, 너비)
    y,        # 타겟값 (품종)
    train_size=0.7,   # 70%를 학습 데이터로 사용
    random_state=1,   # 재현성을 위한 난수 고정
    stratify=y        # 계층적 샘플링 (각 클래스 비율 유지)
)

from sklearn.preprocessing import StandardScaler

sc = StandardScaler()  
sc.fit(X_train) 

X_train_std = sc.transform(X_train)
X_test_std = sc.transform(X_test)

# 결정트리 사용한 예측 모델
from sklearn.tree import DecisionTreeClassifier

dt =DecisionTreeClassifier(
    criterion='gini'
    , max_depth=4
    ,random_state=1
)
# 학습
dt.fit(X_train,y_train)

# 예측
y_pred=dt.predict(X_test_std)

#정확도
from sklearn.metrics import accuracy_score
print(f'정확도 : {accuracy_score(y_pred,y_test):.3f}')
```
![image](https://github.com/user-attachments/assets/e33d03bd-a545-4a34-800b-567597ddc935)

```
from sklearn import datasets
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression

# 붓꽃(iris) 데이터셋 로드
iris = datasets.load_iris()

X = iris['data'][:, [2, 3]]
y = iris['target']

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X,        # 입력 데이터 (꽃잎 길이, 너비)
    y,        # 타겟값 (품종)
    train_size=0.7,   # 70%를 학습 데이터로 사용
    random_state=1,   # 재현성을 위한 난수 고정
    stratify=y        # 계층적 샘플링 (각 클래스 비율 유지)
)

from sklearn.preprocessing import StandardScaler

sc = StandardScaler()  
sc.fit(X_train) 

X_train_std = sc.transform(X_train)
X_test_std = sc.transform(X_test)

# 결정트리 사용한 예측 모델
from sklearn.tree import DecisionTreeClassifier

dt =DecisionTreeClassifier(
    criterion='gini'
    , max_depth=4
    ,random_state=1
)
# 학습
dt.fit(X_train,y_train)

# 예측
y_pred=dt.predict(X_test_std)

#정확도
from sklearn.metrics import accuracy_score
print(f'정확도 : {accuracy_score(y_pred,y_test):.3f}')

from sklearn import tree

feature_names=[
    'Sepal length'
    ,'Sepal Width'
    ,'Petal length'
    ,'Petal Width'
]

tree.plot_tree(
    dt
    ,feature_names=feature_names
    ,filled=True
)

plt.show()
```
![image](https://github.com/user-attachments/assets/89dbcc45-9c23-4bb5-847f-266dbbdd34ed)

```
from sklearn import datasets
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression

# 붓꽃(iris) 데이터셋 로드
iris = datasets.load_iris()

X = iris['data'][:, [2, 3]]
y = iris['target']

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X,        # 입력 데이터 (꽃잎 길이, 너비)
    y,        # 타겟값 (품종)
    train_size=0.7,   # 70%를 학습 데이터로 사용
    random_state=1,   # 재현성을 위한 난수 고정
    stratify=y        # 계층적 샘플링 (각 클래스 비율 유지)
)

from sklearn.preprocessing import StandardScaler

sc = StandardScaler()  
sc.fit(X_train) 

X_train_std = sc.transform(X_train)
X_test_std = sc.transform(X_test)

# 결정트리 사용한 예측 모델
from sklearn.tree import DecisionTreeClassifier

dt =DecisionTreeClassifier(
    criterion='gini'
    , max_depth=10
    ,random_state=1
)
# 학습
dt.fit(X_train,y_train)

# 예측
y_pred=dt.predict(X_test_std)

#정확도
from sklearn.metrics import accuracy_score
print(f'정확도 : {accuracy_score(y_pred,y_test):.3f}')

from sklearn import tree

feature_names=[
    'Sepal length'
    ,'Sepal Width'
    ,'Petal length'
    ,'Petal Width'
]

tree.plot_tree(
    dt
    ,feature_names=feature_names
    ,filled=True
)

plt.show()
```
![image](https://github.com/user-attachments/assets/927cfd91-eb29-4596-922f-8544ea8cc062)

---
## 랜덤 포레스트로 여러 개의 결정 트리 연결

### 배깅(Bagging) vs 부스팅(Boosting)

배깅과 부스팅은 둘 다 앙상블 학습 기법으로, 여러 개의 모델을 결합하여 성능을 향상시키는 방법입니다. 그러나 두 기법은 학습 방식과 예측 방법에서 큰 차이가 있습니다.

#### 1. 배깅 (Bagging, Bootstrap Aggregating)

- **목표**: 여러 개의 모델을 독립적으로 학습시켜 예측을 평균 내거나 다수결로 최종 예측을 결정합니다.
- **과정**:
  - 원본 데이터를 여러 번 **부트스트랩(복원 샘플링)**하여 여러 개의 서로 다른 훈련 데이터를 생성합니다.
  - 각 훈련 데이터로 개별 모델을 학습시킵니다.
  - 각 모델의 예측 결과를 합쳐서 최종 예측을 만듭니다 (회귀의 경우 평균, 분류의 경우 다수결).
- **예시**: 랜덤 포레스트 (Random Forest)
- **특징**:
  - 모델들이 독립적으로 학습되므로 **병렬 처리**에 유리합니다.
  - **과적합**을 줄이는 데 효과적입니다.

#### 2. 부스팅 (Boosting)

- **목표**: 여러 모델을 **순차적으로 학습**시키며, 이전 모델에서 잘못 예측한 데이터에 가중치를 두어 후속 모델이 그 부분을 더 잘 맞추도록 학습시킵니다.
- **과정**:
  - 첫 번째 모델을 학습시키고, 그 모델의 예측 오류에 가중치를 두어 **두 번째 모델**을 학습시킵니다.
  - 이 과정은 **순차적으로 진행**되며, 각 모델은 이전 모델의 약점을 보완하는 방향으로 학습합니다.
  - 최종 예측은 모든 모델의 예측을 **가중합**하여 도출합니다.
- **예시**: AdaBoost, Gradient Boosting, XGBoost, LightGBM
- **특징**:
  - 모델들이 **순차적으로 학습**되므로 **병렬 처리**에는 불리합니다.
  - **약한 학습기(weak learner)**를 사용하여 성능을 점진적으로 향상시킵니다.
  - 과적합에 강하지만, 과도한 반복으로 **과적합**이 발생할 수 있으므로 **조기 종료**가 필요할 수 있습니다.

#### 차이점 요약

| 특징                | 배깅 (Bagging)                        | 부스팅 (Boosting)                     |
|-------------------|-------------------------------------|-------------------------------------|
| **학습 방식**        | 모델들이 독립적으로 학습            | 모델들이 순차적으로 학습            |
| **목표**             | 모델 간 다양성 증가, 과적합 방지     | 모델 성능 점진적 향상               |
| **결과 합성 방식**     | 평균/다수결                          | 가중합                              |
| **병렬 처리**        | 가능                                 | 불가능                              |
| **과적합 방지**      | 효과적                               | 과도한 반복 시 과적합 가능          |

##### 결론

- **배깅**은 모델들이 독립적으로 학습되며 과적합을 줄이는 데 유리하고 병렬화에 적합합니다.
- **부스팅**은 성능 향상에 더 효과적이지만 계산 자원이 많이 들며, 순차적으로 학습하여 모델 간의 오류를


```
from sklearn import datasets
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression

# 붓꽃(iris) 데이터셋 로드
iris = datasets.load_iris()

# 꽃잎 길이와 꽃잎 너비 데이터만 추출 (두 번째와 세 번째 특성)
X = iris['data'][:, [2, 3]]
# 타겟값 (품종 정보)
y = iris['target']

# 데이터를 학습 데이터와 테스트 데이터로 분할
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    X,        # 입력 데이터 (꽃잎 길이, 너비)
    y,        # 타겟값 (품종)
    train_size=0.7,   # 70%를 학습 데이터로 사용
    random_state=1,   # 재현성을 위한 난수 고정
    stratify=y        # 계층적 샘플링 (각 클래스 비율 유지)
)

# 데이터 표준화 (평균이 0이고 분산이 1이 되도록 변환)
from sklearn.preprocessing import StandardScaler

sc = StandardScaler()  
# 학습 데이터를 사용하여 평균과 표준편차 계산
sc.fit(X_train) 

# 학습 데이터와 테스트 데이터를 표준화
X_train_std = sc.transform(X_train)
X_test_std = sc.transform(X_test)

# 랜덤 포레스트 모델 생성
from sklearn.ensemble import RandomForestClassifier

# RandomForestClassifier 모델 초기화
forest = RandomForestClassifier(
    n_estimators=25,   # 학습기 개수 : 트리 개수 (25개의 트리를 사용)
    criterion='gini',   # 분할 기준: 지니 불순도 (Gini Impurity)
    random_state=1,      # 재현성을 위한 난수 고정
    n_jobs=2             # 2개의 CPU 코어를 사용하여 병렬 처리
)

# 학습 데이터로 모델 학습
forest.fit(X_train, y_train)

# 테스트 데이터로 예측값 추출
y_pred = forest.predict(X_test)

# 예측 정확도 출력
from sklearn.metrics import accuracy_score
print(f'정확도 : {accuracy_score(y_pred, y_test):.3f}')
```
![image](https://github.com/user-attachments/assets/d37e5d6e-40c9-438c-9f3e-26238f9e4f89)

---
## k-최근접 이웃

```
from sklearn import datasets
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression

# 붓꽃(iris) 데이터셋 로드
iris = datasets.load_iris()

# 꽃잎 길이와 꽃잎 너비 데이터만 추출 (두 번째와 세 번째 특성)
X = iris['data'][:, [2, 3]]
# 타겟값 (품종 정보)
y = iris['target']

# 데이터를 학습 데이터와 테스트 데이터로 분할
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    X,        # 입력 데이터 (꽃잎 길이, 너비)
    y,        # 타겟값 (품종)
    train_size=0.7,   # 70%를 학습 데이터로 사용
    random_state=1,   # 재현성을 위한 난수 고정
    stratify=y        # 계층적 샘플링 (각 클래스 비율 유지)
)

# 데이터 표준화 (평균이 0이고 분산이 1이 되도록 변환)
from sklearn.preprocessing import StandardScaler

sc = StandardScaler()  
# 학습 데이터를 사용하여 평균과 표준편차 계산
sc.fit(X_train) 

# 학습 데이터와 테스트 데이터를 표준화
X_train_std = sc.transform(X_train)
X_test_std = sc.transform(X_test)

# KNN(K-Nearest Neighbors) 분류기 모델 생성
from sklearn.neighbors import KNeighborsClassifier

# KNeighborsClassifier 모델 초기화
knn = KNeighborsClassifier(
    n_neighbors=3,  # k값 설정: 3개의 최근접 이웃 사용
    p=2,            # 거리 측정 방식: Minkowski 거리 (p=2는 유클리드 거리)
    metric='minkowski'  # Minkowski 거리 사용
)

# 학습 데이터로 모델 학습
knn.fit(X_train_std, y_train)

# 테스트 데이터로 예측값 추출
y_pred = knn.predict(X_test_std)

# 예측 정확도 출력
from sklearn.metrics import accuracy_score
print(f'정확도 : {accuracy_score(y_pred, y_test):.3f}')
```
![image](https://github.com/user-attachments/assets/39abb2cc-d02d-44e9-b860-711ab8a348bf)
