# 사이킷런 머신러닝

## 1. 트리 계열 (Tree-based Models)

트리 계열 알고리즘은 데이터의 특성에 따라 의사결정 트리(Decision Tree)를 구성하여 예측을 수행합니다. 주로 비선형 관계를 잘 처리할 수 있습니다.

- **Decision Tree Classifier (의사결정 트리 분류기)**  
  의사결정 트리는 데이터를 분할하여 분류를 하는 모델입니다. 각 분기점에서 데이터의 특성을 기준으로 두 가지 이상의 범주로 분할합니다. 직관적이고 해석이 쉬운 장점이 있지만, 과적합에 취약할 수 있습니다.

- **Random Forest Classifier (랜덤 포레스트)**  
  여러 개의 의사결정 트리를 생성하고, 그 예측 결과를 종합하여 최종 예측을 도출합니다. 과적합을 줄이고 예측 성능을 향상시킬 수 있습니다.

- **Gradient Boosting Classifier (그라디언트 부스팅)**  
  약한 예측기들을 결합하여 강한 예측기를 만드는 방식입니다. 각 트리가 이전 트리의 오차를 보정하면서 학습합니다.

## 2. 서포트 벡터 머신 (Support Vector Machines, SVM)

서포트 벡터 머신은 데이터의 분포를 고려하여 최적의 결정 경계를 찾는 방식으로 분류를 수행합니다. 주로 고차원 데이터에서 뛰어난 성능을 발휘합니다.

- **SVC (Support Vector Classifier)**  
  커널 트릭(kernel trick)을 사용하여 비선형 데이터를 고차원 공간으로 변환한 후 최적의 분리 경계를 찾습니다.

- **SVR (Support Vector Regression)**  
  분류가 아닌 회귀 문제를 다룰 때 사용됩니다. 데이터를 고차원 공간으로 변환한 후 회귀를 수행하여 예측값을 도출합니다.

## 3. 로지스틱 회귀 (Logistic Regression)

로지스틱 회귀는 선형 회귀 모델을 기반으로 하며, 결과가 특정 범주에 속할 확률을 예측합니다. 주로 이진 분류 문제에 많이 사용됩니다.

- **Logistic Regression (로지스틱 회귀)**  
  입력값에 대해 로지스틱 함수(sigmoid function)를 사용하여 확률값을 출력하고, 이를 기준으로 분류합니다.

- **다중 클래스 분류 (Multinomial Logistic Regression)**  
  로지스틱 회귀는 기본적으로 이진 분류지만, `softmax` 함수를 사용하여 다수의 클래스를 처리할 수 있습니다. `multi_class='ovr'` (One-vs-Rest) 또는 `multi_class='multinomial'`을 설정하여 다중 클래스 분류를 수행합니다.

## 4. K-최근접 이웃 (K-Nearest Neighbors, KNN)

KNN은 분류할 데이터의 가장 가까운 K개의 이웃을 찾아서, 그 이웃들이 속한 클래스에 따라 예측을 수행하는 모델입니다.

- **K-Nearest Neighbors (KNN)**  
  주어진 데이터와 가장 가까운 K개의 데이터를 찾고, 그들의 클래스에서 다수결을 통해 분류합니다.

## 5. 나이브 베이즈 (Naive Bayes)

나이브 베이즈는 베이즈 이론을 기반으로 한 분류 모델입니다. 각 특징이 독립적이라고 가정하고, 조건부 확률을 계산하여 분류합니다.

- **GaussianNB (가우시안 나이브 베이즈)**  
  데이터가 연속형일 때 사용되며, 각 클래스에 대해 특성 값이 가우시안 분포를 따른다고 가정합니다.

- **MultinomialNB (다항 나이브 베이즈)**  
  주로 텍스트 분류에서 사용되며, 각 클래스에 대해 특정 특성이 다항 분포를 따른다고 가정합니다.

## 6. K-평균 군집화 (K-Means Clustering)

K-Means는 분류가 아닌 비지도 학습 알고리즘입니다. 데이터들을 군집화하여 군집의 중심을 찾고, 그 중심을 기준으로 데이터를 그룹화합니다.

- **KMeans (K-평균 군집화)**  
  주어진 K개의 군집에 데이터를 분할합니다. 각 군집은 군집의 중심(centroid)을 기준으로 데이터를 그룹화합니다.
