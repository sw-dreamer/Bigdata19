# 사이킷런 머신러닝

## 1. 트리 계열 (Tree-based Models)

트리 계열 알고리즘은 데이터의 특성에 따라 의사결정 트리(Decision Tree)를 구성하여 예측을 수행합니다. 주로 비선형 관계를 잘 처리할 수 있습니다.

- **Decision Tree Classifier (의사결정 트리 분류기)**  
  의사결정 트리는 데이터를 분할하여 분류를 하는 모델입니다. 각 분기점에서 데이터의 특성을 기준으로 두 가지 이상의 범주로 분할합니다. 직관적이고 해석이 쉬운 장점이 있지만, 과적합에 취약할 수 있습니다.

- **Random Forest Classifier (랜덤 포레스트)**  
  여러 개의 의사결정 트리를 생성하고, 그 예측 결과를 종합하여 최종 예측을 도출합니다. 과적합을 줄이고 예측 성능을 향상시킬 수 있습니다.

- **Gradient Boosting Classifier (그라디언트 부스팅)**  
  약한 예측기들을 결합하여 강한 예측기를 만드는 방식입니다. 각 트리가 이전 트리의 오차를 보정하면서 학습합니다.

## 2. 서포트 벡터 머신 (Support Vector Machines, SVM)

서포트 벡터 머신은 데이터의 분포를 고려하여 최적의 결정 경계를 찾는 방식으로 분류를 수행합니다. 주로 고차원 데이터에서 뛰어난 성능을 발휘합니다.

- **SVC (Support Vector Classifier)**  
  커널 트릭(kernel trick)을 사용하여 비선형 데이터를 고차원 공간으로 변환한 후 최적의 분리 경계를 찾습니다.

- **SVR (Support Vector Regression)**  
  분류가 아닌 회귀 문제를 다룰 때 사용됩니다. 데이터를 고차원 공간으로 변환한 후 회귀를 수행하여 예측값을 도출합니다.

## 3. 로지스틱 회귀 (Logistic Regression)

로지스틱 회귀는 선형 회귀 모델을 기반으로 하며, 결과가 특정 범주에 속할 확률을 예측합니다. 주로 이진 분류 문제에 많이 사용됩니다.

- **Logistic Regression (로지스틱 회귀)**  
  입력값에 대해 로지스틱 함수(sigmoid function)를 사용하여 확률값을 출력하고, 이를 기준으로 분류합니다.

- **다중 클래스 분류 (Multinomial Logistic Regression)**  
  로지스틱 회귀는 기본적으로 이진 분류지만, `softmax` 함수를 사용하여 다수의 클래스를 처리할 수 있습니다. `multi_class='ovr'` (One-vs-Rest) 또는 `multi_class='multinomial'`을 설정하여 다중 클래스 분류를 수행합니다.

## 4. K-최근접 이웃 (K-Nearest Neighbors, KNN)

KNN은 분류할 데이터의 가장 가까운 K개의 이웃을 찾아서, 그 이웃들이 속한 클래스에 따라 예측을 수행하는 모델입니다.

- **K-Nearest Neighbors (KNN)**  
  주어진 데이터와 가장 가까운 K개의 데이터를 찾고, 그들의 클래스에서 다수결을 통해 분류합니다.

## 5. 나이브 베이즈 (Naive Bayes)

나이브 베이즈는 베이즈 이론을 기반으로 한 분류 모델입니다. 각 특징이 독립적이라고 가정하고, 조건부 확률을 계산하여 분류합니다.

- **GaussianNB (가우시안 나이브 베이즈)**  
  데이터가 연속형일 때 사용되며, 각 클래스에 대해 특성 값이 가우시안 분포를 따른다고 가정합니다.

- **MultinomialNB (다항 나이브 베이즈)**  
  주로 텍스트 분류에서 사용되며, 각 클래스에 대해 특정 특성이 다항 분포를 따른다고 가정합니다.

## 6. K-평균 군집화 (K-Means Clustering)

K-Means는 분류가 아닌 비지도 학습 알고리즘입니다. 데이터들을 군집화하여 군집의 중심을 찾고, 그 중심을 기준으로 데이터를 그룹화합니다.

- **KMeans (K-평균 군집화)**  
  주어진 K개의 군집에 데이터를 분할합니다. 각 군집은 군집의 중심(centroid)을 기준으로 데이터를 그룹화합니다.

---

## 혼동 행렬 (Confusion Matrix)

혼동 행렬(Confusion Matrix)은 분류 모델의 성능을 평가하는 데 사용되는 도구로, 예측한 결과와 실제 값을 비교하여 모델의 정확도를 평가합니다. 혼동 행렬은 주로 이진 분류 문제에서 사용되지만, 다중 클래스 분류에서도 적용할 수 있습니다.

### 혼동 행렬의 구성 요소

이진 분류 문제를 예로 들어 혼동 행렬의 구성 요소를 설명하겠습니다. 이진 분류에서는 두 클래스(예: 긍정 클래스와 부정 클래스)가 있습니다.

혼동 행렬은 2x2 형태로 나타내며, 다음과 같이 구성됩니다:

|                | **예측: 긍정** | **예측: 부정** |
|----------------|----------------|----------------|
| **실제: 긍정** | True Positive (TP)  | False Negative (FN) |
| **실제: 부정** | False Positive (FP) | True Negative (TN)  |

#### 각 항목의 의미
- **True Positive (TP)**: 모델이 긍정 클래스를 긍정 클래스로 정확하게 예측한 경우.
- **False Positive (FP)**: 모델이 부정 클래스를 긍정 클래스로 잘못 예측한 경우 (종종 'Type I error'라고 불림).
- **False Negative (FN)**: 모델이 긍정 클래스를 부정 클래스로 잘못 예측한 경우 (종종 'Type II error'라고 불림).
- **True Negative (TN)**: 모델이 부정 클래스를 부정 클래스로 정확하게 예측한 경우.

### 혼동 행렬을 활용한 평가 지표

혼동 행렬을 통해 여러 가지 성능 지표를 계산할 수 있습니다:

1. **정확도 (Accuracy)**
전체 예측 중에서 맞게 예측한 비율입니다.

Accuracy = (TP + TN) / (TP + TN + FP + FN)

2. **정밀도 (Precision)**  
모델이 긍정 클래스라고 예측한 것 중에서 실제로 긍정 클래스인 비율입니다.

Precision = TP / (TP + FP)


3. **재현율 (Recall)**  
실제 긍정 클래스 중에서 모델이 긍정 클래스라고 정확히 예측한 비율입니다.

Recall = TP / (TP + FN)

4. **F1-Score**  
정밀도와 재현율의 조화 평균으로, 불균형 데이터셋에서 중요한 지표입니다.

F1-Score = 2 * (Precision * Recall) / (Precision + Recall)

5. **특이도 (Specificity)**  
실제 부정 클래스 중에서 모델이 부정 클래스라고 정확히 예측한 비율입니다.

Specificity = TN / (TN + FP)

### ROC (Receiver Operating Characteristic) Curve
ROC 곡선은 모델의 True Positive Rate (재현율, Recall)과 False Positive Rate (1 - 특이도, Specificity)을 비교하여 성능을 평가하는 곡선입니다.

- **True Positive Rate (TPR)**: 재현율 (Recall)

  TPR = TP / (TP + FN)

- **False Positive Rate (FPR)**: 1 - 특이도 (1 - Specificity)

  FPR = FP / (FP + TN)


### AUC (Area Under the Curve)
AUC는 ROC 곡선 아래의 면적을 의미합니다. AUC 값은 모델이 얼마나 잘 분류하는지에 대한 성능 지표로, 1에 가까울수록 모델의 성능이 뛰어납니다.

- **AUC**는 ROC 곡선 아래의 면적을 나타내며, 다음과 같은 기준으로 해석됩니다:
  - **AUC = 0.5**: 모델이 랜덤 추측을 하는 것과 동일
  - **AUC > 0.5**: 모델이 데이터의 패턴을 일부 인식하고 있다는 의미
  - **AUC = 1**: 모델이 완벽하게 예측하는 경우

---
## 분류 알고리즘
```
# 데이터 준비 : 분꽃 데이터
from sklearn import datasets
import numpy as np
import matplotlib.pyplot as plt

iris=datasets.load_iris() # 딕셔너리

iris
```
![image](https://github.com/user-attachments/assets/cb6db826-4f38-4809-8053-5a2d98b0a1ab)

```
# 데이터 준비 : 분꽃 데이터
from sklearn import datasets
import numpy as np
import matplotlib.pyplot as plt

iris=datasets.load_iris() # 딕셔너리

print(iris.keys())
print(np.unique(iris['target']))
```
![image](https://github.com/user-attachments/assets/daaab6a5-cfe2-49d1-8c20-25da454833cc)
