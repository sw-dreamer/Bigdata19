# 파이토치를 사용하여 심층 신경망 구현

```
# MNIST 데이터 로딩
import torch
import torchvision
from torchvision import transforms

image_path = './'
transform = transforms.Compose(
    [transforms.ToTensor()]
)

mnist_dataset = torchvision.datasets.MNIST(
    root=image_path,
    train=True,
    transform=transform,
    download=True
)

from torch.utils.data import Subset
mnist_valid_dataset = Subset(
    mnist_dataset,
    torch.arange(10000)
)
mnist_train_dataset = Subset(
    mnist_dataset, 
    torch.arange(10000, len(mnist_dataset))
)
mnist_test_dataset = torchvision.datasets.MNIST(
    root=image_path,
    train=False,
    transform=transform,
    download=False
)
```
![image](https://github.com/user-attachments/assets/cdd5b9c8-27ee-41d8-a61f-d6388efcbcd4)

```
from torch.utils.data import DataLoader

batch_size = 64
torch.manual_seed(1)
train_dl = DataLoader(mnist_train_dataset, batch_size, shuffle=True)
valid_dl = DataLoader(mnist_valid_dataset, batch_size, shuffle=False)
```
```
import torch.nn as nn

# Sequential 생성 : 계산 그래프 정의
# nn.Sequential()을 사용하여 각 레이어를 순차적으로 쌓을 수 있는 모델 객체를 생성
model = nn.Sequential()

# 첫번째 CNN 층을 추가
model.add_module(
    'conv1',  # 레이어의 이름 (모델에 출력될 때 사용)
    nn.Conv2d(  # 2D 합성곱 층 정의
        in_channels=1,  # 입력 채널 수 (흑백 이미지라 1채널)
        out_channels=32,  # 출력 채널 수 (32개의 필터를 사용하여 특징 추출)
        kernel_size=5,  # 커널 크기 (5x5 크기의 필터 사용)
        padding=2  # 패딩 (패딩을 2로 설정해 출력 크기가 입력 크기와 동일하도록 유지)
    )
)

# 첫 번째 ReLU 활성화 함수 추가
model.add_module('relu1', nn.ReLU())  # ReLU는 비선형 활성화 함수로, 음수 값을 0으로 바꾸고 양수는 그대로 둠

# 첫 번째 MaxPooling 층 추가
model.add_module('pool1', nn.MaxPool2d(kernel_size=2))  # 2x2 풀링 (이미지 크기를 절반으로 줄임)

# 두번째 CNN 층 추가
model.add_module(
    'conv2',  # 레이어 이름
    nn.Conv2d(  # 두 번째 2D 합성곱 층 정의
        in_channels=32,  # 입력 채널 수 (이전 레이어의 출력이 32개의 채널로 나오므로 32)
        out_channels=64,  # 출력 채널 수 (64개의 필터를 사용하여 더 많은 특징 추출)
        kernel_size=5,  # 커널 크기 (5x5 크기의 필터 사용)
        padding=2  # 패딩 (입력 이미지 크기와 동일하게 유지)
    )
)

# 두 번째 ReLU 활성화 함수 추가
model.add_module('relu2', nn.ReLU())  # 두 번째 ReLU 활성화 함수로 비선형성을 추가

# 두 번째 MaxPooling 층 추가
model.add_module('pool2', nn.MaxPool2d(kernel_size=2))  # 2x2 풀링 (이미지 크기를 또 절반으로 줄임)

model
```
![image](https://github.com/user-attachments/assets/d7e3b559-8954-4b05-b007-623fa9919527)

```
x=torch.ones(
    (4,1,28,28)
)
print(f'x shape : {x.shape}')
print(f' model(x) shape : {model(x).shape}')
```
![image](https://github.com/user-attachments/assets/868a15eb-f7bc-40dc-be6f-6fd588adc32f)

```
model.add_module('flatten',nn.Flatten())
model
```
![image](https://github.com/user-attachments/assets/4f1469fd-a0ad-4e23-9af2-88ce0eb4cefd)

```
x=torch.ones((4,1,28,28))
model(x).shape
```
![image](https://github.com/user-attachments/assets/6b8227a0-4943-4bb4-a040-116fe159cd40)

```
# 분류에 사용되는 신형 신경망을 2개 추가
model.add_module('fc1',nn.Linear(in_features=3136,out_features=1024))
model.add_module('relu3',nn.ReLU())

'''
드롭아웃(Dropout)은 신경망 모델을 훈련할 때 **과적합(overfitting)**을 방지하는 중요한 기법입니다. 과적합은 모델이 훈련 데이터에 너무 잘 맞추어져서 새로운 데이터(테스트 데이터)에 대한 일반화 성능이 떨어지는 현상입니다.
드롭아웃은 이를 방지하기 위해 다음과 같은 방식으로 동작합니다:

1. 랜덤하게 뉴런을 "끄는" 기법
드롭아웃은 훈련 중에 각 학습 단계에서 무작위로 뉴런을 꺼서 그 뉴런이 네트워크에서 학습을 하지 않게 만듭니다.
예를 들어, p=0.5일 경우, 훈련 중에 각 뉴런의 절반을 무작위로 드롭시킵니다. 이렇게 하면 각 학습 단계에서 서로 다른 서브넷(부분 네트워크)만 사용하여 훈련을 하게 됩니다.

2. 과적합 방지
훈련 데이터에 과도하게 적합되는 것을 방지하기 위해 드롭아웃을 사용합니다. 모델이 특정 뉴런에 의존하지 않고, 여러 뉴런들이 다양한 방식으로 학습을 하도록 유도하여 더 강건한 모델을 만듭니다.
결과적으로 모델이 훈련 데이터에 지나치게 특화되지 않고, 더 잘 일반화되는 효과를 얻을 수 있습니다.

3. 앙상블 효과
드롭아웃을 적용하면, 훈련 중에 네트워크가 여러 개의 서로 다른 네트워크처럼 작동하게 됩니다.
훈련 중에는 각 뉴런이 꺼질 때마다 네트워크의 구조가 다르게 되므로, 드롭아웃은 여러 개의 모델을 훈련시키는 것과 비슷한 효과를 냅니다.
이로 인해 앙상블 효과가 발생하고, 결과적으로 모델의 예측 성능이 향상될 수 있습니다.

4. 테스트 시 드롭아웃 적용 안 함
드롭아웃은 훈련 시에만 적용되고, 테스트 시에는 모든 뉴런을 활성화하여 모델을 평가합니다.
대신, 훈련 중에 드롭아웃을 사용하여 얻은 효과를 고려해, 테스트 시에는 모든 뉴런의 출력이 사용되며, 이를 보정하기 위해 출력값에 스케일링을 적용하기도 합니다.

예시
훈련 중: 예를 들어, 뉴런의 50%를 랜덤하게 드롭시키면, 실제로 훈련 중에 50%의 뉴런이 꺼지고 나머지 뉴런들만 사용됩니다.

테스트 중: 모든 뉴런이 활성화되며, 출력 값은 훈련 시의 평균적인 결과를 반영할 수 있도록 스케일링됩니다.
'''
# dropout은 신경망에서 규제이다.
model.add_module('dropout',nn.Dropout(p=0.5)) # nn.Dropout(p=0.5)는 훈련 중에 뉴런의 50%를 랜덤하게 "끄는" 기술로, 모델의 과적합을 방지하는 데 도움이 됩니다. p=0.5는 50%의 확률로 뉴런을 드롭아웃시키겠다는 의미입니다.
model.add_module('fc2',nn.Linear(in_features=1024,out_features=10))
model
```
![image](https://github.com/user-attachments/assets/6bede801-ffd3-4334-a784-df1edfc3c4a5)

```
x=torch.ones((4,1,28,28))
model(x).shape
```
![image](https://github.com/user-attachments/assets/72cefa86-835c-4094-9ced-9bfa017d403f)

```
model_device=next(model.parameters()).device
print(f'모델이 위치한 디바이스 : {model_device}')

# MNIST 데이터의 디바이스 확인
data_device=mnist_test_dataset.data.device
print(f'MNIST 데이터가 위치한 디바이스 : {data_device}')
```
![image](https://github.com/user-attachments/assets/e23e3f1f-16d1-4c4e-8bf7-24a5d24ca01b)

```
# model을 gpu로 보내기
device=torch.device(
    'cuda' if torch.cuda.is_available() else 'cpu'
)
print(f'현재 사용 가능한 장치 : {device}')

# 모델을 gpu로 보내기
model=model.to(device=device)

# 모델이 어디 있는지 확인
model_device=next(model.parameters()).device
print(f'모델이 위치한 디바이스 : {model_device}')
```
![image](https://github.com/user-attachments/assets/4e857d61-12b8-4d39-9e0f-eef3bec875c3)

```
# 학습을 진행하는 함수
# num_epochs, train_dl, vaild_dl, model, loss_fn, optimizer
# epoch 개수 반복 : num_epochs
# 예측값 추출 : model(학습데이터) : train_dl
# 오차 : loss_fn(예측값, 원래답데이터)
# # 최적화 : optimizer
# 1에폭마다 평가 : valid data + valid data label => valid_dl
# return 에포마다
# 학습데이터에 대한 오차, 검증 데이터 오차, 학습데이터 정확도, 검증 정확도
def train(model, num_epochs, train_dl, valid_dl, loss_fn, optimizer):
    loss_hist_train=[0] * num_epochs # cpu
    accuracy_hist_train=[0] * num_epochs # cpu
    loss_hist_valid=[0] * num_epochs # cpu
    accuracy_hist_valid=[0] * num_epochs # cpu
   
    for epoch in range(num_epochs):
        # 모델을 학습 모드로 설정
        model.train()
        for x_batch, y_batch in train_dl: # 100 , 10 : 10번 반복
            x_batch=x_batch.to(device) # gpu로 보내라
            y_batch=y_batch.to(device) # gpu로 보내라
            pred=model(x_batch) # 예측값 추출 (배치만큼)
            loss=loss_fn(pred, y_batch)
            loss.backward() # 가중치로 미분, 오차가 최소가 되는
            optimizer.step() # 가중치 수정
            optimizer.zero_grad() # 가중치 초기화
           
            # 시각화 데이터 추출
            # 전체 오차 누적 저장
            loss_hist_train[epoch] += loss.item()*y_batch.size(0)
            # 누적 정확도 : 배치사이즈만큼 나온다
            is_correct=(torch.argmax(pred,dim=1)==y_batch).float()
            accuracy_hist_train[epoch] +=is_correct.sum().cpu()
        # 1 epoch마다 평균 오차, 평균 정확도
        loss_hist_train[epoch] /= len(train_dl.dataset)
        accuracy_hist_train[epoch] /= len(train_dl.dataset)
       
        # 모델을 평가 모드로 설정: 결과 위 리스트 대입
        model.eval()
        with torch.no_grad():
            for x_batch, y_batch in valid_dl:
                x_batch=x_batch.to(device)
                y_batch=y_batch.to(device)
               
                pred=model(x_batch) # 예측값 - 원래값값
                loss=loss_fn(pred,y_batch)
                loss_hist_valid[epoch] += loss.item()*y_batch.size(0)
                is_correct = (torch.argmax(pred,dim=1)==y_batch).float()
                accuracy_hist_valid[epoch] += is_correct.sum().cpu()
                # 여기까지는 1배치만큼씩 처리
       
        loss_hist_valid[epoch] /= len(valid_dl.dataset)        
        accuracy_hist_valid[epoch] /= len(valid_dl.dataset)
       
        # 학습데이터 정확도와 검증데이터의 정확도 출력
        print(f'에포크 : {epoch+1},학습정확도 : {accuracy_hist_train[epoch]:.4f}, 검증정확도 : {accuracy_hist_valid[epoch]:.4f} ')
    return loss_hist_train, loss_hist_valid, accuracy_hist_train, accuracy_hist_valid      
```
```
loss_fn=nn.CrossEntropyLoss()
optimizer=torch.optim.Adam(model.parameters(),lr=0.001)
loss_hist_train, loss_hist_valid, acc_hist_train, acc_hist_valid=train(
    model=model
    ,num_epochs=200
    ,train_dl=train_dl
    ,valid_dl=valid_dl
    ,loss_fn=loss_fn
    ,optimizer=optimizer
)
```
![image](https://github.com/user-attachments/assets/111e1f2d-af4d-4d81-adf6-8fdaf8b16250)


