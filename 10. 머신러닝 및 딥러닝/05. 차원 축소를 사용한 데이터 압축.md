# 차원 축소를 사용한 데이터 압축

차원 축소는 고차원 데이터를 더 적은 차원으로 변환하여 중요한 정보를 유지하면서 데이터를 압축하는 기법입니다. 이를 통해 데이터의 복잡성을 줄이고 시각화, 모델 학습 등의 효율성을 높일 수 있습니다. 여기서는 주성분 분석(PCA), 선형 판별 분석(LDA), 그리고 비선형 차원 축소 방법을 다룰 것입니다.

## 1. 주성분 분석(PCA)을 통한 비지도 차원 축소

주성분 분석(PCA)은 **비지도 학습**의 대표적인 차원 축소 기법입니다. PCA는 데이터의 분산을 최대화하는 선형 변환을 찾아 데이터를 저차원으로 변환합니다. 주요 아이디어는 고차원 데이터를 선형 결합하여 더 적은 수의 주성분(Principal Component)으로 압축하는 것입니다. 이를 통해 원본 데이터의 분산을 최대한 보존하면서, 불필요한 특성(잡음)은 제거할 수 있습니다.

### PCA의 주요 단계
1. **데이터 전처리**: 평균을 0으로 맞추기 위해 데이터를 표준화합니다.
2. **공분산 행렬 계산**: 데이터 간의 관계를 이해하기 위해 공분산 행렬을 계산합니다.
3. **고유값과 고유벡터 계산**: 공분산 행렬의 고유값과 고유벡터를 계산하여 주성분을 찾습니다.
4. **주성분 선택**: 가장 큰 고유값에 해당하는 고유벡터를 선택하여 차원 축소를 합니다.

PCA는 비지도 학습이기 때문에 라벨이 없는 데이터에 적용됩니다.

## 2. 선형 판별 분석(LDA)을 통한 지도 방식의 데이터 압축

선형 판별 분석(LDA)은 **지도 학습** 기반의 차원 축소 기법으로, 클래스 간의 분리를 최적화하는 방식입니다. LDA는 데이터의 차원을 축소하면서도 각 클래스 간의 분산을 최대화하고, 클래스 내 분산을 최소화하는 새로운 축을 찾습니다.

### LDA의 주요 단계
1. **클래스 간 분산 행렬과 클래스 내 분산 행렬 계산**: 각 클래스의 데이터가 얼마나 분리되어 있는지를 계산합니다.
2. **고유값 분해**: 클래스 간 분산과 클래스 내 분산을 바탕으로 고유값 분해를 수행하여 변환 행렬을 얻습니다.
3. **주성분 선택**: 변환 행렬에서 가장 큰 고유값에 해당하는 고유벡터를 선택합니다.

LDA는 데이터에 클래스 라벨이 존재할 때 유용하며, 데이터의 차원을 축소하면서도 분류 작업의 성능을 높일 수 있습니다.

## 3. 비선형 차원 축소와 시각화

PCA나 LDA는 모두 **선형** 방법으로 차원 축소를 수행합니다. 그러나 데이터가 비선형 관계를 가질 경우, 선형 기법만으로는 충분한 압축이 어렵습니다. 이때 사용하는 기법들이 **비선형 차원 축소** 방법입니다.

### 대표적인 비선형 차원 축소 기법
- **t-SNE** (t-Distributed Stochastic Neighbor Embedding)
- **UMAP** (Uniform Manifold Approximation and Projection)
- **Isomap**

### t-SNE (t-Distributed Stochastic Neighbor Embedding)
t-SNE는 데이터 포인트 간의 거리가 비선형적으로 보존되도록 하는 차원 축소 기법입니다. 주로 데이터의 **시각화**에 사용되며, 고차원 데이터를 2D 또는 3D로 축소하여 클러스터 구조를 시각적으로 확인할 수 있게 해줍니다.

### UMAP (Uniform Manifold Approximation and Projection)
UMAP은 t-SNE와 비슷한 목적을 가지고 있지만, 더 빠르고 확장성이 좋은 방법입니다. 고차원 데이터를 저차원으로 축소할 때, 데이터의 글로벌 구조를 유지하면서 국소적 구조도 잘 보존합니다.

### Isomap
Isomap은 고차원 데이터의 **기하학적 구조**를 보존하는 비선형 차원 축소 기법입니다. 데이터 포인트 간의 최단 경로를 계산하여 비선형 구조를 학습한 후, 이를 저차원으로 투영합니다.

## 비선형 차원 축소와 시각화
비선형 차원 축소 기법들은 특히 데이터의 클러스터 구조나 복잡한 관계를 시각화할 때 유용합니다. 예를 들어, MNIST와 같은 손글씨 데이터셋에서는 t-SNE나 UMAP을 통해 각 숫자 클래스 간의 관계를 저차원 공간에서 시각적으로 확인할 수 있습니다.

## 요약
- **주성분 분석(PCA)**: 비지도 학습 방식의 차원 축소 기법으로, 데이터를 선형 변환하여 분산을 최대화합니다.
- **선형 판별 분석(LDA)**: 지도 학습 방식의 차원 축소 기법으로, 클래스 간 분산을 최대화하고 클래스 내 분산을 최소화합니다.
- **비선형 차원 축소**: t-SNE, UMAP, Isomap과 같은 기법들은 데이터의 비선형 구조를 보존하며, 주로 데이터 시각화에 활용됩니다.

이러한 방법들을 적절히 선택하여 데이터의 차원을 축소하고 중요한 정보를 유지할 수 있습니다.

---
## 주성분 추출

```
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# 주성분 추출 단계
# 1. 데이터를 표준화 처리
# 2. 공분산 행렬
# 3. 행렬분석(고유값 분해) : 고유값, 고유벡터 분해
# 4. 고유값을 내림차순 정려 => 고유백터주성분)의 순위

# 데이터 로딩
df_wine=pd.read_csv(
    '../wine.data'
    ,header=None
    ,encoding='utf-8'
)

# 데이터와 답을 분리
X,y=df_wine.iloc[:,1:].values,df_wine.iloc[:,0].values
print(f'wine data의 shape : {df_wine.shape}')
print(f'X의 shape : {X.shape}')
print(f'y의 shape : {y.shape}')

# 학습데이터와 테스트데이터 분리
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(
    X
    ,y
    ,test_size=0.3
    ,stratify=y
    ,random_state=0
)
print(f'X_train의 shape : {X_train.shape}')
print(f'X_test의 shape : {X_test.shape}')
print(f'y_train의 shape : {y_train.shape}')
print(f'y_test의 shape : {y_test.shape}')
```
![image](https://github.com/user-attachments/assets/430fa6e1-5284-46c1-a901-00202445171a)

```
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# 주성분 추출 단계
# 1. 데이터를 표준화 처리
# 2. 공분산 행렬
# 3. 행렬분석(고유값 분해) : 고유값, 고유벡터 분해
# 4. 고유값을 내림차순 정려 => 고유백터주성분)의 순위

# 데이터 로딩
df_wine=pd.read_csv(
    '../wine.data'
    ,header=None
    ,encoding='utf-8'
)

# 데이터와 답을 분리
X,y=df_wine.iloc[:,1:].values,df_wine.iloc[:,0].values
# print(f'wine data의 shape : {df_wine.shape}')
# print(f'X의 shape : {X.shape}')
# print(f'y의 shape : {y.shape}')

# 학습데이터와 테스트데이터 분리
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(
    X
    ,y
    ,test_size=0.3
    ,stratify=y
    ,random_state=0
)
# print(f'X_train의 shape : {X_train.shape}')
# print(f'X_test의 shape : {X_test.shape}')
# print(f'y_train의 shape : {y_train.shape}')
# print(f'y_test의 shape : {y_test.shape}')

# 특성을 표준화 : StandardScaler 사용
from sklearn.preprocessing import StandardScaler

# StandardScaler 오브젝트 생성
sc = StandardScaler()

# 학습 데이터에 대해 표준화 진행
X_train_std = sc.fit_transform(X_train)  # fit_transform gives standardized data

# 테스트 데이터는 학습 데이터로부터 계산된 평균과 표준편차를 사용하여 표준화
X_test_std = sc.transform(X_test)

# 공분산 행렬
cov_mat = np.cov(X_train_std.T) 
print(f'공분산\n{cov_mat}')
```
![image](https://github.com/user-attachments/assets/5d51aa5e-32c5-4d57-b5ac-3462137984fe)

```
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# 주성분 추출 단계
# 1. 데이터를 표준화 처리
# 2. 공분산 행렬
# 3. 행렬분석(고유값 분해) : 고유값, 고유벡터 분해
# 4. 고유값을 내림차순 정려 => 고유백터주성분)의 순위

# 데이터 로딩
df_wine=pd.read_csv(
    '../wine.data'
    ,header=None
    ,encoding='utf-8'
)

# 데이터와 답을 분리
X,y=df_wine.iloc[:,1:].values,df_wine.iloc[:,0].values
# print(f'wine data의 shape : {df_wine.shape}')
# print(f'X의 shape : {X.shape}')
# print(f'y의 shape : {y.shape}')

# 학습데이터와 테스트데이터 분리
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(
    X
    ,y
    ,test_size=0.3
    ,stratify=y
    ,random_state=0
)
# print(f'X_train의 shape : {X_train.shape}')
# print(f'X_test의 shape : {X_test.shape}')
# print(f'y_train의 shape : {y_train.shape}')
# print(f'y_test의 shape : {y_test.shape}')

# 특성을 표준화 : StandardScaler 사용
from sklearn.preprocessing import StandardScaler

# StandardScaler 오브젝트 생성
sc = StandardScaler()

# 학습 데이터에 대해 표준화 진행
X_train_std = sc.fit_transform(X_train)  # fit_transform gives standardized data

# 테스트 데이터는 학습 데이터로부터 계산된 평균과 표준편차를 사용하여 표준화
X_test_std = sc.transform(X_test)

# 공분산 행렬
cov_mat = np.cov(X_train_std.T) 
# print(f'공분산\n{cov_mat}')

# 고유값 분해
eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)

# 고윳값 출력
print(f'고윳값\n{eigen_vals}')
print(f'고윳벡터의 차원\n{eigen_vecs.shape}')
print(f'고유벡터\n{eigen_vecs}')
```
![image](https://github.com/user-attachments/assets/102bab5a-c4dd-4854-80d2-4fc6fcb4240a)

```
import numpy as np
import pandas as pd

# 주성분 추출 단계
# 1. 데이터를 표준화 처리
# 2. 공분산 행렬
# 3. 행렬분석(고유값 분해) : 고유값, 고유벡터 분해
# 4. 고유값을 내림차순 정려 => 고유백터주성분)의 순위

# 데이터 로딩
df_wine=pd.read_csv(
    '../wine.data'
    ,header=None
    ,encoding='utf-8'
)

# 데이터와 답을 분리
X,y=df_wine.iloc[:,1:].values,df_wine.iloc[:,0].values
# print(f'wine data의 shape : {df_wine.shape}')
# print(f'X의 shape : {X.shape}')
# print(f'y의 shape : {y.shape}')

# 학습데이터와 테스트데이터 분리
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(
    X
    ,y
    ,test_size=0.3
    ,stratify=y
    ,random_state=0
)
# print(f'X_train의 shape : {X_train.shape}')
# print(f'X_test의 shape : {X_test.shape}')
# print(f'y_train의 shape : {y_train.shape}')
# print(f'y_test의 shape : {y_test.shape}')

# 특성을 표준화 : StandardScaler 사용
from sklearn.preprocessing import StandardScaler

# StandardScaler 오브젝트 생성
sc = StandardScaler()

# 학습 데이터에 대해 표준화 진행
X_train_std = sc.fit_transform(X_train)  # fit_transform gives standardized data

# 테스트 데이터는 학습 데이터로부터 계산된 평균과 표준편차를 사용하여 표준화
X_test_std = sc.transform(X_test)

# 공분산 행렬
cov_mat = np.cov(X_train_std.T) 
# print(f'공분산\n{cov_mat}')

# 고유값 분해
eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)

# 고윳값 출력
# print(f'고윳값\n{eigen_vals}')
# print(f'고윳벡터의 차원\n{eigen_vecs.shape}')
# print(f'고유벡터\n{eigen_vecs}')

# 총분산과 설명된 분산
tot=np.sum(eigen_vals) #고유값의 전체 합
# 고유값 비율
var_exp=[
    (i/tot) for i in sorted(eigen_vals,reverse=True)
]
# 고유값 비율의 누적합
cum_var_exp = np.cumsum(var_exp)

# 시각화
import matplotlib.pyplot as plt

plt.bar(
    range(1,len(var_exp)+1)
    , var_exp
    ,align='center'
)
plt.step(
    range(1,len(var_exp)+1)
    , cum_var_exp
    ,where='mid'
)
plt.grid()
plt.show()
```
![image](https://github.com/user-attachments/assets/156ceab2-ef3d-4e36-ab6f-718b8777be28)

---
## 특성 변환

```
import numpy as np
import pandas as pd

df_wine=pd.read_csv(
    '../wine.data'
    ,header=None
    ,encoding='utf-8'
)

X,y=df_wine.iloc[:,1:].values,df_wine.iloc[:,0].values

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(
    X
    ,y
    ,test_size=0.3
    ,stratify=y
    ,random_state=0
)

from sklearn.preprocessing import StandardScaler

sc = StandardScaler()

X_train_std = sc.fit_transform(X_train)  

X_test_std = sc.transform(X_test)

cov_mat = np.cov(X_train_std.T) 

eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)

tot=np.sum(eigen_vals) 

var_exp=[
    (i/tot) for i in sorted(eigen_vals,reverse=True)
]

cum_var_exp = np.cumsum(var_exp)

# 고유값, 고유 벡터 튜플의 리스트 처리
eigen_pairs=[
    (
        np.abs(eigen_vals[idx])
        , eigen_vecs[:,idx]
    ) for idx in range(len(eigen_vals))
]

# 내림차순 정렬
eigen_pairs.sort(
    key=lambda k :k[0]
    ,reverse=True
)

print(f'eigen_pairs\n{eigen_pairs}')
```
![image](https://github.com/user-attachments/assets/54506186-7b1d-44fa-9a76-029b62185f74)

```
import numpy as np
import pandas as pd

df_wine=pd.read_csv(
    '../wine.data'
    ,header=None
    ,encoding='utf-8'
)

X,y=df_wine.iloc[:,1:].values,df_wine.iloc[:,0].values

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(
    X
    ,y
    ,test_size=0.3
    ,stratify=y
    ,random_state=0
)

from sklearn.preprocessing import StandardScaler

sc = StandardScaler()

X_train_std = sc.fit_transform(X_train)  

X_test_std = sc.transform(X_test)

cov_mat = np.cov(X_train_std.T) 

eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)

tot=np.sum(eigen_vals) 

var_exp=[
    (i/tot) for i in sorted(eigen_vals,reverse=True)
]

cum_var_exp = np.cumsum(var_exp)

# 고유값, 고유 벡터 튜플의 리스트 처리
eigen_pairs=[
    (
        np.abs(eigen_vals[idx])
        , eigen_vecs[:,idx]
    ) for idx in range(len(eigen_vals))
]

# 내림차순 정렬
eigen_pairs.sort(
    key=lambda k :k[0]
    ,reverse=True
)

w = np.hstack((
    eigen_pairs[0][1][:, np.newaxis],  
    eigen_pairs[1][1][:, np.newaxis]   
))

print(f'투영 행렬\n{w}')
```
![image](https://github.com/user-attachments/assets/4c1bfbfd-55a3-491b-8285-0d78dfd3727e)

```
import numpy as np
import pandas as pd

df_wine=pd.read_csv(
    '../wine.data'
    ,header=None
    ,encoding='utf-8'
)

X,y=df_wine.iloc[:,1:].values,df_wine.iloc[:,0].values

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(
    X
    ,y
    ,test_size=0.3
    ,stratify=y
    ,random_state=0
)

from sklearn.preprocessing import StandardScaler

sc = StandardScaler()

X_train_std = sc.fit_transform(X_train)  

X_test_std = sc.transform(X_test)

cov_mat = np.cov(X_train_std.T) 

eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)

tot=np.sum(eigen_vals) 

var_exp=[
    (i/tot) for i in sorted(eigen_vals,reverse=True)
]

cum_var_exp = np.cumsum(var_exp)

# 고유값, 고유 벡터 튜플의 리스트 처리
eigen_pairs=[
    (
        np.abs(eigen_vals[idx])
        , eigen_vecs[:,idx]
    ) for idx in range(len(eigen_vals))
]

# 내림차순 정렬
eigen_pairs.sort(
    key=lambda k :k[0]
    ,reverse=True
)

w = np.hstack((
    eigen_pairs[0][1][:, np.newaxis],  
    eigen_pairs[1][1][:, np.newaxis]   
))

X_train_pca=X_train_std.dot(w)
print(f'X_train_pca.shape : {X_train_pca.shape}')
print(f'X_train_std.shape : {X_train_std.shape}')
```
![image](https://github.com/user-attachments/assets/810791d1-5e19-42e4-8e8c-86cb756e47ba)

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
df_wine=pd.read_csv(
    '../wine.data'
    ,header=None
    ,encoding='utf-8'
)

X,y=df_wine.iloc[:,1:].values,df_wine.iloc[:,0].values

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(
    X
    ,y
    ,test_size=0.3
    ,stratify=y
    ,random_state=0
)

from sklearn.preprocessing import StandardScaler

sc = StandardScaler()

X_train_std = sc.fit_transform(X_train)  

X_test_std = sc.transform(X_test)

cov_mat = np.cov(X_train_std.T) 

eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)

tot=np.sum(eigen_vals) 

var_exp=[
    (i/tot) for i in sorted(eigen_vals,reverse=True)
]

cum_var_exp = np.cumsum(var_exp)

# 고유값, 고유 벡터 튜플의 리스트 처리
eigen_pairs=[
    (
        np.abs(eigen_vals[idx])
        , eigen_vecs[:,idx]
    ) for idx in range(len(eigen_vals))
]

# 내림차순 정렬
eigen_pairs.sort(
    key=lambda k :k[0]
    ,reverse=True
)

w = np.hstack((
    eigen_pairs[0][1][:, np.newaxis],  
    eigen_pairs[1][1][:, np.newaxis]   
))

X_train_pca=X_train_std.dot(w)

colors=['r','b','g']
markers=['o','s','^']
for label, color, marker in zip(np.unique(y_train),colors,markers):
    plt.scatter(
        X_train_pca[y_train==label,0]
        ,X_train_pca[y_train==label,1]
        ,c=color
        ,marker=marker
        ,label=f'Class{label}'
    )
plt.grid()
plt.show()
```
![image](https://github.com/user-attachments/assets/4ec6fc99-4b4b-49c9-a27a-68c909d3ffa9)

---
## 사이킷런의 주성분 분석

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# sklearn PCA
from sklearn.linear_model import LogisticRegression
from sklearn.decomposition import PCA


df_wine=pd.read_csv(
    '../wine.data'
    ,header=None
    ,encoding='utf-8'
)

X,y=df_wine.iloc[:,1:].values,df_wine.iloc[:,0].values

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(
    X
    ,y
    ,test_size=0.3
    ,stratify=y
    ,random_state=0
)

from sklearn.preprocessing import StandardScaler

sc = StandardScaler()

X_train_std = sc.fit_transform(X_train)  

X_test_std = sc.transform(X_test)

# 2차원 차원축소
pca=PCA(
    n_components=2
)
lr=LogisticRegression(
    multi_class='ovr'
    , solver='lbfgs'
)

# 학습데이터/테스트데이터 차원 축소
X_train_pca=pca.fit_transform(X_train_std)
X_test_pca=pca.transform(X_test_std)

# 모델 훈련
lr.fit(X_train_pca,y_train)

from sklearn.metrics import accuracy_score
y_pred = lr.predict(X_test_pca)

print(f'정확도 : {accuracy_score(y_pred,y_test)}')
```
![image](https://github.com/user-attachments/assets/70372ffb-2ea4-4c06-8354-2a5f094e97ed)

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# sklearn PCA
from sklearn.linear_model import LogisticRegression
from sklearn.decomposition import PCA


df_wine=pd.read_csv(
    '../wine.data'
    ,header=None
    ,encoding='utf-8'
)

X,y=df_wine.iloc[:,1:].values,df_wine.iloc[:,0].values

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(
    X
    ,y
    ,test_size=0.3
    ,stratify=y
    ,random_state=0
)

from sklearn.preprocessing import StandardScaler

sc = StandardScaler()

X_train_std = sc.fit_transform(X_train)  

X_test_std = sc.transform(X_test)

# 2차원 차원축소
pca=PCA(
    n_components=None
)
lr=LogisticRegression(
    multi_class='ovr'
    , solver='lbfgs'
)

# 학습데이터/테스트데이터 차원 축소
X_train_pca=pca.fit_transform(X_train_std)
X_test_pca=pca.transform(X_test_std)

print(pca.explained_variance_ratio_)
```
![image](https://github.com/user-attachments/assets/0e08b00d-809f-443b-97dd-4649fff6601b)

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# sklearn PCA
from sklearn.linear_model import LogisticRegression
from sklearn.decomposition import PCA

df_wine = pd.read_csv(
    '../wine.data'
    , header=None
    , encoding='utf-8'
)

X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    X
    , y
    , test_size=0.3
    , stratify=y
    , random_state=0
)

from sklearn.preprocessing import StandardScaler

sc = StandardScaler()

X_train_std = sc.fit_transform(X_train)

X_test_std = sc.transform(X_test)


pca = PCA(
    n_components=0.95
)
pca.fit(X_train_std)
print(f'주성분 개수 : {pca.n_components_}')
print(f'설명된 분산 비율 : {np.sum(pca.explained_variance_ratio_)}')
```
![image](https://github.com/user-attachments/assets/407cad42-315e-4424-9c39-0829c850dee7)

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# sklearn PCA
from sklearn.linear_model import LogisticRegression
from sklearn.decomposition import PCA

df_wine = pd.read_csv(
    '../wine.data'
    , header=None
    , encoding='utf-8'
)

X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    X
    , y
    , test_size=0.3
    , stratify=y
    , random_state=0
)

from sklearn.preprocessing import StandardScaler

sc = StandardScaler()

X_train_std = sc.fit_transform(X_train)

X_test_std = sc.transform(X_test)


pca = PCA(
    n_components='mle'
)
pca.fit(X_train_std)


print(f'주성분 개수 : {pca.n_components_}')
print(f'설명된 분산 비율 : {np.sum(pca.explained_variance_ratio_)}')
```
![image](https://github.com/user-attachments/assets/a88da175-486e-461e-ad09-decf507c2b84)

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# sklearn PCA
from sklearn.linear_model import LogisticRegression
from sklearn.decomposition import PCA

df_wine = pd.read_csv(
    '../wine.data'
    , header=None
    , encoding='utf-8'
)

df_wine.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash',
                   'Alcalinity of ash', 'Magnesium', 'Total phenols',
                   'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins',
                   'Color intensity', 'Hue', 'OD280/OD315 of diluted wines',
                   'Proline']

X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    X
    , y
    , test_size=0.3
    , stratify=y
    , random_state=0
)

from sklearn.preprocessing import StandardScaler

sc = StandardScaler()

X_train_std = sc.fit_transform(X_train)  

X_test_std = sc.transform(X_test)

cov_mat = np.cov(X_train_std.T) 

eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)

tot=np.sum(eigen_vals) 

var_exp=[
    (i/tot) for i in sorted(eigen_vals,reverse=True)
]

cum_var_exp = np.cumsum(var_exp)

# 고유값, 고유 벡터 튜플의 리스트 처리
eigen_pairs=[
    (
        np.abs(eigen_vals[idx])
        , eigen_vecs[:,idx]
    ) for idx in range(len(eigen_vals))
]

# 내림차순 정렬
eigen_pairs.sort(
    key=lambda k :k[0]
    ,reverse=True
)

w = np.hstack((
    eigen_pairs[0][1][:, np.newaxis],  
    eigen_pairs[1][1][:, np.newaxis]   
))


# 피쳐 기여도 평가
loading=eigen_vecs*np.sqrt(eigen_vals)

# print(f'eigen_vals : {eigen_vals.shape}')
# print(f'eigen_vecs : {eigen_vecs.shape}')
# print(f'loading shape : {loading.shape}')

# 첫번째 주성분에 원본 피쳐가 얼마나 기여했는지 시각화
fig, ax = plt.subplots()
ax.bar(
    range(len(loading)) # X 축
    ,loading[:,0] # 첫번재 주성분
    ,align='center'
)
ax.set_xticklabels(df_wine.columns[0:],rotation=90)
plt.show()
```
![image](https://github.com/user-attachments/assets/9cbe0657-e0a3-48e7-bd98-98192296e827)

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# sklearn PCA
from sklearn.linear_model import LogisticRegression
from sklearn.decomposition import PCA

df_wine = pd.read_csv(
    '../wine.data'
    , header=None
    , encoding='utf-8'
)

df_wine.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash',
                   'Alcalinity of ash', 'Magnesium', 'Total phenols',
                   'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins',
                   'Color intensity', 'Hue', 'OD280/OD315 of diluted wines',
                   'Proline']

X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    X
    , y
    , test_size=0.3
    , stratify=y
    , random_state=0
)

from sklearn.preprocessing import StandardScaler

sc = StandardScaler()

X_train_std = sc.fit_transform(X_train)  

X_test_std = sc.transform(X_test)



cov_mat = np.cov(X_train_std.T) 

eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)

tot=np.sum(eigen_vals) 

var_exp=[
    (i/tot) for i in sorted(eigen_vals,reverse=True)
]

cum_var_exp = np.cumsum(var_exp)

# 고유값, 고유 벡터 튜플의 리스트 처리
eigen_pairs=[
    (
        np.abs(eigen_vals[idx])
        , eigen_vecs[:,idx]
    ) for idx in range(len(eigen_vals))
]

# 내림차순 정렬
eigen_pairs.sort(
    key=lambda k :k[0]
    ,reverse=True
)

w = np.hstack((
    eigen_pairs[0][1][:, np.newaxis],  
    eigen_pairs[1][1][:, np.newaxis]   
))

from sklearn.preprocessing import StandardScaler

sc = StandardScaler()

X_train_std = sc.fit_transform(X_train)  

X_test_std = sc.transform(X_test)

# 2차원 차원축소
pca=PCA(
    n_components=2
)
lr=LogisticRegression(
    multi_class='ovr'
    , solver='lbfgs'
)

# 학습데이터/테스트데이터 차원 축소
X_train_pca=pca.fit_transform(X_train_std)
X_test_pca=pca.transform(X_test_std)

# 모델 훈련
lr.fit(X_train_pca,y_train)

from sklearn.metrics import accuracy_score
y_pred = lr.predict(X_test_pca)
# 피쳐 기여도 평가
loading=eigen_vecs*np.sqrt(eigen_vals)

sklearn_loading=pca.components_.T * np.sqrt(pca.explained_variance_)
fig, ax = plt.subplots()
ax.bar(
    range(len(sklearn_loading))
    ,sklearn_loading[:,0]
    ,align='center'
)
ax.set_ylabel
ax.set_xticks(range(len(sklearn_loading)))
ax.set_xticklabels(df_wine.columns[1:],rotation=90)
plt.tight_layout()
plt.show()
```
![image](https://github.com/user-attachments/assets/3d178b3a-a934-499f-a2de-2e3ac7c1004b)

---
## LDA
```
# 사이킷런 패키지 데이터
# 사람얼굴 데이터 : 40명 10장씩 이미지 데이터 : 정체 이미지 개수는 400장
# 1장 이미지(1사람 사진) : 64X64 -> 4096차원
# 전체 데이터 구조 : 400X64X64
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_olivetti_faces # AT&T 얼굴데이터베이스

# ORL : 얼굴 데이터셋 로딩
faces=np.load('./olivetti_faces.npy')
# faces.shape
#첫번째 이미지 출력
person_id = 0
image_idx=0
plt.figure(figsize=(5,5))
plt.imshow(faces[image_idx],cmap='grey')
plt.axis('off')
plt.show()
```
![image](https://github.com/user-attachments/assets/c50c014d-1cb7-4c0f-ba67-fa3586e187c5)

```
# 사이킷런 패키지 데이터
# 사람얼굴 데이터 : 40명 10장씩 이미지 데이터 : 정체 이미지 개수는 400장
# 1장 이미지(1사람 사진) : 64X64 -> 4096차원
# 전체 데이터 구조 : 400X64X64
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_olivetti_faces # AT&T 얼굴데이터베이스

# ORL : 얼굴 데이터셋 로딩
faces=np.load('./olivetti_faces.npy')
# faces.shape
person_id = 0
image_idx=1
plt.figure(figsize=(5,5))
plt.imshow(faces[image_idx],cmap='grey')
plt.axis('off')
plt.show()
```
![image](https://github.com/user-attachments/assets/0c8ad76e-20cf-4072-8990-ad7734a135af)

```
# 사이킷런 패키지 데이터
# 사람얼굴 데이터 : 40명 10장씩 이미지 데이터 : 정체 이미지 개수는 400장
# 1장 이미지(1사람 사진) : 64X64 -> 4096차원
# 전체 데이터 구조 : 400X64X64
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_olivetti_faces 

# ORL : 얼굴 데이터셋 로딩
faces=np.load('./olivetti_faces.npy')
# faces.shape

# person_id = 0
# image_idx=0
num_images=10
for i in range(num_images):
    plt.subplot(2,5,i+1)
    plt.imshow(faces[i],cmap='grey')
    plt.axis('off')
plt.tight_layout()
plt.show()
```
![image](https://github.com/user-attachments/assets/41a5e5a0-c02b-4f37-89c2-5d59c2f2c683)

```
# 사이킷런 패키지 데이터
# 사람얼굴 데이터 : 40명 10장씩 이미지 데이터 : 정체 이미지 개수는 400장
# 1장 이미지(1사람 사진) : 64X64 -> 4096차원
# 전체 데이터 구조 : 400X64X64
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_olivetti_faces 

# LDA 적용해서 분류
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA

from sklearn.metrics import accuracy_score, classification_report

# 이미지 데이터
X=np.load('./olivetti_faces.npy')
# 레이블 로딩
y=np.load('./olivetti_faces_target.npy')

print(f'X shape : {X.shape}')
print(f'y shape : {y.shape}')
print(f'unique y : {np.unique(y)}')

# 학습데이터/테스트데이터 분리
X_train, X_test, y_train,y_test=train_test_split(
    X
    ,y
    ,test_size=0.3
    ,random_state=1
    ,stratify=y
)
print(f'X_train shape : {X_train.shape}')
print(f'X_test shape : {X_test.shape}')
print(f'y_train shape : {y_train.shape}')
print(f'y_test shape : {y_test.shape}')

X_train_2d = X_train.reshape(X_train.shape[0], -1)
X_test_2d = X_test.reshape(X_test.shape[0], -1)
print(f'X_train_2d shape : {X_train_2d.shape}')
print(f'X_test_2d shape : {X_test_2d.shape}')

# LDA 차원 축소
lda=LDA(
    n_components= len(np.unique(y))-1 # 일반적으로 클래스 수 -1
)
X_train_lda=lda.fit_transform(X_train_2d,y_train)
X_test_lda=lda.transform(X_test_2d)
print(f'X_train_lda shape : {X_train_lda.shape}')
print(f'X_test_lda shape : {X_test_lda.shape}')
```
![image](https://github.com/user-attachments/assets/8b230896-0bb8-4ab1-998d-855c4da605cc)

```
# 사이킷런 패키지 데이터
# 사람얼굴 데이터 : 40명 10장씩 이미지 데이터 : 정체 이미지 개수는 400장
# 1장 이미지(1사람 사진) : 64X64 -> 4096차원
# 전체 데이터 구조 : 400X64X64
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_olivetti_faces 

# LDA 적용해서 분류
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA

from sklearn.metrics import accuracy_score, classification_report

# 이미지 데이터
X=np.load('./olivetti_faces.npy')
# 레이블 로딩
y=np.load('./olivetti_faces_target.npy')

print(f'X shape : {X.shape}')
print(f'y shape : {y.shape}')
print(f'unique y : {np.unique(y)}')

# 학습데이터/테스트데이터 분리
X_train, X_test, y_train,y_test=train_test_split(
    X
    ,y
    ,test_size=0.3
    ,random_state=1
    ,stratify=y
)
print(f'X_train shape : {X_train.shape}')
print(f'X_test shape : {X_test.shape}')
print(f'y_train shape : {y_train.shape}')
print(f'y_test shape : {y_test.shape}')

X_train_2d = X_train.reshape(X_train.shape[0], -1)
X_test_2d = X_test.reshape(X_test.shape[0], -1)
print(f'X_train_2d shape : {X_train_2d.shape}')
print(f'X_test_2d shape : {X_test_2d.shape}')

# LDA 차원 축소
lda=LDA(
    n_components= len(np.unique(y))-1 # 일반적으로 클래스 수 -1
)
X_train_lda=lda.fit_transform(X_train_2d,y_train)
X_test_lda=lda.transform(X_test_2d)
print(f'X_train_lda shape : {X_train_lda.shape}')
print(f'X_test_lda shape : {X_test_lda.shape}')

# KNN
knn = KNeighborsClassifier(
    n_neighbors=len(np.unique(y))
)
knn.fit(X_train_lda,y_train)
print(f'knn 학습 : {knn.fit(X_train_lda,y_train)}')

y_pred=knn.predict(
    X_test_lda
)

# 성능평가
acc = accuracy_score(y_pred,y_test)
print(f'LDA를 이용한 얼굴 인식 정확도 : {acc:.3f}')
```
![image](https://github.com/user-attachments/assets/451479ef-d717-47ec-bbce-c248350c14c2)

```
# 사이킷런 패키지 데이터
# 사람얼굴 데이터 : 40명 10장씩 이미지 데이터 : 정체 이미지 개수는 400장
# 1장 이미지(1사람 사진) : 64X64 -> 4096차원
# 전체 데이터 구조 : 400X64X64
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_olivetti_faces 

# LDA 적용해서 분류
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA

from sklearn.metrics import accuracy_score, classification_report

# 이미지 데이터
X=np.load('./olivetti_faces.npy')
# 레이블 로딩
y=np.load('./olivetti_faces_target.npy')

# print(f'X shape : {X.shape}')
# print(f'y shape : {y.shape}')
# print(f'unique y : {np.unique(y)}')

# 학습데이터/테스트데이터 분리
X_train, X_test, y_train,y_test=train_test_split(
    X
    ,y
    ,test_size=0.3
    ,random_state=1
    ,stratify=y
)
# print(f'X_train shape : {X_train.shape}')
# print(f'X_test shape : {X_test.shape}')
# print(f'y_train shape : {y_train.shape}')
# print(f'y_test shape : {y_test.shape}')

X_train_2d = X_train.reshape(X_train.shape[0], -1)
X_test_2d = X_test.reshape(X_test.shape[0], -1)
# print(f'X_train_2d shape : {X_train_2d.shape}')
# print(f'X_test_2d shape : {X_test_2d.shape}')

# LDA 차원 축소
lda=LDA(
    n_components= len(np.unique(y))-1 # 일반적으로 클래스 수 -1
)
X_train_lda=lda.fit_transform(X_train_2d,y_train)
X_test_lda=lda.transform(X_test_2d)
# print(f'X_train_lda shape : {X_train_lda.shape}')
# print(f'X_test_lda shape : {X_test_lda.shape}')

# KNN
knn = KNeighborsClassifier(
    n_neighbors=len(np.unique(y))
)
knn.fit(X_train_lda,y_train)
# print(f'knn 학습 : {knn.fit(X_train_lda,y_train)}')

y_pred=knn.predict(
    X_test_lda
)

# 성능평가
acc = accuracy_score(y_pred,y_test)
# print(f'LDA를 이용한 얼굴 인식 정확도 : {acc:.3f}')

# 시각화
from sklearn.decomposition import PCA


cnt=15
n_components=cnt**2
# print(n_components)
pca=PCA(n_components=n_components, random_state=1)


pca.fit(X_train_2d)


image_original=X_test_2d[0]
image_pca=pca.transform([image_original])


fig, ax=plt.subplots(1,2,figsize=(8,4))
ax[0].imshow(
    image_original.reshape(X_train[0].shape), cmap='grey'
)


ax[1].imshow(
    image_pca.reshape(cnt,-1), cmap='grey'
)
plt.show()
```
![image](https://github.com/user-attachments/assets/656b9749-c932-4adb-bbf0-3c651e54f4d9)

---
```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_digits
digits=load_digits()
# 4개 원본 이미지 시각화
fig, ax=plt.subplots(1,4) # 축을 1행 4열 생성

for idx in range(4):
    ax[idx].imshow(
        digits.images[idx],cmap='Grays'
    )
```
![image](https://github.com/user-attachments/assets/7afb4855-f0d4-4f2b-9e19-a61692d9505d)

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_digits
digits=load_digits()

y_digits=digits.target
X_digits=digits.data
print(f'X digits : {X_digits}')
print(f'y digits : {y_digits}')
print(f'X digits shape : {X_digits.shape}')
print(f'y digits shape: {y_digits.shape}')

# 비선형 차원 축소
from sklearn.manifold import TSNE

tsme=TSNE(
    n_components=2 # 64 차원 데이터셋을 2차원 공간에 투영
    ,init='pca'
    ,random_state=123
)

# 투영 : 변환
X_digits_tmse=tsme.fit_transform(X_digits)

import matplotlib.patheffects as PathEffects


def plot_projection(x, colors):

    f = plt.figure(figsize=(8, 8))
    ax = plt.subplot(aspect='equal')
    for i in range(10):
        plt.scatter(x[colors == i, 0],
                    x[colors == i, 1])

    for i in range(10):

        xtext, ytext = np.median(x[colors == i, :], axis=0)
        txt = ax.text(xtext, ytext, str(i), fontsize=24)
        txt.set_path_effects([
            PathEffects.Stroke(linewidth=5, foreground="w"),
            PathEffects.Normal()])

plot_projection(X_digits_tmse, y_digits)
# plt.savefig('figures/05_13.png', dpi=300)
plt.show()
```
![image](https://github.com/user-attachments/assets/f1b7de90-d133-4f49-80d7-822dfdb90643)

![image](https://github.com/user-attachments/assets/abcef60f-b483-4469-85ab-a01152890f97)

```
# 위스콘신 유방암 데이터 로딩
import numpy as np
import pandas as pd
df = pd.read_csv(
    './wdbc.data'
    ,header=None
)
# 데이터와 레이블 분리 추출
X=df.iloc[:,2:].values #array로 추출
y=df.iloc[:,1].values
print(f'X shape : {X.shape}')
print(f'y shape : {y.shape}')
print(f'unique y : {np.unique(y)}')

# label을 숫자 변환 : 0,1
from sklearn.preprocessing import LabelEncoder
le=LabelEncoder()
y=le.fit_transform(y) # y값을 입력해서 변환(0,1)하고 다시 y에 대입
print(f'unique y : {np.unique(y)}')

# 학습데이터/테스트 데이터 분리
from sklearn.model_selection import train_test_split
X_train, X_test, y_train,y_test=train_test_split(
    X
    ,y
    ,test_size=0.2
    ,stratify=y
    ,random_state=1
)

print(f'X_train.shape : {X_train.shape}')
print(f'X_test.shape : {X_test.shape}')
```
![image](https://github.com/user-attachments/assets/27b47b90-4a45-46cc-b742-aa7b86b010d6)

```
# 위스콘신 유방암 데이터 로딩
import numpy as np
import pandas as pd
df = pd.read_csv(
    './wdbc.data'
    ,header=None
)
# 데이터와 레이블 분리 추출
X=df.iloc[:,2:].values #array로 추출
y=df.iloc[:,1].values
# print(f'X shape : {X.shape}')
# print(f'y shape : {y.shape}')
# print(f'unique y : {np.unique(y)}')

# label을 숫자 변환 : 0,1
from sklearn.preprocessing import LabelEncoder
le=LabelEncoder()
y=le.fit_transform(y) # y값을 입력해서 변환(0,1)하고 다시 y에 대입
# print(f'unique y : {np.unique(y)}')

# 학습데이터/테스트 데이터 분리
from sklearn.model_selection import train_test_split
X_train, X_test, y_train,y_test=train_test_split(
    X
    ,y
    ,test_size=0.2
    ,stratify=y
    ,random_state=1
)

# print(f'X_train.shape : {X_train.shape}')
# print(f'X_test.shape : {X_test.shape}')

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import make_pipeline # 함수

pipe_lr = make_pipeline(
    StandardScaler()
    , PCA(n_components=2)
    ,LogisticRegression(random_state=1)
)

pipe_lr.fit(X_train,y_train)
# StandardScaler.fit(X_train)되고 StandardScaler.transform(X_train) 되고 pca.fit(X_train)되고 pca.transform(X_train)되고 lr.fit(X_train,y_train)
# 학습 종료 : 모델이 완성
y_pred=pipe_lr.predict(X_test)
from sklearn.metrics import accuracy_score
acc=accuracy_score(y_pred,y_test)
print(f'정확도 : {acc}')
```
![image](https://github.com/user-attachments/assets/53abe964-05e6-4610-b5c7-637c3366fcd0)

```
# 위스콘신 유방암 데이터 로딩
import numpy as np
import pandas as pd
df = pd.read_csv(
    './wdbc.data'
    ,header=None
)
# 데이터와 레이블 분리 추출
X=df.iloc[:,2:].values #array로 추출
y=df.iloc[:,1].values
# print(f'X shape : {X.shape}')
# print(f'y shape : {y.shape}')
# print(f'unique y : {np.unique(y)}')

# label을 숫자 변환 : 0,1
from sklearn.preprocessing import LabelEncoder
le=LabelEncoder()
y=le.fit_transform(y) # y값을 입력해서 변환(0,1)하고 다시 y에 대입
# print(f'unique y : {np.unique(y)}')

# 학습데이터/테스트 데이터 분리
from sklearn.model_selection import train_test_split
X_train, X_test, y_train,y_test=train_test_split(
    X
    ,y
    ,test_size=0.2
    ,stratify=y
    ,random_state=1
)

# print(f'X_train.shape : {X_train.shape}')
# print(f'X_test.shape : {X_test.shape}')

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import make_pipeline # 함수

pipe_lr = make_pipeline(
    StandardScaler()
    , PCA(n_components=2)
    ,LogisticRegression(random_state=1)
)

pipe_lr.fit(X_train,y_train)
# StandardScaler.fit(X_train)되고 StandardScaler.transform(X_train) 되고 pca.fit(X_train)되고 pca.transform(X_train)되고 lr.fit(X_train,y_train)
# 학습 종료 : 모델이 완성
y_pred=pipe_lr.predict(X_test)
from sklearn.metrics import accuracy_score
acc=accuracy_score(y_pred,y_test)
# print(f'정확도 : {acc}')

from sklearn import  set_config
set_config(
    display='diagram'
)
pipe_lr
```
![image](https://github.com/user-attachments/assets/723436b8-e534-464c-b64a-c741c538d1fc)

```
# 위스콘신 유방암 데이터 로딩
import numpy as np
import pandas as pd
df = pd.read_csv(
    './wdbc.data'
    ,header=None
)
# 데이터와 레이블 분리 추출
X=df.iloc[:,2:].values #array로 추출
y=df.iloc[:,1].values
# print(f'X shape : {X.shape}')
# print(f'y shape : {y.shape}')
# print(f'unique y : {np.unique(y)}')

# label을 숫자 변환 : 0,1
from sklearn.preprocessing import LabelEncoder
le=LabelEncoder()
y=le.fit_transform(y) # y값을 입력해서 변환(0,1)하고 다시 y에 대입
# print(f'unique y : {np.unique(y)}')

# 학습데이터/테스트 데이터 분리
from sklearn.model_selection import train_test_split
X_train, X_test, y_train,y_test=train_test_split(
    X
    ,y
    ,test_size=0.2
    ,stratify=y
    ,random_state=1
)

# print(f'X_train.shape : {X_train.shape}')
# print(f'X_test.shape : {X_test.shape}')

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import make_pipeline # 함수

pipe_lr = make_pipeline(
    StandardScaler()
    , PCA(n_components=2)
    ,LogisticRegression(random_state=1)
)

pipe_lr.fit(X_train,y_train)
# StandardScaler.fit(X_train)되고 StandardScaler.transform(X_train) 되고 pca.fit(X_train)되고 pca.transform(X_train)되고 lr.fit(X_train,y_train)
# 학습 종료 : 모델이 완성
y_pred=pipe_lr.predict(X_test)
from sklearn.metrics import accuracy_score
acc=accuracy_score(y_pred,y_test)
# print(f'정확도 : {acc}')

from sklearn import  set_config
set_config(
    display='diagram'
)
# pipe_lr
from  sklearn.model_selection import StratifiedKFold

kfold=StratifiedKFold(
    n_splits=10 # 모델이 10개 나온다 -> 정확도 10이다
).split(X_train,y_train)

scores=[]

print(f'X_train shape : {X_train.shape}')
for k,(train,test) in enumerate(kfold):
    # print(f'k : {k}, train shape : {train.shape}, test shape : {test.shape}')
    pipe_lr.fit(
        X_train[train]
        ,y_train[train]
    )
    score=pipe_lr.score(X_train[test],y_train[test])
    scores.append(score)

print(scores)
print(f'평균 값은 : {np.mean(scores)}')
```
![image](https://github.com/user-attachments/assets/2ee64737-7984-4244-89da-6dadc3828e41)

---
## CV

```
import numpy as np
import pandas as pd
df = pd.read_csv(
    './wdbc.data'
    ,header=None
)
X=df.iloc[:,2:].values 
y=df.iloc[:,1].values

from sklearn.preprocessing import LabelEncoder
le=LabelEncoder()
y=le.fit_transform(y) 

from sklearn.model_selection import train_test_split
X_train, X_test, y_train,y_test=train_test_split(
    X
    ,y
    ,test_size=0.2
    ,stratify=y
    ,random_state=1
)

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import make_pipeline 

pipe_lr = make_pipeline(
    StandardScaler()
    , PCA(n_components=2)
    ,LogisticRegression(random_state=1)
)

pipe_lr.fit(X_train,y_train)
y_pred=pipe_lr.predict(X_test)
from sklearn.metrics import accuracy_score
acc=accuracy_score(y_pred,y_test)

from sklearn import  set_config
set_config(
    display='diagram'
)
from  sklearn.model_selection import StratifiedKFold

kfold=StratifiedKFold(
    n_splits=10
).split(X_train,y_train)

scores=[]

for k,(train,test) in enumerate(kfold):
    pipe_lr.fit(
        X_train[train]
        ,y_train[train]
    )
    score=pipe_lr.score(X_train[test],y_train[test])
    scores.append(score)

# cross_val_score
from sklearn.model_selection import cross_val_score

val_scores=cross_val_score(
    estimator=pipe_lr
    , X=X_train
    ,y=y_train
    ,cv=10 # 10 k-fold
    ,n_jobs=1
)
print(f'cross_val_score\n{val_scores}')

print(f'CV 정확도 점수 : {np.mean(val_scores)}')
```
![image](https://github.com/user-attachments/assets/0a0d90c5-da2c-4a37-b281-5ff3de830e6c)

```
import numpy as np
import pandas as pd
df = pd.read_csv(
    './wdbc.data'
    ,header=None
)
X=df.iloc[:,2:].values 
y=df.iloc[:,1].values

from sklearn.preprocessing import LabelEncoder
le=LabelEncoder()
y=le.fit_transform(y) 

from sklearn.model_selection import train_test_split
X_train, X_test, y_train,y_test=train_test_split(
    X
    ,y
    ,test_size=0.2
    ,stratify=y
    ,random_state=1
)

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import make_pipeline 

pipe_lr = make_pipeline(
    StandardScaler()
    , PCA(n_components=2)
    ,LogisticRegression(random_state=1)
)

pipe_lr.fit(X_train,y_train)
y_pred=pipe_lr.predict(X_test)
from sklearn.metrics import accuracy_score
acc=accuracy_score(y_pred,y_test)

from sklearn import  set_config
set_config(
    display='diagram'
)
from  sklearn.model_selection import StratifiedKFold

kfold=StratifiedKFold(
    n_splits=10
).split(X_train,y_train)

scores=[]

for k,(train,test) in enumerate(kfold):
    pipe_lr.fit(
        X_train[train]
        ,y_train[train]
    )
    score=pipe_lr.score(X_train[test],y_train[test])
    scores.append(score)

# cross_val_score
from sklearn.model_selection import cross_val_score

cv_scores=cross_val_score(
    estimator=pipe_lr
    , X=X_train
    ,y=y_train
    ,cv=10 # 10 k-fold
    ,n_jobs=1
)
print('='*50)
print(f'cross_val_score\n{cv_scores}')
print('='*50)

print(f'CV 정확도 점수 : {np.mean(cv_scores)}')
print('='*50)

# cross_validate() : scoring -> 평가 지표 설정
from sklearn.model_selection import cross_validate

val_scores=cross_validate(
    estimator=pipe_lr
    ,X=X_train
    ,y=y_train
    ,scoring=['accuracy']
    ,cv=10
    ,n_jobs=-1
)

print(f'cross_validate\n{val_scores}')
```
![image](https://github.com/user-attachments/assets/21a4cb29-dea6-4752-b48a-bdb842137f4d)

```
import numpy as np
import pandas as pd
df = pd.read_csv(
    './wdbc.data'
    ,header=None
)
X=df.iloc[:,2:].values 
y=df.iloc[:,1].values

from sklearn.preprocessing import LabelEncoder
le=LabelEncoder()
y=le.fit_transform(y) 

from sklearn.model_selection import train_test_split
X_train, X_test, y_train,y_test=train_test_split(
    X
    ,y
    ,test_size=0.2
    ,stratify=y
    ,random_state=1
)

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import make_pipeline 

pipe_lr = make_pipeline(
    StandardScaler()
    , PCA(n_components=2)
    ,LogisticRegression(random_state=1)
)

pipe_lr.fit(X_train,y_train)
y_pred=pipe_lr.predict(X_test)
from sklearn.metrics import accuracy_score
acc=accuracy_score(y_pred,y_test)

from sklearn import  set_config
set_config(
    display='diagram'
)
from  sklearn.model_selection import StratifiedKFold

kfold=StratifiedKFold(
    n_splits=10
).split(X_train,y_train)

scores=[]

for k,(train,test) in enumerate(kfold):
    pipe_lr.fit(
        X_train[train]
        ,y_train[train]
    )
    score=pipe_lr.score(X_train[test],y_train[test])
    scores.append(score)

# cross_val_score
from sklearn.model_selection import cross_val_score

cv_scores=cross_val_score(
    estimator=pipe_lr
    , X=X_train
    ,y=y_train
    ,cv=10 # 10 k-fold
    ,n_jobs=1
)
print('='*50)
print(f'cross_val_score\n{cv_scores}')
print('='*50)

print(f'CV 정확도 점수 : {np.mean(cv_scores)}')
print('='*50)

# cross_validate() : scoring -> 평가 지표 설정
from sklearn.model_selection import cross_validate

val_scores=cross_validate(
    estimator=pipe_lr
    ,X=X_train
    ,y=y_train
    ,scoring=['accuracy']
    ,cv=10
    ,n_jobs=-1
)

print(f'cross_validate\n{val_scores}')
print('='*50)
print(f"val_scores['test_accuracy'] : {val_scores['test_accuracy']}")
print('='*50)
print(f"val_scores['test_accuracy'] : {val_scores['test_accuracy']}")
print(f"np.mean(val_scores['test_accuracy']) : {np.mean(val_scores['test_accuracy'])}")

# cross_val_predict() : method -> 
from sklearn.model_selection import cross_val_predict
preds=cross_val_predict(
    estimator=pipe_lr
    ,X=X_train
    , y = y_train
    ,cv=10
    ,method='predict_proba'
    ,n_jobs=-1
)
print(f'preds :\n {preds}')
print(f'preds의 갯수 : {len(preds)}')
```
![image](https://github.com/user-attachments/assets/646083ae-d2b7-422b-a29d-ddf2b104a956)

```
import numpy as np
import pandas as pd
df = pd.read_csv(
    './wdbc.data'
    ,header=None
)
X=df.iloc[:,2:].values 
y=df.iloc[:,1].values

from sklearn.preprocessing import LabelEncoder
le=LabelEncoder()
y=le.fit_transform(y) 

from sklearn.model_selection import train_test_split
X_train, X_test, y_train,y_test=train_test_split(
    X
    ,y
    ,test_size=0.2
    ,stratify=y
    ,random_state=1
)

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import make_pipeline 

pipe_lr = make_pipeline(
    StandardScaler()
    , PCA(n_components=2)
    ,LogisticRegression(random_state=1)
)

pipe_lr.fit(X_train,y_train)
y_pred=pipe_lr.predict(X_test)
from sklearn.metrics import accuracy_score
acc=accuracy_score(y_pred,y_test)

from sklearn import  set_config
set_config(
    display='diagram'
)
from  sklearn.model_selection import StratifiedKFold

kfold=StratifiedKFold(
    n_splits=10
).split(X_train,y_train)

scores=[]

for k,(train,test) in enumerate(kfold):
    pipe_lr.fit(
        X_train[train]
        ,y_train[train]
    )
    score=pipe_lr.score(X_train[test],y_train[test])
    scores.append(score)

# cross_val_score
from sklearn.model_selection import cross_val_score

cv_scores=cross_val_score(
    estimator=pipe_lr
    , X=X_train
    ,y=y_train
    ,cv=10 # 10 k-fold
    ,n_jobs=1
)

# cross_validate()
from sklearn.model_selection import cross_validate

val_scores=cross_validate(
    estimator=pipe_lr
    ,X=X_train
    ,y=y_train
    ,scoring=['accuracy']
    ,cv=10
    ,n_jobs=-1
)

# cross_val_predict() 
from sklearn.model_selection import cross_val_predict
preds=cross_val_predict(
    estimator=pipe_lr
    ,X=X_train
    , y = y_train
    ,cv=10
    ,method='predict_proba'
    ,n_jobs=-1
)

# 학습 곡선
import matplotlib.pyplot as pt
from sklearn.model_selection import learning_curve

# 파이프 라인 설정
pipe_lr=make_pipeline(
    StandardScaler()
    ,LogisticRegression(
        penalty='l2'
        ,max_iter=10000
    )
)
train_sizes, train_scores, test_scores=learning_curve(
    estimator=pipe_lr
    ,X=X_train
    ,y=y_train
    ,train_sizes=np.linspace(0.1,1.0,10)
    ,cv=10
    ,n_jobs=1
)

print('='*50)
print(f'train size : {train_sizes}')
print('='*50)
print(f'train score : {train_scores}')
print('='*50)
print(f'test scores : {test_scores}')
```
![image](https://github.com/user-attachments/assets/3e4104c9-a727-45a3-a10a-638a12b1fe2b)

```
import numpy as np
import pandas as pd
df = pd.read_csv(
    './wdbc.data'
    ,header=None
)
X=df.iloc[:,2:].values 
y=df.iloc[:,1].values

from sklearn.preprocessing import LabelEncoder
le=LabelEncoder()
y=le.fit_transform(y) 

from sklearn.model_selection import train_test_split
X_train, X_test, y_train,y_test=train_test_split(
    X
    ,y
    ,test_size=0.2
    ,stratify=y
    ,random_state=1
)

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import make_pipeline 

pipe_lr = make_pipeline(
    StandardScaler()
    , PCA(n_components=2)
    ,LogisticRegression(random_state=1)
)

pipe_lr.fit(X_train,y_train)
y_pred=pipe_lr.predict(X_test)
from sklearn.metrics import accuracy_score
acc=accuracy_score(y_pred,y_test)

from sklearn import  set_config
set_config(
    display='diagram'
)
from  sklearn.model_selection import StratifiedKFold

kfold=StratifiedKFold(
    n_splits=10
).split(X_train,y_train)

scores=[]

for k,(train,test) in enumerate(kfold):
    pipe_lr.fit(
        X_train[train]
        ,y_train[train]
    )
    score=pipe_lr.score(X_train[test],y_train[test])
    scores.append(score)

# cross_val_score
from sklearn.model_selection import cross_val_score

cv_scores=cross_val_score(
    estimator=pipe_lr
    , X=X_train
    ,y=y_train
    ,cv=10 # 10 k-fold
    ,n_jobs=1
)

# cross_validate()
from sklearn.model_selection import cross_validate

val_scores=cross_validate(
    estimator=pipe_lr
    ,X=X_train
    ,y=y_train
    ,scoring=['accuracy']
    ,cv=10
    ,n_jobs=-1
)

# cross_val_predict() 
from sklearn.model_selection import cross_val_predict
preds=cross_val_predict(
    estimator=pipe_lr
    ,X=X_train
    , y = y_train
    ,cv=10
    ,method='predict_proba'
    ,n_jobs=-1
)

# 학습 곡선
import matplotlib.pyplot as pt
from sklearn.model_selection import learning_curve

# 파이프 라인 설정
pipe_lr=make_pipeline(
    StandardScaler()
    ,LogisticRegression(
        penalty='l2'
        ,max_iter=10000
    )
)
train_sizes, train_scores, test_scores=learning_curve(
    estimator=pipe_lr
    ,X=X_train
    ,y=y_train
    ,train_sizes=np.linspace(0.1,1.0,10)
    ,cv=10
    ,n_jobs=1
)

print('='*50)
print(f'train size : {train_sizes}')
print(f'train size shape : {train_sizes.shape}')
print('='*50)
print(f'train score : {train_scores}')
print(f'train score shape : {train_scores.shape}')
print('='*50)
print(f'test scores : {test_scores}')
print(f'test scores shape : {test_scores.shape}')
```
![image](https://github.com/user-attachments/assets/e25f778f-4a3b-4f4b-a280-a1bdf0535adb)

```
import numpy as np
import pandas as pd
df = pd.read_csv(
    './wdbc.data'
    ,header=None
)
X=df.iloc[:,2:].values 
y=df.iloc[:,1].values

from sklearn.preprocessing import LabelEncoder
le=LabelEncoder()
y=le.fit_transform(y) 

from sklearn.model_selection import train_test_split
X_train, X_test, y_train,y_test=train_test_split(
    X
    ,y
    ,test_size=0.2
    ,stratify=y
    ,random_state=1
)

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import make_pipeline 

pipe_lr = make_pipeline(
    StandardScaler()
    , PCA(n_components=2)
    ,LogisticRegression(random_state=1)
)

pipe_lr.fit(X_train,y_train)
y_pred=pipe_lr.predict(X_test)
from sklearn.metrics import accuracy_score
acc=accuracy_score(y_pred,y_test)

from sklearn import  set_config
set_config(
    display='diagram'
)
from  sklearn.model_selection import StratifiedKFold

kfold=StratifiedKFold(
    n_splits=10
).split(X_train,y_train)

scores=[]

for k,(train,test) in enumerate(kfold):
    pipe_lr.fit(
        X_train[train]
        ,y_train[train]
    )
    score=pipe_lr.score(X_train[test],y_train[test])
    scores.append(score)

# cross_val_score
from sklearn.model_selection import cross_val_score

cv_scores=cross_val_score(
    estimator=pipe_lr
    , X=X_train
    ,y=y_train
    ,cv=10 # 10 k-fold
    ,n_jobs=1
)

# cross_validate()
from sklearn.model_selection import cross_validate

val_scores=cross_validate(
    estimator=pipe_lr
    ,X=X_train
    ,y=y_train
    ,scoring=['accuracy']
    ,cv=10
    ,n_jobs=-1
)

# cross_val_predict() 
from sklearn.model_selection import cross_val_predict
preds=cross_val_predict(
    estimator=pipe_lr
    ,X=X_train
    , y = y_train
    ,cv=10
    ,method='predict_proba'
    ,n_jobs=-1
)

# 학습 곡선
import matplotlib.pyplot as pt
from sklearn.model_selection import learning_curve

# 파이프 라인 설정
pipe_lr=make_pipeline(
    StandardScaler()
    ,LogisticRegression(
        penalty='l2'
        ,max_iter=10000
    )
)
train_sizes, train_scores, test_scores=learning_curve(
    estimator=pipe_lr
    ,X=X_train
    ,y=y_train
    ,train_sizes=np.linspace(0.1,1.0,10)
    ,cv=10
    ,n_jobs=1
)

np.mean(train_scores,axis=1)
```
![image](https://github.com/user-attachments/assets/ff8795ab-216b-4d63-ab9e-76ae820213c4)

```
import numpy as np
import pandas as pd
df = pd.read_csv(
    './wdbc.data'
    ,header=None
)
X=df.iloc[:,2:].values 
y=df.iloc[:,1].values

from sklearn.preprocessing import LabelEncoder
le=LabelEncoder()
y=le.fit_transform(y) 

from sklearn.model_selection import train_test_split
X_train, X_test, y_train,y_test=train_test_split(
    X
    ,y
    ,test_size=0.2
    ,stratify=y
    ,random_state=1
)

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import make_pipeline 

pipe_lr = make_pipeline(
    StandardScaler()
    , PCA(n_components=2)
    ,LogisticRegression(random_state=1)
)

pipe_lr.fit(X_train,y_train)
y_pred=pipe_lr.predict(X_test)
from sklearn.metrics import accuracy_score
acc=accuracy_score(y_pred,y_test)

from sklearn import  set_config
set_config(
    display='diagram'
)
from  sklearn.model_selection import StratifiedKFold

kfold=StratifiedKFold(
    n_splits=10
).split(X_train,y_train)

scores=[]

for k,(train,test) in enumerate(kfold):
    pipe_lr.fit(
        X_train[train]
        ,y_train[train]
    )
    score=pipe_lr.score(X_train[test],y_train[test])
    scores.append(score)

# cross_val_score
from sklearn.model_selection import cross_val_score

cv_scores=cross_val_score(
    estimator=pipe_lr
    , X=X_train
    ,y=y_train
    ,cv=10 # 10 k-fold
    ,n_jobs=1
)

# cross_validate()
from sklearn.model_selection import cross_validate

val_scores=cross_validate(
    estimator=pipe_lr
    ,X=X_train
    ,y=y_train
    ,scoring=['accuracy']
    ,cv=10
    ,n_jobs=-1
)

# cross_val_predict() 
from sklearn.model_selection import cross_val_predict
preds=cross_val_predict(
    estimator=pipe_lr
    ,X=X_train
    , y = y_train
    ,cv=10
    ,method='predict_proba'
    ,n_jobs=-1
)

# 학습 곡선
import matplotlib.pyplot as pt
from sklearn.model_selection import learning_curve

# 파이프 라인 설정
pipe_lr=make_pipeline(
    StandardScaler()
    ,LogisticRegression(
        penalty='l2'
        ,max_iter=10000
    )
)
train_sizes, train_scores, test_scores=learning_curve(
    estimator=pipe_lr
    ,X=X_train
    ,y=y_train
    ,train_sizes=np.linspace(0.1,1.0,10)
    ,cv=10
    ,n_jobs=1
)

print(f'train_sizes : {train_sizes}')

train_mean=np.mean(train_scores,axis=1)
train_std=np.std(train_scores,axis=1)
test_mean=np.mean(test_scores,axis=1)
test_std=np.std(test_scores,axis=1)

print(f'train mean : {train_mean}')
print(f'train std : {train_std}')
print(f'test mean : {test_mean}')
print(f'test std : {test_std}')
```
![image](https://github.com/user-attachments/assets/500dbfc3-5895-4591-b0d1-6be5a825f159)

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
df = pd.read_csv(
    './wdbc.data'
    ,header=None
)
X=df.iloc[:,2:].values 
y=df.iloc[:,1].values

from sklearn.preprocessing import LabelEncoder
le=LabelEncoder()
y=le.fit_transform(y) 

from sklearn.model_selection import train_test_split
X_train, X_test, y_train,y_test=train_test_split(
    X
    ,y
    ,test_size=0.2
    ,stratify=y
    ,random_state=1
)

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import make_pipeline 

pipe_lr = make_pipeline(
    StandardScaler()
    , PCA(n_components=2)
    ,LogisticRegression(random_state=1)
)

pipe_lr.fit(X_train,y_train)
y_pred=pipe_lr.predict(X_test)
from sklearn.metrics import accuracy_score
acc=accuracy_score(y_pred,y_test)

from sklearn import  set_config
set_config(
    display='diagram'
)
from  sklearn.model_selection import StratifiedKFold

kfold=StratifiedKFold(
    n_splits=10
).split(X_train,y_train)

scores=[]

for k,(train,test) in enumerate(kfold):
    pipe_lr.fit(
        X_train[train]
        ,y_train[train]
    )
    score=pipe_lr.score(X_train[test],y_train[test])
    scores.append(score)

# cross_val_score
from sklearn.model_selection import cross_val_score

cv_scores=cross_val_score(
    estimator=pipe_lr
    , X=X_train
    ,y=y_train
    ,cv=10 # 10 k-fold
    ,n_jobs=1
)

# cross_validate()
from sklearn.model_selection import cross_validate

val_scores=cross_validate(
    estimator=pipe_lr
    ,X=X_train
    ,y=y_train
    ,scoring=['accuracy']
    ,cv=10
    ,n_jobs=-1
)

# cross_val_predict() 
from sklearn.model_selection import cross_val_predict
preds=cross_val_predict(
    estimator=pipe_lr
    ,X=X_train
    , y = y_train
    ,cv=10
    ,method='predict_proba'
    ,n_jobs=-1
)

# 학습 곡선
import matplotlib.pyplot as pt
from sklearn.model_selection import learning_curve

# 파이프 라인 설정
pipe_lr=make_pipeline(
    StandardScaler()
    ,LogisticRegression(
        penalty='l2'
        ,max_iter=10000
    )
)
train_sizes, train_scores, test_scores=learning_curve(
    estimator=pipe_lr
    ,X=X_train
    ,y=y_train
    ,train_sizes=np.linspace(0.1,1.0,10)
    ,cv=10
    ,n_jobs=1
)

train_mean=np.mean(train_scores,axis=1)
train_std=np.std(train_scores,axis=1)
test_mean=np.mean(test_scores,axis=1)
test_std=np.std(test_scores,axis=1)

# 학습 데이터의 결과에 대한 선그래프
plt.plot(
    train_sizes # x축 : 훈련데이터 개수
    ,train_mean # y 축 : 정확도
    , color='blue'
    ,marker='o'
    ,markersize=5
    ,label='Training accuracy'
)
plt.grid()
plt.show()
```
![image](https://github.com/user-attachments/assets/f20ea024-a8ef-4681-8bc7-9dd82f2254d4)

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
df = pd.read_csv(
    './wdbc.data'
    ,header=None
)
X=df.iloc[:,2:].values 
y=df.iloc[:,1].values

from sklearn.preprocessing import LabelEncoder
le=LabelEncoder()
y=le.fit_transform(y) 

from sklearn.model_selection import train_test_split
X_train, X_test, y_train,y_test=train_test_split(
    X
    ,y
    ,test_size=0.2
    ,stratify=y
    ,random_state=1
)

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import make_pipeline 

pipe_lr = make_pipeline(
    StandardScaler()
    , PCA(n_components=2)
    ,LogisticRegression(random_state=1)
)

pipe_lr.fit(X_train,y_train)
y_pred=pipe_lr.predict(X_test)
from sklearn.metrics import accuracy_score
acc=accuracy_score(y_pred,y_test)

from sklearn import  set_config
set_config(
    display='diagram'
)
from  sklearn.model_selection import StratifiedKFold

kfold=StratifiedKFold(
    n_splits=10
).split(X_train,y_train)

scores=[]

for k,(train,test) in enumerate(kfold):
    pipe_lr.fit(
        X_train[train]
        ,y_train[train]
    )
    score=pipe_lr.score(X_train[test],y_train[test])
    scores.append(score)

# cross_val_score
from sklearn.model_selection import cross_val_score

cv_scores=cross_val_score(
    estimator=pipe_lr
    , X=X_train
    ,y=y_train
    ,cv=10 # 10 k-fold
    ,n_jobs=1
)

# cross_validate()
from sklearn.model_selection import cross_validate

val_scores=cross_validate(
    estimator=pipe_lr
    ,X=X_train
    ,y=y_train
    ,scoring=['accuracy']
    ,cv=10
    ,n_jobs=-1
)

# cross_val_predict() 
from sklearn.model_selection import cross_val_predict
preds=cross_val_predict(
    estimator=pipe_lr
    ,X=X_train
    , y = y_train
    ,cv=10
    ,method='predict_proba'
    ,n_jobs=-1
)

# 학습 곡선
import matplotlib.pyplot as pt
from sklearn.model_selection import learning_curve

# 파이프 라인 설정
pipe_lr=make_pipeline(
    StandardScaler()
    ,LogisticRegression(
        penalty='l2'
        ,max_iter=10000
    )
)
train_sizes, train_scores, test_scores=learning_curve(
    estimator=pipe_lr
    ,X=X_train
    ,y=y_train
    ,train_sizes=np.linspace(0.1,1.0,10)
    ,cv=10
    ,n_jobs=1
)

train_mean=np.mean(train_scores,axis=1)
train_std=np.std(train_scores,axis=1)
test_mean=np.mean(test_scores,axis=1)
test_std=np.std(test_scores,axis=1)

# 학습 데이터의 결과에 대한 선그래프
plt.plot(
    train_sizes # x축 : 훈련데이터 개수
    ,train_mean # y 축 : 정확도
    , color='blue'
    ,marker='o'
    ,markersize=5
    ,label='Training accuracy'
)
# 테스트데이터의 정확도에 대한 선그래프
plt.plot(
    train_sizes
    ,test_mean
    ,color='green'
    ,linestyle='--'
    ,marker='s'
    ,markersize=5
    ,label='Validation accuracy'
)
plt.grid()
plt.show()
```
![image](https://github.com/user-attachments/assets/57a23152-3b14-498d-be56-da365c3bdf3a)

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
df = pd.read_csv(
    './wdbc.data'
    ,header=None
)
X=df.iloc[:,2:].values 
y=df.iloc[:,1].values

from sklearn.preprocessing import LabelEncoder
le=LabelEncoder()
y=le.fit_transform(y) 

from sklearn.model_selection import train_test_split
X_train, X_test, y_train,y_test=train_test_split(
    X
    ,y
    ,test_size=0.2
    ,stratify=y
    ,random_state=1
)

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import make_pipeline 

pipe_lr = make_pipeline(
    StandardScaler()
    , PCA(n_components=2)
    ,LogisticRegression(random_state=1)
)

pipe_lr.fit(X_train,y_train)
y_pred=pipe_lr.predict(X_test)
from sklearn.metrics import accuracy_score
acc=accuracy_score(y_pred,y_test)

from sklearn import  set_config
set_config(
    display='diagram'
)
from  sklearn.model_selection import StratifiedKFold

kfold=StratifiedKFold(
    n_splits=10
).split(X_train,y_train)

scores=[]

for k,(train,test) in enumerate(kfold):
    pipe_lr.fit(
        X_train[train]
        ,y_train[train]
    )
    score=pipe_lr.score(X_train[test],y_train[test])
    scores.append(score)

# cross_val_score
from sklearn.model_selection import cross_val_score

cv_scores=cross_val_score(
    estimator=pipe_lr
    , X=X_train
    ,y=y_train
    ,cv=10 # 10 k-fold
    ,n_jobs=1
)

# cross_validate()
from sklearn.model_selection import cross_validate

val_scores=cross_validate(
    estimator=pipe_lr
    ,X=X_train
    ,y=y_train
    ,scoring=['accuracy']
    ,cv=10
    ,n_jobs=-1
)

# cross_val_predict() 
from sklearn.model_selection import cross_val_predict
preds=cross_val_predict(
    estimator=pipe_lr
    ,X=X_train
    , y = y_train
    ,cv=10
    ,method='predict_proba'
    ,n_jobs=-1
)

# 학습 곡선
import matplotlib.pyplot as pt
from sklearn.model_selection import learning_curve

# 파이프 라인 설정
pipe_lr=make_pipeline(
    StandardScaler()
    ,LogisticRegression(
        penalty='l2'
        ,max_iter=10000
    )
)
train_sizes, train_scores, test_scores=learning_curve(
    estimator=pipe_lr
    ,X=X_train
    ,y=y_train
    ,train_sizes=np.linspace(0.1,1.0,10)
    ,cv=10
    ,n_jobs=1
)

train_mean=np.mean(train_scores,axis=1)
train_std=np.std(train_scores,axis=1)
test_mean=np.mean(test_scores,axis=1)
test_std=np.std(test_scores,axis=1)

# 학습 데이터의 결과에 대한 선그래프
plt.plot(
    train_sizes # x축 : 훈련데이터 개수
    ,train_mean # y 축 : 정확도
    , color='blue'
    ,marker='o'
    ,markersize=5
    ,label='Training accuracy'
)
# 테스트데이터의 정확도에 대한 선그래프
plt.plot(
    train_sizes
    ,test_mean
    ,color='green'
    ,linestyle='--'
    ,marker='s'
    ,markersize=5
    ,label='Validation accuracy'
)
plt.ylim(
    [0.8,1.03]
)
plt.grid()
plt.show()
```
![image](https://github.com/user-attachments/assets/6c0e44a0-caf4-443b-baef-1931dd6591b9)

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
df = pd.read_csv(
    './wdbc.data'
    ,header=None
)
X=df.iloc[:,2:].values 
y=df.iloc[:,1].values

from sklearn.preprocessing import LabelEncoder
le=LabelEncoder()
y=le.fit_transform(y) 

from sklearn.model_selection import train_test_split
X_train, X_test, y_train,y_test=train_test_split(
    X
    ,y
    ,test_size=0.2
    ,stratify=y
    ,random_state=1
)

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import make_pipeline 

pipe_lr = make_pipeline(
    StandardScaler()
    , PCA(n_components=2)
    ,LogisticRegression(random_state=1)
)

pipe_lr.fit(X_train,y_train)
y_pred=pipe_lr.predict(X_test)
from sklearn.metrics import accuracy_score
acc=accuracy_score(y_pred,y_test)

from sklearn import  set_config
set_config(
    display='diagram'
)
from  sklearn.model_selection import StratifiedKFold

kfold=StratifiedKFold(
    n_splits=10
).split(X_train,y_train)

scores=[]

for k,(train,test) in enumerate(kfold):
    pipe_lr.fit(
        X_train[train]
        ,y_train[train]
    )
    score=pipe_lr.score(X_train[test],y_train[test])
    scores.append(score)

# cross_val_score
from sklearn.model_selection import cross_val_score

cv_scores=cross_val_score(
    estimator=pipe_lr
    , X=X_train
    ,y=y_train
    ,cv=10 # 10 k-fold
    ,n_jobs=1
)

# cross_validate()
from sklearn.model_selection import cross_validate

val_scores=cross_validate(
    estimator=pipe_lr
    ,X=X_train
    ,y=y_train
    ,scoring=['accuracy']
    ,cv=10
    ,n_jobs=-1
)

# cross_val_predict() 
from sklearn.model_selection import cross_val_predict
preds=cross_val_predict(
    estimator=pipe_lr
    ,X=X_train
    , y = y_train
    ,cv=10
    ,method='predict_proba'
    ,n_jobs=-1
)

# 학습 곡선
import matplotlib.pyplot as pt
from sklearn.model_selection import learning_curve

# 파이프 라인 설정
pipe_lr=make_pipeline(
    StandardScaler()
    ,LogisticRegression(
        penalty='l2'
        ,max_iter=10000
    )
)
train_sizes, train_scores, test_scores=learning_curve(
    estimator=pipe_lr
    ,X=X_train
    ,y=y_train
    ,train_sizes=np.linspace(0.1,1.0,10)
    ,cv=10
    ,n_jobs=1
)

train_mean=np.mean(train_scores,axis=1)
train_std=np.std(train_scores,axis=1)
test_mean=np.mean(test_scores,axis=1)
test_std=np.std(test_scores,axis=1)

# 학습 데이터의 결과에 대한 선그래프
plt.plot(
    train_sizes # x축 : 훈련데이터 개수
    ,train_mean # y 축 : 정확도
    , color='blue'
    ,marker='o'
    ,markersize=5
    ,label='Training accuracy'
)
# 학습데이터 정확도에 대한 편차 그래프
plt.fill_between(
    x=train_sizes # x축
    ,y1=train_mean+train_std #y1축
    ,y2=train_mean-train_std #y2축
    ,alpha=0.15
    ,color='blue'
)

# 테스트데이터의 정확도에 대한 선그래프
plt.plot(
    train_sizes
    ,test_mean
    ,color='green'
    ,linestyle='--'
    ,marker='s'
    ,markersize=5
    ,label='Validation accuracy'
)

# 테스트데이터 정확도에 대한 편차 그래프
plt.fill_between(
    x=train_sizes # x축
    ,y1=test_mean+test_std #y1축
    ,y2=test_mean-test_std #y2축
    ,alpha=0.15
    ,color='red'
)

plt.ylim(
    [0.8,1.03]
)
plt.grid()
plt.show()
```
![image](https://github.com/user-attachments/assets/2f3dc773-0a0f-4994-a8d0-0efa17b2bf52)

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
df = pd.read_csv(
    './wdbc.data'
    ,header=None
)
X=df.iloc[:,2:].values 
y=df.iloc[:,1].values

from sklearn.preprocessing import LabelEncoder
le=LabelEncoder()
y=le.fit_transform(y) 

from sklearn.model_selection import train_test_split
X_train, X_test, y_train,y_test=train_test_split(
    X
    ,y
    ,test_size=0.2
    ,stratify=y
    ,random_state=1
)

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import make_pipeline 

pipe_lr = make_pipeline(
    StandardScaler()
    , PCA(n_components=2)
    ,LogisticRegression(random_state=1)
)

pipe_lr.fit(X_train,y_train)
y_pred=pipe_lr.predict(X_test)
from sklearn.metrics import accuracy_score
acc=accuracy_score(y_pred,y_test)

from sklearn import  set_config
set_config(
    display='diagram'
)
from  sklearn.model_selection import StratifiedKFold

kfold=StratifiedKFold(
    n_splits=10
).split(X_train,y_train)

scores=[]

for k,(train,test) in enumerate(kfold):
    pipe_lr.fit(
        X_train[train]
        ,y_train[train]
    )
    score=pipe_lr.score(X_train[test],y_train[test])
    scores.append(score)

# cross_val_score
from sklearn.model_selection import cross_val_score

cv_scores=cross_val_score(
    estimator=pipe_lr
    , X=X_train
    ,y=y_train
    ,cv=10 # 10 k-fold
    ,n_jobs=1
)

# cross_validate()
from sklearn.model_selection import cross_validate

val_scores=cross_validate(
    estimator=pipe_lr
    ,X=X_train
    ,y=y_train
    ,scoring=['accuracy']
    ,cv=10
    ,n_jobs=-1
)

# cross_val_predict() 
from sklearn.model_selection import cross_val_predict
preds=cross_val_predict(
    estimator=pipe_lr
    ,X=X_train
    , y = y_train
    ,cv=10
    ,method='predict_proba'
    ,n_jobs=-1
)

# 학습 곡선
import matplotlib.pyplot as pt
from sklearn.model_selection import learning_curve

# 파이프 라인 설정
pipe_lr=make_pipeline(
    StandardScaler()
    ,LogisticRegression(
        penalty='l2'
        ,max_iter=10000
    )
)
train_sizes, train_scores, test_scores=learning_curve(
    estimator=pipe_lr
    ,X=X_train
    ,y=y_train
    ,train_sizes=np.linspace(0.1,1.0,10)
    ,cv=10
    ,n_jobs=1
)

train_mean=np.mean(train_scores,axis=1)
train_std=np.std(train_scores,axis=1)
test_mean=np.mean(test_scores,axis=1)
test_std=np.std(test_scores,axis=1)

# 학습 데이터의 결과에 대한 선그래프
plt.plot(
    train_sizes # x축 : 훈련데이터 개수
    ,train_mean # y 축 : 정확도
    , color='blue'
    ,marker='o'
    ,markersize=5
    ,label='Training accuracy'
)
# 학습데이터 정확도에 대한 편차 그래프
plt.fill_between(
    x=train_sizes # x축
    ,y1=train_mean+train_std #y1축
    ,y2=train_mean-train_std #y2축
    ,alpha=0.15
    ,color='blue'
)

# 테스트데이터의 정확도에 대한 선그래프
plt.plot(
    train_sizes
    ,test_mean
    ,color='green'
    ,linestyle='--'
    ,marker='s'
    ,markersize=5
    ,label='Validation accuracy'
)

# 테스트데이터 정확도에 대한 편차 그래프
plt.fill_between(
    x=train_sizes # x축
    ,y1=test_mean+test_std #y1축
    ,y2=test_mean-test_std #y2축
    ,alpha=0.15
    ,color='red'
)

plt.ylim(
    [0.8,1.03]
)

plt.grid()
plt.xlabel('number of training examples')
plt.ylabel('Accuracy')
plt.legend(loc='lower right')
plt.show()
```
![image](https://github.com/user-attachments/assets/794f9996-6e96-4a1d-88b6-78bcad5cbfef)

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
df = pd.read_csv(
    './wdbc.data'
    ,header=None
)
X=df.iloc[:,2:].values 
y=df.iloc[:,1].values

from sklearn.preprocessing import LabelEncoder
le=LabelEncoder()
y=le.fit_transform(y) 

from sklearn.model_selection import train_test_split
X_train, X_test, y_train,y_test=train_test_split(
    X
    ,y
    ,test_size=0.2
    ,stratify=y
    ,random_state=1
)

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import make_pipeline 

pipe_lr = make_pipeline(
    StandardScaler()
    , PCA(n_components=2)
    ,LogisticRegression(random_state=1)
)

pipe_lr.fit(X_train,y_train)
y_pred=pipe_lr.predict(X_test)
from sklearn.metrics import accuracy_score
acc=accuracy_score(y_pred,y_test)

from sklearn import  set_config
set_config(
    display='diagram'
)
from  sklearn.model_selection import StratifiedKFold

kfold=StratifiedKFold(
    n_splits=10
).split(X_train,y_train)

scores=[]

for k,(train,test) in enumerate(kfold):
    pipe_lr.fit(
        X_train[train]
        ,y_train[train]
    )
    score=pipe_lr.score(X_train[test],y_train[test])
    scores.append(score)

# cross_val_score
from sklearn.model_selection import cross_val_score

cv_scores=cross_val_score(
    estimator=pipe_lr
    , X=X_train
    ,y=y_train
    ,cv=10 # 10 k-fold
    ,n_jobs=1
)

# cross_validate()
from sklearn.model_selection import cross_validate

val_scores=cross_validate(
    estimator=pipe_lr
    ,X=X_train
    ,y=y_train
    ,scoring=['accuracy']
    ,cv=10
    ,n_jobs=-1
)

# cross_val_predict() 
from sklearn.model_selection import cross_val_predict
preds=cross_val_predict(
    estimator=pipe_lr
    ,X=X_train
    , y = y_train
    ,cv=10
    ,method='predict_proba'
    ,n_jobs=-1
)

# 학습 곡선
import matplotlib.pyplot as pt
from sklearn.model_selection import learning_curve

# 파이프 라인 설정
pipe_lr=make_pipeline(
    StandardScaler()
    ,LogisticRegression(
        penalty='l2'
        ,max_iter=10000
    )
)
train_sizes, train_scores, test_scores=learning_curve(
    estimator=pipe_lr
    ,X=X_train
    ,y=y_train
    ,train_sizes=np.linspace(0.1,1.0,10)
    ,cv=10
    ,n_jobs=1
)

train_mean=np.mean(train_scores,axis=1)
train_std=np.std(train_scores,axis=1)
test_mean=np.mean(test_scores,axis=1)
test_std=np.std(test_scores,axis=1)

from sklearn.model_selection import LearningCurveDisplay
disp=LearningCurveDisplay(
    train_sizes=train_sizes
    ,train_scores=train_scores
    ,test_scores=test_scores
    ,score_name='Accuracy'
)
disp.plot(
    score_type='both'
)
plt.legend('lower right')
plt.ylim(
    [0.8,1.03]
)
plt.show()
```
![image](https://github.com/user-attachments/assets/22bf0e78-9775-4d97-97e9-7053a0348479)

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
df = pd.read_csv(
    './wdbc.data'
    ,header=None
)
X=df.iloc[:,2:].values 
y=df.iloc[:,1].values

from sklearn.preprocessing import LabelEncoder
le=LabelEncoder()
y=le.fit_transform(y) 

from sklearn.model_selection import train_test_split
X_train, X_test, y_train,y_test=train_test_split(
    X
    ,y
    ,test_size=0.2
    ,stratify=y
    ,random_state=1
)

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import make_pipeline 

pipe_lr = make_pipeline(
    StandardScaler()
    , PCA(n_components=2)
    ,LogisticRegression(random_state=1)
)

pipe_lr.fit(X_train,y_train)
y_pred=pipe_lr.predict(X_test)
from sklearn.metrics import accuracy_score
acc=accuracy_score(y_pred,y_test)

from sklearn import  set_config
set_config(
    display='diagram'
)
from  sklearn.model_selection import StratifiedKFold

kfold=StratifiedKFold(
    n_splits=10
).split(X_train,y_train)

scores=[]

for k,(train,test) in enumerate(kfold):
    pipe_lr.fit(
        X_train[train]
        ,y_train[train]
    )
    score=pipe_lr.score(X_train[test],y_train[test])
    scores.append(score)

# cross_val_score
from sklearn.model_selection import cross_val_score

cv_scores=cross_val_score(
    estimator=pipe_lr
    , X=X_train
    ,y=y_train
    ,cv=10 # 10 k-fold
    ,n_jobs=1
)

# cross_validate()
from sklearn.model_selection import cross_validate

val_scores=cross_validate(
    estimator=pipe_lr
    ,X=X_train
    ,y=y_train
    ,scoring=['accuracy']
    ,cv=10
    ,n_jobs=-1
)

# cross_val_predict() 
from sklearn.model_selection import cross_val_predict
preds=cross_val_predict(
    estimator=pipe_lr
    ,X=X_train
    , y = y_train
    ,cv=10
    ,method='predict_proba'
    ,n_jobs=-1
)

# 학습 곡선
import matplotlib.pyplot as pt
from sklearn.model_selection import learning_curve

# 파이프 라인 설정
pipe_lr=make_pipeline(
    StandardScaler()
    ,LogisticRegression(
        penalty='l2'
        ,max_iter=10000
    )
)

# 검증 곡선 처리
from sklearn.model_selection import validation_curve

# 규제에 적용할 값을 리스토로 생성
# 그런데 이 값이 좋은 것인지 모름
# 첫번째 최적 값을 모른다. => 그리드 서치 사용 필요
# 정밀하게 처리하게 다른 패키지 사용이 필요 : hyperOpt
param_range=[
    0.00001
    ,0.0001
    ,0.001
    ,0.01
    ,0.1
    ,1.0
    ,10.0
    ,100.0
    ,1000.0
    ,10000.0
] # 정확한 값을 모름 더 작을수도 더 클수도 있고 리스트 안에 없을 수도 있음

# 검증 곡선 생성 : 베스트 C 값을 찾는게 목적이다.
train_scores, test_scores=\
    validation_curve(estimator=pipe_lr, # pipe_lr에 학습이 포함, 학습은 하지만 베스트모델 생성x
                     X=X_train,
                     y=y_train,
                     param_name='logisticregression__C',
                     param_range=param_range,                                           
                     cv=10
                     )


# 계산
train_mean=np.mean(train_scores,axis=1)
train_std=np.std(train_scores,axis=1)
test_mean=np.mean(test_scores,axis=1)
test_std=np.std(test_scores,axis=1)

# 차트
plt.plot(
    param_range
    ,train_mean
    ,color='blue'
    ,marker='o'
    ,markersize=5
    ,label='Training Accuracy'
)
plt.fill_between(
    param_range
    ,y1=train_mean+train_std
    ,y2=train_mean-train_std
    ,alpha=0.5
    ,color='blue'
)

plt.plot(
    param_range
    ,test_mean
    ,color='green'
    ,marker='o'
    ,markersize=5
    ,label='Test Accuracy'
)
plt.fill_between(
    param_range
    ,y1=test_mean+test_std
    ,y2=test_mean-test_std
    ,alpha=0.5
    ,color='green'
)
plt.xscale('log')
plt.ylim([0.8,1.005])
plt.show()
```
![image](https://github.com/user-attachments/assets/44eb53c9-d854-43f6-bd27-7bfed213fa82)

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
df = pd.read_csv(
    './wdbc.data'
    ,header=None
)
X=df.iloc[:,2:].values 
y=df.iloc[:,1].values

from sklearn.preprocessing import LabelEncoder
le=LabelEncoder()
y=le.fit_transform(y) 

from sklearn.model_selection import train_test_split
X_train, X_test, y_train,y_test=train_test_split(
    X
    ,y
    ,test_size=0.2
    ,stratify=y
    ,random_state=1
)

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import make_pipeline 

pipe_lr = make_pipeline(
    StandardScaler()
    , PCA(n_components=2)
    ,LogisticRegression(random_state=1)
)

pipe_lr.fit(X_train,y_train)
y_pred=pipe_lr.predict(X_test)
from sklearn.metrics import accuracy_score
acc=accuracy_score(y_pred,y_test)

from sklearn import  set_config
set_config(
    display='diagram'
)
from  sklearn.model_selection import StratifiedKFold

kfold=StratifiedKFold(
    n_splits=10
).split(X_train,y_train)

scores=[]

for k,(train,test) in enumerate(kfold):
    pipe_lr.fit(
        X_train[train]
        ,y_train[train]
    )
    score=pipe_lr.score(X_train[test],y_train[test])
    scores.append(score)

# cross_val_score
from sklearn.model_selection import cross_val_score

cv_scores=cross_val_score(
    estimator=pipe_lr
    , X=X_train
    ,y=y_train
    ,cv=10 # 10 k-fold
    ,n_jobs=1
)

# cross_validate()
from sklearn.model_selection import cross_validate

val_scores=cross_validate(
    estimator=pipe_lr
    ,X=X_train
    ,y=y_train
    ,scoring=['accuracy']
    ,cv=10
    ,n_jobs=-1
)

# cross_val_predict() 
from sklearn.model_selection import cross_val_predict
preds=cross_val_predict(
    estimator=pipe_lr
    ,X=X_train
    , y = y_train
    ,cv=10
    ,method='predict_proba'
    ,n_jobs=-1
)

# 학습 곡선
import matplotlib.pyplot as pt
from sklearn.model_selection import learning_curve

# 파이프 라인 설정
pipe_lr=make_pipeline(
    StandardScaler()
    ,LogisticRegression(
        penalty='l2'
        ,max_iter=10000
    )
)

# 검증 곡선 처리
from sklearn.model_selection import validation_curve

# 규제에 적용할 값을 리스토로 생성
# 그런데 이 값이 좋은 것인지 모름
# 첫번째 최적 값을 모른다. => 그리드 서치 사용 필요
# 정밀하게 처리하게 다른 패키지 사용이 필요 : hyperOpt
param_range=[
    0.001
    ,0.01
    ,0.1
    ,1.0
    ,10.0
    ,100.0
    ,1000.0
] # 정확한 값을 모름 더 작을수도 더 클수도 있고 리스트 안에 없을 수도 있음

# 검증 곡선 생성 : 베스트 C 값을 찾는게 목적이다.
train_scores, test_scores=\
    validation_curve(estimator=pipe_lr, # pipe_lr에 학습이 포함, 학습은 하지만 베스트모델 생성x
                     X=X_train,
                     y=y_train,
                     param_name='logisticregression__C',
                     param_range=param_range,                                           
                     cv=10
                     )


# 계산
train_mean=np.mean(train_scores,axis=1)
train_std=np.std(train_scores,axis=1)
test_mean=np.mean(test_scores,axis=1)
test_std=np.std(test_scores,axis=1)

# 차트
plt.plot(
    param_range
    ,train_mean
    ,color='blue'
    ,marker='o'
    ,markersize=5
    ,label='Training Accuracy'
)
plt.fill_between(
    param_range
    ,y1=train_mean+train_std
    ,y2=train_mean-train_std
    ,alpha=0.5
    ,color='blue'
)

plt.plot(
    param_range
    ,test_mean
    ,color='green'
    ,marker='o'
    ,markersize=5
    ,label='Test Accuracy'
)
plt.fill_between(
    param_range
    ,y1=test_mean+test_std
    ,y2=test_mean-test_std
    ,alpha=0.5
    ,color='green'
)
plt.xscale('log')
plt.ylim([0.8,1.005])
plt.show()
```
![image](https://github.com/user-attachments/assets/6ea13373-d6b1-46b5-a2da-8a0c87cf74be)

---
## 그리드 서치를 사용한 머신 러닝 모델 세부 튜닝

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve
df = pd.read_csv(
    './wdbc.data'
    ,header=None
)
X=df.iloc[:,2:].values 
y=df.iloc[:,1].values

le=LabelEncoder()
y=le.fit_transform(y) 

X_train, X_test, y_train,y_test=train_test_split(
    X
    ,y
    ,test_size=0.2
    ,stratify=y
    ,random_state=1
)

# 그리드 서치

pipe_svc=make_pipeline(
    StandardScaler()
    ,SVC(random_state=1) # 기본 모델 생성
)

param_range=[
    0.00001
    ,0.0001
    ,0.001
    ,0.01
    ,0.1
    ,1.0
    ,10.0
    ,100.0
    ,1000.0
    ,10000.0
    
]
param_grid=[
    {
        'svc__C': param_range,  
        'svc__kernel': ['linear']
    },
    {
        'svc__C': param_range, 
        'svc__gamma': param_range,  
        'svc__kernel': ['rbf']
    }
]


gridsearch=GridSearchCV( # 베스트 모델 : 베스트 파라미터에서 나온 성능을 평가
    estimator=pipe_svc
    ,param_grid=param_grid
    ,scoring='accuracy' # 성능을 평가
    ,cv=10
    ,refit=True
    ,n_jobs=-1
)

gridsearch.fit(X_train,y_train)

print(f'최적의 정확도 : {gridsearch.best_score_}')
print(f'최적의 파라미터 : {gridsearch.best_params_}')
```
![image](https://github.com/user-attachments/assets/b89c2570-da21-4514-be7f-a0feb778fa34)

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve
from sklearn.metrics import accuracy_score
df = pd.read_csv(
    './wdbc.data'
    ,header=None
)
X=df.iloc[:,2:].values 
y=df.iloc[:,1].values

le=LabelEncoder()
y=le.fit_transform(y) 

X_train, X_test, y_train,y_test=train_test_split(
    X
    ,y
    ,test_size=0.2
    ,stratify=y
    ,random_state=1
)

# 그리드 서치

pipe_svc=make_pipeline(
    StandardScaler()
    ,SVC(random_state=1) # 기본 모델 생성
)

param_range=[
    0.00001
    ,0.0001
    ,0.001
    ,0.01
    ,0.1
    ,1.0
    ,10.0
    ,100.0
    ,1000.0
    ,10000.0
    
]
param_grid=[
    {
        'svc__C': param_range,  
        'svc__kernel': ['linear']
    },
    {
        'svc__C': param_range, 
        'svc__gamma': param_range,  
        'svc__kernel': ['rbf']
    }
]


gridsearch=GridSearchCV( # 베스트 모델 : 베스트 파라미터에서 나온 성능을 평가
    estimator=pipe_svc
    ,param_grid=param_grid
    ,scoring='accuracy' # 성능을 평가
    ,cv=10
    ,refit=True
    ,n_jobs=-1
)

gridsearch.fit(X_train,y_train)

print(f'최적의 정확도 : {gridsearch.best_score_}')
print(f'최적의 파라미터 : {gridsearch.best_params_}')
print(f'최고의 모델의 예측값 :  {gridsearch.best_estimator_.predict(X_test)}')
print(f'테스트데이터 정확도 : {accuracy_score(gridsearch.best_estimator_.predict(X_test),y_test)}')
```
![image](https://github.com/user-attachments/assets/e67ff8b0-1462-448b-a047-db1648a6fb7d)

--
## 랜덤 서치

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split, RandomizedSearchCV, learning_curve
from sklearn.metrics import accuracy_score
import scipy.stats
df = pd.read_csv(
    './wdbc.data'
    ,header=None
)
X=df.iloc[:,2:].values 
y=df.iloc[:,1].values

le=LabelEncoder()
y=le.fit_transform(y) 

X_train, X_test, y_train,y_test=train_test_split(
    X
    ,y
    ,test_size=0.2
    ,stratify=y
    ,random_state=1
)

pipe_svc=make_pipeline(
    StandardScaler()
    ,SVC(random_state=1) # 기본 모델 생성
)



# 그리드 서치는 완전 탐색이기 때문에 사용자가 지정한 파라미터 그리드에 최적의 하이퍼파라미터가 포함되어 있다면 반드시 찾을 수 있지만, 하이퍼파라미터 그리드 설정이 크면 그리드 서치 비용이 많이 소요된다. 그래서 여러 파라미터 조합을 샘플링하는 랜덤 서치 방식이 더 효율적이다.
param_range=scipy.stats.loguniform(0.0001,1000.0)

param_grid=[
    {
        'svc__C': param_range,  
        'svc__kernel': ['linear']
    },
    {
        'svc__C': param_range, 
        'svc__gamma': param_range,  
        'svc__kernel': ['rbf']
    }
]

rs=RandomizedSearchCV(
    estimator=pipe_svc
    ,param_distributions=param_grid
    ,scoring='accuracy'
    ,refit=True
    ,n_iter=20
    ,cv=10
    ,random_state=1
    ,n_jobs=-1
)
rs=rs.fit(X_train,y_train)

print(f'최적의 정확도 : {rs.best_score_}')
print(f'최적의 파라미터 : {rs.best_params_}')
print(f'최고의 모델의 예측값 :  {rs.best_estimator_.predict(X_test)}')
print(f'테스트데이터 정확도 : {accuracy_score(rs.best_estimator_.predict(X_test),y_test)}')
```
![image](https://github.com/user-attachments/assets/65b45d76-80bc-4403-b1aa-318327561995)

---

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split, RandomizedSearchCV, learning_curve
from sklearn.metrics import accuracy_score

df = pd.read_csv(
    './wdbc.data'
    ,header=None
)
X=df.iloc[:,2:].values
y=df.iloc[:,1].values

le=LabelEncoder()
y=le.fit_transform(y)

X_train, X_test, y_train,y_test=train_test_split(
    X
    ,y
    ,test_size=0.2
    ,stratify=y
    ,random_state=1
)

pipe_svc=make_pipeline(
    StandardScaler()
    ,SVC(random_state=1) # 기본 모델 생성
)

pipe_svc.fit(X_train,y_train) # 학습 처리
y_pred=pipe_svc.predict(X_test)
from sklearn.metrics import confusion_matrix

confmat=confusion_matrix(y_true=y_test, y_pred=y_pred)
print(confmat)
print(y_test)
print(y_pred)
```
![image](https://github.com/user-attachments/assets/c22f59a1-ddb6-408d-90de-848d6944925d)

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split, RandomizedSearchCV, learning_curve
from sklearn.metrics import accuracy_score

df = pd.read_csv(
    './wdbc.data'
    ,header=None
)
X=df.iloc[:,2:].values
y=df.iloc[:,1].values

le=LabelEncoder()
y=le.fit_transform(y)

X_train, X_test, y_train,y_test=train_test_split(
    X
    ,y
    ,test_size=0.2
    ,stratify=y
    ,random_state=1
)

pipe_svc=make_pipeline(
    StandardScaler()
    ,SVC(random_state=1) # 기본 모델 생성
)

pipe_svc.fit(X_train,y_train) # 학습 처리
y_pred=pipe_svc.predict(X_test)
from sklearn.metrics import confusion_matrix

confmat=confusion_matrix(y_true=y_test, y_pred=y_pred)

# ConfusionMatrixDisplay
from sklearn.metrics import ConfusionMatrixDisplay
ConfusionMatrixDisplay.from_estimator(pipe_svc,X_test,y_test)
plt.show()
```
![image](https://github.com/user-attachments/assets/dff9fa03-85ec-42ee-aa2d-6ed8ec9e068c)

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split, RandomizedSearchCV, learning_curve
from sklearn.metrics import accuracy_score

df = pd.read_csv(
    './wdbc.data'
    ,header=None
)
X=df.iloc[:,2:].values
y=df.iloc[:,1].values

le=LabelEncoder()
y=le.fit_transform(y)

X_train, X_test, y_train,y_test=train_test_split(
    X
    ,y
    ,test_size=0.2
    ,stratify=y
    ,random_state=1
)

pipe_svc=make_pipeline(
    StandardScaler()
    ,SVC(random_state=1) # 기본 모델 생성
)

pipe_svc.fit(X_train,y_train) # 학습 처리
y_pred=pipe_svc.predict(X_test)
from sklearn.metrics import confusion_matrix

confmat=confusion_matrix(y_true=y_test, y_pred=y_pred)

# ConfusionMatrixDisplay
from sklearn.metrics import ConfusionMatrixDisplay
ConfusionMatrixDisplay.from_estimator(pipe_svc,
                                      X_test,
                                      y_test,
                                      normalize='all')
plt.show()
```
![image](https://github.com/user-attachments/assets/7b2af584-eb08-4873-9dcd-de1fdb38d3c5)

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split, RandomizedSearchCV, learning_curve
from sklearn.metrics import accuracy_score

df = pd.read_csv(
    './wdbc.data'
    ,header=None
)
X=df.iloc[:,2:].values
y=df.iloc[:,1].values

le=LabelEncoder()
y=le.fit_transform(y)

X_train, X_test, y_train,y_test=train_test_split(
    X
    ,y
    ,test_size=0.2
    ,stratify=y
    ,random_state=1
)

pipe_svc=make_pipeline(
    StandardScaler()
    ,SVC(random_state=1) # 기본 모델 생성
)

pipe_svc.fit(X_train,y_train) # 학습 처리
y_pred=pipe_svc.predict(X_test)

#정밀도(예측), 재현율(실제), f1
from sklearn.metrics import precision_score,recall_score,f1_score
precision_score_=precision_score(y_true=y_test,y_pred=y_pred)
print(f'정밀도(positive 예측) : {precision_score_}')
recall_score_=recall_score(y_true=y_test,y_pred=y_pred)
print(f'재현율 : {recall_score_}')
f1_score_=f1_score(y_true=y_test,y_pred=y_pred)
print(f'f1 : {f1_score_}')
```
![image](https://github.com/user-attachments/assets/84025711-ab1e-449b-b73a-47dde7f3e7d7)

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split, RandomizedSearchCV, learning_curve
from sklearn.metrics import accuracy_score

df = pd.read_csv(
    './wdbc.data'
    ,header=None
)
X=df.iloc[:,2:].values
y=df.iloc[:,1].values

le=LabelEncoder()
y=le.fit_transform(y)

X_train, X_test, y_train,y_test=train_test_split(
    X
    ,y
    ,test_size=0.2
    ,stratify=y
    ,random_state=1
)

pipe_svc=make_pipeline(
    StandardScaler()
    ,SVC(random_state=1) # 기본 모델 생성
)

pipe_svc.fit(X_train,y_train) # 학습 처리
y_pred=pipe_svc.predict(X_test)

# 정밀도(예측), 재현율(실제), f1
# 재현율 : 암진단, 금융사기 (놓치면 위험한 경우)
# 정밀도 : 스팸 매일, 추천시스템(잘못 긍정 예측시 비용이 큰 경우)
from sklearn.metrics import precision_score,recall_score,f1_score
precision_score_=precision_score(y_true=y_test,y_pred=y_pred)
print(f'정밀도(positive 예측) : {precision_score_}')
recall_score_=recall_score(y_true=y_test,y_pred=y_pred)
print(f'재현율 : {recall_score_}')
f1_score_=f1_score(y_true=y_test,y_pred=y_pred)
print(f'f1 : {f1_score_}')
```
![image](https://github.com/user-attachments/assets/b230bb4f-f482-4935-9f4c-d7d39688fed8)

---
## 불균형 데이터 처리

```
pip install imbalanced-learn
```
![image](https://github.com/user-attachments/assets/311311b6-fb27-4a4c-bf07-456b3bb0c09b)

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split, RandomizedSearchCV, learning_curve
from sklearn.metrics import accuracy_score

df = pd.read_csv(
    './wdbc.data'
    ,header=None
)
X = df.iloc[:, 2:].values
y = df.iloc[:, 1].values

le = LabelEncoder()
y = le.fit_transform(y)

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=1
)

# Under-sampling using RandomUnderSampler
from imblearn.under_sampling import RandomUnderSampler
rus = RandomUnderSampler(random_state=42)
X_under_resampled, y_under_resampled = rus.fit_resample(X, y)
print("Under-sampled class distribution:", np.bincount(y_under_resampled))

# Over-sampling using SMOTE
from imblearn.over_sampling import SMOTE
smote = SMOTE(random_state=1)
X_over_resampled, y_over_resampled = smote.fit_resample(X, y)
print("Over-sampled class distribution:", np.bincount(y_over_resampled))

from imblearn.combine import SMOTETomek
smote_tomek=SMOTETomek(random_state=42)
X_combine_resampled,y_combine_resampled=smote_tomek.fit_resample(X,y)
print(np.bincount(y_combine_resampled))
```
![image](https://github.com/user-attachments/assets/ad381dce-d68a-4096-a16c-b7d6f988f9f7)

### under/over : 평가지표는 정확도가 아닌 다른 지표 사용 필요
### 정밀도.재현율,fi,auc
