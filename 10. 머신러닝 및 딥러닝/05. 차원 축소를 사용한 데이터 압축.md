# 차원 축소를 사용한 데이터 압축

차원 축소는 고차원 데이터를 더 적은 차원으로 변환하여 중요한 정보를 유지하면서 데이터를 압축하는 기법입니다. 이를 통해 데이터의 복잡성을 줄이고 시각화, 모델 학습 등의 효율성을 높일 수 있습니다. 여기서는 주성분 분석(PCA), 선형 판별 분석(LDA), 그리고 비선형 차원 축소 방법을 다룰 것입니다.

## 1. 주성분 분석(PCA)을 통한 비지도 차원 축소

주성분 분석(PCA)은 **비지도 학습**의 대표적인 차원 축소 기법입니다. PCA는 데이터의 분산을 최대화하는 선형 변환을 찾아 데이터를 저차원으로 변환합니다. 주요 아이디어는 고차원 데이터를 선형 결합하여 더 적은 수의 주성분(Principal Component)으로 압축하는 것입니다. 이를 통해 원본 데이터의 분산을 최대한 보존하면서, 불필요한 특성(잡음)은 제거할 수 있습니다.

### PCA의 주요 단계
1. **데이터 전처리**: 평균을 0으로 맞추기 위해 데이터를 표준화합니다.
2. **공분산 행렬 계산**: 데이터 간의 관계를 이해하기 위해 공분산 행렬을 계산합니다.
3. **고유값과 고유벡터 계산**: 공분산 행렬의 고유값과 고유벡터를 계산하여 주성분을 찾습니다.
4. **주성분 선택**: 가장 큰 고유값에 해당하는 고유벡터를 선택하여 차원 축소를 합니다.

PCA는 비지도 학습이기 때문에 라벨이 없는 데이터에 적용됩니다.

## 2. 선형 판별 분석(LDA)을 통한 지도 방식의 데이터 압축

선형 판별 분석(LDA)은 **지도 학습** 기반의 차원 축소 기법으로, 클래스 간의 분리를 최적화하는 방식입니다. LDA는 데이터의 차원을 축소하면서도 각 클래스 간의 분산을 최대화하고, 클래스 내 분산을 최소화하는 새로운 축을 찾습니다.

### LDA의 주요 단계
1. **클래스 간 분산 행렬과 클래스 내 분산 행렬 계산**: 각 클래스의 데이터가 얼마나 분리되어 있는지를 계산합니다.
2. **고유값 분해**: 클래스 간 분산과 클래스 내 분산을 바탕으로 고유값 분해를 수행하여 변환 행렬을 얻습니다.
3. **주성분 선택**: 변환 행렬에서 가장 큰 고유값에 해당하는 고유벡터를 선택합니다.

LDA는 데이터에 클래스 라벨이 존재할 때 유용하며, 데이터의 차원을 축소하면서도 분류 작업의 성능을 높일 수 있습니다.

## 3. 비선형 차원 축소와 시각화

PCA나 LDA는 모두 **선형** 방법으로 차원 축소를 수행합니다. 그러나 데이터가 비선형 관계를 가질 경우, 선형 기법만으로는 충분한 압축이 어렵습니다. 이때 사용하는 기법들이 **비선형 차원 축소** 방법입니다.

### 대표적인 비선형 차원 축소 기법
- **t-SNE** (t-Distributed Stochastic Neighbor Embedding)
- **UMAP** (Uniform Manifold Approximation and Projection)
- **Isomap**

### t-SNE (t-Distributed Stochastic Neighbor Embedding)
t-SNE는 데이터 포인트 간의 거리가 비선형적으로 보존되도록 하는 차원 축소 기법입니다. 주로 데이터의 **시각화**에 사용되며, 고차원 데이터를 2D 또는 3D로 축소하여 클러스터 구조를 시각적으로 확인할 수 있게 해줍니다.

### UMAP (Uniform Manifold Approximation and Projection)
UMAP은 t-SNE와 비슷한 목적을 가지고 있지만, 더 빠르고 확장성이 좋은 방법입니다. 고차원 데이터를 저차원으로 축소할 때, 데이터의 글로벌 구조를 유지하면서 국소적 구조도 잘 보존합니다.

### Isomap
Isomap은 고차원 데이터의 **기하학적 구조**를 보존하는 비선형 차원 축소 기법입니다. 데이터 포인트 간의 최단 경로를 계산하여 비선형 구조를 학습한 후, 이를 저차원으로 투영합니다.

## 비선형 차원 축소와 시각화
비선형 차원 축소 기법들은 특히 데이터의 클러스터 구조나 복잡한 관계를 시각화할 때 유용합니다. 예를 들어, MNIST와 같은 손글씨 데이터셋에서는 t-SNE나 UMAP을 통해 각 숫자 클래스 간의 관계를 저차원 공간에서 시각적으로 확인할 수 있습니다.

## 요약
- **주성분 분석(PCA)**: 비지도 학습 방식의 차원 축소 기법으로, 데이터를 선형 변환하여 분산을 최대화합니다.
- **선형 판별 분석(LDA)**: 지도 학습 방식의 차원 축소 기법으로, 클래스 간 분산을 최대화하고 클래스 내 분산을 최소화합니다.
- **비선형 차원 축소**: t-SNE, UMAP, Isomap과 같은 기법들은 데이터의 비선형 구조를 보존하며, 주로 데이터 시각화에 활용됩니다.

이러한 방법들을 적절히 선택하여 데이터의 차원을 축소하고 중요한 정보를 유지할 수 있습니다.

---
## 주성분 추출

```
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# 주성분 추출 단계
# 1. 데이터를 표준화 처리
# 2. 공분산 행렬
# 3. 행렬분석(고유값 분해) : 고유값, 고유벡터 분해
# 4. 고유값을 내림차순 정려 => 고유백터주성분)의 순위

# 데이터 로딩
df_wine=pd.read_csv(
    '../wine.data'
    ,header=None
    ,encoding='utf-8'
)

# 데이터와 답을 분리
X,y=df_wine.iloc[:,1:].values,df_wine.iloc[:,0].values
print(f'wine data의 shape : {df_wine.shape}')
print(f'X의 shape : {X.shape}')
print(f'y의 shape : {y.shape}')

# 학습데이터와 테스트데이터 분리
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(
    X
    ,y
    ,test_size=0.3
    ,stratify=y
    ,random_state=0
)
print(f'X_train의 shape : {X_train.shape}')
print(f'X_test의 shape : {X_test.shape}')
print(f'y_train의 shape : {y_train.shape}')
print(f'y_test의 shape : {y_test.shape}')
```
![image](https://github.com/user-attachments/assets/430fa6e1-5284-46c1-a901-00202445171a)

```
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# 주성분 추출 단계
# 1. 데이터를 표준화 처리
# 2. 공분산 행렬
# 3. 행렬분석(고유값 분해) : 고유값, 고유벡터 분해
# 4. 고유값을 내림차순 정려 => 고유백터주성분)의 순위

# 데이터 로딩
df_wine=pd.read_csv(
    '../wine.data'
    ,header=None
    ,encoding='utf-8'
)

# 데이터와 답을 분리
X,y=df_wine.iloc[:,1:].values,df_wine.iloc[:,0].values
# print(f'wine data의 shape : {df_wine.shape}')
# print(f'X의 shape : {X.shape}')
# print(f'y의 shape : {y.shape}')

# 학습데이터와 테스트데이터 분리
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(
    X
    ,y
    ,test_size=0.3
    ,stratify=y
    ,random_state=0
)
# print(f'X_train의 shape : {X_train.shape}')
# print(f'X_test의 shape : {X_test.shape}')
# print(f'y_train의 shape : {y_train.shape}')
# print(f'y_test의 shape : {y_test.shape}')

# 특성을 표준화 : StandardScaler 사용
from sklearn.preprocessing import StandardScaler

# StandardScaler 오브젝트 생성
sc = StandardScaler()

# 학습 데이터에 대해 표준화 진행
X_train_std = sc.fit_transform(X_train)  # fit_transform gives standardized data

# 테스트 데이터는 학습 데이터로부터 계산된 평균과 표준편차를 사용하여 표준화
X_test_std = sc.transform(X_test)

# 공분산 행렬
cov_mat = np.cov(X_train_std.T) 
print(f'공분산\n{cov_mat}')
```
![image](https://github.com/user-attachments/assets/5d51aa5e-32c5-4d57-b5ac-3462137984fe)

```
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# 주성분 추출 단계
# 1. 데이터를 표준화 처리
# 2. 공분산 행렬
# 3. 행렬분석(고유값 분해) : 고유값, 고유벡터 분해
# 4. 고유값을 내림차순 정려 => 고유백터주성분)의 순위

# 데이터 로딩
df_wine=pd.read_csv(
    '../wine.data'
    ,header=None
    ,encoding='utf-8'
)

# 데이터와 답을 분리
X,y=df_wine.iloc[:,1:].values,df_wine.iloc[:,0].values
# print(f'wine data의 shape : {df_wine.shape}')
# print(f'X의 shape : {X.shape}')
# print(f'y의 shape : {y.shape}')

# 학습데이터와 테스트데이터 분리
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(
    X
    ,y
    ,test_size=0.3
    ,stratify=y
    ,random_state=0
)
# print(f'X_train의 shape : {X_train.shape}')
# print(f'X_test의 shape : {X_test.shape}')
# print(f'y_train의 shape : {y_train.shape}')
# print(f'y_test의 shape : {y_test.shape}')

# 특성을 표준화 : StandardScaler 사용
from sklearn.preprocessing import StandardScaler

# StandardScaler 오브젝트 생성
sc = StandardScaler()

# 학습 데이터에 대해 표준화 진행
X_train_std = sc.fit_transform(X_train)  # fit_transform gives standardized data

# 테스트 데이터는 학습 데이터로부터 계산된 평균과 표준편차를 사용하여 표준화
X_test_std = sc.transform(X_test)

# 공분산 행렬
cov_mat = np.cov(X_train_std.T) 
# print(f'공분산\n{cov_mat}')

# 고유값 분해
eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)

# 고윳값 출력
print(f'고윳값\n{eigen_vals}')
print(f'고윳벡터의 차원\n{eigen_vecs.shape}')
print(f'고유벡터\n{eigen_vecs}')
```
![image](https://github.com/user-attachments/assets/102bab5a-c4dd-4854-80d2-4fc6fcb4240a)

```
import numpy as np
import pandas as pd

# 주성분 추출 단계
# 1. 데이터를 표준화 처리
# 2. 공분산 행렬
# 3. 행렬분석(고유값 분해) : 고유값, 고유벡터 분해
# 4. 고유값을 내림차순 정려 => 고유백터주성분)의 순위

# 데이터 로딩
df_wine=pd.read_csv(
    '../wine.data'
    ,header=None
    ,encoding='utf-8'
)

# 데이터와 답을 분리
X,y=df_wine.iloc[:,1:].values,df_wine.iloc[:,0].values
# print(f'wine data의 shape : {df_wine.shape}')
# print(f'X의 shape : {X.shape}')
# print(f'y의 shape : {y.shape}')

# 학습데이터와 테스트데이터 분리
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(
    X
    ,y
    ,test_size=0.3
    ,stratify=y
    ,random_state=0
)
# print(f'X_train의 shape : {X_train.shape}')
# print(f'X_test의 shape : {X_test.shape}')
# print(f'y_train의 shape : {y_train.shape}')
# print(f'y_test의 shape : {y_test.shape}')

# 특성을 표준화 : StandardScaler 사용
from sklearn.preprocessing import StandardScaler

# StandardScaler 오브젝트 생성
sc = StandardScaler()

# 학습 데이터에 대해 표준화 진행
X_train_std = sc.fit_transform(X_train)  # fit_transform gives standardized data

# 테스트 데이터는 학습 데이터로부터 계산된 평균과 표준편차를 사용하여 표준화
X_test_std = sc.transform(X_test)

# 공분산 행렬
cov_mat = np.cov(X_train_std.T) 
# print(f'공분산\n{cov_mat}')

# 고유값 분해
eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)

# 고윳값 출력
# print(f'고윳값\n{eigen_vals}')
# print(f'고윳벡터의 차원\n{eigen_vecs.shape}')
# print(f'고유벡터\n{eigen_vecs}')

# 총분산과 설명된 분산
tot=np.sum(eigen_vals) #고유값의 전체 합
# 고유값 비율
var_exp=[
    (i/tot) for i in sorted(eigen_vals,reverse=True)
]
# 고유값 비율의 누적합
cum_var_exp = np.cumsum(var_exp)

# 시각화
import matplotlib.pyplot as plt

plt.bar(
    range(1,len(var_exp)+1)
    , var_exp
    ,align='center'
)
plt.step(
    range(1,len(var_exp)+1)
    , cum_var_exp
    ,where='mid'
)
plt.grid()
plt.show()
```
![image](https://github.com/user-attachments/assets/156ceab2-ef3d-4e36-ab6f-718b8777be28)

---
## 특성 변환

```
import numpy as np
import pandas as pd

df_wine=pd.read_csv(
    '../wine.data'
    ,header=None
    ,encoding='utf-8'
)

X,y=df_wine.iloc[:,1:].values,df_wine.iloc[:,0].values

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(
    X
    ,y
    ,test_size=0.3
    ,stratify=y
    ,random_state=0
)

from sklearn.preprocessing import StandardScaler

sc = StandardScaler()

X_train_std = sc.fit_transform(X_train)  

X_test_std = sc.transform(X_test)

cov_mat = np.cov(X_train_std.T) 

eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)

tot=np.sum(eigen_vals) 

var_exp=[
    (i/tot) for i in sorted(eigen_vals,reverse=True)
]

cum_var_exp = np.cumsum(var_exp)

# 고유값, 고유 벡터 튜플의 리스트 처리
eigen_pairs=[
    (
        np.abs(eigen_vals[idx])
        , eigen_vecs[:,idx]
    ) for idx in range(len(eigen_vals))
]

# 내림차순 정렬
eigen_pairs.sort(
    key=lambda k :k[0]
    ,reverse=True
)

print(f'eigen_pairs\n{eigen_pairs}')
```
![image](https://github.com/user-attachments/assets/54506186-7b1d-44fa-9a76-029b62185f74)

```
import numpy as np
import pandas as pd

df_wine=pd.read_csv(
    '../wine.data'
    ,header=None
    ,encoding='utf-8'
)

X,y=df_wine.iloc[:,1:].values,df_wine.iloc[:,0].values

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(
    X
    ,y
    ,test_size=0.3
    ,stratify=y
    ,random_state=0
)

from sklearn.preprocessing import StandardScaler

sc = StandardScaler()

X_train_std = sc.fit_transform(X_train)  

X_test_std = sc.transform(X_test)

cov_mat = np.cov(X_train_std.T) 

eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)

tot=np.sum(eigen_vals) 

var_exp=[
    (i/tot) for i in sorted(eigen_vals,reverse=True)
]

cum_var_exp = np.cumsum(var_exp)

# 고유값, 고유 벡터 튜플의 리스트 처리
eigen_pairs=[
    (
        np.abs(eigen_vals[idx])
        , eigen_vecs[:,idx]
    ) for idx in range(len(eigen_vals))
]

# 내림차순 정렬
eigen_pairs.sort(
    key=lambda k :k[0]
    ,reverse=True
)

w = np.hstack((
    eigen_pairs[0][1][:, np.newaxis],  
    eigen_pairs[1][1][:, np.newaxis]   
))

print(f'투영 행렬\n{w}')
```
![image](https://github.com/user-attachments/assets/4c1bfbfd-55a3-491b-8285-0d78dfd3727e)

```
import numpy as np
import pandas as pd

df_wine=pd.read_csv(
    '../wine.data'
    ,header=None
    ,encoding='utf-8'
)

X,y=df_wine.iloc[:,1:].values,df_wine.iloc[:,0].values

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(
    X
    ,y
    ,test_size=0.3
    ,stratify=y
    ,random_state=0
)

from sklearn.preprocessing import StandardScaler

sc = StandardScaler()

X_train_std = sc.fit_transform(X_train)  

X_test_std = sc.transform(X_test)

cov_mat = np.cov(X_train_std.T) 

eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)

tot=np.sum(eigen_vals) 

var_exp=[
    (i/tot) for i in sorted(eigen_vals,reverse=True)
]

cum_var_exp = np.cumsum(var_exp)

# 고유값, 고유 벡터 튜플의 리스트 처리
eigen_pairs=[
    (
        np.abs(eigen_vals[idx])
        , eigen_vecs[:,idx]
    ) for idx in range(len(eigen_vals))
]

# 내림차순 정렬
eigen_pairs.sort(
    key=lambda k :k[0]
    ,reverse=True
)

w = np.hstack((
    eigen_pairs[0][1][:, np.newaxis],  
    eigen_pairs[1][1][:, np.newaxis]   
))

X_train_pca=X_train_std.dot(w)
print(f'X_train_pca.shape : {X_train_pca.shape}')
print(f'X_train_std.shape : {X_train_std.shape}')
```
![image](https://github.com/user-attachments/assets/810791d1-5e19-42e4-8e8c-86cb756e47ba)

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
df_wine=pd.read_csv(
    '../wine.data'
    ,header=None
    ,encoding='utf-8'
)

X,y=df_wine.iloc[:,1:].values,df_wine.iloc[:,0].values

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(
    X
    ,y
    ,test_size=0.3
    ,stratify=y
    ,random_state=0
)

from sklearn.preprocessing import StandardScaler

sc = StandardScaler()

X_train_std = sc.fit_transform(X_train)  

X_test_std = sc.transform(X_test)

cov_mat = np.cov(X_train_std.T) 

eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)

tot=np.sum(eigen_vals) 

var_exp=[
    (i/tot) for i in sorted(eigen_vals,reverse=True)
]

cum_var_exp = np.cumsum(var_exp)

# 고유값, 고유 벡터 튜플의 리스트 처리
eigen_pairs=[
    (
        np.abs(eigen_vals[idx])
        , eigen_vecs[:,idx]
    ) for idx in range(len(eigen_vals))
]

# 내림차순 정렬
eigen_pairs.sort(
    key=lambda k :k[0]
    ,reverse=True
)

w = np.hstack((
    eigen_pairs[0][1][:, np.newaxis],  
    eigen_pairs[1][1][:, np.newaxis]   
))

X_train_pca=X_train_std.dot(w)

colors=['r','b','g']
markers=['o','s','^']
for label, color, marker in zip(np.unique(y_train),colors,markers):
    plt.scatter(
        X_train_pca[y_train==label,0]
        ,X_train_pca[y_train==label,1]
        ,c=color
        ,marker=marker
        ,label=f'Class{label}'
    )
plt.grid()
plt.show()
```
![image](https://github.com/user-attachments/assets/4ec6fc99-4b4b-49c9-a27a-68c909d3ffa9)

---
## 사이킷런의 주성분 분석

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# sklearn PCA
from sklearn.linear_model import LogisticRegression
from sklearn.decomposition import PCA


df_wine=pd.read_csv(
    '../wine.data'
    ,header=None
    ,encoding='utf-8'
)

X,y=df_wine.iloc[:,1:].values,df_wine.iloc[:,0].values

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(
    X
    ,y
    ,test_size=0.3
    ,stratify=y
    ,random_state=0
)

from sklearn.preprocessing import StandardScaler

sc = StandardScaler()

X_train_std = sc.fit_transform(X_train)  

X_test_std = sc.transform(X_test)

# 2차원 차원축소
pca=PCA(
    n_components=2
)
lr=LogisticRegression(
    multi_class='ovr'
    , solver='lbfgs'
)

# 학습데이터/테스트데이터 차원 축소
X_train_pca=pca.fit_transform(X_train_std)
X_test_pca=pca.transform(X_test_std)

# 모델 훈련
lr.fit(X_train_pca,y_train)

from sklearn.metrics import accuracy_score
y_pred = lr.predict(X_test_pca)

print(f'정확도 : {accuracy_score(y_pred,y_test)}')
```
![image](https://github.com/user-attachments/assets/70372ffb-2ea4-4c06-8354-2a5f094e97ed)

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# sklearn PCA
from sklearn.linear_model import LogisticRegression
from sklearn.decomposition import PCA


df_wine=pd.read_csv(
    '../wine.data'
    ,header=None
    ,encoding='utf-8'
)

X,y=df_wine.iloc[:,1:].values,df_wine.iloc[:,0].values

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(
    X
    ,y
    ,test_size=0.3
    ,stratify=y
    ,random_state=0
)

from sklearn.preprocessing import StandardScaler

sc = StandardScaler()

X_train_std = sc.fit_transform(X_train)  

X_test_std = sc.transform(X_test)

# 2차원 차원축소
pca=PCA(
    n_components=None
)
lr=LogisticRegression(
    multi_class='ovr'
    , solver='lbfgs'
)

# 학습데이터/테스트데이터 차원 축소
X_train_pca=pca.fit_transform(X_train_std)
X_test_pca=pca.transform(X_test_std)

print(pca.explained_variance_ratio_)
```
![image](https://github.com/user-attachments/assets/0e08b00d-809f-443b-97dd-4649fff6601b)

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# sklearn PCA
from sklearn.linear_model import LogisticRegression
from sklearn.decomposition import PCA

df_wine = pd.read_csv(
    '../wine.data'
    , header=None
    , encoding='utf-8'
)

X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    X
    , y
    , test_size=0.3
    , stratify=y
    , random_state=0
)

from sklearn.preprocessing import StandardScaler

sc = StandardScaler()

X_train_std = sc.fit_transform(X_train)

X_test_std = sc.transform(X_test)


pca = PCA(
    n_components=0.95
)
pca.fit(X_train_std)
print(f'주성분 개수 : {pca.n_components_}')
print(f'설명된 분산 비율 : {np.sum(pca.explained_variance_ratio_)}')
```
![image](https://github.com/user-attachments/assets/407cad42-315e-4424-9c39-0829c850dee7)

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# sklearn PCA
from sklearn.linear_model import LogisticRegression
from sklearn.decomposition import PCA

df_wine = pd.read_csv(
    '../wine.data'
    , header=None
    , encoding='utf-8'
)

X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    X
    , y
    , test_size=0.3
    , stratify=y
    , random_state=0
)

from sklearn.preprocessing import StandardScaler

sc = StandardScaler()

X_train_std = sc.fit_transform(X_train)

X_test_std = sc.transform(X_test)


pca = PCA(
    n_components='mle'
)
pca.fit(X_train_std)


print(f'주성분 개수 : {pca.n_components_}')
print(f'설명된 분산 비율 : {np.sum(pca.explained_variance_ratio_)}')
```
![image](https://github.com/user-attachments/assets/a88da175-486e-461e-ad09-decf507c2b84)
