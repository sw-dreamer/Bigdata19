# 차원 축소를 사용한 데이터 압축

차원 축소는 고차원 데이터를 더 적은 차원으로 변환하여 중요한 정보를 유지하면서 데이터를 압축하는 기법입니다. 이를 통해 데이터의 복잡성을 줄이고 시각화, 모델 학습 등의 효율성을 높일 수 있습니다. 여기서는 주성분 분석(PCA), 선형 판별 분석(LDA), 그리고 비선형 차원 축소 방법을 다룰 것입니다.

## 1. 주성분 분석(PCA)을 통한 비지도 차원 축소

주성분 분석(PCA)은 **비지도 학습**의 대표적인 차원 축소 기법입니다. PCA는 데이터의 분산을 최대화하는 선형 변환을 찾아 데이터를 저차원으로 변환합니다. 주요 아이디어는 고차원 데이터를 선형 결합하여 더 적은 수의 주성분(Principal Component)으로 압축하는 것입니다. 이를 통해 원본 데이터의 분산을 최대한 보존하면서, 불필요한 특성(잡음)은 제거할 수 있습니다.

### PCA의 주요 단계
1. **데이터 전처리**: 평균을 0으로 맞추기 위해 데이터를 표준화합니다.
2. **공분산 행렬 계산**: 데이터 간의 관계를 이해하기 위해 공분산 행렬을 계산합니다.
3. **고유값과 고유벡터 계산**: 공분산 행렬의 고유값과 고유벡터를 계산하여 주성분을 찾습니다.
4. **주성분 선택**: 가장 큰 고유값에 해당하는 고유벡터를 선택하여 차원 축소를 합니다.

PCA는 비지도 학습이기 때문에 라벨이 없는 데이터에 적용됩니다.

## 2. 선형 판별 분석(LDA)을 통한 지도 방식의 데이터 압축

선형 판별 분석(LDA)은 **지도 학습** 기반의 차원 축소 기법으로, 클래스 간의 분리를 최적화하는 방식입니다. LDA는 데이터의 차원을 축소하면서도 각 클래스 간의 분산을 최대화하고, 클래스 내 분산을 최소화하는 새로운 축을 찾습니다.

### LDA의 주요 단계
1. **클래스 간 분산 행렬과 클래스 내 분산 행렬 계산**: 각 클래스의 데이터가 얼마나 분리되어 있는지를 계산합니다.
2. **고유값 분해**: 클래스 간 분산과 클래스 내 분산을 바탕으로 고유값 분해를 수행하여 변환 행렬을 얻습니다.
3. **주성분 선택**: 변환 행렬에서 가장 큰 고유값에 해당하는 고유벡터를 선택합니다.

LDA는 데이터에 클래스 라벨이 존재할 때 유용하며, 데이터의 차원을 축소하면서도 분류 작업의 성능을 높일 수 있습니다.

## 3. 비선형 차원 축소와 시각화

PCA나 LDA는 모두 **선형** 방법으로 차원 축소를 수행합니다. 그러나 데이터가 비선형 관계를 가질 경우, 선형 기법만으로는 충분한 압축이 어렵습니다. 이때 사용하는 기법들이 **비선형 차원 축소** 방법입니다.

### 대표적인 비선형 차원 축소 기법
- **t-SNE** (t-Distributed Stochastic Neighbor Embedding)
- **UMAP** (Uniform Manifold Approximation and Projection)
- **Isomap**

### t-SNE (t-Distributed Stochastic Neighbor Embedding)
t-SNE는 데이터 포인트 간의 거리가 비선형적으로 보존되도록 하는 차원 축소 기법입니다. 주로 데이터의 **시각화**에 사용되며, 고차원 데이터를 2D 또는 3D로 축소하여 클러스터 구조를 시각적으로 확인할 수 있게 해줍니다.

### UMAP (Uniform Manifold Approximation and Projection)
UMAP은 t-SNE와 비슷한 목적을 가지고 있지만, 더 빠르고 확장성이 좋은 방법입니다. 고차원 데이터를 저차원으로 축소할 때, 데이터의 글로벌 구조를 유지하면서 국소적 구조도 잘 보존합니다.

### Isomap
Isomap은 고차원 데이터의 **기하학적 구조**를 보존하는 비선형 차원 축소 기법입니다. 데이터 포인트 간의 최단 경로를 계산하여 비선형 구조를 학습한 후, 이를 저차원으로 투영합니다.

## 비선형 차원 축소와 시각화
비선형 차원 축소 기법들은 특히 데이터의 클러스터 구조나 복잡한 관계를 시각화할 때 유용합니다. 예를 들어, MNIST와 같은 손글씨 데이터셋에서는 t-SNE나 UMAP을 통해 각 숫자 클래스 간의 관계를 저차원 공간에서 시각적으로 확인할 수 있습니다.

## 요약
- **주성분 분석(PCA)**: 비지도 학습 방식의 차원 축소 기법으로, 데이터를 선형 변환하여 분산을 최대화합니다.
- **선형 판별 분석(LDA)**: 지도 학습 방식의 차원 축소 기법으로, 클래스 간 분산을 최대화하고 클래스 내 분산을 최소화합니다.
- **비선형 차원 축소**: t-SNE, UMAP, Isomap과 같은 기법들은 데이터의 비선형 구조를 보존하며, 주로 데이터 시각화에 활용됩니다.

이러한 방법들을 적절히 선택하여 데이터의 차원을 축소하고 중요한 정보를 유지할 수 있습니다.

---
## 주성분 추출

```
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# 주성분 추출 단계
# 1. 데이터를 표준화 처리
# 2. 공분산 행렬
# 3. 행렬분석(고유값 분해) : 고유값, 고유벡터 분해
# 4. 고유값을 내림차순 정려 => 고유백터주성분)의 순위

# 데이터 로딩
df_wine=pd.read_csv(
    '../wine.data'
    ,header=None
    ,encoding='utf-8'
)

# 데이터와 답을 분리
X,y=df_wine.iloc[:,1:].values,df_wine.iloc[:,0].values
print(f'wine data의 shape : {df_wine.shape}')
print(f'X의 shape : {X.shape}')
print(f'y의 shape : {y.shape}')

# 학습데이터와 테스트데이터 분리
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(
    X
    ,y
    ,test_size=0.3
    ,stratify=y
    ,random_state=0
)
print(f'X_train의 shape : {X_train.shape}')
print(f'X_test의 shape : {X_test.shape}')
print(f'y_train의 shape : {y_train.shape}')
print(f'y_test의 shape : {y_test.shape}')
```
![image](https://github.com/user-attachments/assets/430fa6e1-5284-46c1-a901-00202445171a)

```
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# 주성분 추출 단계
# 1. 데이터를 표준화 처리
# 2. 공분산 행렬
# 3. 행렬분석(고유값 분해) : 고유값, 고유벡터 분해
# 4. 고유값을 내림차순 정려 => 고유백터주성분)의 순위

# 데이터 로딩
df_wine=pd.read_csv(
    '../wine.data'
    ,header=None
    ,encoding='utf-8'
)

# 데이터와 답을 분리
X,y=df_wine.iloc[:,1:].values,df_wine.iloc[:,0].values
# print(f'wine data의 shape : {df_wine.shape}')
# print(f'X의 shape : {X.shape}')
# print(f'y의 shape : {y.shape}')

# 학습데이터와 테스트데이터 분리
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(
    X
    ,y
    ,test_size=0.3
    ,stratify=y
    ,random_state=0
)
# print(f'X_train의 shape : {X_train.shape}')
# print(f'X_test의 shape : {X_test.shape}')
# print(f'y_train의 shape : {y_train.shape}')
# print(f'y_test의 shape : {y_test.shape}')

# 특성을 표준화 : StandardScaler 사용
from sklearn.preprocessing import StandardScaler

# StandardScaler 오브젝트 생성
sc = StandardScaler()

# 학습 데이터에 대해 표준화 진행
X_train_std = sc.fit_transform(X_train)  # fit_transform gives standardized data

# 테스트 데이터는 학습 데이터로부터 계산된 평균과 표준편차를 사용하여 표준화
X_test_std = sc.transform(X_test)

# 공분산 행렬
cov_mat = np.cov(X_train_std.T) 
print(f'공분산\n{cov_mat}')
```
![image](https://github.com/user-attachments/assets/5d51aa5e-32c5-4d57-b5ac-3462137984fe)

```
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# 주성분 추출 단계
# 1. 데이터를 표준화 처리
# 2. 공분산 행렬
# 3. 행렬분석(고유값 분해) : 고유값, 고유벡터 분해
# 4. 고유값을 내림차순 정려 => 고유백터주성분)의 순위

# 데이터 로딩
df_wine=pd.read_csv(
    '../wine.data'
    ,header=None
    ,encoding='utf-8'
)

# 데이터와 답을 분리
X,y=df_wine.iloc[:,1:].values,df_wine.iloc[:,0].values
# print(f'wine data의 shape : {df_wine.shape}')
# print(f'X의 shape : {X.shape}')
# print(f'y의 shape : {y.shape}')

# 학습데이터와 테스트데이터 분리
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(
    X
    ,y
    ,test_size=0.3
    ,stratify=y
    ,random_state=0
)
# print(f'X_train의 shape : {X_train.shape}')
# print(f'X_test의 shape : {X_test.shape}')
# print(f'y_train의 shape : {y_train.shape}')
# print(f'y_test의 shape : {y_test.shape}')

# 특성을 표준화 : StandardScaler 사용
from sklearn.preprocessing import StandardScaler

# StandardScaler 오브젝트 생성
sc = StandardScaler()

# 학습 데이터에 대해 표준화 진행
X_train_std = sc.fit_transform(X_train)  # fit_transform gives standardized data

# 테스트 데이터는 학습 데이터로부터 계산된 평균과 표준편차를 사용하여 표준화
X_test_std = sc.transform(X_test)

# 공분산 행렬
cov_mat = np.cov(X_train_std.T) 
# print(f'공분산\n{cov_mat}')

# 고유값 분해
eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)

# 고윳값 출력
print(f'고윳값\n{eigen_vals}')
print(f'고윳벡터의 차원\n{eigen_vecs.shape}')
print(f'고유벡터\n{eigen_vecs}')
```
![image](https://github.com/user-attachments/assets/102bab5a-c4dd-4854-80d2-4fc6fcb4240a)

```
import numpy as np
import pandas as pd

# 주성분 추출 단계
# 1. 데이터를 표준화 처리
# 2. 공분산 행렬
# 3. 행렬분석(고유값 분해) : 고유값, 고유벡터 분해
# 4. 고유값을 내림차순 정려 => 고유백터주성분)의 순위

# 데이터 로딩
df_wine=pd.read_csv(
    '../wine.data'
    ,header=None
    ,encoding='utf-8'
)

# 데이터와 답을 분리
X,y=df_wine.iloc[:,1:].values,df_wine.iloc[:,0].values
# print(f'wine data의 shape : {df_wine.shape}')
# print(f'X의 shape : {X.shape}')
# print(f'y의 shape : {y.shape}')

# 학습데이터와 테스트데이터 분리
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(
    X
    ,y
    ,test_size=0.3
    ,stratify=y
    ,random_state=0
)
# print(f'X_train의 shape : {X_train.shape}')
# print(f'X_test의 shape : {X_test.shape}')
# print(f'y_train의 shape : {y_train.shape}')
# print(f'y_test의 shape : {y_test.shape}')

# 특성을 표준화 : StandardScaler 사용
from sklearn.preprocessing import StandardScaler

# StandardScaler 오브젝트 생성
sc = StandardScaler()

# 학습 데이터에 대해 표준화 진행
X_train_std = sc.fit_transform(X_train)  # fit_transform gives standardized data

# 테스트 데이터는 학습 데이터로부터 계산된 평균과 표준편차를 사용하여 표준화
X_test_std = sc.transform(X_test)

# 공분산 행렬
cov_mat = np.cov(X_train_std.T) 
# print(f'공분산\n{cov_mat}')

# 고유값 분해
eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)

# 고윳값 출력
# print(f'고윳값\n{eigen_vals}')
# print(f'고윳벡터의 차원\n{eigen_vecs.shape}')
# print(f'고유벡터\n{eigen_vecs}')

# 총분산과 설명된 분산
tot=np.sum(eigen_vals) #고유값의 전체 합
# 고유값 비율
var_exp=[
    (i/tot) for i in sorted(eigen_vals,reverse=True)
]
# 고유값 비율의 누적합
cum_var_exp = np.cumsum(var_exp)

# 시각화
import matplotlib.pyplot as plt

plt.bar(
    range(1,len(var_exp)+1)
    , var_exp
    ,align='center'
)
plt.step(
    range(1,len(var_exp)+1)
    , cum_var_exp
    ,where='mid'
)
plt.grid()
plt.show()
```
![image](https://github.com/user-attachments/assets/156ceab2-ef3d-4e36-ab6f-718b8777be28)

---
## 특성 변환

```
import numpy as np
import pandas as pd

df_wine=pd.read_csv(
    '../wine.data'
    ,header=None
    ,encoding='utf-8'
)

X,y=df_wine.iloc[:,1:].values,df_wine.iloc[:,0].values

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(
    X
    ,y
    ,test_size=0.3
    ,stratify=y
    ,random_state=0
)

from sklearn.preprocessing import StandardScaler

sc = StandardScaler()

X_train_std = sc.fit_transform(X_train)  

X_test_std = sc.transform(X_test)

cov_mat = np.cov(X_train_std.T) 

eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)

tot=np.sum(eigen_vals) 

var_exp=[
    (i/tot) for i in sorted(eigen_vals,reverse=True)
]

cum_var_exp = np.cumsum(var_exp)

# 고유값, 고유 벡터 튜플의 리스트 처리
eigen_pairs=[
    (
        np.abs(eigen_vals[idx])
        , eigen_vecs[:,idx]
    ) for idx in range(len(eigen_vals))
]

# 내림차순 정렬
eigen_pairs.sort(
    key=lambda k :k[0]
    ,reverse=True
)

print(f'eigen_pairs\n{eigen_pairs}')
```
![image](https://github.com/user-attachments/assets/54506186-7b1d-44fa-9a76-029b62185f74)

```
import numpy as np
import pandas as pd

df_wine=pd.read_csv(
    '../wine.data'
    ,header=None
    ,encoding='utf-8'
)

X,y=df_wine.iloc[:,1:].values,df_wine.iloc[:,0].values

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(
    X
    ,y
    ,test_size=0.3
    ,stratify=y
    ,random_state=0
)

from sklearn.preprocessing import StandardScaler

sc = StandardScaler()

X_train_std = sc.fit_transform(X_train)  

X_test_std = sc.transform(X_test)

cov_mat = np.cov(X_train_std.T) 

eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)

tot=np.sum(eigen_vals) 

var_exp=[
    (i/tot) for i in sorted(eigen_vals,reverse=True)
]

cum_var_exp = np.cumsum(var_exp)

# 고유값, 고유 벡터 튜플의 리스트 처리
eigen_pairs=[
    (
        np.abs(eigen_vals[idx])
        , eigen_vecs[:,idx]
    ) for idx in range(len(eigen_vals))
]

# 내림차순 정렬
eigen_pairs.sort(
    key=lambda k :k[0]
    ,reverse=True
)

w = np.hstack((
    eigen_pairs[0][1][:, np.newaxis],  
    eigen_pairs[1][1][:, np.newaxis]   
))

print(f'투영 행렬\n{w}')
```
![image](https://github.com/user-attachments/assets/4c1bfbfd-55a3-491b-8285-0d78dfd3727e)

```
import numpy as np
import pandas as pd

df_wine=pd.read_csv(
    '../wine.data'
    ,header=None
    ,encoding='utf-8'
)

X,y=df_wine.iloc[:,1:].values,df_wine.iloc[:,0].values

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(
    X
    ,y
    ,test_size=0.3
    ,stratify=y
    ,random_state=0
)

from sklearn.preprocessing import StandardScaler

sc = StandardScaler()

X_train_std = sc.fit_transform(X_train)  

X_test_std = sc.transform(X_test)

cov_mat = np.cov(X_train_std.T) 

eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)

tot=np.sum(eigen_vals) 

var_exp=[
    (i/tot) for i in sorted(eigen_vals,reverse=True)
]

cum_var_exp = np.cumsum(var_exp)

# 고유값, 고유 벡터 튜플의 리스트 처리
eigen_pairs=[
    (
        np.abs(eigen_vals[idx])
        , eigen_vecs[:,idx]
    ) for idx in range(len(eigen_vals))
]

# 내림차순 정렬
eigen_pairs.sort(
    key=lambda k :k[0]
    ,reverse=True
)

w = np.hstack((
    eigen_pairs[0][1][:, np.newaxis],  
    eigen_pairs[1][1][:, np.newaxis]   
))

X_train_pca=X_train_std.dot(w)
print(f'X_train_pca.shape : {X_train_pca.shape}')
print(f'X_train_std.shape : {X_train_std.shape}')
```
![image](https://github.com/user-attachments/assets/810791d1-5e19-42e4-8e8c-86cb756e47ba)

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
df_wine=pd.read_csv(
    '../wine.data'
    ,header=None
    ,encoding='utf-8'
)

X,y=df_wine.iloc[:,1:].values,df_wine.iloc[:,0].values

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(
    X
    ,y
    ,test_size=0.3
    ,stratify=y
    ,random_state=0
)

from sklearn.preprocessing import StandardScaler

sc = StandardScaler()

X_train_std = sc.fit_transform(X_train)  

X_test_std = sc.transform(X_test)

cov_mat = np.cov(X_train_std.T) 

eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)

tot=np.sum(eigen_vals) 

var_exp=[
    (i/tot) for i in sorted(eigen_vals,reverse=True)
]

cum_var_exp = np.cumsum(var_exp)

# 고유값, 고유 벡터 튜플의 리스트 처리
eigen_pairs=[
    (
        np.abs(eigen_vals[idx])
        , eigen_vecs[:,idx]
    ) for idx in range(len(eigen_vals))
]

# 내림차순 정렬
eigen_pairs.sort(
    key=lambda k :k[0]
    ,reverse=True
)

w = np.hstack((
    eigen_pairs[0][1][:, np.newaxis],  
    eigen_pairs[1][1][:, np.newaxis]   
))

X_train_pca=X_train_std.dot(w)

colors=['r','b','g']
markers=['o','s','^']
for label, color, marker in zip(np.unique(y_train),colors,markers):
    plt.scatter(
        X_train_pca[y_train==label,0]
        ,X_train_pca[y_train==label,1]
        ,c=color
        ,marker=marker
        ,label=f'Class{label}'
    )
plt.grid()
plt.show()
```
![image](https://github.com/user-attachments/assets/4ec6fc99-4b4b-49c9-a27a-68c909d3ffa9)

---
## 사이킷런의 주성분 분석

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# sklearn PCA
from sklearn.linear_model import LogisticRegression
from sklearn.decomposition import PCA


df_wine=pd.read_csv(
    '../wine.data'
    ,header=None
    ,encoding='utf-8'
)

X,y=df_wine.iloc[:,1:].values,df_wine.iloc[:,0].values

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(
    X
    ,y
    ,test_size=0.3
    ,stratify=y
    ,random_state=0
)

from sklearn.preprocessing import StandardScaler

sc = StandardScaler()

X_train_std = sc.fit_transform(X_train)  

X_test_std = sc.transform(X_test)

# 2차원 차원축소
pca=PCA(
    n_components=2
)
lr=LogisticRegression(
    multi_class='ovr'
    , solver='lbfgs'
)

# 학습데이터/테스트데이터 차원 축소
X_train_pca=pca.fit_transform(X_train_std)
X_test_pca=pca.transform(X_test_std)

# 모델 훈련
lr.fit(X_train_pca,y_train)

from sklearn.metrics import accuracy_score
y_pred = lr.predict(X_test_pca)

print(f'정확도 : {accuracy_score(y_pred,y_test)}')
```
![image](https://github.com/user-attachments/assets/70372ffb-2ea4-4c06-8354-2a5f094e97ed)

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# sklearn PCA
from sklearn.linear_model import LogisticRegression
from sklearn.decomposition import PCA


df_wine=pd.read_csv(
    '../wine.data'
    ,header=None
    ,encoding='utf-8'
)

X,y=df_wine.iloc[:,1:].values,df_wine.iloc[:,0].values

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(
    X
    ,y
    ,test_size=0.3
    ,stratify=y
    ,random_state=0
)

from sklearn.preprocessing import StandardScaler

sc = StandardScaler()

X_train_std = sc.fit_transform(X_train)  

X_test_std = sc.transform(X_test)

# 2차원 차원축소
pca=PCA(
    n_components=None
)
lr=LogisticRegression(
    multi_class='ovr'
    , solver='lbfgs'
)

# 학습데이터/테스트데이터 차원 축소
X_train_pca=pca.fit_transform(X_train_std)
X_test_pca=pca.transform(X_test_std)

print(pca.explained_variance_ratio_)
```
![image](https://github.com/user-attachments/assets/0e08b00d-809f-443b-97dd-4649fff6601b)

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# sklearn PCA
from sklearn.linear_model import LogisticRegression
from sklearn.decomposition import PCA

df_wine = pd.read_csv(
    '../wine.data'
    , header=None
    , encoding='utf-8'
)

X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    X
    , y
    , test_size=0.3
    , stratify=y
    , random_state=0
)

from sklearn.preprocessing import StandardScaler

sc = StandardScaler()

X_train_std = sc.fit_transform(X_train)

X_test_std = sc.transform(X_test)


pca = PCA(
    n_components=0.95
)
pca.fit(X_train_std)
print(f'주성분 개수 : {pca.n_components_}')
print(f'설명된 분산 비율 : {np.sum(pca.explained_variance_ratio_)}')
```
![image](https://github.com/user-attachments/assets/407cad42-315e-4424-9c39-0829c850dee7)

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# sklearn PCA
from sklearn.linear_model import LogisticRegression
from sklearn.decomposition import PCA

df_wine = pd.read_csv(
    '../wine.data'
    , header=None
    , encoding='utf-8'
)

X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    X
    , y
    , test_size=0.3
    , stratify=y
    , random_state=0
)

from sklearn.preprocessing import StandardScaler

sc = StandardScaler()

X_train_std = sc.fit_transform(X_train)

X_test_std = sc.transform(X_test)


pca = PCA(
    n_components='mle'
)
pca.fit(X_train_std)


print(f'주성분 개수 : {pca.n_components_}')
print(f'설명된 분산 비율 : {np.sum(pca.explained_variance_ratio_)}')
```
![image](https://github.com/user-attachments/assets/a88da175-486e-461e-ad09-decf507c2b84)

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# sklearn PCA
from sklearn.linear_model import LogisticRegression
from sklearn.decomposition import PCA

df_wine = pd.read_csv(
    '../wine.data'
    , header=None
    , encoding='utf-8'
)

df_wine.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash',
                   'Alcalinity of ash', 'Magnesium', 'Total phenols',
                   'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins',
                   'Color intensity', 'Hue', 'OD280/OD315 of diluted wines',
                   'Proline']

X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    X
    , y
    , test_size=0.3
    , stratify=y
    , random_state=0
)

from sklearn.preprocessing import StandardScaler

sc = StandardScaler()

X_train_std = sc.fit_transform(X_train)  

X_test_std = sc.transform(X_test)

cov_mat = np.cov(X_train_std.T) 

eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)

tot=np.sum(eigen_vals) 

var_exp=[
    (i/tot) for i in sorted(eigen_vals,reverse=True)
]

cum_var_exp = np.cumsum(var_exp)

# 고유값, 고유 벡터 튜플의 리스트 처리
eigen_pairs=[
    (
        np.abs(eigen_vals[idx])
        , eigen_vecs[:,idx]
    ) for idx in range(len(eigen_vals))
]

# 내림차순 정렬
eigen_pairs.sort(
    key=lambda k :k[0]
    ,reverse=True
)

w = np.hstack((
    eigen_pairs[0][1][:, np.newaxis],  
    eigen_pairs[1][1][:, np.newaxis]   
))


# 피쳐 기여도 평가
loading=eigen_vecs*np.sqrt(eigen_vals)

# print(f'eigen_vals : {eigen_vals.shape}')
# print(f'eigen_vecs : {eigen_vecs.shape}')
# print(f'loading shape : {loading.shape}')

# 첫번째 주성분에 원본 피쳐가 얼마나 기여했는지 시각화
fig, ax = plt.subplots()
ax.bar(
    range(len(loading)) # X 축
    ,loading[:,0] # 첫번재 주성분
    ,align='center'
)
ax.set_xticklabels(df_wine.columns[0:],rotation=90)
plt.show()
```
![image](https://github.com/user-attachments/assets/9cbe0657-e0a3-48e7-bd98-98192296e827)

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# sklearn PCA
from sklearn.linear_model import LogisticRegression
from sklearn.decomposition import PCA

df_wine = pd.read_csv(
    '../wine.data'
    , header=None
    , encoding='utf-8'
)

df_wine.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash',
                   'Alcalinity of ash', 'Magnesium', 'Total phenols',
                   'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins',
                   'Color intensity', 'Hue', 'OD280/OD315 of diluted wines',
                   'Proline']

X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    X
    , y
    , test_size=0.3
    , stratify=y
    , random_state=0
)

from sklearn.preprocessing import StandardScaler

sc = StandardScaler()

X_train_std = sc.fit_transform(X_train)  

X_test_std = sc.transform(X_test)



cov_mat = np.cov(X_train_std.T) 

eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)

tot=np.sum(eigen_vals) 

var_exp=[
    (i/tot) for i in sorted(eigen_vals,reverse=True)
]

cum_var_exp = np.cumsum(var_exp)

# 고유값, 고유 벡터 튜플의 리스트 처리
eigen_pairs=[
    (
        np.abs(eigen_vals[idx])
        , eigen_vecs[:,idx]
    ) for idx in range(len(eigen_vals))
]

# 내림차순 정렬
eigen_pairs.sort(
    key=lambda k :k[0]
    ,reverse=True
)

w = np.hstack((
    eigen_pairs[0][1][:, np.newaxis],  
    eigen_pairs[1][1][:, np.newaxis]   
))

from sklearn.preprocessing import StandardScaler

sc = StandardScaler()

X_train_std = sc.fit_transform(X_train)  

X_test_std = sc.transform(X_test)

# 2차원 차원축소
pca=PCA(
    n_components=2
)
lr=LogisticRegression(
    multi_class='ovr'
    , solver='lbfgs'
)

# 학습데이터/테스트데이터 차원 축소
X_train_pca=pca.fit_transform(X_train_std)
X_test_pca=pca.transform(X_test_std)

# 모델 훈련
lr.fit(X_train_pca,y_train)

from sklearn.metrics import accuracy_score
y_pred = lr.predict(X_test_pca)
# 피쳐 기여도 평가
loading=eigen_vecs*np.sqrt(eigen_vals)

sklearn_loading=pca.components_.T * np.sqrt(pca.explained_variance_)
fig, ax = plt.subplots()
ax.bar(
    range(len(sklearn_loading))
    ,sklearn_loading[:,0]
    ,align='center'
)
ax.set_ylabel
ax.set_xticks(range(len(sklearn_loading)))
ax.set_xticklabels(df_wine.columns[1:],rotation=90)
plt.tight_layout()
plt.show()
```
![image](https://github.com/user-attachments/assets/3d178b3a-a934-499f-a2de-2e3ac7c1004b)

---
## LDA
```
# 사이킷런 패키지 데이터
# 사람얼굴 데이터 : 40명 10장씩 이미지 데이터 : 정체 이미지 개수는 400장
# 1장 이미지(1사람 사진) : 64X64 -> 4096차원
# 전체 데이터 구조 : 400X64X64
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_olivetti_faces # AT&T 얼굴데이터베이스

# ORL : 얼굴 데이터셋 로딩
faces=np.load('./olivetti_faces.npy')
# faces.shape
#첫번째 이미지 출력
person_id = 0
image_idx=0
plt.figure(figsize=(5,5))
plt.imshow(faces[image_idx],cmap='grey')
plt.axis('off')
plt.show()
```
![image](https://github.com/user-attachments/assets/c50c014d-1cb7-4c0f-ba67-fa3586e187c5)

```
# 사이킷런 패키지 데이터
# 사람얼굴 데이터 : 40명 10장씩 이미지 데이터 : 정체 이미지 개수는 400장
# 1장 이미지(1사람 사진) : 64X64 -> 4096차원
# 전체 데이터 구조 : 400X64X64
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_olivetti_faces # AT&T 얼굴데이터베이스

# ORL : 얼굴 데이터셋 로딩
faces=np.load('./olivetti_faces.npy')
# faces.shape
person_id = 0
image_idx=1
plt.figure(figsize=(5,5))
plt.imshow(faces[image_idx],cmap='grey')
plt.axis('off')
plt.show()
```
![image](https://github.com/user-attachments/assets/0c8ad76e-20cf-4072-8990-ad7734a135af)

```
# 사이킷런 패키지 데이터
# 사람얼굴 데이터 : 40명 10장씩 이미지 데이터 : 정체 이미지 개수는 400장
# 1장 이미지(1사람 사진) : 64X64 -> 4096차원
# 전체 데이터 구조 : 400X64X64
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_olivetti_faces 

# ORL : 얼굴 데이터셋 로딩
faces=np.load('./olivetti_faces.npy')
# faces.shape

# person_id = 0
# image_idx=0
num_images=10
for i in range(num_images):
    plt.subplot(2,5,i+1)
    plt.imshow(faces[i],cmap='grey')
    plt.axis('off')
plt.tight_layout()
plt.show()
```
![image](https://github.com/user-attachments/assets/41a5e5a0-c02b-4f37-89c2-5d59c2f2c683)

```
# 사이킷런 패키지 데이터
# 사람얼굴 데이터 : 40명 10장씩 이미지 데이터 : 정체 이미지 개수는 400장
# 1장 이미지(1사람 사진) : 64X64 -> 4096차원
# 전체 데이터 구조 : 400X64X64
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_olivetti_faces 

# LDA 적용해서 분류
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA

from sklearn.metrics import accuracy_score, classification_report

# 이미지 데이터
X=np.load('./olivetti_faces.npy')
# 레이블 로딩
y=np.load('./olivetti_faces_target.npy')

print(f'X shape : {X.shape}')
print(f'y shape : {y.shape}')
print(f'unique y : {np.unique(y)}')

# 학습데이터/테스트데이터 분리
X_train, X_test, y_train,y_test=train_test_split(
    X
    ,y
    ,test_size=0.3
    ,random_state=1
    ,stratify=y
)
print(f'X_train shape : {X_train.shape}')
print(f'X_test shape : {X_test.shape}')
print(f'y_train shape : {y_train.shape}')
print(f'y_test shape : {y_test.shape}')

X_train_2d = X_train.reshape(X_train.shape[0], -1)
X_test_2d = X_test.reshape(X_test.shape[0], -1)
print(f'X_train_2d shape : {X_train_2d.shape}')
print(f'X_test_2d shape : {X_test_2d.shape}')

# LDA 차원 축소
lda=LDA(
    n_components= len(np.unique(y))-1 # 일반적으로 클래스 수 -1
)
X_train_lda=lda.fit_transform(X_train_2d,y_train)
X_test_lda=lda.transform(X_test_2d)
print(f'X_train_lda shape : {X_train_lda.shape}')
print(f'X_test_lda shape : {X_test_lda.shape}')
```
![image](https://github.com/user-attachments/assets/8b230896-0bb8-4ab1-998d-855c4da605cc)

```
# 사이킷런 패키지 데이터
# 사람얼굴 데이터 : 40명 10장씩 이미지 데이터 : 정체 이미지 개수는 400장
# 1장 이미지(1사람 사진) : 64X64 -> 4096차원
# 전체 데이터 구조 : 400X64X64
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_olivetti_faces 

# LDA 적용해서 분류
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA

from sklearn.metrics import accuracy_score, classification_report

# 이미지 데이터
X=np.load('./olivetti_faces.npy')
# 레이블 로딩
y=np.load('./olivetti_faces_target.npy')

print(f'X shape : {X.shape}')
print(f'y shape : {y.shape}')
print(f'unique y : {np.unique(y)}')

# 학습데이터/테스트데이터 분리
X_train, X_test, y_train,y_test=train_test_split(
    X
    ,y
    ,test_size=0.3
    ,random_state=1
    ,stratify=y
)
print(f'X_train shape : {X_train.shape}')
print(f'X_test shape : {X_test.shape}')
print(f'y_train shape : {y_train.shape}')
print(f'y_test shape : {y_test.shape}')

X_train_2d = X_train.reshape(X_train.shape[0], -1)
X_test_2d = X_test.reshape(X_test.shape[0], -1)
print(f'X_train_2d shape : {X_train_2d.shape}')
print(f'X_test_2d shape : {X_test_2d.shape}')

# LDA 차원 축소
lda=LDA(
    n_components= len(np.unique(y))-1 # 일반적으로 클래스 수 -1
)
X_train_lda=lda.fit_transform(X_train_2d,y_train)
X_test_lda=lda.transform(X_test_2d)
print(f'X_train_lda shape : {X_train_lda.shape}')
print(f'X_test_lda shape : {X_test_lda.shape}')

# KNN
knn = KNeighborsClassifier(
    n_neighbors=len(np.unique(y))
)
knn.fit(X_train_lda,y_train)
print(f'knn 학습 : {knn.fit(X_train_lda,y_train)}')

y_pred=knn.predict(
    X_test_lda
)

# 성능평가
acc = accuracy_score(y_pred,y_test)
print(f'LDA를 이용한 얼굴 인식 정확도 : {acc:.3f}')
```
![image](https://github.com/user-attachments/assets/451479ef-d717-47ec-bbce-c248350c14c2)

```
# 사이킷런 패키지 데이터
# 사람얼굴 데이터 : 40명 10장씩 이미지 데이터 : 정체 이미지 개수는 400장
# 1장 이미지(1사람 사진) : 64X64 -> 4096차원
# 전체 데이터 구조 : 400X64X64
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_olivetti_faces 

# LDA 적용해서 분류
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA

from sklearn.metrics import accuracy_score, classification_report

# 이미지 데이터
X=np.load('./olivetti_faces.npy')
# 레이블 로딩
y=np.load('./olivetti_faces_target.npy')

# print(f'X shape : {X.shape}')
# print(f'y shape : {y.shape}')
# print(f'unique y : {np.unique(y)}')

# 학습데이터/테스트데이터 분리
X_train, X_test, y_train,y_test=train_test_split(
    X
    ,y
    ,test_size=0.3
    ,random_state=1
    ,stratify=y
)
# print(f'X_train shape : {X_train.shape}')
# print(f'X_test shape : {X_test.shape}')
# print(f'y_train shape : {y_train.shape}')
# print(f'y_test shape : {y_test.shape}')

X_train_2d = X_train.reshape(X_train.shape[0], -1)
X_test_2d = X_test.reshape(X_test.shape[0], -1)
# print(f'X_train_2d shape : {X_train_2d.shape}')
# print(f'X_test_2d shape : {X_test_2d.shape}')

# LDA 차원 축소
lda=LDA(
    n_components= len(np.unique(y))-1 # 일반적으로 클래스 수 -1
)
X_train_lda=lda.fit_transform(X_train_2d,y_train)
X_test_lda=lda.transform(X_test_2d)
# print(f'X_train_lda shape : {X_train_lda.shape}')
# print(f'X_test_lda shape : {X_test_lda.shape}')

# KNN
knn = KNeighborsClassifier(
    n_neighbors=len(np.unique(y))
)
knn.fit(X_train_lda,y_train)
# print(f'knn 학습 : {knn.fit(X_train_lda,y_train)}')

y_pred=knn.predict(
    X_test_lda
)

# 성능평가
acc = accuracy_score(y_pred,y_test)
# print(f'LDA를 이용한 얼굴 인식 정확도 : {acc:.3f}')

# 시각화
from sklearn.decomposition import PCA


cnt=15
n_components=cnt**2
# print(n_components)
pca=PCA(n_components=n_components, random_state=1)


pca.fit(X_train_2d)


image_original=X_test_2d[0]
image_pca=pca.transform([image_original])


fig, ax=plt.subplots(1,2,figsize=(8,4))
ax[0].imshow(
    image_original.reshape(X_train[0].shape), cmap='grey'
)


ax[1].imshow(
    image_pca.reshape(cnt,-1), cmap='grey'
)
plt.show()
```
![image](https://github.com/user-attachments/assets/656b9749-c932-4adb-bbf0-3c651e54f4d9)
