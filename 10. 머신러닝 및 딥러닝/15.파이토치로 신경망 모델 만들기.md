
```
import torch
import numpy as np
import matplotlib.pyplot as plt

X_train=np.arange(
    10
    ,dtype='float32'
).reshape(
    (10,1)
)

y_train=np.array(
    [1.0,1.3,3.1,2.0,5.0,6.3,6.6,7.4,8.0,9.0]
    ,dtype='float32'
)

plt.plot(X_train,y_train,'o',markersize=10)
plt.xlabel('x')
plt.ylabel('y')
plt.show()
```
![image](https://github.com/user-attachments/assets/6e1ac30f-3f85-4b0d-acc8-3111e07bd2cc)

```
from torch.utils.data import TensorDataset, DataLoader
# ì‹ ê²½ë§ì€ ë¹„ëª¨ìˆ˜ì  ì¶”ì •ì´ë¯€ë¡œ ì •ê·œë¶„í¬í™”ë¥¼ í•˜ë©´ ì˜í–¥ì´ ìˆì§€ë§Œ, sklearnì€ ëª¨ìˆ˜ì  ì¶”ì •ì´ë¯€ë¡œ ì •ê·œë¶„í¬í™”ë¥¼ í•´ë„ ì„±ëŠ¥ì´ í¬ê²Œ í–¥ìƒë˜ì§€ ì•ŠëŠ”ë‹¤.
# X_train ë°ì´í„°ë¥¼ ì •ê·œí™” (í‰ê·  0, í‘œì¤€í¸ì°¨ 1ë¡œ ìŠ¤ì¼€ì¼ë§)
X_train_norm = (X_train - np.mean(X_train)) / np.std(X_train)

# ì •ê·œí™”ëœ X_train ë°ì´í„°ì˜ í˜•íƒœì™€ íƒ€ì… ì¶œë ¥
print(f'X_train_norm shape : {X_train_norm.shape}')
print(f'X_train_norm type : {type(X_train_norm)}')
print(X_train_norm)
print('='*100)

# ë„˜íŒŒì´ ë°°ì—´ì„ íŒŒì´í† ì¹˜ í…ì„œë¡œ ë³€í™˜
X_train_norm = torch.from_numpy(X_train_norm)

# í…ì„œë¡œ ë³€í™˜ëœ í›„ì˜ í˜•íƒœì™€ íƒ€ì… ì¶œë ¥
print(f'X_train_norm shape : {X_train_norm.shape}')
print(f'X_train_norm type : {type(X_train_norm)}')
print(X_train_norm)
print('='*100)

# y_trainì´ ë„˜íŒŒì´ ë°°ì—´ì´ë¼ë©´ íŒŒì´í† ì¹˜ í…ì„œ(float íƒ€ì…)ë¡œ ë³€í™˜
# ì´ë¯¸ í…ì„œë¼ë©´ float íƒ€ì…ìœ¼ë¡œ ìºìŠ¤íŒ…
# if isinstance(y_train, np.ndarray):
#     y_train = torch.from_numpy(y_train).float()
# else:
#     y_train = y_train.float()
y_train=torch.from_numpy(y_train).float()
# X_train_normê³¼ y_trainì„ ë¬¶ì–´ì„œ í•˜ë‚˜ì˜ TensorDataset ìƒì„±
train_ds = TensorDataset(X_train_norm, y_train)

# ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì„¤ì • (ì—¬ê¸°ì„œëŠ” 1ë¡œ ì„¤ì •)í•˜ê³ , ë°ì´í„°ë¥¼ ì…”í”Œí•´ì„œ ë¡œë”©í•  ìˆ˜ ìˆë„ë¡ DataLoader ìƒì„±
batch_size = 1
train_dl = DataLoader(train_ds, batch_size, shuffle=True)

# DataLoader ê°ì²´ ì¶œë ¥ (ë‚´ë¶€ì— ìˆëŠ” ë°ì´í„°ë‚˜ ì„¤ì • í™•ì¸ìš©)
print(train_dl)

```
```
X_train_norm shape : (10, 1)
X_train_norm type : <class 'numpy.ndarray'>
[[-1.5666989 ]
 [-1.2185436 ]
 [-0.87038827]
 [-0.52223295]
 [-0.17407766]
 [ 0.17407766]
 [ 0.52223295]
 [ 0.87038827]
 [ 1.2185436 ]
 [ 1.5666989 ]]
====================================================================================================
X_train_norm shape : torch.Size([10, 1])
X_train_norm type : <class 'torch.Tensor'>
tensor([[-1.5667],
        [-1.2185],
        [-0.8704],
        [-0.5222],
        [-0.1741],
        [ 0.1741],
        [ 0.5222],
        [ 0.8704],
        [ 1.2185],
        [ 1.5667]])
====================================================================================================
<torch.utils.data.dataloader.DataLoader object at 0x0000014A17A0AFB0>
```
```
# PyTorchì—ì„œ ê°™ì€ ë‚œìˆ˜ ê²°ê³¼ë¥¼ ì–»ê¸° ìœ„í•´ ì‹œë“œë¥¼ ì„¤ì •
torch.manual_seed(1)

# ê°€ì¤‘ì¹˜ 1ê°œ ì„ ì–¸ : ì„ì˜ê°’ìœ¼ë¡œ ì´ˆê¸°í™”
# ì •ê·œë¶„í¬ì—ì„œ ë‚œìˆ˜ í•˜ë‚˜ ìƒì„± (ëª¨ë¸ì˜ weight ì´ˆê¸°í™”)
weight = torch.randn(1)

# weightì— ëŒ€í•´ gradientë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆë„ë¡ ì„¤ì • : weightë¥¼ ë¯¸ë¶„ ê°€ëŠ¥í•˜ê²Œ ë§Œë“¤ê² ë‹¤
weight.requires_grad_() 

# bias 1ê°œ ì„ ì–¸
# biasë¥¼ 0ìœ¼ë¡œ ì´ˆê¸°í™”í•˜ê³ , gradient ê³„ì‚° í—ˆìš©
bias = torch.zeros(1, requires_grad=True)

# ì„ í˜• ëª¨ë¸ ì •ì˜: ì…ë ¥ xbì— ëŒ€í•´ wx + b ê³„ì‚°
def model(x_batch):
    return x_batch @ weight + bias  # @ëŠ” í–‰ë ¬ ê³±

# ì†ì‹¤ í•¨ìˆ˜ ì •ì˜: í‰ê·  ì œê³± ì˜¤ì°¨(MSE)
def loss_fn(input, target): # input : y^(ì˜ˆì¸¡ê°’), target : y ì‹¤ì œê°’
    return (input - target).pow(2).mean()

# í•™ìŠµë¥  ì„¤ì •
learning_rate = 0.001

# ì´ í•™ìŠµí•  epoch ìˆ˜ : ì „ì²´ ë°ì´í„° ëª‡ë²ˆ ë°˜ë³µí•´ì„œ ì‚¬ìš©í•  ê²ƒì¸ì§€
num_epochs = 200

# ë¡œê·¸ë¥¼ ì¶œë ¥í•  epoch ê°„ê²© : ëª‡ë²ˆë§ˆë‹¤ ì†ì‹¤(ì˜¤ì°¨)ì„ ì¶œë ¥í•œ ê²ƒì¸ì§€? ì§€ì •
log_epochs = 10

# ì—í­ë§Œí¼ ë°˜ë³µ (ì „ì²´ ë°ì´í„°ì…‹ì„ num_epochsë²ˆ í•™ìŠµ)
for epoch in range(num_epochs):
    # ë¯¸ë‹ˆ ë°°ì¹˜ í•™ìŠµ
    # ë°ì´í„° ì¶”ì¶œ(train_dl)í•´ì„œ ëª¨ë¸ì— ë„£ì–´ì•¼í•œë‹¤
    for x_batch, y_batch in train_dl:  # train_dlì€ ë°ì´í„° ë¡œë” x_batch: ë°ì´í„°, y_batch : ë ˆì´ë¸”
        pred = model(x_batch)  # ëª¨ë¸ ì˜ˆì¸¡ê°’ ê³„ì‚°
        loss = loss_fn(pred, y_batch)  # ì†ì‹¤ ê³„ì‚° 
        loss.backward()  # ì˜¤ì°¨ ì—­ì „íŒŒ : ì†ì‹¤ í•¨ìˆ˜ì— ëŒ€í•œ weight, biasì˜ gradient ê³„ì‚°
        
    # ì¶”ë¡  : new dataë¥¼ ë„£ì–´ì„œ ì˜ˆì¸¡ê°’ì„ ì¶”ì¶œ : ê³„ì‚° ê·¸ë˜í”„ X, ê°€ì¤‘ì¹˜ê°’ì„ ì €ì¥ X
    # weightì™€ bias ì—…ë°ì´íŠ¸ (ê²½ì‚¬í•˜ê°•ë²•)
    with torch.no_grad():  # gradient ê³„ì‚° ë¹„í™œì„±í™” (ë©”ëª¨ë¦¬ ì ˆì•½) : ê°€ì¤‘ì¹˜ ê°’ë“¤ì„ êµ¬í•´ì„œ ì¶œë ¥
        weight -= weight.grad * learning_rate  # weight ì—…ë°ì´íŠ¸
        bias -= bias.grad * learning_rate      # bias ì—…ë°ì´íŠ¸
        weight.grad.zero_()  # gradientë¥¼ 0ìœ¼ë¡œ ì´ˆê¸°í™”
        bias.grad.zero_()    # gradientë¥¼ 0ìœ¼ë¡œ ì´ˆê¸°í™”

    # ì¼ì • ì—í­ë§ˆë‹¤ ì†ì‹¤ ì¶œë ¥ : ì—í¬í¬ 10ì˜ ë°°ìˆ˜ë§ˆë‹¤ ì˜¤ì°¨ë¥¼ ì¶œë ¥ë ¥
    if epoch % log_epochs == 0:
        print(f'ì—í¬í¬ : {epoch} ì†ì‹¤ : {loss.item()}')

print('='*100)
print(f'ìµœì¢… íŒŒë¼ë¯¸í„° : {weight.item()} {bias.item()}')
```
![image](https://github.com/user-attachments/assets/7aaba317-192f-4562-83e1-54c9d0c44c95)

```
X_test = np.linspace(0, 9, num=100, dtype='float32').reshape(-1, 1)
X_test_norm = (X_test - np.mean(X_train)) / np.std(X_train)
X_test_norm = torch.from_numpy(X_test_norm)
y_pred = model(X_test_norm).detach()  # ê³„ì‚° ê·¸ë˜í”„ì—ì„œ í…ì„œë¥¼ ë¶„ë¦¬í•©ë‹ˆë‹¤.
fig = plt.figure(figsize=(13, 5))
ax = fig.add_subplot(1, 2, 1)
plt.plot(X_train_norm, y_train, 'o', markersize=10)
plt.plot(X_test_norm, y_pred, '--', lw=3)
plt.legend(['Training examples', 'Linear Reg.'], fontsize=15)
ax.set_xlabel('x', size=15)
ax.set_ylabel('y', size=15)
ax.tick_params(axis='both', which='major', labelsize=15)
#plt.savefig('figures/12_08.pdf')
plt.show()
```
![image](https://github.com/user-attachments/assets/8888ece9-e7fe-461f-a681-913f52b062cd)

---
## torch.nnì™€ torch.optim ëª¨ë“ˆë¡œ ëª¨ë¸ í›ˆë ¨
```
import torch.nn as nn

input_size=1
output_size=1

learning_rate = 0.001
num_epochs = 200

# ì‹ ê²½ë§ êµ¬ì„±
model = nn.Linear(input_size,output_size)

# ì†ì‹¤ í•¨ìˆ˜ ì •ì˜
loss_fn=nn.MSELoss(
    reduction='mean'
)

optimizer=torch.optim.SGD(
    model.parameters()
    ,lr=learning_rate
)

for epoch in range(num_epochs):
    for x_batch,y_batch in train_dl:
        # ì˜ˆì¸¡ì„ ìƒì„±
        pred=model(x_batch)[:,0]
        # ì†ì‹¤ì„ ê³„ì‚°
        loss = loss_fn(pred,y_batch)
        # ê·¸ë ˆë””ì–¸íŠ¸ë¥¼ ê³„ì‚°
        loss.backward()
        # ê·¸ë ˆë””ì–¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸
        optimizer.step()
        # ê·¸ë ˆë””ì–¸íŠ¸ë¥¼ 0ìœ¼ë¡œ ì´ˆê¸°í™”
        optimizer.zero_grad()
    if epoch % log_epochs ==0:
        print(f'ì—í¬í¬ : {epoch} ì†ì‹¤ : {loss.item()}')

print(f'ìµœì¢… íŒŒë¼ë¯¸í„° : {weight.item()} {bias.item()}')
```
![image](https://github.com/user-attachments/assets/7c379809-7792-42cf-b589-fd76211142d7)

```
print('ìµœì¢… íŒŒë¼ë¯¸í„°:', model.weight.item(), model.bias.item())


X_test = np.linspace(0, 9, num=100, dtype='float32').reshape(-1, 1)
X_test_norm = (X_test - np.mean(X_train)) / np.std(X_train)
X_test_norm = torch.from_numpy(X_test_norm)
y_pred = model(X_test_norm).detach()

fig = plt.figure(figsize=(13, 5))
ax = fig.add_subplot(1, 2, 1)
plt.plot(X_train_norm, y_train, 'o', markersize=10)
plt.plot(X_test_norm, y_pred, '--', lw=3)
plt.legend(['Training examples', 'Linear reg.'], fontsize=15)
ax.set_xlabel('x', size=15)
ax.set_ylabel('y', size=15)
ax.tick_params(axis='both', which='major', labelsize=15)


#plt.savefig('ch12-linreg-2.pdf')


plt.show()
```
![image](https://github.com/user-attachments/assets/c88ff10b-0220-45e1-b687-607a053dddcb)

---
## ë¶“ê½ƒ ë°ì´í„°ì…‹ì„ ë¶„ë¥˜í•˜ëŠ” ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡  ë§Œë“¤ê¸°
```
import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

# =============================
# Iris ë°ì´í„°ì…‹ ë¡œë“œ ë° ë¶„í• 
# =============================

# Iris ë°ì´í„°ì…‹ ë¡œë“œ (ì…ë ¥ X: ê½ƒë°›ì¹¨/ê½ƒì ê¸¸ì´ì™€ ë„ˆë¹„, ë ˆì´ë¸” y: í’ˆì¢…)
iris = load_iris()
X = iris['data']          # ì…ë ¥ íŠ¹ì„± (ì´ 4ê°œ íŠ¹ì„±)
y = iris['target']        # ì¶œë ¥ ë ˆì´ë¸” (0: setosa, 1: versicolor, 2: virginica)

# ë°ì´í„° íƒ€ì… í™•ì¸
print(f'Xì˜ íƒ€ì… : {type(X)}')
print(f'yì˜ íƒ€ì… : {type(y)}')

# í›ˆë ¨/í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¡œ ë¶„í•  (1/3ì€ í…ŒìŠ¤íŠ¸ìš©, í´ë˜ìŠ¤ ë¹„ìœ¨ ìœ ì§€ ìœ„í•´ stratify ì‚¬ìš©)
X_train, X_test, y_train, y_test = train_test_split(
    X,
    y,
    test_size=1./3,
    random_state=1,
    stratify=y
)

# =============================
# ë°ì´í„° ì „ì²˜ë¦¬ ë° PyTorch Dataset ì¤€ë¹„
# =============================

from torch.utils.data import TensorDataset, DataLoader

# ì…ë ¥ íŠ¹ì„± ì •ê·œí™” (í‰ê·  0, í‘œì¤€í¸ì°¨ 1) â†’ í•™ìŠµ ì•ˆì •ì„± í–¥ìƒ
X_train_norm = (X_train - np.mean(X_train)) / np.std(X_train)

# numpy â†’ torch tensor ë³€í™˜ (ëª¨ë¸ í•™ìŠµì„ ìœ„í•œ ì¤€ë¹„)
X_train_norm = torch.from_numpy(X_train_norm).float()  # ì…ë ¥ì€ floatí˜•
y_train = torch.from_numpy(y_train).long()             # ë¼ë²¨ì€ longí˜• (CrossEntropyLossìš©)

# TensorDataset: ì…ë ¥/ë¼ë²¨ì„ ë¬¶ì–´ì„œ Dataset í˜•íƒœë¡œ ë³€í™˜
train_ds = TensorDataset(X_train_norm, y_train)

# DataLoader: ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë°ì´í„°ë¥¼ ë¡œë“œ + ì…”í”Œ
torch.manual_seed(1)       # ëœë¤ ì‹œë“œ ê³ ì • (ë™ì¼ ê²°ê³¼ ì¬í˜„ì„ ìœ„í•´)
batch_size = 2
train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)

# =============================
# ì‹ ê²½ë§ ëª¨ë¸ ì •ì˜
# =============================

# ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡  (MLP) êµ¬ì¡° ì •ì˜:
# ì…ë ¥ì¸µ(4) â†’ ì€ë‹‰ì¸µ1(16) + í™œì„±í™” â†’ ì€ë‹‰ì¸µ2(16) + í™œì„±í™” â†’ ì¶œë ¥ì¸µ(3 í´ë˜ìŠ¤)

class Model(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super().__init__()  # nn.Module ì´ˆê¸°í™”
        self.layer1 = nn.Linear(input_size, hidden_size)     # ì…ë ¥ â†’ ì€ë‹‰1
        self.layer2 = nn.Linear(hidden_size, output_size)    # ì€ë‹‰2 â†’ ì¶œë ¥

    def forward(self, x):  # ìˆœì „íŒŒ ì •ì˜
        x = self.layer1(x)             # ì²« ë²ˆì§¸ ì„ í˜•ë³€í™˜
        x = nn.Sigmoid()(x)            # ì²« ë²ˆì§¸ ì€ë‹‰ì¸µ í™œì„±í™” í•¨ìˆ˜
        x = self.layer2(x)             # ë‘ ë²ˆì§¸ ì„ í˜•ë³€í™˜
        x = nn.Softmax(dim=1)(x)       # ë‹¤ì¤‘ í´ë˜ìŠ¤ í™•ë¥  ì¶œë ¥ (dim=1ì€ ë°°ì¹˜ ê¸°ì¤€)
        return x

# ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
input_size = X_train_norm.shape[1]     # ì…ë ¥ íŠ¹ì„± ìˆ˜ = 4
hidden_size = 16                       # ì€ë‹‰ì¸µ ë…¸ë“œ ìˆ˜ (í•˜ì´í¼íŒŒë¼ë¯¸í„°)
output_size = 3                        # í´ë˜ìŠ¤ ìˆ˜ = 3 (setosa, versicolor, virginica)
model = Model(input_size=input_size, hidden_size=hidden_size, output_size=output_size)

# ì˜µí‹°ë§ˆì´ì €ì™€ ì†ì‹¤ í•¨ìˆ˜
import torch.optim as optim

learning_rate=0.001

# ì†ì‹¤ í•¨ìˆ˜ object ì •ì˜
loss_fn=nn.CrossEntropyLoss() # ë‹¤ì¤‘ë¶„ë¥˜ì¼ë•Œ CrossEntropyLoss ì‚¬ìš©

optimizer=torch.optim.Adam(model.parameters(),lr=learning_rate)

num_epochs=200
loss_hist=[0]*num_epochs
accuracy_hist=[0]*num_epochs
for epoch in range(num_epochs):
    for x_batch, y_batch in train_dl:
        pred=model(x_batch)
        loss=loss_fn(pred,y_batch)
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
        loss_hist[epoch] += loss.item() * y_batch.size(0)
        is_correct=(torch.argmax(pred,dim=1)==y_batch).float()
        accuracy_hist[epoch] += is_correct.sum()
    loss_hist[epoch] /= len(train_dl.dataset)
    accuracy_hist[epoch] /= len(train_dl.dataset)

fig = plt.figure(figsize=(12,5))
ax = fig.add_subplot(1,2,1)
ax.plot(loss_hist,lw=3)
ax.set_title('Training loss',size=15)
ax.set_xlabel('Epoch',size=15)
ax.tick_params(axis='both',which='major',labelsize=15)
ax = fig.add_subplot(1,2,2)
ax.plot(accuracy_hist,lw=3)
ax.set_title('Training accuracy',size=15)
ax.set_xlabel('Epoch',size=15)
ax.tick_params(axis='both',which='major',labelsize=15)
plt.show()
```
![image](https://github.com/user-attachments/assets/48ce0c66-ad1a-4038-aa09-2b50aa40300e)

```
# ëª¨ë¸ í‰ê°€ : í…ŒìŠ¤íŠ¸ ë°ì´í„° -> X_test, y_test

# ë°ì´í„° ì •ê·œí™”
X_test_norm=torch.from_numpy(
    (X_test - np.mean(X_train))/np.std(X_train)
).float()
if isinstance(y_test, np.ndarray):
    y_test = torch.from_numpy(y_test).float()
pred_test=model(X_test_norm)
correct=(
    (torch.argmax(pred_test,dim=1) == y_test).float()
)
accuracy=correct.mean()
print(f'í…ŒìŠ¤íŠ¸ ì •í™•ë„ : {accuracy}')
```
![image](https://github.com/user-attachments/assets/97b74ee5-1f54-478e-96a2-feb0db451c01)

```
# í›ˆë ¨ëœ ëª¨ë¸ ì €ì¥
path='iris_classifier.pt'
torch.save(model,path)
```
![image](https://github.com/user-attachments/assets/4e1c629b-c86c-4840-af05-b46ed2a5e448)

```
# ì €ì¥ëœ ëª¨ë¸  ë¡œë“œ
path='iris_classifier.pt'
load_model=torch.load(path)
load_model.eval()
```
![image](https://github.com/user-attachments/assets/c4e0a4d4-ede9-4a63-b9ef-fbb49e0cc10b)

```
# ë¡œë“œ í‰ê°€
pred_test=load_model(X_test_norm)
correct=(
   (torch.argmax(pred_test,dim=1) == y_test).float() 
)
accuracy=correct.mean()
print(f'í…ŒìŠ¤íŠ¸ ì •í™•ë„ : {accuracy}')
```
![image](https://github.com/user-attachments/assets/62bedde1-121a-428a-a846-4eb09093cbdc)

```
# í•™ìŠµëœ í›ˆë ¨ íŒŒë¼ë¯¸í„°ë§Œ ì €ì¥í•˜ê³  ì‹¶ìœ¼ë©´
path='iris_classifier_state.pt'
torch.save(model.state_dict(),path)
```
![image](https://github.com/user-attachments/assets/1befdebd-5617-47aa-a0ca-cf932951a689)

```
# ì €ì¥ëœ íŒŒë¼ë¯¸í„° ë¡œë“œ
model_new_pa=Model(input_size,hidden_size,output_size)
model_new_pa.load_state_dict(torch.load(path))
```
![image](https://github.com/user-attachments/assets/9841436e-47e4-43b3-a638-48f306986993)

---
## ë‹¤ì¸µ ì‹ ê²½ë§ì˜ í™œì„±í™” í•¨ìˆ˜ ì„ íƒ

ì‹ ê²½ë§(Neural Network)ì—ì„œ **í™œì„±í™” í•¨ìˆ˜(Activation Function)** ëŠ” ë‰´ëŸ°ì˜ ì¶œë ¥ê°’ì„ ê²°ì •í•©ë‹ˆë‹¤.  
ë¹„ì„ í˜•ì„±ì„ ë¶€ì—¬í•˜ì—¬ ë³µì¡í•œ ë¬¸ì œë¥¼ í’€ ìˆ˜ ìˆê²Œ ë„ì™€ì£¼ëŠ” ì¤‘ìš”í•œ ìš”ì†Œì…ë‹ˆë‹¤.

### 1. âœ… ë¡œì§€ìŠ¤í‹± í•¨ìˆ˜ (Logistic, Sigmoid)

- **ìˆ˜ì‹**

  $$
  \[
  \sigma(x) = \frac{1}{1 + e^{-x}}
  \]
  $$

- **ì¶œë ¥ ë²”ìœ„**: `0 ~ 1`

- **íŠ¹ì§•**:
  - Sì ê³¡ì„  (sigmoid curve)
  - **ì´ì§„ ë¶„ë¥˜**ì˜ ì¶œë ¥ì¸µì—ì„œ ìì£¼ ì‚¬ìš©ë¨

- **ì¥ì **:
  - ì¶œë ¥ê°’ì„ **í™•ë¥ ì²˜ëŸ¼ í•´ì„** ê°€ëŠ¥

- **ë‹¨ì **:
  - **Vanishing Gradient** ë¬¸ì œ ë°œìƒ (ê¸°ìš¸ê¸° â†’ 0)
  - ì¶œë ¥ê°’ì´ 0 ì¤‘ì‹¬ì´ ì•„ë‹˜ â†’ í•™ìŠµ ì†ë„ ëŠë¦¼


### 2. âœ… ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜ (Softmax)

- **ìˆ˜ì‹** (í´ë˜ìŠ¤ iì— ëŒ€í•´)
  
  $$
  \[
  \text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j} e^{z_j}}
  \]
  $$


- **ì¶œë ¥ ë²”ìœ„**: `0 ~ 1` (ì „ì²´ í•©ì€ 1)

- **íŠ¹ì§•**:
  - **ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜**ì—ì„œ ì¶œë ¥ì¸µì— ì‚¬ìš©
  - ê° í´ë˜ìŠ¤ì— ëŒ€í•œ í™•ë¥  ì œê³µ

- **ì¥ì **:
  - í™•ë¥  ë¶„í¬ë¡œ í•´ì„ ê°€ëŠ¥

- **ë‹¨ì **:
  - í° ê°’ì— ë¯¼ê° â†’ í•™ìŠµ ë¶ˆì•ˆì • ê°€ëŠ¥


### 3. âœ… í•˜ì´í¼ë³¼ë¦­ íƒ„ì  íŠ¸ í•¨ìˆ˜ (tanh)

- **ìˆ˜ì‹**
  
  $$
  \[
  \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
  \]
  $$

- **ì¶œë ¥ ë²”ìœ„**: `-1 ~ 1`

- **íŠ¹ì§•**:
  - sigmoidë³´ë‹¤ **ì¶œë ¥ì´ 0 ì¤‘ì‹¬**

- **ì¥ì **:
  - í•™ìŠµ ì†ë„ê°€ ë¹ ë¦„
  - sigmoidë³´ë‹¤ gradientê°€ í¼

- **ë‹¨ì **:
  - ì—¬ì „íˆ **Vanishing Gradient** ë¬¸ì œ ì¡´ì¬


### 4. âœ… ReLU í•¨ìˆ˜ (Rectified Linear Unit)

- **ìˆ˜ì‹**
  
  $$
  \[
  f(x) = \max(0, x)
  \]
  $$

- **ì¶œë ¥ ë²”ìœ„**: `0 ~ âˆ`

- **íŠ¹ì§•**:
  - ê°€ì¥ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” í™œì„±í™” í•¨ìˆ˜

- **ì¥ì **:
  - ê³„ì‚° ê°„ë‹¨, í•™ìŠµ ë¹ ë¦„
  - Vanishing Gradient ë¬¸ì œ ê°ì†Œ

- **ë‹¨ì **:
  - **Dead Neuron ë¬¸ì œ** ë°œìƒ ê°€ëŠ¥ (ìŒìˆ˜ ì…ë ¥ â†’ ì¶œë ¥ 0 â†’ í•™ìŠµ ë©ˆì¶¤)


### 5. ğŸ” ReLU ë³€í˜•ë“¤

#### 5-1. âœ… Leaky ReLU

- **ìˆ˜ì‹**
  
  $$
  \[
  f(x) = \begin{cases}
  x & \text{if } x \geq 0 \\
  \alpha x & \text{if } x < 0 \quad (\alpha \approx 0.01)
  \end{cases}
  \]
  $$

- **íŠ¹ì§•**:
  - ìŒìˆ˜ ì…ë ¥ì—ë„ **ì‘ì€ ê¸°ìš¸ê¸° ìœ ì§€**
  - Dead Neuron ë¬¸ì œ ì™„í™”


#### 5-2. âœ… Parametric ReLU (PReLU)

- **Leaky ReLU**ì™€ ë¹„ìŠ·í•˜ì§€ë§Œ,  
  ğŸ‘‰ `Î±` ê°’ì„ **í•™ìŠµì„ í†µí•´ ìë™ìœ¼ë¡œ ì¡°ì •**

- **íŠ¹ì§•**:
  - ìœ ì—°í•˜ê²Œ ìŒìˆ˜ ì˜ì—­ì„ ì¡°ì ˆ ê°€ëŠ¥


#### 5-3. âœ… ELU (Exponential Linear Unit)

- **ìˆ˜ì‹**
  
  $$
  \[
  f(x) = \begin{cases}
  x & \text{if } x \geq 0 \\
  \alpha (e^x - 1) & \text{if } x < 0
  \end{cases}
  \]
  $$

- **íŠ¹ì§•**:
  - ì¶œë ¥ì´ 0 ì¤‘ì‹¬
  - ìŒìˆ˜ ì˜ì—­ì—ì„œë„ **ë¶€ë“œëŸ¬ìš´ ë³€í™”**


#### 5-4. âœ… SELU (Scaled Exponential Linear Unit)

- **ìˆ˜ì‹**
  
  $$
  \[
  f(x) = \lambda \begin{cases}
  x & \text{if } x > 0 \\
  \alpha (e^x - 1) & \text{if } x \leq 0
  \end{cases}
  \]
  $$
  
- **íŠ¹ì§•**:
  - **Self-normalizing** ê¸°ëŠ¥
  - íŠ¹ì • ì¡°ê±´ì—ì„œë§Œ ì‚¬ìš© ì¶”ì²œ (ì˜ˆ: AlphaDropout, íŠ¹ì • ì´ˆê¸°í™”)

#### 5-5 âœ… GELU ( Gaussian Error Linear Unit)

##### ğŸ”¢ ìˆ˜ì‹

$$
\text{GELU}(x) = x \cdot \Phi(x)
$$

> $\Phi(x)$ëŠ” í‘œì¤€ ì •ê·œë¶„í¬ì˜ ëˆ„ì  ë¶„í¬ í•¨ìˆ˜(CDF)

ë˜ëŠ” ê·¼ì‚¬ í‘œí˜„ìœ¼ë¡œëŠ”:

$$
\text{GELU}(x) \approx 0.5x \left(1 + \tanh\left[\sqrt{\frac{2}{\pi}} \left(x + 0.044715x^3\right)\right]\right)
$$

##### ğŸ“Œ íŠ¹ì§•

- ë¶€ë“œëŸ¬ìš´ ë¹„ì„ í˜• í•¨ìˆ˜
- ì‘ì€ ì…ë ¥ë„ ì™„ì „íˆ ì£½ì´ì§€ ì•ŠìŒ (ReLU ëŒ€ë¹„ ì´ì )
- **Transformer ê³„ì—´ ëª¨ë¸**ì—ì„œ ë„ë¦¬ ì‚¬ìš© (ì˜ˆ: BERT, GPT ë“±)
- í•™ìŠµ ì•ˆì •ì„±ê³¼ í‘œí˜„ë ¥ì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥
  
##### âœ… ìš”ì•½í‘œ

| í•¨ìˆ˜         | ì¶œë ¥ ë²”ìœ„     | ì¤‘ì‹¬ | ì¥ì                         | ë‹¨ì                       |
|--------------|---------------|------|-----------------------------|---------------------------|
| Sigmoid      | (0, 1)        | âŒ   | í™•ë¥  í•´ì„ ê°€ëŠ¥              | ê¸°ìš¸ê¸° ì†Œì‹¤, ëŠë¦° í•™ìŠµ     |
| Softmax      | (0, 1), í•©=1  | âŒ   | ë‹¤ì¤‘ í´ë˜ìŠ¤ í™•ë¥             | í° ê°’ì— ë¯¼ê°              |
| tanh         | (-1, 1)       | âœ…   | ë¹ ë¥¸ í•™ìŠµ, 0 ì¤‘ì‹¬           | ê¸°ìš¸ê¸° ì†Œì‹¤               |
| ReLU         | [0, âˆ)        | âŒ   | ë¹ ë¦„, ê°„ë‹¨                  | Dead Neuron ë¬¸ì œ          |
| Leaky ReLU   | (-âˆ, âˆ)       | âŒ   | Dead Neuron ì™„í™”            | Î± ìˆ˜ë™ ì„¤ì •               |
| PReLU        | (-âˆ, âˆ)       | âŒ   | Î± ìë™ í•™ìŠµ                 | ë³µì¡ë„ ì¦ê°€               |
| ELU          | (-Î±, âˆ)       | âœ…   | 0 ì¤‘ì‹¬, ë¶€ë“œëŸ¬ìš´ ê¸°ìš¸ê¸°     | ê³„ì‚° ë³µì¡                 |
| SELU         | ì •ê·œí™”ëœ ë²”ìœ„ | âœ…   | ìë™ ì •ê·œí™”(Self-normalizing)| ì¡°ê±´ì´ ê¹Œë‹¤ë¡œì›€           |

---
