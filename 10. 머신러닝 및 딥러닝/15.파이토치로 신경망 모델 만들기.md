
```
import torch
import numpy as np
import matplotlib.pyplot as plt

X_train=np.arange(
    10
    ,dtype='float32'
).reshape(
    (10,1)
)

y_train=np.array(
    [1.0,1.3,3.1,2.0,5.0,6.3,6.6,7.4,8.0,9.0]
    ,dtype='float32'
)

plt.plot(X_train,y_train,'o',markersize=10)
plt.xlabel('x')
plt.ylabel('y')
plt.show()
```
![image](https://github.com/user-attachments/assets/6e1ac30f-3f85-4b0d-acc8-3111e07bd2cc)

```
from torch.utils.data import TensorDataset, DataLoader
# 신경망은 비모수적 추정이므로 정규분포화를 하면 영향이 있지만, sklearn은 모수적 추정이므로 정규분포화를 해도 성능이 크게 향상되지 않는다.
# X_train 데이터를 정규화 (평균 0, 표준편차 1로 스케일링)
X_train_norm = (X_train - np.mean(X_train)) / np.std(X_train)

# 정규화된 X_train 데이터의 형태와 타입 출력
print(f'X_train_norm shape : {X_train_norm.shape}')
print(f'X_train_norm type : {type(X_train_norm)}')
print(X_train_norm)
print('='*100)

# 넘파이 배열을 파이토치 텐서로 변환
X_train_norm = torch.from_numpy(X_train_norm)

# 텐서로 변환된 후의 형태와 타입 출력
print(f'X_train_norm shape : {X_train_norm.shape}')
print(f'X_train_norm type : {type(X_train_norm)}')
print(X_train_norm)
print('='*100)

# y_train이 넘파이 배열이라면 파이토치 텐서(float 타입)로 변환
# 이미 텐서라면 float 타입으로 캐스팅
# if isinstance(y_train, np.ndarray):
#     y_train = torch.from_numpy(y_train).float()
# else:
#     y_train = y_train.float()
y_train=torch.from_numpy(y_train).float()
# X_train_norm과 y_train을 묶어서 하나의 TensorDataset 생성
train_ds = TensorDataset(X_train_norm, y_train)

# 배치 사이즈 설정 (여기서는 1로 설정)하고, 데이터를 셔플해서 로딩할 수 있도록 DataLoader 생성
batch_size = 1
train_dl = DataLoader(train_ds, batch_size, shuffle=True)

# DataLoader 객체 출력 (내부에 있는 데이터나 설정 확인용)
print(train_dl)

```
```
X_train_norm shape : (10, 1)
X_train_norm type : <class 'numpy.ndarray'>
[[-1.5666989 ]
 [-1.2185436 ]
 [-0.87038827]
 [-0.52223295]
 [-0.17407766]
 [ 0.17407766]
 [ 0.52223295]
 [ 0.87038827]
 [ 1.2185436 ]
 [ 1.5666989 ]]
====================================================================================================
X_train_norm shape : torch.Size([10, 1])
X_train_norm type : <class 'torch.Tensor'>
tensor([[-1.5667],
        [-1.2185],
        [-0.8704],
        [-0.5222],
        [-0.1741],
        [ 0.1741],
        [ 0.5222],
        [ 0.8704],
        [ 1.2185],
        [ 1.5667]])
====================================================================================================
<torch.utils.data.dataloader.DataLoader object at 0x0000014A17A0AFB0>
```
```
# PyTorch에서 같은 난수 결과를 얻기 위해 시드를 설정
torch.manual_seed(1)

# 가중치 1개 선언 : 임의값으로 초기화
# 정규분포에서 난수 하나 생성 (모델의 weight 초기화)
weight = torch.randn(1)

# weight에 대해 gradient를 계산할 수 있도록 설정 : weight를 미분 가능하게 만들겠다
weight.requires_grad_() 

# bias 1개 선언
# bias를 0으로 초기화하고, gradient 계산 허용
bias = torch.zeros(1, requires_grad=True)

# 선형 모델 정의: 입력 xb에 대해 wx + b 계산
def model(x_batch):
    return x_batch @ weight + bias  # @는 행렬 곱

# 손실 함수 정의: 평균 제곱 오차(MSE)
def loss_fn(input, target): # input : y^(예측값), target : y 실제값
    return (input - target).pow(2).mean()

# 학습률 설정
learning_rate = 0.001

# 총 학습할 epoch 수 : 전체 데이터 몇번 반복해서 사용할 것인지
num_epochs = 200

# 로그를 출력할 epoch 간격 : 몇번마다 손실(오차)을 출력한 것인지? 지정
log_epochs = 10

# 에폭만큼 반복 (전체 데이터셋을 num_epochs번 학습)
for epoch in range(num_epochs):
    # 미니 배치 학습
    # 데이터 추출(train_dl)해서 모델에 넣어야한다
    for x_batch, y_batch in train_dl:  # train_dl은 데이터 로더 x_batch: 데이터, y_batch : 레이블
        pred = model(x_batch)  # 모델 예측값 계산
        loss = loss_fn(pred, y_batch)  # 손실 계산 
        loss.backward()  # 오차 역전파 : 손실 함수에 대한 weight, bias의 gradient 계산
        
    # 추론 : new data를 넣어서 예측값을 추출 : 계산 그래프 X, 가중치값을 저장 X
    # weight와 bias 업데이트 (경사하강법)
    with torch.no_grad():  # gradient 계산 비활성화 (메모리 절약) : 가중치 값들을 구해서 출력
        weight -= weight.grad * learning_rate  # weight 업데이트
        bias -= bias.grad * learning_rate      # bias 업데이트
        weight.grad.zero_()  # gradient를 0으로 초기화
        bias.grad.zero_()    # gradient를 0으로 초기화

    # 일정 에폭마다 손실 출력 : 에포크 10의 배수마다 오차를 출력력
    if epoch % log_epochs == 0:
        print(f'에포크 : {epoch} 손실 : {loss.item()}')

print('='*100)
print(f'최종 파라미터 : {weight.item()} {bias.item()}')
```
![image](https://github.com/user-attachments/assets/7aaba317-192f-4562-83e1-54c9d0c44c95)

```
X_test = np.linspace(0, 9, num=100, dtype='float32').reshape(-1, 1)
X_test_norm = (X_test - np.mean(X_train)) / np.std(X_train)
X_test_norm = torch.from_numpy(X_test_norm)
y_pred = model(X_test_norm).detach()  # 계산 그래프에서 텐서를 분리합니다.
fig = plt.figure(figsize=(13, 5))
ax = fig.add_subplot(1, 2, 1)
plt.plot(X_train_norm, y_train, 'o', markersize=10)
plt.plot(X_test_norm, y_pred, '--', lw=3)
plt.legend(['Training examples', 'Linear Reg.'], fontsize=15)
ax.set_xlabel('x', size=15)
ax.set_ylabel('y', size=15)
ax.tick_params(axis='both', which='major', labelsize=15)
#plt.savefig('figures/12_08.pdf')
plt.show()
```
![image](https://github.com/user-attachments/assets/8888ece9-e7fe-461f-a681-913f52b062cd)

---
## torch.nn와 torch.optim 모듈로 모델 훈련
```
import torch.nn as nn

input_size=1
output_size=1

learning_rate = 0.001
num_epochs = 200

# 신경망 구성
model = nn.Linear(input_size,output_size)

# 손실 함수 정의
loss_fn=nn.MSELoss(
    reduction='mean'
)

optimizer=torch.optim.SGD(
    model.parameters()
    ,lr=learning_rate
)

for epoch in range(num_epochs):
    for x_batch,y_batch in train_dl:
        # 예측을 생성
        pred=model(x_batch)[:,0]
        # 손실을 계산
        loss = loss_fn(pred,y_batch)
        # 그레디언트를 계산
        loss.backward()
        # 그레디언트를 사용하여 파라미터를 업데이트
        optimizer.step()
        # 그레디언트를 0으로 초기화
        optimizer.zero_grad()
    if epoch % log_epochs ==0:
        print(f'에포크 : {epoch} 손실 : {loss.item()}')

print(f'최종 파라미터 : {weight.item()} {bias.item()}')
```
![image](https://github.com/user-attachments/assets/7c379809-7792-42cf-b589-fd76211142d7)

```
print('최종 파라미터:', model.weight.item(), model.bias.item())


X_test = np.linspace(0, 9, num=100, dtype='float32').reshape(-1, 1)
X_test_norm = (X_test - np.mean(X_train)) / np.std(X_train)
X_test_norm = torch.from_numpy(X_test_norm)
y_pred = model(X_test_norm).detach()

fig = plt.figure(figsize=(13, 5))
ax = fig.add_subplot(1, 2, 1)
plt.plot(X_train_norm, y_train, 'o', markersize=10)
plt.plot(X_test_norm, y_pred, '--', lw=3)
plt.legend(['Training examples', 'Linear reg.'], fontsize=15)
ax.set_xlabel('x', size=15)
ax.set_ylabel('y', size=15)
ax.tick_params(axis='both', which='major', labelsize=15)


#plt.savefig('ch12-linreg-2.pdf')


plt.show()
```
![image](https://github.com/user-attachments/assets/c88ff10b-0220-45e1-b687-607a053dddcb)

---
## 붓꽃 데이터셋을 분류하는 다층 퍼셉트론 만들기
```
import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

# =============================
# Iris 데이터셋 로드 및 분할
# =============================

# Iris 데이터셋 로드 (입력 X: 꽃받침/꽃잎 길이와 너비, 레이블 y: 품종)
iris = load_iris()
X = iris['data']          # 입력 특성 (총 4개 특성)
y = iris['target']        # 출력 레이블 (0: setosa, 1: versicolor, 2: virginica)

# 데이터 타입 확인
print(f'X의 타입 : {type(X)}')
print(f'y의 타입 : {type(y)}')

# 훈련/테스트 세트로 분할 (1/3은 테스트용, 클래스 비율 유지 위해 stratify 사용)
X_train, X_test, y_train, y_test = train_test_split(
    X,
    y,
    test_size=1./3,
    random_state=1,
    stratify=y
)

# =============================
# 데이터 전처리 및 PyTorch Dataset 준비
# =============================

from torch.utils.data import TensorDataset, DataLoader

# 입력 특성 정규화 (평균 0, 표준편차 1) → 학습 안정성 향상
X_train_norm = (X_train - np.mean(X_train)) / np.std(X_train)

# numpy → torch tensor 변환 (모델 학습을 위한 준비)
X_train_norm = torch.from_numpy(X_train_norm).float()  # 입력은 float형
y_train = torch.from_numpy(y_train).long()             # 라벨은 long형 (CrossEntropyLoss용)

# TensorDataset: 입력/라벨을 묶어서 Dataset 형태로 변환
train_ds = TensorDataset(X_train_norm, y_train)

# DataLoader: 배치 단위로 데이터를 로드 + 셔플
torch.manual_seed(1)       # 랜덤 시드 고정 (동일 결과 재현을 위해)
batch_size = 2
train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)

# =============================
# 신경망 모델 정의
# =============================

# 다층 퍼셉트론 (MLP) 구조 정의:
# 입력층(4) → 은닉층1(16) + 활성화 → 은닉층2(16) + 활성화 → 출력층(3 클래스)

class Model(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super().__init__()  # nn.Module 초기화
        self.layer1 = nn.Linear(input_size, hidden_size)     # 입력 → 은닉1
        self.layer2 = nn.Linear(hidden_size, output_size)    # 은닉2 → 출력

    def forward(self, x):  # 순전파 정의
        x = self.layer1(x)             # 첫 번째 선형변환
        x = nn.Sigmoid()(x)            # 첫 번째 은닉층 활성화 함수
        x = self.layer2(x)             # 두 번째 선형변환
        x = nn.Softmax(dim=1)(x)       # 다중 클래스 확률 출력 (dim=1은 배치 기준)
        return x

# 모델 인스턴스 생성
input_size = X_train_norm.shape[1]     # 입력 특성 수 = 4
hidden_size = 16                       # 은닉층 노드 수 (하이퍼파라미터)
output_size = 3                        # 클래스 수 = 3 (setosa, versicolor, virginica)
model = Model(input_size=input_size, hidden_size=hidden_size, output_size=output_size)

# 옵티마이저와 손실 함수
import torch.optim as optim

learning_rate=0.001

# 손실 함수 object 정의
loss_fn=nn.CrossEntropyLoss() # 다중분류일때 CrossEntropyLoss 사용

optimizer=torch.optim.Adam(model.parameters(),lr=learning_rate)

num_epochs=200
loss_hist=[0]*num_epochs
accuracy_hist=[0]*num_epochs
for epoch in range(num_epochs):
    for x_batch, y_batch in train_dl:
        pred=model(x_batch)
        loss=loss_fn(pred,y_batch)
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
        loss_hist[epoch] += loss.item() * y_batch.size(0)
        is_correct=(torch.argmax(pred,dim=1)==y_batch).float()
        accuracy_hist[epoch] += is_correct.sum()
    loss_hist[epoch] /= len(train_dl.dataset)
    accuracy_hist[epoch] /= len(train_dl.dataset)

fig = plt.figure(figsize=(12,5))
ax = fig.add_subplot(1,2,1)
ax.plot(loss_hist,lw=3)
ax.set_title('Training loss',size=15)
ax.set_xlabel('Epoch',size=15)
ax.tick_params(axis='both',which='major',labelsize=15)
ax = fig.add_subplot(1,2,2)
ax.plot(accuracy_hist,lw=3)
ax.set_title('Training accuracy',size=15)
ax.set_xlabel('Epoch',size=15)
ax.tick_params(axis='both',which='major',labelsize=15)
plt.show()
```
![image](https://github.com/user-attachments/assets/48ce0c66-ad1a-4038-aa09-2b50aa40300e)
