
```
import torch
import numpy as np
import matplotlib.pyplot as plt

X_train=np.arange(
    10
    ,dtype='float32'
).reshape(
    (10,1)
)

y_train=np.array(
    [1.0,1.3,3.1,2.0,5.0,6.3,6.6,7.4,8.0,9.0]
    ,dtype='float32'
)

plt.plot(X_train,y_train,'o',markersize=10)
plt.xlabel('x')
plt.ylabel('y')
plt.show()
```
![image](https://github.com/user-attachments/assets/6e1ac30f-3f85-4b0d-acc8-3111e07bd2cc)

```
from torch.utils.data import TensorDataset, DataLoader
# 신경망은 비모수적 추정이므로 정규분포화를 하면 영향이 있지만, sklearn은 모수적 추정이므로 정규분포화를 해도 성능이 크게 향상되지 않는다.
# X_train 데이터를 정규화 (평균 0, 표준편차 1로 스케일링)
X_train_norm = (X_train - np.mean(X_train)) / np.std(X_train)

# 정규화된 X_train 데이터의 형태와 타입 출력
print(f'X_train_norm shape : {X_train_norm.shape}')
print(f'X_train_norm type : {type(X_train_norm)}')
print(X_train_norm)
print('='*100)

# 넘파이 배열을 파이토치 텐서로 변환
X_train_norm = torch.from_numpy(X_train_norm)

# 텐서로 변환된 후의 형태와 타입 출력
print(f'X_train_norm shape : {X_train_norm.shape}')
print(f'X_train_norm type : {type(X_train_norm)}')
print(X_train_norm)
print('='*100)

# y_train이 넘파이 배열이라면 파이토치 텐서(float 타입)로 변환
# 이미 텐서라면 float 타입으로 캐스팅
# if isinstance(y_train, np.ndarray):
#     y_train = torch.from_numpy(y_train).float()
# else:
#     y_train = y_train.float()
y_train=torch.from_numpy(y_train).float()
# X_train_norm과 y_train을 묶어서 하나의 TensorDataset 생성
train_ds = TensorDataset(X_train_norm, y_train)

# 배치 사이즈 설정 (여기서는 1로 설정)하고, 데이터를 셔플해서 로딩할 수 있도록 DataLoader 생성
batch_size = 1
train_dl = DataLoader(train_ds, batch_size, shuffle=True)

# DataLoader 객체 출력 (내부에 있는 데이터나 설정 확인용)
print(train_dl)

```
```
X_train_norm shape : (10, 1)
X_train_norm type : <class 'numpy.ndarray'>
[[-1.5666989 ]
 [-1.2185436 ]
 [-0.87038827]
 [-0.52223295]
 [-0.17407766]
 [ 0.17407766]
 [ 0.52223295]
 [ 0.87038827]
 [ 1.2185436 ]
 [ 1.5666989 ]]
====================================================================================================
X_train_norm shape : torch.Size([10, 1])
X_train_norm type : <class 'torch.Tensor'>
tensor([[-1.5667],
        [-1.2185],
        [-0.8704],
        [-0.5222],
        [-0.1741],
        [ 0.1741],
        [ 0.5222],
        [ 0.8704],
        [ 1.2185],
        [ 1.5667]])
====================================================================================================
<torch.utils.data.dataloader.DataLoader object at 0x0000014A17A0AFB0>
```
```
# PyTorch에서 같은 난수 결과를 얻기 위해 시드를 설정
torch.manual_seed(1)

# 가중치 1개 선언 : 임의값으로 초기화
# 정규분포에서 난수 하나 생성 (모델의 weight 초기화)
weight = torch.randn(1)

# weight에 대해 gradient를 계산할 수 있도록 설정 : weight를 미분 가능하게 만들겠다
weight.requires_grad_() 

# bias 1개 선언
# bias를 0으로 초기화하고, gradient 계산 허용
bias = torch.zeros(1, requires_grad=True)

# 선형 모델 정의: 입력 xb에 대해 wx + b 계산
def model(x_batch):
    return x_batch @ weight + bias  # @는 행렬 곱

# 손실 함수 정의: 평균 제곱 오차(MSE)
def loss_fn(input, target): # input : y^(예측값), target : y 실제값
    return (input - target).pow(2).mean()

# 학습률 설정
learning_rate = 0.001

# 총 학습할 epoch 수 : 전체 데이터 몇번 반복해서 사용할 것인지
num_epochs = 200

# 로그를 출력할 epoch 간격 : 몇번마다 손실(오차)을 출력한 것인지? 지정
log_epochs = 10

# 에폭만큼 반복 (전체 데이터셋을 num_epochs번 학습)
for epoch in range(num_epochs):
    # 미니 배치 학습
    # 데이터 추출(train_dl)해서 모델에 넣어야한다
    for x_batch, y_batch in train_dl:  # train_dl은 데이터 로더 x_batch: 데이터, y_batch : 레이블
        pred = model(x_batch)  # 모델 예측값 계산
        loss = loss_fn(pred, y_batch)  # 손실 계산 
        loss.backward()  # 오차 역전파 : 손실 함수에 대한 weight, bias의 gradient 계산
        
    # 추론 : new data를 넣어서 예측값을 추출 : 계산 그래프 X, 가중치값을 저장 X
    # weight와 bias 업데이트 (경사하강법)
    with torch.no_grad():  # gradient 계산 비활성화 (메모리 절약) : 가중치 값들을 구해서 출력
        weight -= weight.grad * learning_rate  # weight 업데이트
        bias -= bias.grad * learning_rate      # bias 업데이트
        weight.grad.zero_()  # gradient를 0으로 초기화
        bias.grad.zero_()    # gradient를 0으로 초기화

    # 일정 에폭마다 손실 출력 : 에포크 10의 배수마다 오차를 출력력
    if epoch % log_epochs == 0:
        print(f'에포크 : {epoch} 손실 : {loss.item()}')

print('='*100)
print(f'최종 파라미터 : {weight.item()} {bias.item()}')
```
![image](https://github.com/user-attachments/assets/7aaba317-192f-4562-83e1-54c9d0c44c95)

```
X_test = np.linspace(0, 9, num=100, dtype='float32').reshape(-1, 1)
X_test_norm = (X_test - np.mean(X_train)) / np.std(X_train)
X_test_norm = torch.from_numpy(X_test_norm)
y_pred = model(X_test_norm).detach()  # 계산 그래프에서 텐서를 분리합니다.
fig = plt.figure(figsize=(13, 5))
ax = fig.add_subplot(1, 2, 1)
plt.plot(X_train_norm, y_train, 'o', markersize=10)
plt.plot(X_test_norm, y_pred, '--', lw=3)
plt.legend(['Training examples', 'Linear Reg.'], fontsize=15)
ax.set_xlabel('x', size=15)
ax.set_ylabel('y', size=15)
ax.tick_params(axis='both', which='major', labelsize=15)
#plt.savefig('figures/12_08.pdf')
plt.show()
```
![image](https://github.com/user-attachments/assets/8888ece9-e7fe-461f-a681-913f52b062cd)

---
## torch.nn와 torch.optim 모듈로 모델 훈련
```
import torch.nn as nn

input_size=1
output_size=1

learning_rate = 0.001
num_epochs = 200

# 신경망 구성
model = nn.Linear(input_size,output_size)

# 손실 함수 정의
loss_fn=nn.MSELoss(
    reduction='mean'
)

optimizer=torch.optim.SGD(
    model.parameters()
    ,lr=learning_rate
)

for epoch in range(num_epochs):
    for x_batch,y_batch in train_dl:
        # 예측을 생성
        pred=model(x_batch)[:,0]
        # 손실을 계산
        loss = loss_fn(pred,y_batch)
        # 그레디언트를 계산
        loss.backward()
        # 그레디언트를 사용하여 파라미터를 업데이트
        optimizer.step()
        # 그레디언트를 0으로 초기화
        optimizer.zero_grad()
    if epoch % log_epochs ==0:
        print(f'에포크 : {epoch} 손실 : {loss.item()}')

print(f'최종 파라미터 : {weight.item()} {bias.item()}')
```
![image](https://github.com/user-attachments/assets/7c379809-7792-42cf-b589-fd76211142d7)

```
print('최종 파라미터:', model.weight.item(), model.bias.item())


X_test = np.linspace(0, 9, num=100, dtype='float32').reshape(-1, 1)
X_test_norm = (X_test - np.mean(X_train)) / np.std(X_train)
X_test_norm = torch.from_numpy(X_test_norm)
y_pred = model(X_test_norm).detach()

fig = plt.figure(figsize=(13, 5))
ax = fig.add_subplot(1, 2, 1)
plt.plot(X_train_norm, y_train, 'o', markersize=10)
plt.plot(X_test_norm, y_pred, '--', lw=3)
plt.legend(['Training examples', 'Linear reg.'], fontsize=15)
ax.set_xlabel('x', size=15)
ax.set_ylabel('y', size=15)
ax.tick_params(axis='both', which='major', labelsize=15)


#plt.savefig('ch12-linreg-2.pdf')


plt.show()
```
![image](https://github.com/user-attachments/assets/c88ff10b-0220-45e1-b687-607a053dddcb)

---
## 붓꽃 데이터셋을 분류하는 다층 퍼셉트론 만들기
```
import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

# =============================
# Iris 데이터셋 로드 및 분할
# =============================

# Iris 데이터셋 로드 (입력 X: 꽃받침/꽃잎 길이와 너비, 레이블 y: 품종)
iris = load_iris()
X = iris['data']          # 입력 특성 (총 4개 특성)
y = iris['target']        # 출력 레이블 (0: setosa, 1: versicolor, 2: virginica)

# 데이터 타입 확인
print(f'X의 타입 : {type(X)}')
print(f'y의 타입 : {type(y)}')

# 훈련/테스트 세트로 분할 (1/3은 테스트용, 클래스 비율 유지 위해 stratify 사용)
X_train, X_test, y_train, y_test = train_test_split(
    X,
    y,
    test_size=1./3,
    random_state=1,
    stratify=y
)

# =============================
# 데이터 전처리 및 PyTorch Dataset 준비
# =============================

from torch.utils.data import TensorDataset, DataLoader

# 입력 특성 정규화 (평균 0, 표준편차 1) → 학습 안정성 향상
X_train_norm = (X_train - np.mean(X_train)) / np.std(X_train)

# numpy → torch tensor 변환 (모델 학습을 위한 준비)
X_train_norm = torch.from_numpy(X_train_norm).float()  # 입력은 float형
y_train = torch.from_numpy(y_train).long()             # 라벨은 long형 (CrossEntropyLoss용)

# TensorDataset: 입력/라벨을 묶어서 Dataset 형태로 변환
train_ds = TensorDataset(X_train_norm, y_train)

# DataLoader: 배치 단위로 데이터를 로드 + 셔플
torch.manual_seed(1)       # 랜덤 시드 고정 (동일 결과 재현을 위해)
batch_size = 2
train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)

# =============================
# 신경망 모델 정의
# =============================

# 다층 퍼셉트론 (MLP) 구조 정의:
# 입력층(4) → 은닉층1(16) + 활성화 → 은닉층2(16) + 활성화 → 출력층(3 클래스)

class Model(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super().__init__()  # nn.Module 초기화
        self.layer1 = nn.Linear(input_size, hidden_size)     # 입력 → 은닉1
        self.layer2 = nn.Linear(hidden_size, output_size)    # 은닉2 → 출력

    def forward(self, x):  # 순전파 정의
        x = self.layer1(x)             # 첫 번째 선형변환
        x = nn.Sigmoid()(x)            # 첫 번째 은닉층 활성화 함수
        x = self.layer2(x)             # 두 번째 선형변환
        x = nn.Softmax(dim=1)(x)       # 다중 클래스 확률 출력 (dim=1은 배치 기준)
        return x

# 모델 인스턴스 생성
input_size = X_train_norm.shape[1]     # 입력 특성 수 = 4
hidden_size = 16                       # 은닉층 노드 수 (하이퍼파라미터)
output_size = 3                        # 클래스 수 = 3 (setosa, versicolor, virginica)
model = Model(input_size=input_size, hidden_size=hidden_size, output_size=output_size)

# 옵티마이저와 손실 함수
import torch.optim as optim

learning_rate=0.001

# 손실 함수 object 정의
loss_fn=nn.CrossEntropyLoss() # 다중분류일때 CrossEntropyLoss 사용

optimizer=torch.optim.Adam(model.parameters(),lr=learning_rate)

num_epochs=200
loss_hist=[0]*num_epochs
accuracy_hist=[0]*num_epochs
for epoch in range(num_epochs):
    for x_batch, y_batch in train_dl:
        pred=model(x_batch)
        loss=loss_fn(pred,y_batch)
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
        loss_hist[epoch] += loss.item() * y_batch.size(0)
        is_correct=(torch.argmax(pred,dim=1)==y_batch).float()
        accuracy_hist[epoch] += is_correct.sum()
    loss_hist[epoch] /= len(train_dl.dataset)
    accuracy_hist[epoch] /= len(train_dl.dataset)

fig = plt.figure(figsize=(12,5))
ax = fig.add_subplot(1,2,1)
ax.plot(loss_hist,lw=3)
ax.set_title('Training loss',size=15)
ax.set_xlabel('Epoch',size=15)
ax.tick_params(axis='both',which='major',labelsize=15)
ax = fig.add_subplot(1,2,2)
ax.plot(accuracy_hist,lw=3)
ax.set_title('Training accuracy',size=15)
ax.set_xlabel('Epoch',size=15)
ax.tick_params(axis='both',which='major',labelsize=15)
plt.show()
```
![image](https://github.com/user-attachments/assets/48ce0c66-ad1a-4038-aa09-2b50aa40300e)

```
# 모델 평가 : 테스트 데이터 -> X_test, y_test

# 데이터 정규화
X_test_norm=torch.from_numpy(
    (X_test - np.mean(X_train))/np.std(X_train)
).float()
if isinstance(y_test, np.ndarray):
    y_test = torch.from_numpy(y_test).float()
pred_test=model(X_test_norm)
correct=(
    (torch.argmax(pred_test,dim=1) == y_test).float()
)
accuracy=correct.mean()
print(f'테스트 정확도 : {accuracy}')
```
![image](https://github.com/user-attachments/assets/97b74ee5-1f54-478e-96a2-feb0db451c01)

```
# 훈련된 모델 저장
path='iris_classifier.pt'
torch.save(model,path)
```
![image](https://github.com/user-attachments/assets/4e1c629b-c86c-4840-af05-b46ed2a5e448)

```
# 저장된 모델  로드
path='iris_classifier.pt'
load_model=torch.load(path)
load_model.eval()
```
![image](https://github.com/user-attachments/assets/c4e0a4d4-ede9-4a63-b9ef-fbb49e0cc10b)

```
# 로드 평가
pred_test=load_model(X_test_norm)
correct=(
   (torch.argmax(pred_test,dim=1) == y_test).float() 
)
accuracy=correct.mean()
print(f'테스트 정확도 : {accuracy}')
```
![image](https://github.com/user-attachments/assets/62bedde1-121a-428a-a846-4eb09093cbdc)

```
# 학습된 훈련 파라미터만 저장하고 싶으면
path='iris_classifier_state.pt'
torch.save(model.state_dict(),path)
```
![image](https://github.com/user-attachments/assets/1befdebd-5617-47aa-a0ca-cf932951a689)

```
# 저장된 파라미터 로드
model_new_pa=Model(input_size,hidden_size,output_size)
model_new_pa.load_state_dict(torch.load(path))
```
![image](https://github.com/user-attachments/assets/9841436e-47e4-43b3-a638-48f306986993)

---
## 다층 신경망의 활성화 함수 선택

신경망(Neural Network)에서 **활성화 함수(Activation Function)** 는 뉴런의 출력값을 결정합니다.  
비선형성을 부여하여 복잡한 문제를 풀 수 있게 도와주는 중요한 요소입니다.

### 1. ✅ 로지스틱 함수 (Logistic, Sigmoid)

- **수식**

  $$
  \[
  \sigma(x) = \frac{1}{1 + e^{-x}}
  \]
  $$

- **출력 범위**: `0 ~ 1`

- **특징**:
  - S자 곡선 (sigmoid curve)
  - **이진 분류**의 출력층에서 자주 사용됨

- **장점**:
  - 출력값을 **확률처럼 해석** 가능

- **단점**:
  - **Vanishing Gradient** 문제 발생 (기울기 → 0)
  - 출력값이 0 중심이 아님 → 학습 속도 느림


### 2. ✅ 소프트맥스 함수 (Softmax)

- **수식** (클래스 i에 대해)
  
  $$
  \[
  \text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j} e^{z_j}}
  \]
  $$

- **출력 범위**: `0 ~ 1` (전체 합은 1)

- **특징**:
  - **다중 클래스 분류**에서 출력층에 사용
  - 각 클래스에 대한 확률 제공

- **장점**:
  - 확률 분포로 해석 가능

- **단점**:
  - 큰 값에 민감 → 학습 불안정 가능


### 3. ✅ 하이퍼볼릭 탄젠트 함수 (tanh)

- **수식**
  
  $$
  \[
  \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
  \]
  $$

- **출력 범위**: `-1 ~ 1`

- **특징**:
  - sigmoid보다 **출력이 0 중심**

- **장점**:
  - 학습 속도가 빠름
  - sigmoid보다 gradient가 큼

- **단점**:
  - 여전히 **Vanishing Gradient** 문제 존재


### 4. ✅ ReLU 함수 (Rectified Linear Unit)

- **수식**
  
  $$
  \[
  f(x) = \max(0, x)
  \]
  $$

- **출력 범위**: `0 ~ ∞`

- **특징**:
  - 가장 널리 사용되는 활성화 함수

- **장점**:
  - 계산 간단, 학습 빠름
  - Vanishing Gradient 문제 감소

- **단점**:
  - **Dead Neuron 문제** 발생 가능 (음수 입력 → 출력 0 → 학습 멈춤)


### 5. 🔍 ReLU 변형들

#### 5-1. ✅ Leaky ReLU

- **수식**
  
  $$
  \[
  f(x) = \begin{cases}
  x & \text{if } x \geq 0 \\
  \alpha x & \text{if } x < 0 \quad (\alpha \approx 0.01)
  \end{cases}
  \]
  $$

- **특징**:
  - 음수 입력에도 **작은 기울기 유지**
  - Dead Neuron 문제 완화


#### 5-2. ✅ Parametric ReLU (PReLU)

- **Leaky ReLU**와 비슷하지만,  
  👉 `α` 값을 **학습을 통해 자동으로 조정**

- **특징**:
  - 유연하게 음수 영역을 조절 가능


#### 5-3. ✅ ELU (Exponential Linear Unit)

- **수식**
  
  $$
  \[
  f(x) = \begin{cases}
  x & \text{if } x \geq 0 \\
  \alpha (e^x - 1) & \text{if } x < 0
  \end{cases}
  \]
  $$

- **특징**:
  - 출력이 0 중심
  - 음수 영역에서도 **부드러운 변화**


#### 5-4. ✅ SELU (Scaled Exponential Linear Unit)

- **수식**
  
  $$
  \[
  f(x) = \lambda \begin{cases}
  x & \text{if } x > 0 \\
  \alpha (e^x - 1) & \text{if } x \leq 0
  \end{cases}
  \]
  $$
  
- **특징**:
  - **Self-normalizing** 기능
  - 특정 조건에서만 사용 추천 (예: AlphaDropout, 특정 초기화)


### ✅ 요약표

| 함수         | 출력 범위     | 중심 | 장점                        | 단점                      |
|--------------|---------------|------|-----------------------------|---------------------------|
| Sigmoid      | (0, 1)        | ❌   | 확률 해석 가능              | 기울기 소실, 느린 학습     |
| Softmax      | (0, 1), 합=1  | ❌   | 다중 클래스 확률            | 큰 값에 민감              |
| tanh         | (-1, 1)       | ✅   | 빠른 학습, 0 중심           | 기울기 소실               |
| ReLU         | [0, ∞)        | ❌   | 빠름, 간단                  | Dead Neuron 문제          |
| Leaky ReLU   | (-∞, ∞)       | ❌   | Dead Neuron 완화            | α 수동 설정               |
| PReLU        | (-∞, ∞)       | ❌   | α 자동 학습                 | 복잡도 증가               |
| ELU          | (-α, ∞)       | ✅   | 0 중심, 부드러운 기울기     | 계산 복잡                 |
| SELU         | 정규화된 범위 | ✅   | 자동 정규화(Self-normalizing)| 조건이 까다로움           |

---
