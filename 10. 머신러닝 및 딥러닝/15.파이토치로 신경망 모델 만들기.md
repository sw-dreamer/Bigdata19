
```
import torch
import numpy as np
import matplotlib.pyplot as plt

X_train=np.arange(
    10
    ,dtype='float32'
).reshape(
    (10,1)
)

y_train=np.array(
    [1.0,1.3,3.1,2.0,5.0,6.3,6.6,7.4,8.0,9.0]
    ,dtype='float32'
)

plt.plot(X_train,y_train,'o',markersize=10)
plt.xlabel('x')
plt.ylabel('y')
plt.show()
```
![image](https://github.com/user-attachments/assets/6e1ac30f-3f85-4b0d-acc8-3111e07bd2cc)

```
from torch.utils.data import TensorDataset, DataLoader
# 신경망은 비모수적 추정이므로 정규분포화를 하면 영향이 있지만, sklearn은 모수적 추정이므로 정규분포화를 해도 성능이 크게 향상되지 않는다.
# X_train 데이터를 정규화 (평균 0, 표준편차 1로 스케일링)
X_train_norm = (X_train - np.mean(X_train)) / np.std(X_train)

# 정규화된 X_train 데이터의 형태와 타입 출력
print(f'X_train_norm shape : {X_train_norm.shape}')
print(f'X_train_norm type : {type(X_train_norm)}')
print(X_train_norm)
print('='*100)

# 넘파이 배열을 파이토치 텐서로 변환
X_train_norm = torch.from_numpy(X_train_norm)

# 텐서로 변환된 후의 형태와 타입 출력
print(f'X_train_norm shape : {X_train_norm.shape}')
print(f'X_train_norm type : {type(X_train_norm)}')
print(X_train_norm)
print('='*100)

# y_train이 넘파이 배열이라면 파이토치 텐서(float 타입)로 변환
# 이미 텐서라면 float 타입으로 캐스팅
# if isinstance(y_train, np.ndarray):
#     y_train = torch.from_numpy(y_train).float()
# else:
#     y_train = y_train.float()
y_train=torch.from_numpy(y_train).float()
# X_train_norm과 y_train을 묶어서 하나의 TensorDataset 생성
train_ds = TensorDataset(X_train_norm, y_train)

# 배치 사이즈 설정 (여기서는 1로 설정)하고, 데이터를 셔플해서 로딩할 수 있도록 DataLoader 생성
batch_size = 1
train_dl = DataLoader(train_ds, batch_size, shuffle=True)

# DataLoader 객체 출력 (내부에 있는 데이터나 설정 확인용)
print(train_dl)

```
```
X_train_norm shape : (10, 1)
X_train_norm type : <class 'numpy.ndarray'>
[[-1.5666989 ]
 [-1.2185436 ]
 [-0.87038827]
 [-0.52223295]
 [-0.17407766]
 [ 0.17407766]
 [ 0.52223295]
 [ 0.87038827]
 [ 1.2185436 ]
 [ 1.5666989 ]]
====================================================================================================
X_train_norm shape : torch.Size([10, 1])
X_train_norm type : <class 'torch.Tensor'>
tensor([[-1.5667],
        [-1.2185],
        [-0.8704],
        [-0.5222],
        [-0.1741],
        [ 0.1741],
        [ 0.5222],
        [ 0.8704],
        [ 1.2185],
        [ 1.5667]])
====================================================================================================
<torch.utils.data.dataloader.DataLoader object at 0x0000014A17A0AFB0>
```
```
# PyTorch에서 같은 난수 결과를 얻기 위해 시드를 설정
torch.manual_seed(1)

# 가중치 1개 선언 : 임의값으로 초기화
# 정규분포에서 난수 하나 생성 (모델의 weight 초기화)
weight = torch.randn(1)

# weight에 대해 gradient를 계산할 수 있도록 설정 : weight를 미분 가능하게 만들겠다
weight.requires_grad_() 

# bias 1개 선언
# bias를 0으로 초기화하고, gradient 계산 허용
bias = torch.zeros(1, requires_grad=True)

# 선형 모델 정의: 입력 xb에 대해 wx + b 계산
def model(x_batch):
    return x_batch @ weight + bias  # @는 행렬 곱

# 손실 함수 정의: 평균 제곱 오차(MSE)
def loss_fn(input, target): # input : y^(예측값), target : y 실제값
    return (input - target).pow(2).mean()

# 학습률 설정
learning_rate = 0.001

# 총 학습할 epoch 수 : 전체 데이터 몇번 반복해서 사용할 것인지
num_epochs = 200

# 로그를 출력할 epoch 간격 : 몇번마다 손실(오차)을 출력한 것인지? 지정
log_epochs = 10

# 에폭만큼 반복 (전체 데이터셋을 num_epochs번 학습)
for epoch in range(num_epochs):
    # 미니 배치 학습
    # 데이터 추출(train_dl)해서 모델에 넣어야한다
    for x_batch, y_batch in train_dl:  # train_dl은 데이터 로더 x_batch: 데이터, y_batch : 레이블
        pred = model(x_batch)  # 모델 예측값 계산
        loss = loss_fn(pred, y_batch)  # 손실 계산 
        loss.backward()  # 오차 역전파 : 손실 함수에 대한 weight, bias의 gradient 계산
        
    # 추론 : new data를 넣어서 예측값을 추출 : 계산 그래프 X, 가중치값을 저장 X
    # weight와 bias 업데이트 (경사하강법)
    with torch.no_grad():  # gradient 계산 비활성화 (메모리 절약) : 가중치 값들을 구해서 출력
        weight -= weight.grad * learning_rate  # weight 업데이트
        bias -= bias.grad * learning_rate      # bias 업데이트
        weight.grad.zero_()  # gradient를 0으로 초기화
        bias.grad.zero_()    # gradient를 0으로 초기화

    # 일정 에폭마다 손실 출력 : 에포크 10의 배수마다 오차를 출력력
    if epoch % log_epochs == 0:
        print(f'에포크 : {epoch} 손실 : {loss.item()}')

print('='*100)
print(f'최종 파라미터 : {weight.item()} {bias.item()}')
```
![image](https://github.com/user-attachments/assets/7aaba317-192f-4562-83e1-54c9d0c44c95)

```
X_test = np.linspace(0, 9, num=100, dtype='float32').reshape(-1, 1)
X_test_norm = (X_test - np.mean(X_train)) / np.std(X_train)
X_test_norm = torch.from_numpy(X_test_norm)
y_pred = model(X_test_norm).detach()  # 계산 그래프에서 텐서를 분리합니다.
fig = plt.figure(figsize=(13, 5))
ax = fig.add_subplot(1, 2, 1)
plt.plot(X_train_norm, y_train, 'o', markersize=10)
plt.plot(X_test_norm, y_pred, '--', lw=3)
plt.legend(['Training examples', 'Linear Reg.'], fontsize=15)
ax.set_xlabel('x', size=15)
ax.set_ylabel('y', size=15)
ax.tick_params(axis='both', which='major', labelsize=15)
#plt.savefig('figures/12_08.pdf')
plt.show()
```
![image](https://github.com/user-attachments/assets/8888ece9-e7fe-461f-a681-913f52b062cd)

---
## torch.nn와 torch.optim 모듈로 모델 훈련
