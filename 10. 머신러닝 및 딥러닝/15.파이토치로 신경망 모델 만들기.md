
```
import torch
import numpy as np
import matplotlib.pyplot as plt

X_train=np.arange(
    10
    ,dtype='float32'
).reshape(
    (10,1)
)

y_train=np.array(
    [1.0,1.3,3.1,2.0,5.0,6.3,6.6,7.4,8.0,9.0]
    ,dtype='float32'
)

plt.plot(X_train,y_train,'o',markersize=10)
plt.xlabel('x')
plt.ylabel('y')
plt.show()
```
![image](https://github.com/user-attachments/assets/6e1ac30f-3f85-4b0d-acc8-3111e07bd2cc)

```
from torch.utils.data import TensorDataset, DataLoader
X_train_norm=(X_train - np.mean(X_train))/np.std(X_train)
print(f'X_train_norm shape : {X_train_norm.shape}')
print(f'X_train_norm type : {type(X_train_norm)}')
print(X_train_norm)
print('='*100)
X_train_norm=torch.from_numpy(X_train_norm)
print(f'X_train_norm shape : {X_train_norm.shape}')
print(f'X_train_norm type : {type(X_train_norm)}')
print(X_train_norm)
print('='*100)
if isinstance(y_train, np.ndarray):
    y_train = torch.from_numpy(y_train).float()
else:
    y_train = y_train.float()

train_ds=TensorDataset(X_train_norm,y_train)
batch_size=1
train_dl=DataLoader(train_ds,batch_size,shuffle=True)
print(train_dl)
```
```
X_train_norm shape : (10, 1)
X_train_norm type : <class 'numpy.ndarray'>
[[-1.5666989 ]
 [-1.2185436 ]
 [-0.87038827]
 [-0.52223295]
 [-0.17407766]
 [ 0.17407766]
 [ 0.52223295]
 [ 0.87038827]
 [ 1.2185436 ]
 [ 1.5666989 ]]
====================================================================================================
X_train_norm shape : torch.Size([10, 1])
X_train_norm type : <class 'torch.Tensor'>
tensor([[-1.5667],
        [-1.2185],
        [-0.8704],
        [-0.5222],
        [-0.1741],
        [ 0.1741],
        [ 0.5222],
        [ 0.8704],
        [ 1.2185],
        [ 1.5667]])
====================================================================================================
<torch.utils.data.dataloader.DataLoader object at 0x0000014A17A0AFB0>
```
```
# PyTorch에서 같은 난수 결과를 얻기 위해 시드를 설정
torch.manual_seed(1)

# 정규분포에서 난수 하나 생성 (모델의 weight 초기화)
weight = torch.randn(1)

# weight에 대해 gradient를 계산할 수 있도록 설정
weight.requires_grad_()

# bias를 0으로 초기화하고, gradient 계산 허용
bias = torch.zeros(1, requires_grad=True)

# 선형 모델 정의: 입력 xb에 대해 wx + b 계산
def model(xb):
    return xb @ weight + bias  # @는 행렬 곱

# 손실 함수 정의: 평균 제곱 오차(MSE)
def loss_fn(input, target):
    return (input - target).pow(2).mean()

# 학습률 설정
learning_rate = 0.001

# 총 학습할 epoch 수
num_epochs = 200

# 로그를 출력할 epoch 간격
log_epochs = 10

# 에폭만큼 반복 (전체 데이터셋을 num_epochs번 학습)
for epoch in range(num_epochs):
    # 미니 배치 학습
    for x_batch, y_batch in train_dl:  # train_dl은 데이터 로더
        pred = model(x_batch)  # 모델 예측값 계산
        loss = loss_fn(pred, y_batch.long())  # 손실 계산 (정수형 타겟으로 변환)
        loss.backward()  # 손실 함수에 대한 weight, bias의 gradient 계산

    # weight와 bias 업데이트 (경사하강법)
    with torch.no_grad():  # gradient 계산 비활성화 (메모리 절약)
        weight -= weight.grad * learning_rate  # weight 업데이트
        bias -= bias.grad * learning_rate      # bias 업데이트
        weight.grad.zero_()  # gradient를 0으로 초기화
        bias.grad.zero_()    # gradient를 0으로 초기화

    # 일정 에폭마다 손실 출력
    if epoch % log_epochs == 0:
        print(f'에포크 : {epoch} 손실 : {loss.item()}')

print('='*100)
print(f'최종 파라미터 : {weight.item()} {bias.item()}')
```
![image](https://github.com/user-attachments/assets/cc83aa0c-04d0-4369-a704-951a7cc28124)

