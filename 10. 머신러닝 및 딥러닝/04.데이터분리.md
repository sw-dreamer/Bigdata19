# 데이터 분리

---
## 데이터셋을 훈련 데이터셋과 테스트 데이터셋으로 나누기

```
import pandas as pd
import numpy as np

df_wine = pd.read_csv('https://archive.ics.uci.edu/'
                      'ml/machine-learning-databases/wine/wine.data',
                      header=None)

# UCI 머신러닝 저장소의 Wine 데이터셋에 접근되지 않을 때
# 다음 코드의 주석을 제거하고 로컬 경로에서 데이터셋을 읽으세요:

# df_wine = pd.read_csv('wine.data', header=None)


df_wine.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash',
                   'Alcalinity of ash', 'Magnesium', 'Total phenols',
                   'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins',
                   'Color intensity', 'Hue', 'OD280/OD315 of diluted wines',
                   'Proline']

print('Class labels', np.unique(df_wine['Class label']))
df_wine.head()
```
![image](https://github.com/user-attachments/assets/24c5beed-dfb8-43a4-91e4-ccc4f53d7129)

```
import pandas as pd
import numpy as np

df_wine = pd.read_csv('https://archive.ics.uci.edu/'
                      'ml/machine-learning-databases/wine/wine.data',
                      header=None)

df_wine.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash',
                   'Alcalinity of ash', 'Magnesium', 'Total phenols',
                   'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins',
                   'Color intensity', 'Hue', 'OD280/OD315 of diluted wines',
                   'Proline']

df_wine.info()
```
![image](https://github.com/user-attachments/assets/fb9406ee-5539-4fdc-8c66-376035441246)

```
import pandas as pd
import numpy as np

df_wine = pd.read_csv('https://archive.ics.uci.edu/'
                      'ml/machine-learning-databases/wine/wine.data',
                      header=None)

df_wine.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash',
                   'Alcalinity of ash', 'Magnesium', 'Total phenols',
                   'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins',
                   'Color intensity', 'Hue', 'OD280/OD315 of diluted wines',
                   'Proline']

df_wine.describe()
```
![image](https://github.com/user-attachments/assets/0ffab463-f4fa-4ec0-9b70-7892beef6b11)

```
import pandas as pd
import numpy as np

df_wine = pd.read_csv('https://archive.ics.uci.edu/'
                      'ml/machine-learning-databases/wine/wine.data',
                      header=None)

df_wine.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash',
                   'Alcalinity of ash', 'Magnesium', 'Total phenols',
                   'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins',
                   'Color intensity', 'Hue', 'OD280/OD315 of diluted wines',
                   'Proline']

from sklearn.model_selection import train_test_split

# 데이터(X)와 답(y) 분리
X=df_wine.iloc[:,1:].values # 모든행, 1열부터 끝까지
y=df_wine.iloc[:,0].values # 레이블(답), 0열부터 끝까지

# 학습/테스트 데이터(답) 분리
X_train, X_test, y_train, y_test= train_test_split(
    X
    ,y
    ,test_size=0.3
    ,random_state=1
    ,stratify=y 
)
print(f'X_train.shape : {X_train.shape}')
print(f'X_test.shape : {X_test.shape}')
print(f'y_train.shape : {y_train.shape}')
print(f'y_test.shape : {y_test.shape}')
print('='*50)
print(f'np.bincount(y_train) : {np.bincount(y_train)}')
print(f'np.bincount(y_test) : {np.bincount(y_test)}')
```
![image](https://github.com/user-attachments/assets/7a09b08f-8b71-4b55-a668-8c171b12b69f)
---
## Scaler 종류

데이터 전처리에서 사용하는 다양한 스케일링 기법에 대해 설명합니다. 각 스케일러는 데이터를 특정 범위나 분포로 변환하는 방식이 다릅니다.

### 1. StandardScaler
- **목적**: 데이터를 평균이 0이고 표준편차가 1인 정규분포로 변환합니다.
- **사용법**: 데이터에서 평균을 빼고, 표준편차로 나눕니다.
- **공식**:
  
  ![StandardScaler](https://latex.codecogs.com/png.latex?X_%7B%5Ctext%7Bscaled%7D%7D%20%3D%20%5Cfrac%7BX%20-%20%5Cmu%7D%7B%5Csigma%7D)

  여기서, $\mu$는 평균, $\sigma$는 표준편차입니다.
- **특징**:
  - 이상치에 민감합니다.
  - 데이터가 정규분포를 따를 때 가장 효과적입니다.

### 2. MinMaxScaler
- **목적**: 데이터를 지정된 범위로 변환합니다. 보통 0과 1 사이로 변환됩니다.
- **사용법**: 데이터의 최소값과 최대값을 이용하여 데이터를 변환합니다.
- **공식**:
  
  ![MinMaxScaler](https://latex.codecogs.com/png.latex?X_%7B%5Ctext%7Bscaled%7D%7D%20%3D%20%5Cfrac%7BX%20-%20X_%7B%5Ctext%7Bmin%7D%7D%7D%7BX_%7B%5Ctext%7Bmax%7D%7D%20-%20X_%7B%5Ctext%7Bmin%7D%7D%7D)

  여기서, $X_{\text{min}}$과 $X_{\text{max}}$는 데이터의 최소값과 최대값입니다.
- **특징**:
  - 이상치가 있을 경우, 이상치가 다른 데이터의 스케일에 큰 영향을 미칩니다.
  - 모든 데이터가 같은 범위(보통 0~1)로 변환되기 때문에 다른 모델링에서 더 유용할 수 있습니다.

## 3. RobustScaler
- **목적**: 데이터를 중앙값(median)과 사분위수 범위(IQR)를 기준으로 변환합니다. 이상치에 덜 민감하게 변환됩니다.
- **사용법**: 중앙값을 빼고, IQR(75th percentile - 25th percentile)로 나눕니다.
- **공식**
  
  ![RobustScaler](https://latex.codecogs.com/png.latex?X_%7B%5Ctext%7Bscaled%7D%7D%20%3D%20%5Cfrac%7BX%20-%20%5Ctext%7BMedian%7D%28X%29%7D%7B%5Ctext%7BIQR%7D%28X%29%7D)

  여기서, $\text{Median}(X)$은 중앙값, $\text{IQR}(X)$는 사분위수 범위입니다.
- **특징**:
  - 이상치에 강인합니다.
  - 데이터가 이상치에 민감할 때 유용합니다.

## 4. MaxAbsScaler
- **목적**: 데이터를 절대값의 최대값으로 나누어, 데이터를 -1과 1 사이로 변환합니다.
- **사용법**: 각 데이터 포인트를 절대값의 최대값으로 나눕니다.
- **공식**
  
  ![image](https://github.com/user-attachments/assets/b142beca-6f2c-43cf-a99d-6023290fbf87)

  여기서, $|X_{\text{max}}|$는 절대값이 가장 큰 데이터 값입니다.
- **특징**:
  - 데이터가 양수와 음수를 포함하는 경우에도 데이터를 0과 ±1 사이로 변환합니다.
  - 이상치에 강인하며, 데이터가 스케일을 유지하면서 변화합니다.
  - 주로 희소 행렬(sparse matrix)에서 유용하게 사용됩니다.

## 요약
- **StandardScaler**: 평균이 0, 표준편차가 1인 정규분포로 변환 (정규분포 데이터에 적합)
- **MinMaxScaler**: 데이터를 특정 범위(보통 0과 1)로 변환 (이상치에 민감)
- **RobustScaler**: 중앙값과 IQR을 기준으로 변환 (이상치에 강인)
- **MaxAbsScaler**: 절대값 최대값으로 나누어 -1과 1 사이로 변환 (희소 행렬에 적합)

각 스케일러는 데이터의 특성에 따라 선택해야 하므로, 이상치의 유무나 데이터의 분포를 고려하여 적절한 스케일러를 선택하는 것이 중요합니다.


---
```
import pandas as pd
import numpy as np

df_wine = pd.read_csv('https://archive.ics.uci.edu/'
                      'ml/machine-learning-databases/wine/wine.data',
                      header=None)

df_wine.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash',
                   'Alcalinity of ash', 'Magnesium', 'Total phenols',
                   'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins',
                   'Color intensity', 'Hue', 'OD280/OD315 of diluted wines',
                   'Proline']

from sklearn.model_selection import train_test_split

# 데이터(X)와 답(y) 분리
X=df_wine.iloc[:,1:].values # 모든행, 1열부터 끝까지
y=df_wine.iloc[:,0].values # 레이블(답), 0열부터 끝까지

# 학습/테스트 데이터(답) 분리
X_train, X_test, y_train, y_test= train_test_split(
    X
    ,y
    ,test_size=0.3
    ,random_state=1
    ,stratify=y 
)

from sklearn.preprocessing import MinMaxScaler

mms=MinMaxScaler()

mms.fit(X_train) # 13개의 feature의 min/max 구해진다

X_train_norm=mms.transform(X_train)
X_test_norm=mms.transform(X_test)

X_train_norm.shape
```
![image](https://github.com/user-attachments/assets/fadc9b06-743e-41fe-9954-466428916af1)

```
import pandas as pd
import numpy as np

df_wine = pd.read_csv('https://archive.ics.uci.edu/'
                      'ml/machine-learning-databases/wine/wine.data',
                      header=None)

df_wine.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash',
                   'Alcalinity of ash', 'Magnesium', 'Total phenols',
                   'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins',
                   'Color intensity', 'Hue', 'OD280/OD315 of diluted wines',
                   'Proline']

from sklearn.model_selection import train_test_split

# 데이터(X)와 답(y) 분리
X=df_wine.iloc[:,1:].values # 모든행, 1열부터 끝까지
y=df_wine.iloc[:,0].values # 레이블(답), 0열부터 끝까지

# 학습/테스트 데이터(답) 분리
X_train, X_test, y_train, y_test= train_test_split(
    X
    ,y
    ,test_size=0.3
    ,random_state=1
    ,stratify=y 
)

from sklearn.preprocessing import MinMaxScaler

mms=MinMaxScaler()

mms.fit(X_train) # 13개의 feature의 min/max 구해진다

X_train_norm=mms.transform(X_train)
X_test_norm=mms.transform(X_test)

print(f'X_train_norm.max() : {X_train_norm.max()}')
print(f'X_train_norm.min() : {X_train_norm.min()}')
```
![image](https://github.com/user-attachments/assets/c339efaf-ab46-44ab-84cf-49baa4e43af9)

```
import numpy as np
ex = np.array(
    [0,1,2,3,4,5]
)
print(f'표준화 : {(ex-ex.mean())/ex.std()}')
print(f'정규화 : {(ex-ex.min())/ex.max()-ex.min()}')
```
![image](https://github.com/user-attachments/assets/64b6cb25-a1af-4eaf-a9d7-81426e0a703e)

---
## 유용한 특성 선택
```
import pandas as pd
import numpy as np

df_wine = pd.read_csv('https://archive.ics.uci.edu/'
                      'ml/machine-learning-databases/wine/wine.data',
                      header=None)

df_wine.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash',
                   'Alcalinity of ash', 'Magnesium', 'Total phenols',
                   'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins',
                   'Color intensity', 'Hue', 'OD280/OD315 of diluted wines',
                   'Proline']

from sklearn.model_selection import train_test_split

# 데이터(X)와 답(y) 분리
X=df_wine.iloc[:,1:].values # 모든행, 1열부터 끝까지
y=df_wine.iloc[:,0].values # 레이블(답), 0열부터 끝까지

# 학습/테스트 데이터(답) 분리
X_train, X_test, y_train, y_test= train_test_split(
    X
    ,y
    ,test_size=0.3
    ,random_state=1
    ,stratify=y 
)

from sklearn.preprocessing import MinMaxScaler

mms=MinMaxScaler()

mms.fit(X_train) # 13개의 feature의 min/max 구해진다

X_train_norm=mms.transform(X_train)
X_test_norm=mms.transform(X_test)


from sklearn.preprocessing import StandardScaler
ss = StandardScaler()
ss.fit(X_train)
X_train_std=ss.transform(X_train)
X_test_std=ss.transform(X_test)


# 3중 분류
from sklearn.linear_model import LogisticRegression
lr = LogisticRegression(
    penalty='l1'
    ,C=1.0
    ,solver='liblinear'
    ,multi_class='ovr'
)

lr1= LogisticRegression(
    penalty='l1'
    ,C=1.0
    ,solver='liblinear'
    ,multi_class='ovr'
)

lr.fit(X_train_norm,y_train)
print(f'훈련 정확도 : {lr.score(X_train_norm,y_train)}')
print(f'테스트 정확도 : {lr.score(X_test_norm,y_test)}')
print('='*50)
lr1.fit(X_train_std,y_train)
print(f'훈련 정확도 : {lr1.score(X_train_std,y_train)}')
print(f'테스트 정확도 : {lr1.score(X_test_std,y_test)}')

```
![image](https://github.com/user-attachments/assets/4d7f2cd8-e2a5-4e3f-8bcc-fabc5b356667)

---
## 순차 특성 선택 알고리즘

```
# 피쳐 선택

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.neighbors import KNeighborsClassifier
from sklearn.feature_selection import SequentialFeatureSelector
df_wine = pd.read_csv('https://archive.ics.uci.edu/'
                      'ml/machine-learning-databases/wine/wine.data',
                      header=None)

df_wine.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash',
                   'Alcalinity of ash', 'Magnesium', 'Total phenols',
                   'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins',
                   'Color intensity', 'Hue', 'OD280/OD315 of diluted wines',
                   'Proline']

from sklearn.model_selection import train_test_split

# 데이터(X)와 답(y) 분리
X=df_wine.iloc[:,1:].values # 모든행, 1열부터 끝까지
y=df_wine.iloc[:,0].values # 레이블(답), 0열부터 끝까지

# 학습/테스트 데이터(답) 분리
X_train, X_test, y_train, y_test= train_test_split(
    X
    ,y
    ,test_size=0.3
    ,random_state=1
    ,stratify=y 
)

from sklearn.preprocessing import StandardScaler
ss = StandardScaler()
ss.fit(X_train)
X_train_std=ss.transform(X_train)
X_test_std=ss.transform(X_test)

knn = KNeighborsClassifier(
    n_neighbors=5
)

scores=[] # 정확도를 저장하는 리스트
f_masks=[]
for n_feature in range(1,13):
    # print('='*100)
    # print(n_feature)
    sfs= SequentialFeatureSelector(
        knn
        , n_features_to_select=n_feature
    )
    sfs.fit(X_train_std,y_train)
    f_mask= sfs.support_ # 선택된 feature boolean 타입으로 저장
    f_masks.append(f_mask)
    knn.fit(
        X_train_std[:,f_mask]
        ,y_train
    )
    scores.append(knn.score(X_train_std[:,f_mask],y_train))
    

print(scores)
print(f_masks)
```
![image](https://github.com/user-attachments/assets/4e34facd-c87a-46e2-a92f-6d01d6331838)

```
# 피쳐 선택

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.neighbors import KNeighborsClassifier
from sklearn.feature_selection import SequentialFeatureSelector
df_wine = pd.read_csv('https://archive.ics.uci.edu/'
                      'ml/machine-learning-databases/wine/wine.data',
                      header=None)

df_wine.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash',
                   'Alcalinity of ash', 'Magnesium', 'Total phenols',
                   'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins',
                   'Color intensity', 'Hue', 'OD280/OD315 of diluted wines',
                   'Proline']

from sklearn.model_selection import train_test_split

# 데이터(X)와 답(y) 분리
X=df_wine.iloc[:,1:].values # 모든행, 1열부터 끝까지
y=df_wine.iloc[:,0].values # 레이블(답), 0열부터 끝까지

# 학습/테스트 데이터(답) 분리
X_train, X_test, y_train, y_test= train_test_split(
    X
    ,y
    ,test_size=0.3
    ,random_state=1
    ,stratify=y 
)

from sklearn.preprocessing import StandardScaler
ss = StandardScaler()
ss.fit(X_train)
X_train_std=ss.transform(X_train)
X_test_std=ss.transform(X_test)

knn = KNeighborsClassifier(
    n_neighbors=5
)

scores=[] # 정확도를 저장하는 리스트
f_masks=[]
for n_feature in range(1,13):
    # print('='*100)
    # print(n_feature)
    sfs= SequentialFeatureSelector(
        knn
        , n_features_to_select=n_feature
    )
    sfs.fit(X_train_std,y_train)
    f_mask= sfs.support_ # 선택된 feature boolean 타입으로 저장
    f_masks.append(f_mask)
    knn.fit(
        X_train_std[:,f_mask]
        ,y_train
    )
    scores.append(knn.score(X_train_std[:,f_mask],y_train))
    

plt.plot(range(1,13),scores,marker='o')
plt.grid()
plt.show()
```
![image](https://github.com/user-attachments/assets/ac994719-f3dc-4347-8d9d-71025da6023a)

```
# 피쳐 선택

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.neighbors import KNeighborsClassifier
from sklearn.feature_selection import SequentialFeatureSelector
df_wine = pd.read_csv('https://archive.ics.uci.edu/'
                      'ml/machine-learning-databases/wine/wine.data',
                      header=None)

df_wine.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash',
                   'Alcalinity of ash', 'Magnesium', 'Total phenols',
                   'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins',
                   'Color intensity', 'Hue', 'OD280/OD315 of diluted wines',
                   'Proline']

from sklearn.model_selection import train_test_split

# 데이터(X)와 답(y) 분리
X=df_wine.iloc[:,1:].values # 모든행, 1열부터 끝까지
y=df_wine.iloc[:,0].values # 레이블(답), 0열부터 끝까지

# 학습/테스트 데이터(답) 분리
X_train, X_test, y_train, y_test= train_test_split(
    X
    ,y
    ,test_size=0.3
    ,random_state=1
    ,stratify=y 
)

from sklearn.preprocessing import StandardScaler
ss = StandardScaler()
ss.fit(X_train)
X_train_std=ss.transform(X_train)
X_test_std=ss.transform(X_test)

from sklearn.ensemble import RandomForestClassifier

feat_labels=df_wine.columns[1:]
print(f'feat_labels : {feat_labels}')

forest = RandomForestClassifier(
    n_estimators=500
    , random_state=1    
)
forest.fit(X_train,y_train) # 학습 -> 모델 완성
print(f'forest.fit(X_train,y_train) : {forest.fit(X_train,y_train)}')
```
![image](https://github.com/user-attachments/assets/fe9309f9-9cdc-4994-8c3a-82aee1be303a)

```
# 피쳐 선택

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.neighbors import KNeighborsClassifier
from sklearn.feature_selection import SequentialFeatureSelector
df_wine = pd.read_csv('https://archive.ics.uci.edu/'
                      'ml/machine-learning-databases/wine/wine.data',
                      header=None)

df_wine.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash',
                   'Alcalinity of ash', 'Magnesium', 'Total phenols',
                   'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins',
                   'Color intensity', 'Hue', 'OD280/OD315 of diluted wines',
                   'Proline']

from sklearn.model_selection import train_test_split

# 데이터(X)와 답(y) 분리
X=df_wine.iloc[:,1:].values # 모든행, 1열부터 끝까지
y=df_wine.iloc[:,0].values # 레이블(답), 0열부터 끝까지

# 학습/테스트 데이터(답) 분리
X_train, X_test, y_train, y_test= train_test_split(
    X
    ,y
    ,test_size=0.3
    ,random_state=1
    ,stratify=y 
)

from sklearn.preprocessing import StandardScaler
ss = StandardScaler()
ss.fit(X_train)
X_train_std=ss.transform(X_train)
X_test_std=ss.transform(X_test)

from sklearn.ensemble import RandomForestClassifier
print('='*50)
feat_labels=df_wine.columns[1:]
print(f'feat_labels : {feat_labels}')
print('='*50)
forest = RandomForestClassifier(
    n_estimators=500
    , random_state=1    
)
forest.fit(X_train,y_train) # 학습 -> 모델 완성
print(f'forest.fit(X_train,y_train) : {forest.fit(X_train,y_train)}')
print('='*50)
importances=forest.feature_importances_
print(f'피처 중요도 추출 \n{importances}')
print('='*50)
importances_sort_order_asc = np.argsort(importances)
print(f'중요도 오름차순 정렬 : {importances_sort_order_asc}')
print('='*50)
indices=np.argsort(importances)[::-1]
print(f'중요도 내림차순 정렬 : {indices}')
print('='*50)

print(f'반복을 위해서 확인 : {X_train.shape[1]}')
print('='*50)
# # 피쳐 중요도 출력
print('피쳐 중요도 출력')
for f in range(X_train.shape[1]):
    print(f'{f+1:2})\t{feat_labels[indices[f]]:<30}\t{importances[indices[f]]}')

    # print(
    #     "%2d) %-*s %f"
    #     % (f+1,30,feat_labels[indices[f]],importances[indices[f]])
    # )
plt.title('Feature Importance')
plt.bar(
    range(X_train.shape[1])
    , importances[indices]
    ,align='center'
)
plt.show()
```
![image](https://github.com/user-attachments/assets/de7152ef-dc67-4003-8e6c-de9dd4b94ad3)

![image](https://github.com/user-attachments/assets/4cf55870-ce97-440b-9751-441a7c50ab10)

```
# 피쳐 선택

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.neighbors import KNeighborsClassifier
from sklearn.feature_selection import SequentialFeatureSelector
df_wine = pd.read_csv('https://archive.ics.uci.edu/'
                      'ml/machine-learning-databases/wine/wine.data',
                      header=None)

df_wine.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash',
                   'Alcalinity of ash', 'Magnesium', 'Total phenols',
                   'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins',
                   'Color intensity', 'Hue', 'OD280/OD315 of diluted wines',
                   'Proline']

from sklearn.model_selection import train_test_split

# 데이터(X)와 답(y) 분리
X=df_wine.iloc[:,1:].values # 모든행, 1열부터 끝까지
y=df_wine.iloc[:,0].values # 레이블(답), 0열부터 끝까지

# 학습/테스트 데이터(답) 분리
X_train, X_test, y_train, y_test= train_test_split(
    X
    ,y
    ,test_size=0.3
    ,random_state=1
    ,stratify=y 
)

from sklearn.preprocessing import StandardScaler
ss = StandardScaler()
ss.fit(X_train)
X_train_std=ss.transform(X_train)
X_test_std=ss.transform(X_test)

from sklearn.ensemble import RandomForestClassifier
feat_labels=df_wine.columns[1:]
forest = RandomForestClassifier(
    n_estimators=500
    , random_state=1    
)
forest.fit(X_train,y_train) # 학습 -> 모델 완성
importances=forest.feature_importances_
importances_sort_order_asc = np.argsort(importances)
indices=np.argsort(importances)[::-1]
    
plt.title('Feature Importance')
plt.bar(
    range(X_train.shape[1])
    , importances[indices]
    ,align='center'
)
plt.xticks(
    range(X_train.shape[1])
    ,feat_labels[indices]
    ,rotation=90
)
plt.show()
```
![image](https://github.com/user-attachments/assets/7575faab-2e4c-4f43-9070-1370f924d3ba)

```
# 피쳐 선택

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.neighbors import KNeighborsClassifier
from sklearn.feature_selection import SequentialFeatureSelector
df_wine = pd.read_csv('https://archive.ics.uci.edu/'
                      'ml/machine-learning-databases/wine/wine.data',
                      header=None)

df_wine.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash',
                   'Alcalinity of ash', 'Magnesium', 'Total phenols',
                   'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins',
                   'Color intensity', 'Hue', 'OD280/OD315 of diluted wines',
                   'Proline']

from sklearn.model_selection import train_test_split

# 데이터(X)와 답(y) 분리
X=df_wine.iloc[:,1:].values # 모든행, 1열부터 끝까지
y=df_wine.iloc[:,0].values # 레이블(답), 0열부터 끝까지

# 학습/테스트 데이터(답) 분리
X_train, X_test, y_train, y_test= train_test_split(
    X
    ,y
    ,test_size=0.3
    ,random_state=1
    ,stratify=y 
)

from sklearn.preprocessing import StandardScaler
ss = StandardScaler()
ss.fit(X_train)
X_train_std=ss.transform(X_train)
X_test_std=ss.transform(X_test)


from sklearn.ensemble import RandomForestClassifier
print('='*50)
feat_labels=df_wine.columns[1:]
print(f'feat_labels : {feat_labels}')
print('='*50)
forest = RandomForestClassifier(
    n_estimators=500
    , random_state=1    
)
forest.fit(X_train,y_train) # 학습 -> 모델 완성
importances=forest.feature_importances_
importances_sort_order_asc = np.argsort(importances)
indices=np.argsort(importances)[::-1]
    
print('='*100) 
from sklearn.feature_selection import SelectFromModel
sfm=SelectFromModel(
    forest
    ,threshold=-.1
    ,prefit=True
)

X_selected=sfm.transform(X_train)
print(f'13개 피쳐에서 중요도가 0.1큰 피쳐를 선택 \n {X_selected}')
print(f'13개 피쳐에서 중요도가 0.1큰 피쳐의 shape : {X_selected.shape}')

print('='*50)
print(f'이 임계 조건을 만족하는 feature 수 : {X_selected.shape[1]}')
for f in range(X_selected.shape[1]):
    print(f'{f+1:2})\t{feat_labels[indices[f]]:<30}\t{importances[indices[f]]}')

```
![image](https://github.com/user-attachments/assets/a04606e7-4079-4d5b-9b85-f229a389a7d8)
