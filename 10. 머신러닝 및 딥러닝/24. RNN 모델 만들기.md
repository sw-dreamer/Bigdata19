
```
import torch
import torch.nn as nn

from torch.utils.data.dataset import random_split
import re
import numpy as np
import pandas as pd
from collections import Counter, OrderedDict
```
```
# RNN 클래스 정의 (nn.Module을 상속받음)
class RNN(nn.Module):
    # 생성자: RNN의 레이어들을 초기화
    def __init__(self, input_size, hidden_size):
        super().__init__()
        
        # RNN 레이어 정의:
        # input_size = 입력 피처의 수
        # hidden_size = 은닉 상태의 크기 (각 RNN 단계의 출력 크기)
        # num_layers = RNN 층의 개수 (여기서는 2개 층을 쌓음)
        # batch_first=True: 입력과 출력의 형태를 (배치 크기, 시퀀스 길이, 입력 크기)로 맞춤
        self.rnn = nn.RNN(
            input_size,  # 입력의 특성 개수
            hidden_size, # 은닉 상태의 크기
            num_layers=2, # RNN 층의 개수
            batch_first=True # 배치 차원이 첫 번째로 올 수 있도록 설정
        )

        # 주석 처리된 코드 부분은 다른 RNN 유형인 GRU나 LSTM을 사용하려는 경우
        # self.rnn = nn.GRU(input_size, hidden_size, num_layers=2, batch_first=True)
        # self.rnn = nn.LSTM(input_size, hidden_size, num_layers=2, batch_first=True)

        # 출력 레이어: RNN의 출력을 받아서 최종적으로 1차원 출력값을 생성 (회귀 문제)
        self.fc = nn.Linear(hidden_size, 1)

    # 순전파 (forward) 함수: 모델에 데이터를 입력하여 출력을 얻는 과정
    def forward(self, x):
        # 입력 데이터를 RNN 레이어에 통과시킴
        # RNN은 두 개의 출력값을 반환: 각 시간 단계의 출력과 마지막 은닉 상태
        _, hidden = self.rnn(x)

        # 마지막 층의 은닉 상태를 가져옴
        # hidden[-1,:,:]은 마지막 층의 은닉 상태를 가져오는데, shape은 (batch_size, hidden_size)
        out = hidden[-1, :, :]

        # 마지막 은닉 상태를 Fully Connected (FC) 레이어를 통해 처리
        out = self.fc(out)

        # 최종 출력 반환 (shape: batch_size, 1)
        return out

# RNN 모델 인스턴스 생성 (입력 크기 64, 은닉 크기 32)
model = RNN(64, 32)

# 모델 아키텍처 출력
print(model)

# 배치 크기 5, 시퀀스 길이 3, 입력 크기 64인 임의의 입력 텐서를 생성
# 모델을 통해 출력을 얻고 출력값을 출력
print(model(torch.randn(5, 3, 64)))
```
![image](https://github.com/user-attachments/assets/45af7c88-943f-48c9-b791-2d15080c76b1)
