
```
import torch
import torch.nn as nn

from torch.utils.data.dataset import random_split
import re
import numpy as np
import pandas as pd
from collections import Counter, OrderedDict
```
```
# RNN 클래스 정의 (nn.Module을 상속받음)
class RNN(nn.Module):
    # 생성자: RNN의 레이어들을 초기화
    def __init__(self, input_size, hidden_size):
        super().__init__()
        
        # RNN 레이어 정의:
        # input_size = 입력 피처의 수
        # hidden_size = 은닉 상태의 크기 (각 RNN 단계의 출력 크기)
        # num_layers = RNN 층의 개수 (여기서는 2개 층을 쌓음)
        # batch_first=True: 입력과 출력의 형태를 (배치 크기, 시퀀스 길이, 입력 크기)로 맞춤
        self.rnn = nn.RNN(
            input_size,  # 입력의 특성 개수
            hidden_size, # 은닉 상태의 크기
            num_layers=2, # RNN 층의 개수
            batch_first=True # 배치 차원이 첫 번째로 올 수 있도록 설정
        )

        # 주석 처리된 코드 부분은 다른 RNN 유형인 GRU나 LSTM을 사용하려는 경우
        # self.rnn = nn.GRU(input_size, hidden_size, num_layers=2, batch_first=True)
        # self.rnn = nn.LSTM(input_size, hidden_size, num_layers=2, batch_first=True)

        # 출력 레이어: RNN의 출력을 받아서 최종적으로 1차원 출력값을 생성 (회귀 문제)
        self.fc = nn.Linear(hidden_size, 1)

    # 순전파 (forward) 함수: 모델에 데이터를 입력하여 출력을 얻는 과정
    def forward(self, x):
        # 입력 데이터를 RNN 레이어에 통과시킴
        # RNN은 두 개의 출력값을 반환: 각 시간 단계의 출력과 마지막 은닉 상태
        _, hidden = self.rnn(x)

        # 마지막 층의 은닉 상태를 가져옴
        # hidden[-1,:,:]은 마지막 층의 은닉 상태를 가져오는데, shape은 (batch_size, hidden_size)
        out = hidden[-1, :, :]

        # 마지막 은닉 상태를 Fully Connected (FC) 레이어를 통해 처리
        out = self.fc(out)

        # 최종 출력 반환 (shape: batch_size, 1)
        return out

# RNN 모델 인스턴스 생성 (입력 크기 64, 은닉 크기 32)
model = RNN(64, 32)

# 모델 아키텍처 출력
print(model)

# 배치 크기 5, 시퀀스 길이 3, 입력 크기 64인 임의의 입력 텐서를 생성
# 모델을 통해 출력을 얻고 출력값을 출력
print(model(torch.randn(5, 3, 64)))
```
![image](https://github.com/user-attachments/assets/45af7c88-943f-48c9-b791-2d15080c76b1)

```
# 감성 분석
class RNN(nn.Module):
    def __init__(self, voab_size,embed_dim,rnn_hidden_size,fc_hidden_size):
        super().__init__()
        self.embedding=nn.Embedding(
            voab_size
            ,embed_dim
            ,padding_idx=0
        )
        self.rnn=nn.LSTM(
            input_size=embed_dim
            ,hidden_size=rnn_hidden_size
            ,batch_first=True
        )
        self.fc1=nn.Linear(rnn_hidden_size,fc_hidden_size)
        self.relu=nn.ReLU()
        self.fc2=nn.Linear(fc_hidden_size,1)
        self.sigmoid=nn.Sigmoid()
    
    def forward(self,text,lengths):
        out=self.embedding(text)
        out=nn.utils.rnn.pack_padded_sequence(
            out
            ,lengths.cpu().numpy()
            ,enforce_sorted=False
            ,batch_first=True
        )
        out,(hidden,cell)=self.rnn(out)
        out=hidden[-1,:,:]
        out=self.fc1(out)
        out=self.relu(out)
        out=self.fc2(out)
        out=self.sigmoid(out)
        return out
```
```
vocab_size=len(vocab)
embed_dim=20
rnn_hidden_size=64
fc_hidden_size=64
torch.manual_seed(1)
model=RNN(voab_size=vocab_size,embed_dim=embed_dim,rnn_hidden_size=rnn_hidden_size,fc_hidden_size=fc_hidden_size)
model
```
![image](https://github.com/user-attachments/assets/ae4942d5-a3c4-4fa4-9f0b-78b8eb0909ff)

```
model=model.to(device=device)
# 학습 + 평가 함수 선언 (1 epoch당 1번 실행 함 )
def train(model, dataloader, optimizer, loss_fn):  # train_dl
    model.train()
    total_acc, total_loss = 0, 0
    for text_batch, label_batch, lengths in dataloader:
        optimizer.zero_grad() # 가중치 초기화 
        # 학습
        pred = model(text_batch, lengths)[:,0]  # 배치사이즈만큼 다 가져오고, 나머지 값들 다 가져오기 
        loss = loss_fn(pred, label_batch) # BCELoss
        # 업데이트
        loss.backward()
        optimizer.step()
        total_acc += ((pred > 0.5).float() == label_batch).float().sum().item()
        total_loss += loss.item() * label_batch.size(0) # 텐서에서 꺼낼 때 item()으로 꺼내야 함
        #  label_batch.size(0) => shape의 0번째 값을 가져오는것 
    avg_acc = total_acc/len(dataloader.dataset)
    avg_loss = total_loss/len(dataloader.dataset)
    return avg_acc, avg_loss
```
```
# 검증 함수
def evaluate(model, dataloader, loss_fn): # valid_dl
    model.eval()
    total_acc, total_loss = 0, 0
    with torch.no_grad(): # 미분하지 말아라(학습하지 말아라)
        for text_batch, label_batch, lengths in dataloader:
            pred = model(text_batch, lengths)[ : , 0] # batch_size = 5 -> [5,1]
            loss = loss_fn(pred,label_batch)
            
            total_acc += ((pred > 0.5).float() == label_batch).float().sum().item()
            total_loss += loss.item() * label_batch.size(0)
    return total_acc/len(list(dataloader.dataset)), total_loss/len(list(dataloader.dataset))
```
```
bacth_size=32
train_dl=DataLoader(
    train_dataset
    ,batch_size=bacth_size
    ,shuffle=True
    ,collate_fn=collate_batch
)

valid_dl=DataLoader(
    valid_dataset
    ,batch_size=bacth_size
    ,shuffle=False
    ,collate_fn=collate_batch
)

test_dl=DataLoader(
    test_dataset
    ,batch_size=bacth_size
    ,shuffle=False
    ,collate_fn=collate_batch
)
```
```
loss_fn = nn.BCELoss()
optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)

num_epochs = 10

torch.manual_seed(1)

for epoch in range(num_epochs):
    acc_train, loss_train = train(model, train_dl, optimizer, loss_fn)
    acc_valid, loss_valid = evaluate(model, valid_dl, loss_fn)
    print(f'epoch : {epoch}, 학습정확도 : {acc_train:.4f}. 검증정확도 : {acc_valid:.4f}')
```
![image](https://github.com/user-attachments/assets/c9e8611f-3f02-497b-8da3-74e71b6dd93d)
```
acc_test, _ = evaluate(model, test_dl, loss_fn)
acc_test
```
![image](https://github.com/user-attachments/assets/bf6abd61-ef97-43d2-b275-feccc3fc4f6e)
