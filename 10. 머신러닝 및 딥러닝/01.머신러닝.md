# 머신러닝
---
## 환경설정

### 기싱환경
```
conda create -n pyml python=3.10
```
![image](https://github.com/user-attachments/assets/ce192c6e-cc99-49da-9f83-2b12191633c3)

![image](https://github.com/user-attachments/assets/19e305ab-8dbb-44cb-80f5-9f548f5c97b7)

```
conda activate pyml
```
![image](https://github.com/user-attachments/assets/5f9bb3db-97f7-49cd-acc2-7cfdb1697474)

```
conda install numpy==1.21.2 scipy==1.7.0 scikit-learn==1.2 matplotlib==3.4.3  pandas==1.3.2 ipykernel
```
![image](https://github.com/user-attachments/assets/97b82766-7bce-4f7c-9f21-746a2ce5d83f)

```
conda install numpy==1.21.2 scipy scikit-learn==1.2 matplotlib==3.4.3  pandas ipykernel
```
![image](https://github.com/user-attachments/assets/f68a8e55-83ca-4b9f-8867-233291d73b7c)

```
conda install numpy==1.21.2 scipy scikit-learn==1.2 matplotlib  pandas ipykernel
```
![image](https://github.com/user-attachments/assets/196d51fd-8b61-47f3-a14c-460ce582e681)

```
conda install numpy==1.23.3 scipy scikit-learn==1.2 matplotlib  pandas ipykernel
```
![image](https://github.com/user-attachments/assets/617195b8-a561-4108-bbf4-2ddeb16f5ef1)

```
pip freeze > requirements.txt
```
![image](https://github.com/user-attachments/assets/1677c9b1-0562-4e2b-b867-9523cc200f8c)

만약에 requirements.txt를 설치하고 싶으면 아래 명령어를 실행하면 된다.
```
pip install -r requirements.txt
```

---

## numpy

1. 넘파이란
   - 파이썬에서 수치 계산을 효율적으로 할 수 있도록 지원하는 라이브러리
   - 다차원 배열 객체(ndarray)를 제공
   - 벡터, 행렬 연산과 같은 고속으로 수행할 수 있도록 최적화되어 있음
   - 과학 계산, 머신러닝, 딥러닝, 데이터 분석등 다양한 분야에 사용
    
2. 특징
   - 고속 연산 지원 : c언어로 구현되어 내부 연산이 빠르다
   - 다차원 배열 지원
   - 브로드캐스팅 지원 : 서로 다른 크기의 배열 간 연산을 가능
   - 다양한 수학 함수 지원 : 선형대수, 통계, 랜덤 샘플링 등의 다양한 함수 지원
   - python 리스트보다 메모리를 효율적으로 사용

3. numpy 기본 사용법
   - 설치 : pip(conda) install numpy
   - import numpy as np

```
import numpy as np

# 주요 기능 및 예제

# 배열 생성 (np.array())
# 1차원 배열 생성
arr1 = np.array([1,2,3,4,5])

print(f'arr1 : {arr1}')

# 2차원 배열 생성
arr2 = np.array([
    [1,2,3]
    ,[4,5,6]
])
print(f'arr2 : {arr2}')

# 3차원 배열 생성

arr3 =np.array(
    [
        [
            [1,2]
            ,[3,4]
            ,[5,6]
            ,[7,8]
        ]
    ]
)
print(f'arr3 : {arr3}')
```
![image](https://github.com/user-attachments/assets/81aa6f7b-7aed-4f5c-b595-35936db79445)

```
import numpy as np

# 주요 기능 및 예제

# 배열 생성 (np.array())
# 1차원 배열 생성
arr1 = np.array([1,2,3,4,5])

# 2차원 배열 생성
arr2 = np.array([
    [1,2,3]
    ,[4,5,6]
])


# 3차원 배열 생성

arr3 =np.array(
    [
        [
            [1,2]
            ,[3,4]
            ,[5,6]
            ,[7,8]
        ]
    ]
)
print('='*50)

print(f'arr1 배열의 차원 : {arr1.ndim}')
print(f'arr2 배열의 차원 : {arr2.ndim}')
print(f'arr3 배열의 차원 : {arr3.ndim}')
print('='*50)
print(f'arr1 배열의 크기 : {arr1.shape}')
print(f'arr2 배열의 크기 : {arr2.shape}')
print(f'arr3 배열의 크기 : {arr3.shape}')

print('='*50)

print(f'arr1 배열의 원소 개수 : {arr1.size}')
print(f'arr2 배열의 원소 개수 : {arr2.size}')
print(f'arr3 배열의 원소 개수 : {arr3.size}')

print('='*50)

print(f'arr1 배열의 데이터 타입 : {arr1.dtype}')
print(f'arr2 배열의 데이터 타입 : {arr2.dtype}')
print(f'arr3 배열의 데이터 타입 : {arr3.dtype}') # ndarray는 동일타입만 저장 할 수 있다
```
![image](https://github.com/user-attachments/assets/a2760dec-fd88-4c21-8157-1a0e43b706e2)

```
import numpy as np
# 특정값을 갖는 배열 생성
# 0값을 갖는 배열 : np.zeros((행,열))
zeroes =np.zeros(
    (3,3)
)
zeroes
```
![image](https://github.com/user-attachments/assets/2613dd34-7f86-4229-a437-574730734353)

```
import numpy as np
# 특정값을 갖는 배열 생성
# 0값을 갖는 배열 : np.zeros((행,열))
zeroes_ =np.zeros(
    (3,3)
)
print(zeroes_)
print('='*50)
# 1값을 갖는 배열 : np.ones((행,열))
ones_ =np.ones(
    (3,3)
)
print(ones_)
print('='*50)
# 특정값으로 채운 배열 : np.full((행,열),특정값)

full=np.full((2,2),5)
print(full)

print('='*50)
# 연속된 숫자로 채운 배열 생성 : np.arange(start,end, stop)
arr4=np.arange(1,10,2)
print(arr4)
print('='*50)

# np.linspace(start,end,count)
lin_arr=np.linspace(0,100,5) # 0~100 사이에서 5개 값으로 생성
print(lin_arr)
```
![image](https://github.com/user-attachments/assets/dc3c08d7-c94a-49f2-a615-04af3b193942)

```
import numpy as np

# 난수 배열 생성

# numpy의 random모듈을 사용하면 다양한 난수 배열을 생성 할 수 있다.

# np.random.rand(행,열) : 0~1사이의 난수를 가진 배열 생성
np.random.seed(42) # seed에 의해 rand 값이 바뀔수도 있고 안 바뀔수도 있다

rand_arr = np.random.rand(2,3)
print(rand_arr)
```
![image](https://github.com/user-attachments/assets/947023bd-f155-4a66-94c2-24b4c223db30)

```
import numpy as np

# 난수 배열 생성

# numpy의 random모듈을 사용하면 다양한 난수 배열을 생성 할 수 있다.

# np.random.rand(행,열) : 0~1사이의 난수를 가진 배열 생성
np.random.seed(42) # seed에 의해 rand 값이 바뀔수도 있고 안 바뀔수도 있다

rand_arr = np.random.rand(2,3)
print(rand_arr)

# 정규분포를 따르는 난수 배열 생성 : np.random.randn(행,열)
randn_arr=np.random.randn(2,2)
print(randn_arr)

# 특정 범위의 정수 난수 배열 생성 : np.random.randint(start,ent,[(행,열)])
randint_arr= np.random.randint(1,10,[3,4])
print(randint_arr)
```
![image](https://github.com/user-attachments/assets/4023bd24-a25d-43ba-aec1-448a0525d54c)

```
import numpy as np

# 배열 연산(더하기, 빼기, 곱셈, 나눗셈, 제곱)
a_vector =np.array([1,2,3])
b_vector =np.array([4,5,6])
print(f'덧셈 : {a_vector+b_vector}')
print(f'뺄셈 : {a_vector-b_vector}')
print(f'곱셈 : {a_vector*b_vector}')
print(f'제곱 : {a_vector**b_vector}')
```
![image](https://github.com/user-attachments/assets/f22eca3b-8e9f-434e-a1e9-ad9237c49ba3)

```
import numpy as np

# 배열 인덱싱 및 슬라이싱
arr=np.array([
    [1,2,3]
    ,[4,5,6]
    ,[7,8,9]
])

print(f'arr : {arr}')
print(f'첫번째 행, 두번째 열 : {arr[0][1]}') # arr[행][열]
print(f'첫번째 행, 두번째 열 : {arr[0,1]}') # arr[행,열]

# 특정 행 슬라이싱
print(f'첫번째 행 : {arr[0]}')
print(f'첫번째 행 : {arr[0,]}')
print(f'첫번째 행 : {arr[0,:]}')

# 특정 열 슬라이싱
print(f'두번째 열 : {arr[:,1]}')

# 부분 배열 가져오기
print(f'2x2 부분 배열 : {arr[1:3,1:3]}')
```
![image](https://github.com/user-attachments/assets/2c63edbe-ad55-418a-9c69-91304e275994)

```
import numpy as np

# 선형대수 연산
# 선형 변환(Linear Transformation)은 벡터나 행렬을 다른 벡터나 행렬로 변환하는 수학적 과정
# 특정 방향으로 확장/축소하는 역할
A = np.array([
    [1,2]
    ,[3,4]
])
B = np.array([
    [1,1]
    ,[2,2]
])

# 점 곱 
# 주어진 데이터(A)를 가중치(B)로 선형 변환한 것이다.
dot_prod=np.dot(A,B)
print(f'행 열 곱 : {dot_prod}')
```
![image](https://github.com/user-attachments/assets/58069830-1c49-4a17-a646-3ff64da28bfe)

```
import numpy as np

# 전체 행렬 : transpose
A = np.array([
    [1,2]
    ,[3,4]
])
B = np.array([
    [1,1]
    ,[2,2]
])

A,A.T,A.transpose()
```
![image](https://github.com/user-attachments/assets/5281654f-1165-429e-804f-e53f6174cdb5)

```
import numpy as np

a=np.array([1,2,3])
print(f'a shape : {a.shape}')
b=np.array([1,2,3])
print(f'b shape : {b.shape}')
print(np.dot(a,b))
print(np.dot(a.T,b))
```
![image](https://github.com/user-attachments/assets/b1029566-640d-4b8c-ba45-49df18d822a2)

```
import numpy as np

a = np.array([[1, 2, 3], [4, 5, 6]])
print(f'a shape : {a.shape}')

b = np.array([1, 2, 3])
print(f'b shape : {b.shape}')

print(np.dot(a, b))

try:
    print(np.dot(a.T, b))
except Exception as e:
    print('오류: ' + str(e))
```
![image](https://github.com/user-attachments/assets/b25cf09e-fbf0-493f-9f67-4fcb8f5795f9)

```
import numpy as np

# 역행렬
# 역행렬 존재하는 조건
# 1. 정사각 행렬(Square Matrix) (nxn)
# 2. 행렬식이 0이 아니어야한다. 행렬식이 0 이면 역행렬이 존재하지 않고 이러한 행렬을 특이행렬(Singular Matrix)라고 한다.
#    AxA-1 = A-1
#    단위 행렬(Identity Matrix), 행렬 곱셈의 항등원 역할



# 행렬식 : 정사각 행렬의 스칼라(단일값) 값을 의미한다.
# 특정 행렬(데이터)이 선형변환에서 공간을 얼마나 변환시키는지를 나타나는 값

# 행렬식이 0이면 해당 행렬이 역행렬을 가질 수 없다(det(A)=0)
# 해당 행렬이 선형 종속(Linear Dependent)이 된다

# 행렬식이 0이 아니면
# 행렬이 가역(invertible)하며, 역행렬이 존재한다.

# 역행렬과 행렬식 계산
# 1. 역행렬 구하기(np.linalg.inv())

# 행렬 A : 2X2 정사각행렬
A = np.array([
    [4,7]
    ,[2,6]
]
)

# 역행렬 구하기
A_inv = np.linalg.inv(A)

print(A_inv)

# 단위 행렬 구하기
I = np.dot(A,A_inv)
print(I)

```
![image](https://github.com/user-attachments/assets/2cea4b7d-3551-4b2a-9523-e76ff2ce5764)

```
import numpy as np
# 역행렬 존재하지 않는 행렬
B = np.array([
    [4,12]
    ,[2,6]
]
)


B_inv = np.linalg.inv(B)

print(B_inv)

B_I = np.dot(B,B_inv)
print(B_I)
```
![image](https://github.com/user-attachments/assets/daa078ff-70a9-4178-a56c-558d5be38794)

```
import numpy as np

# 행렬식 구하기 (np.linalg.det())

# 3x3 정사각 행렬
B = np.array(
    [
        [1,2,3]
        ,[4,5,6]
        ,[7,8,9]
    ]
)
print(B)

# 역행렬 구하기
B_inv = np.linalg.inv(B)
print(B_inv)

# 행렬식 구하기
B_det = np.linalg.det(B) # 행렬식 결과값은 스칼라값(단일값)이 나온다

print(B_det)
```
![image](https://github.com/user-attachments/assets/9c66ba69-f873-4048-bae0-da03d31aa9bd)

```
import numpy as np

# 행렬식 구하기 (np.linalg.det())

# 3x3 정사각 행렬
B = np.array(
    [
        [1,2,3]
        ,[4,5,6]
        ,[7,8,10]
    ]
)
print(B)

# 역행렬 구하기
B_inv = np.linalg.inv(B)
print(B_inv)

# 행렬식 구하기
B_det = np.linalg.det(B)

print(B_det)
```
![image](https://github.com/user-attachments/assets/59187ccc-4df2-480b-b304-c67ef2bfc3c4)

```
import numpy as np

# 특이 행렬(Singular Matrix)

C = np.array(
    [
        [1,2]
        ,[2,4]
    ]
)

print(C)

# 행렬식 확인
print(f'행렬식 값 : {np.linalg.det(C)}')
```
![image](https://github.com/user-attachments/assets/a2d686a9-43ea-4de7-8ece-7dcf989ac1dc)

```
import numpy as np

# 특이 행렬(Singular Matrix)

C = np.array(
    [
        [1,2]
        ,[2,4]
    ]
)

print(C)

# 행렬식 확인
print(f'행렬식 값 : {np.linalg.det(C)}')

try:
    c_inv = np.linalg.inv(C)
    print(f'역행렬 : {c_inv}')
except np.linalg.LinAlgError:
    print('역행렬을 가질 수 없는 특이 행렬이다.')

```
![image](https://github.com/user-attachments/assets/e6b68221-b6fd-4689-b3d7-84eae7dbb137)

---

### 역행렬의 해석과 의미

1. 역행렬의 존재 조건
- 행렬  A가 정사각 행렬이어야 한다.
- 행렬식이 0이 아니어야한다.
- 선행독립(Linear Independent)인 열벡터를 가져야 한다.
    - 즉, 행렬의 열벡터들이 서로 독립적이면 역행렬이 존재하게 된다.

2. 해석
2.1 선형변환 관점
- 행렬 A는 공간을 변형시키는 선형 변환으로 볼 수 있다.
- 역행렬은 이 변환을 되돌리는 역할을 한다.(공간을 복원하는 역할)

2.2 기하학적 관점
- 역행렬이 존재 : 변환이 가역적이라면, 좌표 공간을 다시 원래대로 복원
- 역행렬이 존재 x : 변환이 공간을 축소시키거나, 특정 차원으로 줄여 되둘리 수 없는 변환(Singular Transformation)이 된다.

### Singular Tranformation
- 역행렬이 존재하지 않는 경우는 공간을 완전히 축소시키는 변환을 의미한다.
- 2D 공간에서 1D 공간으로 압축되거나, 3D 공간에서 2D 평면으로 축소되는 등의 변환을 수행

---
## 지도학습에서 분류와 회귀 모델

지도학습은 **입력 데이터(features)**와 **타겟 값(target)**을 이용하여 모델을 학습시키는 방식입니다. 여기서 타겟 값은 우리가 예측하려는 정답을 말합니다.

- **입력 데이터**는 학습 과정에서 모델에게 제공되는 정보입니다.
  - 입력 데이터는 **column**.
- **타겟 값**은 우리가 예측하고자 하는 값으로, 학습 후 모델이 예측을 해야 할 정답입니다.
  - 타겟은 정답(ground truth).
  - 타겟 = 레이블, 두 용어 모두 예측해야 할 목표 값 또는 정답을 지칭.

### 1. 분류 모델 (Classification Model)

- **타겟 값**: 분류 문제에서 타겟 값은 범주형 데이터입니다. 예를 들어, "생존 여부"나 "승패"와 같은 값을 예측합니다.

#### 예시:
- **입력 데이터(features)**: 나이, 성별, 혈압 등
- **타겟 값(target)**: "die" 또는 "live" (이진 분류의 경우), 또는 "class A", "class B", "class C" (다중 클래스 분류의 경우)

모델은 입력 데이터를 학습하여, 주어진 입력에 대해 정확한 범주를 예측하려고 합니다. 예측 값은 범주(예: "live", "die", "win", "lose")로 나오게 됩니다.

### 2. 회귀 모델 (Regression Model)

- **타겟 값**: 회귀 문제에서 타겟 값은 연속적인 수치입니다. 예를 들어, 주식 가격, 온도, 부동산 가격 등을 예측합니다.

#### 예시:
- **입력 데이터**: 지역, 면적, 방 개수 등
- **타겟 값**: "가격" (연속적인 수치 값, 예: 3,000,000원)

모델은 입력 데이터를 기반으로, 주어진 입력에 대해 연속적인 숫자를 예측하려고 합니다. 예측 값은 실수 값(예: 150.5, 3,200,000) 형태로 나오게 됩니다.

### 지도학습의 학습 과정

1. **모델 학습**: 모델은 입력 데이터와 타겟 값을 사용하여 패턴을 학습합니다.
2. **예측**: 학습이 완료된 후, 모델은 새로운 입력 데이터에 대해 예측을 하게 됩니다.
3. **오차 계산**: 예측값과 실제 타겟 값(정답) 사이의 차이를 계산하여 오차를 평가합니다.
4. **모델 개선**: 모델은 이 오차를 최소화하도록 학습을 계속하며, 더 정확한 예측을 하도록 개선됩니다.

**예측 값(predicted value)**은 **타겟(target)**이나 **레이블(label)**과는 다릅니다. 예측 값은 모델이 실제로 예측한 값을 의미하고, 타겟 또는 레이블은 실제 정답을 의미합니다.

- **타겟 (target) 또는 레이블 (label)**: 실제로 우리가 모델에게 예측하도록 제공한 값입니다. 즉, 훈련 데이터에서 정답입니다.
- **예측 값 (predicted value)**: 모델이 예측한 값입니다. 훈련 후, 모델이 주어진 입력에 대해 예측하는 값이죠.

#### 예시:
**분류 모델**에서는:
- **타겟 (레이블)**은 실제 클래스 (예: 0 또는 1, 혹은 'cat', 'dog')
- **예측 값**은 모델이 예측한 클래스 (예: 모델이 0 또는 1로 예측한 값)

**회귀 모델**에서는:
- **타겟 (레이블)**은 실제 값 (예: 집의 가격)
- **예측 값**은 모델이 예측한 값 (예: 모델이 예측한 가격)

### 요약:
- **분류 모델**은 범주형 타겟 값을 예측합니다. (예: "승" vs "패")
- **회귀 모델**은 연속적인 수치 값을 예측합니다. (예: 주식 가격, 온도)
- 두 모델 모두 타겟 값(원래 답)을 통해 예측을 학습하며, 이를 바탕으로 새로운 입력 데이터에 대해 예측값을 제공합니다.

### Features와 타겟 값의 관계

- **Features**는 모델이 학습할 때 사용하는 입력 데이터입니다. 쉽게 말해, 예측을 위한 정보입니다. 모델이 예측을 하기 위해 사용하는 특성(혹은 변수)들을 의미합니다. 각 feature는 데이터를 설명하는 속성이나 특징이라고 할 수 있습니다.

#### Features의 예시:
1. **생존 여부 예측 (분류 문제)**
   - **Features**: 나이, 성별, 탑승 클래스, 승선 항구, 가족 수 등
   - **타겟 값**: 생존 여부 (예: "live" 또는 "die")

2. **주택 가격 예측 (회귀 문제)**
   - **Features**: 집의 크기, 방 개수, 위치, 연도 등
   - **타겟 값**: 주택 가격 (예: 3,000,000원)

3. **스팸 이메일 분류 (분류 문제)**
   - **Features**: 이메일의 제목, 내용에 포함된 특정 단어들, 발신자 정보 등
   - **타겟 값**: 이메일이 스팸인지 아닌지 (예: "spam" 또는 "not spam")

#### Features의 특성:
- **숫자형 Feature**: 나이, 크기, 가격 등과 같이 연속적인 값을 가지는 특성입니다.
  - 예: 나이 (25세), 집 크기 (100제곱미터)
- **범주형 Feature**: 성별, 국가, 색상 등과 같이 특정 범주에 속하는 값을 가지는 특성입니다.
  - 범주는 최소 2개 이상의 카테고리
  - 예: 성별 (남성/여성), 국가 (한국/미국/일본)
- **이진형 Feature**: 두 가지 선택지(예/아니오, 0/1 등)만 존재하는 특성입니다.
  - 2개의 선택지만 있음
  - 범주형이 정확히 2개의 카테고리만을 가지고 있다면 이진형 features라고도 함
  - 예: 결혼 여부 (결혼함/안 함), 신용카드 사용 여부 (사용/안 함)

#### Features와 타겟 값의 관계:
- **Features**는 입력값으로 사용되며, 이 데이터를 통해 모델은 예측을 하게 됩니다.
- **타겟 값**은 모델이 예측하려는 값으로, Features를 기반으로 모델이 예측하려는 결과입니다.

#### 예시 정리:
- **Features**: "부서" (예: 부서 A, 부서 B 등)
- **Target**: "각 부서의 평균 급여" (예: $3000)
- **Predicted Value**: 모델이 예측한 "부서별 급여" (예: 모델이 예측한 부서 A의 급여가 $2800일 수 있음)

### 강화 학습에서 보상 설계 (Reward Design)

강화 학습에서 보상(reward)은 에이전트가 어떤 행동을 했을 때 환경이 에이전트에게 주는 피드백입니다. 보상은 에이전트가 무엇을 학습하고 추구해야 하는지에 대한 중요한 신호를 제공합니다. 이를 통해 에이전트는 좋은 행동과 나쁜 행동을 구분하고, 최적의 행동을 학습하게 됩니다.

#### 보상 설계의 중요성
- 보상 신호는 에이전트가 학습을 통해 추구해야 할 목표를 정의합니다. 즉, 에이전트가 무엇을 해야 잘한다고 판단할지를 결정하는 중요한 역할을 합니다.
- 예를 들어, 게임에서 목표가 "적을 처치하는 것"이라면, 에이전트가 적을 처치했을 때 높은 보상을 주고, 그 외에는 적은 보상을 주는 식으로 보상을 설계할 수 있습니다.

#### 보상의 최대화 설정 과정
1. **목표 정의 (Objective)**:
   - 강화 학습을 시키는 사람은 에이전트가 수행해야 할 목표를 설정합니다. 목표는 게임의 승리, 로봇의 경로 탐색, 자율 주행 차량의 안전한 주행 등 다양할 수 있습니다.

2. **보상 설계 (Reward Function)**:
   - 목표를 달성하기 위해 **보상 함수(reward function)**를 설계합니다. 보상 함수는 주어진 상태와 행동에 대해 에이전트가 어떤 보상을 받을지 정의하는 함수입니다.
   - 예를 들어:
     - 보상: 에이전트가 올바른 행동을 취했을 때 주는 양의 보상.
     - 벌점: 에이전트가 잘못된 행동을 했을 때 주는 음의 보상(벌점).

3. **보상 설계 예시**:
   - 게임 환경: 적을 처치할 때 +10 보상, 플레이어가 죽을 때 -10 벌


---
## 퍼셉트론

- 퍼셉트론(Perceptron)은 인공 신경망에서 가장 기본적인 형태로, 이진 분류(binary classification)를 위한 알고리즘입니다. 주로 머신러닝의 기초로 다루어지며, "단층 퍼셉트론"이 가장 간단한 형태입니다.

### 동작 원리
- 퍼셉트론은 주어진 입력 데이터에 대해 출력을 예측하는 과정에서 **가중치(weights)**와 **편향(bias)**을 학습하여 분류 작업을 수행합니다. 퍼셉트론의 핵심은 **선형 결정 경계(linear decision boundary)**를 만들어서 데이터를 분류하는 것입니다.

퍼셉트론의 동작 과정은 다음과 같습니다:

#### 1. 입력 데이터와 가중치 초기화
- 입력값 **X**와 이를 조정하는 **가중치(w)**가 있습니다.
- 가중치는 일반적으로 **난수**로 초기화되며, 훈련을 통해 조정됩니다.
- 편향 **b** 역시 초기화됩니다.

#### 2. 선형 결합 계산
- 각 입력 값에 대해 가중치를 곱하고, 그 값을 모두 더한 후 편향을 더하여 **선형 결합**을 계산합니다.
- 이 계산은 `net_input(X)` 메소드에서 `np.dot(X, w) + b`와 같이 수행됩니다.

#### 3. 활성화 함수 (Activation Function)
- 퍼셉트론은 선형 결합된 값을 활성화 함수에 통과시켜 출력을 만듭니다.
- 퍼셉트론에서 사용하는 활성화 함수는 **단위 계단 함수(unit step function)**입니다.
    - 이 함수는 출력이 0 이상이면 1을 반환하고, 0 미만이면 0을 반환합니다.
    - 이 과정은 `predict(X)` 메소드에서 `np.where(self.net_input(X) >= 0.0, 1, 0)`로 수행됩니다.

#### 4. 오차 계산 및 가중치 업데이트
- 예측값과 실제 목표값(타겟값) 사이의 차이를 **오차(error)**라고 합니다.
- 퍼셉트론은 이 오차를 기반으로 **가중치(weight)**와 **편향(bias)**을 업데이트합니다.
- 업데이트 공식은 `w = w + eta * error * X`로, 여기서 **eta**는 학습률(learning rate)을 의미합니다. 학습률은 가중치 업데이트의 크기를 조절합니다.
- 이 과정을 반복하여 가중치가 점차적으로 학습됩니다.

#### 5. 훈련 반복
- 이 과정은 주어진 **에포크(epoch)** 수만큼 반복됩니다. 각 에포크마다 데이터셋을 한번씩 학습하며, 가중치를 업데이트합니다.

### 퍼셉트론의 특징
- **단층 퍼셉트론**은 선형적으로 구분 가능한 데이터만 처리할 수 있습니다. 즉, 입력 데이터가 선형 결정 경계로 나눠질 수 있을 때만 잘 동작합니다.
- XOR 문제와 같은 비선형적으로 구분되는 문제를 해결할 수 없습니다. 이 문제를 해결하려면 **다층 퍼셉트론(Multi-layer Perceptron, MLP)**을 사용해야 합니다.
- 퍼셉트론은 매우 간단하고, 학습 속도가 빠르며, 구현이 쉽다는 장점이 있습니다. 그러나 실제 복잡한 문제를 해결하기에는 한계가 있습니다.

### 퍼셉트론의 수학적 표현
퍼셉트론의 핵심은 다음의 수식으로 표현됩니다:

$$
y = 
\begin{cases} 
1 & \text{if } w \cdot x + b \geq 0 \\
0 & \text{if } w \cdot x + b < 0 
\end{cases}
$$


여기서:
- \( x \)는 입력 벡터,
- \( w \)는 가중치 벡터,
- \( b \)는 편향,
- \( y \)는 예측된 클래스(0 또는 1)입니다.
  
```
import numpy as np

class Perceptron:
    '''퍼셉트론 분류기
    
    매개변수
    -------------
    eta : float
        학습률(0.0과 1.0 사이)
    n-iter L int
    훈련데이터셋 반복 회수 : 에포크
    
    random_state : int
    난수 생성 시드(난수값을 고정)
    
    속성(클래스 내부에서 사용할 변수)
    w_ : 1d array
        학습할 가중치
    b_ : 스칼라
        학습할 가중치
    
    error : list
    에포크마다 누적된 분류 오류(오차)
    
    '''
    
    def __init__(self,eta=0.01,n_iter=50,random_state=1):
        self.eta=eta
        self.n_iter=n_iter
        self.random_state=random_state
        
    def fit(self,X,y): # 학습 메소드
        '''
        훈련 데이터 학습
        
        매개변수
        --------------------------------------------------
        X : {array-like}, shape=[n_samples,n_features]
        n_samples 개의 샘플데이터, n_feautres개의 특성(컬럼)으로 이뤄진 훈련데이터
        
        y : array-like, shape=[n_samples]
        원래 답(타깃 값)
        
        반환 값(return value)
        ----------------------------
        self : object
        '''
        rgen=np.random.RandomState(self.random_state) # 시드 적용
        self.w_=rgen.normal(
            loc=0.0
            ,scale=0.01
            ,size=X.shape[1] # X 2차원 행렬 -> (행,열)
        )
        
        self.b_= np.float_(0.)
        self_errors_=[] # 빈리스트 : 오차 저장할 리스트
        
        #학습
        for _ in range(self.n_iter): # 순수하게 반복만 진행
            errors = 0 # 1회 학습시 나오는 오차를 저장하는 변수
            for xi, target in zip(X,y):
                update = self.eta *(target-self.predict(xi)) # 오차를 구한다
                self.w_ += update*xi
                self.b_ += update
                errors += int(update!=0.0)
            
            self_errors_.append(errors) # 1에포크 마다  오차를 저장하는 함수
        return self # java의 this와 같음 : 오브젝트를 접근 할 수 있는 변수
    
    def net_input(self,X):
        '''
        입력 계산 : Z
        '''
        return np.dot(X,self.w_) + self.b_
    
    def predict(self, X): # 입력 데이터를 받아서 예측값을 반환 : 0,1
        # np.where(w조건, 참일경우, 거짓일 경우)
        return np.where(self.net_input(X)>=0.0,1,0)
```

```
# 붓꽃 데이터셋에서 퍼셉트론 훈련
import os
import pandas as pd
s= 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'
print('url : ',s)

df = pd.read_csv(
    s
    , header = None
    , encoding='utf-8'
)

df.tail()
```
![image](https://github.com/user-attachments/assets/718b6a86-fac6-449a-b889-a59fc7cce1a4)

```
# 붓꽃 데이터셋에서 퍼셉트론 훈련
import os
import pandas as pd
s= 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'


df = pd.read_csv(
    s
    , header = None
    , encoding='utf-8'
)

df[4].unique()
```
![image](https://github.com/user-attachments/assets/cbfb8f74-cc5c-4780-8ba8-07439622885d)

```
# 붓꽃 데이터셋에서 퍼셉트론 훈련
import os
import pandas as pd
s= 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'


df = pd.read_csv(
    s
    , header = None
    , encoding='utf-8'
)

# 시각화
import matplotlib.pyplot as plt

# setosa랑 versicolor를 선택
df.iloc[0:100,4].values #품정 추출
```
![image](https://github.com/user-attachments/assets/ff98857b-9245-4c28-893d-f77a76ddd811)

```
# 붓꽃 데이터셋에서 퍼셉트론 훈련
import os
import pandas as pd
s= 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'


df = pd.read_csv(
    s
    , header = None
    , encoding='utf-8'
)

# 시각화
import matplotlib.pyplot as plt

# setosa랑 versicolor를 선택
y=df.iloc[0:100,4].values #품정 추출

# setosa이면 0, versicolor이면 1로 출력

np.where(y=='Iris-setosa',0,1)
```
![image](https://github.com/user-attachments/assets/03a1cd79-035a-4d48-aac2-2464dbf0f1c9)

```
# 붓꽃 데이터셋에서 퍼셉트론 훈련
import os
import pandas as pd
s= 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'


df = pd.read_csv(
    s
    , header = None
    , encoding='utf-8'
)

# 시각화
import matplotlib.pyplot as plt

# setosa랑 versicolor를 선택
y=df.iloc[0:100,4].values #품정 추출

# setosa이면 0, versicolor이면 1로 출력

y=np.where(y=='Iris-setosa',0,1)

# 꽃의 길이 df[0],df[2], 꽃의 넓이 : df[1], df[3]
# data 추출 : 꽃받침, 꽃잎의 길이 데이터를 추출
X=df.iloc[0:100,[0,2]].values
X

```
![image](https://github.com/user-attachments/assets/318240b2-e263-4e43-80be-f288485ce9ba)

```
# 붓꽃 데이터셋에서 퍼셉트론 훈련
import os
import pandas as pd
s= 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'


df = pd.read_csv(
    s
    , header = None
    , encoding='utf-8'
)

# 시각화
import matplotlib.pyplot as plt

# setosa랑 versicolor를 선택
y=df.iloc[0:100,4].values #품정 추출

# setosa이면 0, versicolor이면 1로 출력

y=np.where(y=='Iris-setosa',0,1)

# 꽃의 길이 df[0],df[2], 꽃의 넓이 : df[1], df[3]
# data 추출 : 꽃받침, 꽃잎의 길이 데이터를 추출
X=df.iloc[0:100,[0,2]].values

# setosa와 versicolor 데이터 분포 확인

# 산점도 setosa
plt.figure(figsize=(10,8))
plt.scatter(
    x=X[:50,0]
    ,y=X[:50,1]
    ,color='red'
    ,marker='o'
    ,label='setosa'
)
plt.show()

```
![image](https://github.com/user-attachments/assets/dab8d629-09c8-487e-8fb9-f32a2a86f432)

```
# 붓꽃 데이터셋에서 퍼셉트론 훈련
import os
import pandas as pd
s= 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'


df = pd.read_csv(
    s
    , header = None
    , encoding='utf-8'
)

# 시각화
import matplotlib.pyplot as plt

# setosa랑 versicolor를 선택
y=df.iloc[0:100,4].values #품정 추출

# setosa이면 0, versicolor이면 1로 출력

y=np.where(y=='Iris-setosa',0,1)

# 꽃의 길이 df[0],df[2], 꽃의 넓이 : df[1], df[3]
# data 추출 : 꽃받침, 꽃잎의 길이 데이터를 추출
X=df.iloc[0:100,[0,2]].values

# setosa와 versicolor 데이터 분포 확인

# 산점도 setosa
plt.figure(figsize=(10,8))
plt.scatter(
    x=X[:50,0]
    ,y=X[:50,1]
    ,color='red'
    ,marker='o'
    ,label='setosa'
)

plt.scatter(
    x=X[50:100,0]
    ,y=X[50:100,1]
    ,color='blue'
    ,marker='o'
    ,label='versicolor'

)


plt.show()

```
![image](https://github.com/user-attachments/assets/8e05a556-c413-4087-b8e9-cb6387398cde)

```
# 붓꽃 데이터셋에서 퍼셉트론 훈련
import os
import pandas as pd
s= 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'


df = pd.read_csv(
    s
    , header = None
    , encoding='utf-8'
)

# 시각화
import matplotlib.pyplot as plt

# setosa랑 versicolor를 선택
y=df.iloc[0:100,4].values #품정 추출

# setosa이면 0, versicolor이면 1로 출력

y=np.where(y=='Iris-setosa',0,1)

# 꽃의 길이 df[0],df[2], 꽃의 넓이 : df[1], df[3]
# data 추출 : 꽃받침, 꽃잎의 길이 데이터를 추출
X=df.iloc[0:100,[0,2]].values

# setosa와 versicolor 데이터 분포 확인

# 산점도 setosa
plt.figure(figsize=(10,8))
plt.scatter(
    x=X[:50,0]
    ,y=X[:50,1]
    ,color='red'
    ,marker='o'
    ,label='setosa'
)

plt.scatter(
    x=X[50:100,0]
    ,y=X[50:100,1]
    ,color='blue'
    ,marker='o'
    ,label='versicolor'

)

plt.xlabel('Sepal length [cm]')
plt.ylabel('Petal length [cm]')
plt.legend(loc='upper left')

plt.show()
```
![image](https://github.com/user-attachments/assets/3477269c-9447-4e9d-a593-30cfb6b1a3a2)

### 시그모이드 함수(Sigmoid Function)

시그모이드 함수는 실수 값을 입력받아 그 출력을 0과 1 사이로 제한하는 특성을 가진 함수입니다. 이 함수는 주로 인공 신경망에서 활성화 함수로 사용됩니다. 시그모이드 함수는 다음과 같은 수식으로 정의됩니다.

세개 이상의 품종을 예측 하기 위해서는 **softmax**를 사용하는게 좋다

이진 분류 문제에서는 **시그모이드 함수**가 적합합니다.

#### 시그모이드 함수의 수식

$$
f(x) = \frac{1}{1 + e^{-x}}
$$


여기서 \( e \)는 자연 상수(약 2.718), \( x \)는 입력 값입니다.

#### 시그모이드 함수의 특징

1. **출력 범위**: 시그모이드 함수의 출력값은 항상 0과 1 사이에 있습니다. 즉, \( 0 < f(x) < 1 \)입니다.
2. **비선형성**: 시그모이드 함수는 비선형 함수로, 입력값의 변화에 비례하여 출력을 부드럽게 변화시킵니다. 이는 신경망에서 비선형적인 학습을 가능하게 합니다.
3. **미분 가능**: 시그모이드 함수는 미분이 가능하며, 이는 역전파 알고리즘을 사용할 때 유용합니다. 시그모이드 함수의 미분은 다음과 같습니다:

$$
f'(x) = f(x) \cdot (1 - f(x))
$$

4. **대칭성**: 시그모이드 함수는 원점에 대해 대칭적이지 않지만, 매우 작은 음수 입력에 대해 출력값이 0에 가까워지고, 양수 입력에 대해 1에 가까워지는 특성을 가집니다.

#### 시그모이드 함수의 그래프

시그모이드 함수는 S자 형태의 곡선을 그리며, 매우 작은 값에서 0에 가까워지고, 매우 큰 값에서는 1에 가까워집니다.

![image](https://github.com/user-attachments/assets/c6860750-4f70-4613-b12e-08429a26bd74)


#### 시그모이드 함수의 장점과 단점

##### 장점:
- **출력 범위 제한**: 출력값이 0과 1 사이로 제한되어, 확률을 다룰 때 유용합니다.
- **미분 가능성**: 신경망에서 학습을 위해 필요한 미분이 가능합니다.

##### 단점:
- **기울기 소실 문제**: 입력값이 너무 크거나 작을 경우, 함수의 기울기가 거의 0에 가까워져 학습이 느려질 수 있습니다.
- **비대칭성**: 출력 값이 0과 1 사이로 제한되어 있기 때문에, 0을 기준으로 대칭적인 출력을 제공하는 다른 함수들이 더 적합한 경우도 있습니다.

#### 시그모이드 함수의 활용

- **신경망**: 시그모이드 함수는 초기 신경망에서 활성화 함수로 사용되었지만, 현재는 ReLU와 같은 다른 함수들이 더 널리 사용됩니다.
- **로지스틱 회귀**: 이진 분류 문제에서 출력 값을 확률로 해석하기 위해 시그모이드 함수가 사용됩니다.

---
## Adaline (Adaptive Linear Neuron) 설명

### 개요

Adaline (Adaptive Linear Neuron)은 선형 회귀와 비슷한 개념을 가진 인공 신경망 모델입니다. Adaline은 주로 **Gradient Descent** 알고리즘을 사용하여 모델을 훈련시킵니다. 이 모델은 **활성화 함수**로 **항등 함수 (Identity Function)**를 사용하며, 출력은 입력의 가중치와 바이어스의 선형 조합입니다.

Adaline은 **퍼셉트론**과 비슷하지만, 퍼셉트론은 이진 분류에서 **단계 함수 (Step Function)**를 사용하여 출력을 이진값(0 또는 1)으로 결정하는 반면, Adaline은 **연속적인 값을 출력**합니다. 또한, Adaline은 평균 제곱 오차(MSE)를 손실 함수로 사용하며, 이를 최소화하기 위해 경사 하강법(Gradient Descent)을 적용합니다.

### 수학적 모델

Adaline은 다음과 같은 수식으로 정의됩니다:

- **순 입력 (Net Input)**:  
  $
  \[
  z = w_1x_1 + w_2x_2 + \dots + w_nx_n + b
  \]
  $
  여기서 \(x_1, x_2, \dots, x_n\)은 입력 값, \(w_1, w_2, \dots, w_n\)은 가중치, \(b\)는 바이어스입니다.

- **출력 (Output)**:  
  Adaline은 활성화 함수로 항등 함수를 사용합니다. 즉, 출력 \(y\)는 순 입력 \(z\)와 동일합니다.  
  $
  \[
  y = z
  \]
  $
  
- **오차 (Error)**:  
  실제 값 \(t\)와 모델의 예측값 \(y\) 사이의 오차는 다음과 같이 계산됩니다:
  $
  \[
  e = t - y
  \]
  $
  
- **가중치 업데이트 (Weight Update)**:  
  가중치는 다음과 같은 규칙에 따라 업데이트됩니다:
  $
  \[
  w_i = w_i + \eta \cdot e \cdot x_i
  \]
  $
  여기서 \(\eta\)는 학습률(learning rate), \(e\)는 오차, \(x_i\)는 입력값입니다.

## 모델 훈련

Adaline은 **경사 하강법 (Gradient Descent)**을 이용하여 가중치와 바이어스를 업데이트합니다. 경사 하강법은 손실 함수(평균 제곱 오차, MSE)를 최소화하기 위해 가중치와 바이어스의 기울기를 계산하고 이를 따라 업데이트합니다.

### 학습 과정:
1. **초기화**: 가중치와 바이어스를 작은 임의의 값으로 초기화합니다.
2. **예측**: 훈련 데이터에 대해 예측값을 계산합니다.
3. **오차 계산**: 실제 값과 예측값의 차이를 구합니다.
4. **가중치 업데이트**: 경사 하강법을 사용하여 가중치와 바이어스를 업데이트합니다.
5. **손실 계산**: 오차의 제곱 평균을 계산하여 손실값을 추적합니다.
6. **반복**: 주어진 반복 횟수만큼 2-5단계를 반복합니다.

## 장점

- **단순성**: Adaline은 퍼셉트론보다 구현이 간단하고, 선형 회귀 모델에 기반하여 수학적으로 이해하기 쉬운 모델입니다.
- **연속적인 출력을 제공**: Adaline은 퍼셉트론처럼 0 또는 1로 분류하는 대신, 연속적인 값을 출력하므로 회귀 문제에도 유용하게 사용될 수 있습니다.

## 단점

- **선형 모델**: Adaline은 선형 모델이기 때문에, 비선형적인 데이터에 대해서는 성능이 좋지 않습니다.
- **오류 민감도**: 경사 하강법을 사용하기 때문에, 학습률(\(\eta\))이 너무 크면 발산할 수 있으며, 너무 작으면 수렴이 매우 느려질 수 있습니다.

## 결론

Adaline은 간단하고 직관적인 선형 모델로, 특히 선형적으로 분리 가능한 데이터를 다룰 때 유용합니다. 그러나 비선형 문제에 대해서는 퍼셉트론이나 다른 비선형 모델들이 더 적합합니다.

```
# 필요한 라이브러리 임포트
import numpy as np
import matplotlib.pyplot as plt 


# Adaline (Adaptive Linear Neuron) 모델 클래스 정의
class AdalineGD:
    # 모델 초기화
    def __init__(self, eta=0.01, n_iter=50, random_state=1):
        self.eta = eta  # 학습률 (step size)
        self.n_iter = n_iter  # 훈련할 반복 횟수
        self.random_state = random_state  # 랜덤 시드를 설정하여 재현 가능하도록 설정
    
    # 훈련 데이터를 기반으로 모델 학습
    # method self를 주는 이유 : 맴버 변수 생성하거나 접근하게 하려고 준다
    def fit(self, X, y): # X : 학습 데이터, y : 원래 답
        rgen = np.random.RandomState(self.random_state)  # 주어진 시드로 랜덤 넘버 생성기 초기화
        # 가중치(w_)를 작은 랜덤 값으로 초기화
        self.w_ = rgen.normal(
            loc=0.0 # 평균
            , scale=0.01 # 표준편차
            , size=X.shape[1] # 개수 지정 : X의 피처 개수만큼
        )  # 작은 값으로 초기화
        self.b_ = np.float_(0.)  # 바이어스(bias)는 0으로 초기화, numpy 2.x일때는 np.float64를 쓰면 된다.

        # 오차 저장 : 시각화
        self.losses_ = []  # 각 반복에서의 손실값을 저장할 리스트
        
        # 주어진 반복 횟수만큼 Gradient Descent 수행
        for i in range(self.n_iter):
            # 입력값 X와 가중치 w_의 내적 + 바이어스를 계산하여 순 입력 값 구하기
            net_input = self.net_input(X)
            # 순 입력 값에 활성화 함수 적용 (Adaline은 선형 활성화 함수)
            output = self.activation(net_input)
            # 실제 값(y)와 예측 값(output)의 오차 계산
            errors = (y - output)
            # 가중치 업데이트 (오차에 대한 기울기)
            self.w_ += self.eta * 2.0 * X.T.dot(errors) / X.shape[0]
            # 바이어스 업데이트 (오차 평균에 대한 기울기)
            self.b_ += self.eta * 2.0 * errors.mean()
            # 손실값 계산 (평균 제곱 오차)
            loss = (errors ** 2).mean()
            self.losses_.append(loss)  # 손실값을 리스트에 저장
        
        return self  # 훈련된 모델 반환
    
    # 순 입력 계산 (X * 가중치 + 바이어스)
    def net_input(self, X):
        return np.dot(X, self.w_) + self.b_
    
    # 활성화 함수 (Adaline에서는 항등 함수, 즉 입력 그대로 반환)
    def activation(self, X):
        return X
    
    # 모델을 이용한 예측 함수
    def predict(self, X):
        # 예측값이 0 이상이면 1, 아니면 -1로 반환
        return np.where(self.activation(self.net_input(X)) >= 0.0, 1, -1)
```


```
import numpy as np
import matplotlib.pyplot as plt 


# Adaline (Adaptive Linear Neuron) 모델 클래스 정의
class AdalineGD:
    # 모델 초기화
    def __init__(self, eta=0.01, n_iter=50, random_state=1):
        self.eta = eta  # 학습률 (step size)
        self.n_iter = n_iter  # 훈련할 반복 횟수
        self.random_state = random_state  # 랜덤 시드를 설정하여 재현 가능하도록 설정
    
    # 훈련 데이터를 기반으로 모델 학습
    # method self를 주는 이유 : 맴버 변수 생성하거나 접근하게 하려고 준다
    def fit(self, X, y): # X : 학습 데이터, y : 원래 답
        rgen = np.random.RandomState(self.random_state)  # 주어진 시드로 랜덤 넘버 생성기 초기화
        # 가중치(w_)를 작은 랜덤 값으로 초기화
        self.w_ = rgen.normal(
            loc=0.0 # 평균
            , scale=0.01 # 표준편차
            , size=X.shape[1]
        )  # 작은 값으로 초기화
        self.b_ = np.float_(0.)  # 바이어스(bias)는 0으로 초기화
        self.losses_ = []  # 각 반복에서의 손실값을 저장할 리스트
        
        # 주어진 반복 횟수만큼 Gradient Descent 수행
        for i in range(self.n_iter):
            # 입력값 X와 가중치 w_의 내적 + 바이어스를 계산하여 순 입력 값 구하기
            net_input = self.net_input(X)
            # 순 입력 값에 활성화 함수 적용 (Adaline은 선형 활성화 함수)
            output = self.activation(net_input)
            # 실제 값(y)와 예측 값(output)의 오차 계산
            errors = (y - output)
            # 가중치 업데이트 (오차에 대한 기울기)
            self.w_ += self.eta * 2.0 * X.T.dot(errors) / X.shape[0]
            # 바이어스 업데이트 (오차 평균에 대한 기울기)
            self.b_ += self.eta * 2.0 * errors.mean()
            # 손실값 계산 (평균 제곱 오차)
            loss = (errors ** 2).mean()
            self.losses_.append(loss)  # 손실값을 리스트에 저장
        
        return self  # 훈련된 모델 반환
    
    # 순 입력 계산 (X * 가중치 + 바이어스)
    def net_input(self, X):
        return np.dot(X, self.w_) + self.b_
    
    # 활성화 함수 (Adaline에서는 항등 함수, 즉 입력 그대로 반환)
    def activation(self, X):
        return X
    
    # 모델을 이용한 예측 함수
    def predict(self, X):
        # 예측값이 0 이상이면 1, 아니면 -1로 반환
        return np.where(self.activation(self.net_input(X)) >= 0.0, 1, -1)

# 붓꽃 데이터셋
import os
import pandas as pd
s= 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'


df = pd.read_csv(
    s
    , header = None
    , encoding='utf-8'
)

# setosa랑 versicolor를 선택
y=df.iloc[0:100,4].values #품정 추출

# setosa이면 0, versicolor이면 1로 출력

y=np.where(y=='Iris-setosa',0,1)

# 꽃의 길이 df[0],df[2], 꽃의 넓이 : df[1], df[3]
# data 추출 : 꽃받침, 꽃잎의 길이 데이터를 추출
X=df.iloc[0:100,[0,2]].values


# 오차 시각화
fig, ax = plt.subplots(
    nrows=1
    , ncols=2
    , figsize=(10, 4)
)

ada1 = AdalineGD(n_iter=15, eta=0.1).fit(X, y)
ax[0].plot(
    range(1, len(ada1.losses_) + 1),
    ada1.losses_,
    marker='o'
)
ax[0].set_xlabel('Epochs') 
ax[0].set_ylabel('Mean squared error')  
ax[0].set_title('Adaline - Learning rate 0.1')

ada2 = AdalineGD(n_iter=15, eta=0.0001).fit(X, y)
ax[1].plot(
    range(1, len(ada2.losses_) + 1),
    ada2.losses_,
    marker='o'
)
ax[1].set_xlabel('Epochs') 
ax[1].set_ylabel('Mean squared error') 
ax[1].set_title('Adaline - Learning rate 0.0001')

plt.show()

```
![image](https://github.com/user-attachments/assets/c4d0dd8e-f6bc-490b-935f-34b92f5d92e4)

```
import numpy as np
import matplotlib.pyplot as plt 


# Adaline (Adaptive Linear Neuron) 모델 클래스 정의
class AdalineGD:
    # 모델 초기화
    def __init__(self, eta=0.01, n_iter=50, random_state=1):
        self.eta = eta  # 학습률 (step size)
        self.n_iter = n_iter  # 훈련할 반복 횟수
        self.random_state = random_state  # 랜덤 시드를 설정하여 재현 가능하도록 설정
    
    # 훈련 데이터를 기반으로 모델 학습
    # method self를 주는 이유 : 맴버 변수 생성하거나 접근하게 하려고 준다
    def fit(self, X, y): # X : 학습 데이터, y : 원래 답
        rgen = np.random.RandomState(self.random_state)  # 주어진 시드로 랜덤 넘버 생성기 초기화
        # 가중치(w_)를 작은 랜덤 값으로 초기화
        self.w_ = rgen.normal(
            loc=0.0 # 평균
            , scale=0.01 # 표준편차
            , size=X.shape[1]
        )  # 작은 값으로 초기화
        self.b_ = np.float_(0.)  # 바이어스(bias)는 0으로 초기화
        self.losses_ = []  # 각 반복에서의 손실값을 저장할 리스트
        
        # 주어진 반복 횟수만큼 Gradient Descent 수행
        for i in range(self.n_iter):
            # 입력값 X와 가중치 w_의 내적 + 바이어스를 계산하여 순 입력 값 구하기
            net_input = self.net_input(X)
            # 순 입력 값에 활성화 함수 적용 (Adaline은 선형 활성화 함수)
            output = self.activation(net_input)
            # 실제 값(y)와 예측 값(output)의 오차 계산
            errors = (y - output)
            # 가중치 업데이트 (오차에 대한 기울기)
            self.w_ += self.eta * 2.0 * X.T.dot(errors) / X.shape[0]
            # 바이어스 업데이트 (오차 평균에 대한 기울기)
            self.b_ += self.eta * 2.0 * errors.mean()
            # 손실값 계산 (평균 제곱 오차)
            loss = (errors ** 2).mean()
            self.losses_.append(loss)  # 손실값을 리스트에 저장
        
        return self  # 훈련된 모델 반환
    
    # 순 입력 계산 (X * 가중치 + 바이어스)
    def net_input(self, X):
        return np.dot(X, self.w_) + self.b_
    
    # 활성화 함수 (Adaline에서는 항등 함수, 즉 입력 그대로 반환)
    def activation(self, X):
        return X
    
    # 모델을 이용한 예측 함수
    def predict(self, X):
        # 예측값이 0 이상이면 1, 아니면 -1로 반환
        return np.where(self.activation(self.net_input(X)) >= 0.0, 1, -1)

# 붓꽃 데이터셋
import os
import pandas as pd
s= 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'


df = pd.read_csv(
    s
    , header = None
    , encoding='utf-8'
)

# setosa랑 versicolor를 선택
y=df.iloc[0:100,4].values #품정 추출

# setosa이면 0, versicolor이면 1로 출력

y=np.where(y=='Iris-setosa',0,1)

# 꽃의 길이 df[0],df[2], 꽃의 넓이 : df[1], df[3]
# data 추출 : 꽃받침, 꽃잎의 길이 데이터를 추출
X=df.iloc[0:100,[0,2]].values


# 오차 시각화
fig, ax = plt.subplots(
    nrows=1
    , ncols=2
    , figsize=(10, 4)
)

ada1 = AdalineGD(n_iter=15, eta=0.1).fit(X, y)
ax[0].plot(
    range(1, len(ada1.losses_) + 1),
    np.log10(ada1.losses_),
    marker='o'
)
ax[0].set_xlabel('Epochs') 
ax[0].set_ylabel('log(Mean squared error)')  
ax[0].set_title('Adaline - Learning rate 0.1')

ada2 = AdalineGD(n_iter=15, eta=0.0001).fit(X, y)
ax[1].plot(
    range(1, len(ada2.losses_) + 1),
    ada2.losses_,
    marker='o'
)
ax[1].set_xlabel('Epochs') 
ax[1].set_ylabel('Mean squared error') 
ax[1].set_title('Adaline - Learning rate 0.0001')

plt.show()

```
![image](https://github.com/user-attachments/assets/e7276395-a44c-401f-a3d4-6d4fd10c4ee9)

```
import numpy as np
import matplotlib.pyplot as plt 


# Adaline (Adaptive Linear Neuron) 모델 클래스 정의
class AdalineGD:
    # 모델 초기화
    def __init__(self, eta=0.01, n_iter=50, random_state=1):
        self.eta = eta  # 학습률 (step size)
        self.n_iter = n_iter  # 훈련할 반복 횟수
        self.random_state = random_state  # 랜덤 시드를 설정하여 재현 가능하도록 설정
    
    # 훈련 데이터를 기반으로 모델 학습
    # method self를 주는 이유 : 맴버 변수 생성하거나 접근하게 하려고 준다
    def fit(self, X, y): # X : 학습 데이터, y : 원래 답
        rgen = np.random.RandomState(self.random_state)  # 주어진 시드로 랜덤 넘버 생성기 초기화
        # 가중치(w_)를 작은 랜덤 값으로 초기화
        self.w_ = rgen.normal(
            loc=0.0 # 평균
            , scale=0.01 # 표준편차
            , size=X.shape[1]
        )  # 작은 값으로 초기화
        self.b_ = np.float_(0.)  # 바이어스(bias)는 0으로 초기화
        self.losses_ = []  # 각 반복에서의 손실값을 저장할 리스트
        
        # 주어진 반복 횟수만큼 Gradient Descent 수행
        for i in range(self.n_iter):
            # 입력값 X와 가중치 w_의 내적 + 바이어스를 계산하여 순 입력 값 구하기
            net_input = self.net_input(X)
            # 순 입력 값에 활성화 함수 적용 (Adaline은 선형 활성화 함수)
            output = self.activation(net_input)
            # 실제 값(y)와 예측 값(output)의 오차 계산
            errors = (y - output)
            # 가중치 업데이트 (오차에 대한 기울기)
            self.w_ += self.eta * 2.0 * X.T.dot(errors) / X.shape[0]
            # 바이어스 업데이트 (오차 평균에 대한 기울기)
            self.b_ += self.eta * 2.0 * errors.mean()
            # 손실값 계산 (평균 제곱 오차)
            loss = (errors ** 2).mean()
            self.losses_.append(loss)  # 손실값을 리스트에 저장
        
        return self  # 훈련된 모델 반환
    
    # 순 입력 계산 (X * 가중치 + 바이어스)
    def net_input(self, X):
        return np.dot(X, self.w_) + self.b_
    
    # 활성화 함수 (Adaline에서는 항등 함수, 즉 입력 그대로 반환)
    def activation(self, X):
        return X
    
    # 모델을 이용한 예측 함수
    def predict(self, X):
        # 예측값이 0 이상이면 1, 아니면 -1로 반환
        return np.where(self.activation(self.net_input(X)) >= 0.0, 1, -1)

# 붓꽃 데이터셋
import os
import pandas as pd
s= 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'


df = pd.read_csv(
    s
    , header = None
    , encoding='utf-8'
)

# setosa랑 versicolor를 선택
y=df.iloc[0:100,4].values #품정 추출

# setosa이면 0, versicolor이면 1로 출력

y=np.where(y=='Iris-setosa',0,1)

# 꽃의 길이 df[0],df[2], 꽃의 넓이 : df[1], df[3]
# data 추출 : 꽃받침, 꽃잎의 길이 데이터를 추출
X=df.iloc[0:100,[0,2]].values


# 오차 시각화
fig, ax = plt.subplots(
    nrows=1
    , ncols=2
    , figsize=(10, 4)
)

ada1 = AdalineGD(n_iter=3000, eta=0.1).fit(X, y)
ax[0].plot(
    range(1, len(ada1.losses_) + 1),
    np.log10(ada1.losses_),
    marker='o'
)
ax[0].set_xlabel('Epochs') 
ax[0].set_ylabel('log(Mean squared error)')  
ax[0].set_title('Adaline - Learning rate 0.1')

ada2 = AdalineGD(n_iter=3000, eta=0.0001).fit(X, y)
ax[1].plot(
    range(1, len(ada2.losses_) + 1),
    ada2.losses_,
    marker='o'
)
ax[1].set_xlabel('Epochs') 
ax[1].set_ylabel('Mean squared error') 
ax[1].set_title('Adaline - Learning rate 0.0001')

plt.show()

```
![image](https://github.com/user-attachments/assets/98a075ed-a877-46a2-94bc-b244b764347d)

```
import numpy as np
import matplotlib.pyplot as plt 


# Adaline (Adaptive Linear Neuron) 모델 클래스 정의
class AdalineGD:
    # 모델 초기화
    def __init__(self, eta=0.01, n_iter=50, random_state=1):
        self.eta = eta  # 학습률 (step size)
        self.n_iter = n_iter  # 훈련할 반복 횟수
        self.random_state = random_state  # 랜덤 시드를 설정하여 재현 가능하도록 설정
    
    # 훈련 데이터를 기반으로 모델 학습
    # method self를 주는 이유 : 맴버 변수 생성하거나 접근하게 하려고 준다
    def fit(self, X, y): # X : 학습 데이터, y : 원래 답
        rgen = np.random.RandomState(self.random_state)  # 주어진 시드로 랜덤 넘버 생성기 초기화
        # 가중치(w_)를 작은 랜덤 값으로 초기화
        self.w_ = rgen.normal(
            loc=0.0 # 평균
            , scale=0.01 # 표준편차
            , size=X.shape[1]
        )  # 작은 값으로 초기화
        self.b_ = np.float_(0.)  # 바이어스(bias)는 0으로 초기화
        self.losses_ = []  # 각 반복에서의 손실값을 저장할 리스트
        
        # 주어진 반복 횟수만큼 Gradient Descent 수행
        for i in range(self.n_iter):
            # 입력값 X와 가중치 w_의 내적 + 바이어스를 계산하여 순 입력 값 구하기
            net_input = self.net_input(X)
            # 순 입력 값에 활성화 함수 적용 (Adaline은 선형 활성화 함수)
            output = self.activation(net_input)
            # 실제 값(y)와 예측 값(output)의 오차 계산
            errors = (y - output)
            # 가중치 업데이트 (오차에 대한 기울기)
            self.w_ += self.eta * 2.0 * X.T.dot(errors) / X.shape[0]
            # 바이어스 업데이트 (오차 평균에 대한 기울기)
            self.b_ += self.eta * 2.0 * errors.mean()
            # 손실값 계산 (평균 제곱 오차)
            loss = (errors ** 2).mean()
            self.losses_.append(loss)  # 손실값을 리스트에 저장
        
        return self  # 훈련된 모델 반환
    
    # 순 입력 계산 (X * 가중치 + 바이어스)
    def net_input(self, X):
        return np.dot(X, self.w_) + self.b_
    
    # 활성화 함수 (Adaline에서는 항등 함수, 즉 입력 그대로 반환)
    def activation(self, X):
        return X
    
    # 모델을 이용한 예측 함수
    def predict(self, X):
        # 예측값이 0 이상이면 1, 아니면 -1로 반환
        return np.where(self.activation(self.net_input(X)) >= 0.0, 1, -1)

# 붓꽃 데이터셋
import os
import pandas as pd
s= 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'


df = pd.read_csv(
    s
    , header = None
    , encoding='utf-8'
)

# setosa랑 versicolor를 선택
y=df.iloc[0:100,4].values #품정 추출

# setosa이면 0, versicolor이면 1로 출력

y=np.where(y=='Iris-setosa',0,1)

# 꽃의 길이 df[0],df[2], 꽃의 넓이 : df[1], df[3]
# data 추출 : 꽃받침, 꽃잎의 길이 데이터를 추출
X=df.iloc[0:100,[0,2]].values


# 오차 시각화
fig, ax = plt.subplots(
    nrows=1
    , ncols=2
    , figsize=(10, 4)
)

ada1 = AdalineGD(n_iter=300000, eta=0.1).fit(X, y)
ax[0].plot(
    range(1, len(ada1.losses_) + 1),
    np.log10(ada1.losses_),
    marker='o'
)
ax[0].set_xlabel('Epochs') 
ax[0].set_ylabel('log(Mean squared error)')  
ax[0].set_title('Adaline - Learning rate 0.1')

ada2 = AdalineGD(n_iter=300000, eta=0.0001).fit(X, y)
ax[1].plot(
    range(1, len(ada2.losses_) + 1),
    ada2.losses_,
    marker='o'
)
ax[1].set_xlabel('Epochs') 
ax[1].set_ylabel('Mean squared error') 
ax[1].set_title('Adaline - Learning rate 0.0001')

plt.show()
```
![image](https://github.com/user-attachments/assets/5a9f69fd-72fa-43ee-8610-409b2b82f10a)

```
import numpy as np
import matplotlib.pyplot as plt 
import os
import pandas as pd
from mlxtend.plotting import plot_decision_regions

# Adaline (Adaptive Linear Neuron) 모델 클래스 정의
class AdalineGD:
    # 모델 초기화
    def __init__(self, eta=0.01, n_iter=50, random_state=1):
        self.eta = eta  # 학습률 (step size)
        self.n_iter = n_iter  # 훈련할 반복 횟수
        self.random_state = random_state  # 랜덤 시드를 설정하여 재현 가능하도록 설정
    
    # 훈련 데이터를 기반으로 모델 학습
    # method self를 주는 이유 : 맴버 변수 생성하거나 접근하게 하려고 준다
    def fit(self, X, y): # X : 학습 데이터, y : 원래 답
        rgen = np.random.RandomState(self.random_state)  # 주어진 시드로 랜덤 넘버 생성기 초기화
        # 가중치(w_)를 작은 랜덤 값으로 초기화
        self.w_ = rgen.normal(
            loc=0.0 # 평균
            , scale=0.01 # 표준편차
            , size=X.shape[1]
        )  # 작은 값으로 초기화
        self.b_ = np.float_(0.)  # 바이어스(bias)는 0으로 초기화
        self.losses_ = []  # 각 반복에서의 손실값을 저장할 리스트
        
        # 주어진 반복 횟수만큼 Gradient Descent 수행
        for i in range(self.n_iter):
            # 입력값 X와 가중치 w_의 내적 + 바이어스를 계산하여 순 입력 값 구하기
            net_input = self.net_input(X)
            # 순 입력 값에 활성화 함수 적용 (Adaline은 선형 활성화 함수)
            output = self.activation(net_input)
            # 실제 값(y)와 예측 값(output)의 오차 계산
            errors = (y - output)
            # 가중치 업데이트 (오차에 대한 기울기)
            self.w_ += self.eta * 2.0 * X.T.dot(errors) / X.shape[0]
            # 바이어스 업데이트 (오차 평균에 대한 기울기)
            self.b_ += self.eta * 2.0 * errors.mean()
            # 손실값 계산 (평균 제곱 오차)
            loss = (errors ** 2).mean()
            self.losses_.append(loss)  # 손실값을 리스트에 저장
        
        return self  # 훈련된 모델 반환
    
    # 순 입력 계산 (X * 가중치 + 바이어스)
    def net_input(self, X):
        return np.dot(X, self.w_) + self.b_
    
    # 활성화 함수 (Adaline에서는 항등 함수, 즉 입력 그대로 반환)
    def activation(self, X):
        return X
    
    # 모델을 이용한 예측 함수
    def predict(self, X):
        # 예측값이 0 이상이면 1, 아니면 -1로 반환
        return np.where(self.activation(self.net_input(X)) >= 0.0, 1, -1)

s = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'  # Iris dataset URL

# Load the data into a DataFrame
df = pd.read_csv(s, header=None, encoding='utf-8')

# Create the target variable
y = df.iloc[0:100, 4].values
y = np.where(y == 'Iris-setosa', 0, 1)  # Convert labels to binary (0 or 1)

# Extract the features (sepal length and petal length)
X = df.iloc[0:100, [0, 2]].values

# Standardize the features
X_std = np.copy(X)
X_std[:, 0] = (X[:, 0] - X[:, 0].mean()) / X[:, 0].std()  # Standardize the first feature (sepal length)
X_std[:, 1] = (X[:, 1] - X[:, 1].mean()) / X[:, 1].std()  # Standardize the second feature (petal length)


ada_gd =AdalineGD(n_iter=20,eta=0.5)
ada_gd.fit(X_std,y)


# 오차 시각화
fig, ax = plt.subplots(
    nrows=1
    , ncols=2
    , figsize=(10, 4)
)

ada1 = AdalineGD(n_iter=20, eta=0.5).fit(X, y) # 표준화 하지 않은 데이터
ax[0].plot(
    range(1, len(ada1.losses_) + 1),
    ada1.losses_,
    marker='o'
)


ada2 = AdalineGD(n_iter=20, eta=0.5).fit(X_std, y) # 표준화 한 데이터 
ax[1].plot(
    range(1, len(ada2.losses_) + 1),
    ada2.losses_,
    marker='o'
)

plt.show()

```
![image](https://github.com/user-attachments/assets/5cd3f7e4-d418-495f-bd39-bf83320f941c)

---
## 확률적 경사 하강법
```
import numpy as np
import matplotlib.pyplot as plt 
import os
import pandas as pd

class AdalineSGD:
    # 3개의 boolean type : 에포크별 가중치 초기화 여부, 섞어야하고, 시드 여부
    def __init__(self, eta=0.01, n_iter=10, shuffle=True, random_state=None):
        '''
        Adaline(Adaptive Linear Neuron) 모델을 위한 초기화 함수
        
        Parameters:
        eta : 학습률 (learning rate), 가중치 업데이트의 크기
        n_iter : 학습을 위한 에포크 수 (반복 횟수)
        shuffle : 데이터를 섞을지 여부
        random_state : 랜덤 시드를 고정하려면 값 설정 (None이면 랜덤)
        '''
        self.eta = eta
        self.n_iter = n_iter
        self.w_initialized = False  # 가중치 초기화 여부
        self.shuffle = shuffle
        self.random_state = random_state

    def fit(self, X, y):
        '''
        주어진 훈련 데이터 X, 타겟 값 y에 대해 학습
        
        Parameters:
        X : 훈련 데이터 (특징 벡터)
        y : 타겟 값 (실제 레이블)
        
        Returns:
        self : 학습된 모델 반환
        '''
        self._initialize_weights(X.shape[1])  # 가중치 초기화
        self.losses_ = []  # 에포크별 손실을 기록할 리스트
        
        for i in range(self.n_iter):
            # 데이터 섞기 (shuffle이 True일 경우)
            if self.shuffle:
                X, y = self._shuffle(X, y)
            
            losses = []  # 각 샘플별 손실을 기록할 리스트
            # 샘플 하나씩 학습 (온라인 학습 방식)
            for xi, target in zip(X, y):  # xi, target은 하나의 샘플과 그에 해당하는 레이블
                losses.append(self._update_weights(xi, target))  # 가중치 업데이트하고 손실 계산
            
            avg_loss = np.mean(losses)  # 평균 손실 계산
            self.losses_.append(avg_loss)  # 에포크별 손실 기록
        
        return self

    def partial_fit(self, X, y):
        '''
        가중치를 초기화하지 않고 주어진 데이터로 추가 학습
        
        Parameters:
        X : 훈련 데이터 (특징 벡터)
        y : 타겟 값 (실제 레이블)
        
        Returns:
        self : 학습된 모델 반환
        '''
        if not self.w_initialized:
            self._initialize_weights(X.shape[1])  # 초기화되지 않았다면 가중치 초기화
        
        if y.ravel().shape[0] > 1:  # 부분 데이터로 학습, ravel 함수는 주로 NumPy 라이브러리에서 사용되는 함수로, 다차원 배열을 1차원 배열로 평탄화(flatten)하는 데 사용
            for xi, target in zip(X, y):
                self._update_weights(xi, target)
        else:  # 전체 데이터에 학습
            self._update_weights(X, y)
        
        return self

    def _shuffle(self, X, y):
        '''
        훈련 데이터를 무작위로 섞기
        
        Parameters:
        X : 훈련 데이터 (특징 벡터)
        y : 타겟 값 (실제 레이블)
        
        Returns:
        X, y : 섞인 데이터와 레이블 반환
        '''
        idx = self.rgen.permutation(len(y))  # 무작위 순서 생성
        return X[idx], y[idx]  # 섞은 데이터와 레이블 반환

    def _initialize_weights(self, feature_count):
        '''가중치를 랜덤한 작은 값으로 초기화'''
        self.rgen = np.random.RandomState(self.random_state)  # 난수 생성기 초기화 (시드 설정), random_state=None여서 난수 생성 값 고정 x
        self.w_ = self.rgen.normal(loc=0.0, scale=0.01, size=feature_count)  # 평균 0, 표준편차 0.01로 가중치 초기화
        self.b_ = np.float_(0,)  # 바이어스 초기화
        self.w_initialized = True  # 가중치가 초기화되었음을 표시

    def _update_weights(self, xi, target):
        '''Adaline 학습 규칙을 적용하여 가중치 업데이트'''
        output = self.activation(self.net_input(xi))  # 예측값 계산
        error = (target - output)  # 오차 계산
        self.w_ += self.eta * 2.0 * xi * (error)  # 가중치 업데이트
        self.b_ += self.eta * 2.0 * error  # 바이어스 업데이트
        loss = error**2  # 손실 계산 (제곱 오차)
        return loss  # 손실 값 반환

    def net_input(self, X):
        '''입력 값 계산 (가중치와 특성의 선형 결합)'''
        return np.dot(X, self.w_) + self.b_  # X와 가중치 w_의 내적, 바이어스 b_ 추가

    def activation(self, X):
        '''활성화 함수 (Adaline에서는 단순 선형 함수)'''
        return X  # 이 경우 활성화 함수는 그대로 반환 (선형 활성화)

    def predict(self, X):
        '''입력 X에 대해 예측된 클래스 반환'''
        return np.where(self.activation(self.net_input(X)) >= 0.5, 1, 0)  # 활성화 값이 0.5 이상이면 1, 그 이하는 0으로 예측

```

```
import numpy as np
arr=np.array(
    [
        [1,2]
        ,[3,4]
    ]
)
print(arr)
print(type(arr))
print(arr.shape)
print(arr.ndim)
print(arr.ravel())
```
![image](https://github.com/user-attachments/assets/f1bbca27-ea00-45c0-a598-9427423f648e)

```
import numpy as np
import matplotlib.pyplot as plt 
import os
import pandas as pd

s = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'  # Iris dataset URL

# Load the data into a DataFrame
df = pd.read_csv(s, header=None, encoding='utf-8')

# Create the target variable
y = df.iloc[0:100, 4].values
y = np.where(y == 'Iris-setosa', 0, 1)  # Convert labels to binary (0 or 1)

# Extract the features (sepal length and petal length)
X = df.iloc[0:100, [0, 2]].values

# Standardize the features
X_std = np.copy(X)
X_std[:, 0] = (X[:, 0] - X[:, 0].mean()) / X[:, 0].std()  # Standardize the first feature (sepal length)
X_std[:, 1] = (X[:, 1] - X[:, 1].mean()) / X[:, 1].std()  # Standardize the second feature (petal length)



class AdalineSGD:
    # 3개의 boolean type : 에포크별 가중치 초기화 여부, 섞어야하고, 시드 여부
    def __init__(self, eta=0.01, n_iter=10, shuffle=True, random_state=None):
        '''
        Adaline(Adaptive Linear Neuron) 모델을 위한 초기화 함수
        
        Parameters:
        eta : 학습률 (learning rate), 가중치 업데이트의 크기
        n_iter : 학습을 위한 에포크 수 (반복 횟수)
        shuffle : 데이터를 섞을지 여부
        random_state : 랜덤 시드를 고정하려면 값 설정 (None이면 랜덤)
        '''
        self.eta = eta
        self.n_iter = n_iter
        self.w_initialized = False  # 가중치 초기화 여부
        self.shuffle = shuffle
        self.random_state = random_state

    def fit(self, X, y):
        '''
        주어진 훈련 데이터 X, 타겟 값 y에 대해 학습
        
        Parameters:
        X : 훈련 데이터 (특징 벡터)
        y : 타겟 값 (실제 레이블)
        
        Returns:
        self : 학습된 모델 반환
        '''
        self._initialize_weights(X.shape[1])  # 가중치 초기화
        self.losses_ = []  # 에포크별 손실을 기록할 리스트
        
        for i in range(self.n_iter):
            # 데이터 섞기 (shuffle이 True일 경우)
            if self.shuffle:
                X, y = self._shuffle(X, y)
            
            losses = []  # 각 샘플별 손실을 기록할 리스트
            # 샘플 하나씩 학습 (온라인 학습 방식)
            for xi, target in zip(X, y):  # xi, target은 하나의 샘플과 그에 해당하는 레이블
                losses.append(self._update_weights(xi, target))  # 가중치 업데이트하고 손실 계산
            
            avg_loss = np.mean(losses)  # 평균 손실 계산
            self.losses_.append(avg_loss)  # 에포크별 손실 기록
        
        return self

    def partial_fit(self, X, y):
        '''
        가중치를 초기화하지 않고 주어진 데이터로 추가 학습
        
        Parameters:
        X : 훈련 데이터 (특징 벡터)
        y : 타겟 값 (실제 레이블)
        
        Returns:
        self : 학습된 모델 반환
        '''
        if not self.w_initialized:
            self._initialize_weights(X.shape[1])  # 초기화되지 않았다면 가중치 초기화
        
        if y.ravel().shape[0] > 1:  # 부분 데이터로 학습, ravel 함수는 주로 NumPy 라이브러리에서 사용되는 함수로, 다차원 배열을 1차원 배열로 평탄화(flatten)하는 데 사용
            for xi, target in zip(X, y):
                self._update_weights(xi, target)
        else:  # 전체 데이터에 학습
            self._update_weights(X, y)
        
        return self

    def _shuffle(self, X, y):
        '''
        훈련 데이터를 무작위로 섞기
        
        Parameters:
        X : 훈련 데이터 (특징 벡터)
        y : 타겟 값 (실제 레이블)
        
        Returns:
        X, y : 섞인 데이터와 레이블 반환
        '''
        idx = self.rgen.permutation(len(y))  # 무작위 순서 생성
        return X[idx], y[idx]  # 섞은 데이터와 레이블 반환

    def _initialize_weights(self, feature_count):
        '''가중치를 랜덤한 작은 값으로 초기화'''
        self.rgen = np.random.RandomState(self.random_state)  # 난수 생성기 초기화 (시드 설정), random_state=None여서 난수 생성 값 고정 x
        self.w_ = self.rgen.normal(loc=0.0, scale=0.01, size=feature_count)  # 평균 0, 표준편차 0.01로 가중치 초기화
        self.b_ = np.float_(0,)  # 바이어스 초기화
        self.w_initialized = True  # 가중치가 초기화되었음을 표시

    def _update_weights(self, xi, target):
        '''Adaline 학습 규칙을 적용하여 가중치 업데이트'''
        output = self.activation(self.net_input(xi))  # 예측값 계산
        error = (target - output)  # 오차 계산
        self.w_ += self.eta * 2.0 * xi * (error)  # 가중치 업데이트
        self.b_ += self.eta * 2.0 * error  # 바이어스 업데이트
        loss = error**2  # 손실 계산 (제곱 오차)
        return loss  # 손실 값 반환

    def net_input(self, X):
        '''입력 값 계산 (가중치와 특성의 선형 결합)'''
        return np.dot(X, self.w_) + self.b_  # X와 가중치 w_의 내적, 바이어스 b_ 추가

    def activation(self, X):
        '''활성화 함수 (Adaline에서는 단순 선형 함수)'''
        return X  # 이 경우 활성화 함수는 그대로 반환 (선형 활성화)

    def predict(self, X):
        '''입력 X에 대해 예측된 클래스 반환'''
        return np.where(self.activation(self.net_input(X)) >= 0.5, 1, 0)  # 활성화 값이 0.5 이상이면 1, 그 이하는 0으로 예측


# 오차 시각화
fig, ax = plt.subplots(
    nrows=1
    , ncols=2
    , figsize=(10, 4)
)

ada1 = AdalineSGD(n_iter=15, eta=0.01).fit(X, y) 
ax[0].plot(
    range(1, len(ada1.losses_) + 1),
    ada1.losses_,
    marker='o'
)


ada2 = AdalineSGD(n_iter=15, eta=0.01).fit(X, y) 
ax[1].plot(
    range(1, len(ada2.losses_) + 1),
    ada2.losses_,
    marker='o'
)

plt.show()
```
![image](https://github.com/user-attachments/assets/9e0c2b05-87e1-4ec2-92aa-a284b541d25b)

```
import numpy as np
import matplotlib.pyplot as plt 
import os
import pandas as pd

s = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'  # Iris dataset URL

# Load the data into a DataFrame
df = pd.read_csv(s, header=None, encoding='utf-8')

# Create the target variable
y = df.iloc[0:100, 4].values
y = np.where(y == 'Iris-setosa', 0, 1)  # Convert labels to binary (0 or 1)

# Extract the features (sepal length and petal length)
X = df.iloc[0:100, [0, 2]].values

# Standardize the features
X_std = np.copy(X)
X_std[:, 0] = (X[:, 0] - X[:, 0].mean()) / X[:, 0].std()  # Standardize the first feature (sepal length)
X_std[:, 1] = (X[:, 1] - X[:, 1].mean()) / X[:, 1].std()  # Standardize the second feature (petal length)


# Adaline (Adaptive Linear Neuron) 모델 클래스 정의
class AdalineGD:
    # 모델 초기화
    def __init__(self, eta=0.01, n_iter=50, random_state=1):
        self.eta = eta  # 학습률 (step size)
        self.n_iter = n_iter  # 훈련할 반복 횟수
        self.random_state = random_state  # 랜덤 시드를 설정하여 재현 가능하도록 설정
    
    # 훈련 데이터를 기반으로 모델 학습
    # method self를 주는 이유 : 맴버 변수 생성하거나 접근하게 하려고 준다
    def fit(self, X, y): # X : 학습 데이터, y : 원래 답
        rgen = np.random.RandomState(self.random_state)  # 주어진 시드로 랜덤 넘버 생성기 초기화
        # 가중치(w_)를 작은 랜덤 값으로 초기화
        self.w_ = rgen.normal(
            loc=0.0 # 평균
            , scale=0.01 # 표준편차
            , size=X.shape[1]
        )  # 작은 값으로 초기화
        self.b_ = np.float_(0.)  # 바이어스(bias)는 0으로 초기화
        self.losses_ = []  # 각 반복에서의 손실값을 저장할 리스트
        
        # 주어진 반복 횟수만큼 Gradient Descent 수행
        for i in range(self.n_iter):
            # 입력값 X와 가중치 w_의 내적 + 바이어스를 계산하여 순 입력 값 구하기
            net_input = self.net_input(X)
            # 순 입력 값에 활성화 함수 적용 (Adaline은 선형 활성화 함수)
            output = self.activation(net_input)
            # 실제 값(y)와 예측 값(output)의 오차 계산
            errors = (y - output)
            # 가중치 업데이트 (오차에 대한 기울기)
            self.w_ += self.eta * 2.0 * X.T.dot(errors) / X.shape[0]
            # 바이어스 업데이트 (오차 평균에 대한 기울기)
            self.b_ += self.eta * 2.0 * errors.mean()
            # 손실값 계산 (평균 제곱 오차)
            loss = (errors ** 2).mean()
            self.losses_.append(loss)  # 손실값을 리스트에 저장
        
        return self  # 훈련된 모델 반환
    
    # 순 입력 계산 (X * 가중치 + 바이어스)
    def net_input(self, X):
        return np.dot(X, self.w_) + self.b_
    
    # 활성화 함수 (Adaline에서는 항등 함수, 즉 입력 그대로 반환)
    def activation(self, X):
        return X
    
    # 모델을 이용한 예측 함수
    def predict(self, X):
        # 예측값이 0 이상이면 1, 아니면 -1로 반환
        return np.where(self.activation(self.net_input(X)) >= 0.0, 1, -1)

class AdalineSGD:
    # 3개의 boolean type : 에포크별 가중치 초기화 여부, 섞어야하고, 시드 여부
    def __init__(self, eta=0.01, n_iter=10, shuffle=True, random_state=None):
        '''
        Adaline(Adaptive Linear Neuron) 모델을 위한 초기화 함수
        
        Parameters:
        eta : 학습률 (learning rate), 가중치 업데이트의 크기
        n_iter : 학습을 위한 에포크 수 (반복 횟수)
        shuffle : 데이터를 섞을지 여부
        random_state : 랜덤 시드를 고정하려면 값 설정 (None이면 랜덤)
        '''
        self.eta = eta
        self.n_iter = n_iter
        self.w_initialized = False  # 가중치 초기화 여부
        self.shuffle = shuffle
        self.random_state = random_state

    def fit(self, X, y):
        '''
        주어진 훈련 데이터 X, 타겟 값 y에 대해 학습
        
        Parameters:
        X : 훈련 데이터 (특징 벡터)
        y : 타겟 값 (실제 레이블)
        
        Returns:
        self : 학습된 모델 반환
        '''
        self._initialize_weights(X.shape[1])  # 가중치 초기화
        self.losses_ = []  # 에포크별 손실을 기록할 리스트
        
        for i in range(self.n_iter):
            # 데이터 섞기 (shuffle이 True일 경우)
            if self.shuffle:
                X, y = self._shuffle(X, y)
            
            losses = []  # 각 샘플별 손실을 기록할 리스트
            # 샘플 하나씩 학습 (온라인 학습 방식)
            for xi, target in zip(X, y):  # xi, target은 하나의 샘플과 그에 해당하는 레이블
                losses.append(self._update_weights(xi, target))  # 가중치 업데이트하고 손실 계산
            
            avg_loss = np.mean(losses)  # 평균 손실 계산
            self.losses_.append(avg_loss)  # 에포크별 손실 기록
        
        return self

    def partial_fit(self, X, y):
        '''
        가중치를 초기화하지 않고 주어진 데이터로 추가 학습
        
        Parameters:
        X : 훈련 데이터 (특징 벡터)
        y : 타겟 값 (실제 레이블)
        
        Returns:
        self : 학습된 모델 반환
        '''
        if not self.w_initialized:
            self._initialize_weights(X.shape[1])  # 초기화되지 않았다면 가중치 초기화
        
        if y.ravel().shape[0] > 1:  # 부분 데이터로 학습, ravel 함수는 주로 NumPy 라이브러리에서 사용되는 함수로, 다차원 배열을 1차원 배열로 평탄화(flatten)하는 데 사용
            for xi, target in zip(X, y):
                self._update_weights(xi, target)
        else:  # 전체 데이터에 학습
            self._update_weights(X, y)
        
        return self

    def _shuffle(self, X, y):
        '''
        훈련 데이터를 무작위로 섞기
        
        Parameters:
        X : 훈련 데이터 (특징 벡터)
        y : 타겟 값 (실제 레이블)
        
        Returns:
        X, y : 섞인 데이터와 레이블 반환
        '''
        idx = self.rgen.permutation(len(y))  # 무작위 순서 생성
        return X[idx], y[idx]  # 섞은 데이터와 레이블 반환

    def _initialize_weights(self, feature_count):
        '''가중치를 랜덤한 작은 값으로 초기화'''
        self.rgen = np.random.RandomState(self.random_state)  # 난수 생성기 초기화 (시드 설정), random_state=None여서 난수 생성 값 고정 x
        self.w_ = self.rgen.normal(loc=0.0, scale=0.01, size=feature_count)  # 평균 0, 표준편차 0.01로 가중치 초기화
        self.b_ = np.float_(0,)  # 바이어스 초기화
        self.w_initialized = True  # 가중치가 초기화되었음을 표시

    def _update_weights(self, xi, target):
        '''Adaline 학습 규칙을 적용하여 가중치 업데이트'''
        output = self.activation(self.net_input(xi))  # 예측값 계산
        error = (target - output)  # 오차 계산
        self.w_ += self.eta * 2.0 * xi * (error)  # 가중치 업데이트
        self.b_ += self.eta * 2.0 * error  # 바이어스 업데이트
        loss = error**2  # 손실 계산 (제곱 오차)
        return loss  # 손실 값 반환

    def net_input(self, X):
        '''입력 값 계산 (가중치와 특성의 선형 결합)'''
        return np.dot(X, self.w_) + self.b_  # X와 가중치 w_의 내적, 바이어스 b_ 추가

    def activation(self, X):
        '''활성화 함수 (Adaline에서는 단순 선형 함수)'''
        return X  # 이 경우 활성화 함수는 그대로 반환 (선형 활성화)

    def predict(self, X):
        '''입력 X에 대해 예측된 클래스 반환'''
        return np.where(self.activation(self.net_input(X)) >= 0.5, 1, 0)  # 활성화 값이 0.5 이상이면 1, 그 이하는 0으로 예측


# 오차 시각화
fig, ax = plt.subplots(
    nrows=1
    , ncols=2
    , figsize=(10, 4)
)

ada1 = AdalineGD(n_iter=15, eta=0.01).fit(X, y) 
ax[0].plot(
    range(1, len(ada1.losses_) + 1),
    ada1.losses_,
    marker='o'
)
ax[0].set_title('AdalineGD')


ada2 = AdalineSGD(n_iter=15, eta=0.01).fit(X, y) 
ax[1].plot(
    range(1, len(ada2.losses_) + 1),
    ada2.losses_,
    marker='o'
)
ax[1].set_title('AdalineSGD')
plt.show()
```
![image](https://github.com/user-attachments/assets/4300b98a-0a1f-4092-aebe-f6e5d84aff89)

