```
import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns

from scipy import stats

import warnings
warnings.filterwarnings('ignore')
```
```
# 보스톤 주택 가격(연속값) 예측(회귀모델)
# 데이터 로딩
boston_df=pd.read_csv(
    './boston_house_prices.csv'
    ,encoding='utf-8'
    ,header=1
)

boston_df.head()
```
![image](https://github.com/user-attachments/assets/7f014ca8-1bd8-4847-8c6f-65497276e7a5)

```
boston_df.info()
```
![image](https://github.com/user-attachments/assets/424e5e2e-0daf-492e-9cf8-4a6cef524c89)

```
boston_df.describe()
```
![image](https://github.com/user-attachments/assets/2fd74ec5-d3ec-45b1-a0e4-1f0ea629233d)

```
boston_df.columns
```
![image](https://github.com/user-attachments/assets/9e4c89a0-c924-4e24-aace-5e56273a5f31)

```
fig,axs=plt.subplots(
    figsize=(16,8)
    ,ncols=4
    ,nrows=2
)
lm_feautres=['RM','ZN','INDUS','NOX','AGE','PTRATIO','LSTAT','RAD']

for i, feature in enumerate(lm_feautres):
    row =int(i/4)
    col = i%4
    # 사본의 regplot을 이용해 산점도와 선형 회귀 직선을 함께 표현
    sns.regplot(
        x=feature
        ,y='MEDV'
        ,data=boston_df
        ,ax=axs[row][col]
    )
plt.show()
```
![image](https://github.com/user-attachments/assets/f748c32a-dbb7-4943-85e8-812366f72759)

```
# 2개의 행과 4개의 열을 가진 subplots를 이용. axs는 4x2개의 ax를 가짐.
fig, axs = plt.subplots(figsize=(16,8) , ncols=4 , nrows=2)
lm_features = ['RM','ZN','INDUS','NOX','AGE','PTRATIO','LSTAT','RAD']
for i , feature in enumerate(lm_features):
    row = int(i/4)
    col = i%4
    # 시본의 regplot을 이용해 산점도와 선형 회귀 직선을 함께 표현
    sns.regplot(x=feature , y='MEDV',data=boston_df , ax=axs[row][col])


# 이미지 파일 저장
fig1 = plt.gcf()
fig1.savefig('p322_boston.tif', format='tif', dpi=300, bbox_inches='tight')
```

```
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# 데이터 레이블 분리
y_target=boston_df['MEDV']
X_data=boston_df.drop(['MEDV'],axis=1,inplace=False)# False : 삭제한 DF를 반환

```

```
# 학습/테스트 데이터 분리
X_train,X_test,y_train,y_test=train_test_split(
    X_data
    ,y_target
    ,test_size=0.3
    ,random_state=0
)
```
```
# LinearRegression object create
lr =LinearRegression()
lr.fit(X_train,y_train) # 학습
y_pred=lr.predict(X_test) # 테스트 데이터로 예측값 출력

# 오차(원래답-예측값) : MSE
mse=mean_squared_error(y_test,y_pred)
rmse=np.sqrt(mse) # 제곱근
print(f'MSE : {mse}, RMSE : {rmse}')
print(f'Variance score : {r2_score(y_test,y_pred)}') # 모델의 설명력
```
![image](https://github.com/user-attachments/assets/d90e433c-923c-4b64-a35b-e53feece208b)

```
# 피쳐의 영향력 : 개수 값을 출력
print(f'회귀 계수 값 " {lr.coef_}')
```
![image](https://github.com/user-attachments/assets/3ff69b2f-cefb-412b-a984-8dbab38950ea)

```
coeff=pd.Series(
    data=np.round(lr.coef_,1)
    ,index=X_data.columns
)
coeff
```
![image](https://github.com/user-attachments/assets/e3023479-6964-46fc-ac79-ddfba815a9fa)

```
coeff=pd.Series(
    data=np.round(lr.coef_,1)
    ,index=X_data.columns
)
coeff.sort_values(ascending=False)
```
![image](https://github.com/user-attachments/assets/009b9f84-ccd0-405a-97d3-8ce1057e7645)

```
X_data.info()
```
![image](https://github.com/user-attachments/assets/9e6ed5f3-cf8c-4908-b400-9ff26eec2c76)

```
y_target.info()
```
![image](https://github.com/user-attachments/assets/69e1bcf0-b2c1-4acc-9b93-ac98dbc95dad)

```
pip install statsmodels
```
![image](https://github.com/user-attachments/assets/b6f8296e-102a-48eb-8ba9-156f3e73e00c)

```
# 상수항 추가
import statsmodels.api as sm

X=sm.add_constant(X_data)
X
```
![image](https://github.com/user-attachments/assets/3896f3de-5fa5-4795-8fdc-fa887f7dd90c)

```
# 상수항 추가
import statsmodels.api as sm

X=sm.add_constant(X_data)

# OLS(회구 모델)
model=sm.OLS(y_target,X)

# 학습
result=model.fit()

# 결과 출력
print(result.summary())
```
![image](https://github.com/user-attachments/assets/10a9e995-91bd-458c-8936-96fdb4d67e53)

---
## OLS 회귀 분석 결과 해석

### 주요 지표

- **R-squared (결정계수)**: 0.741  
  이 값은 독립변수들이 종속변수(MEDV, 주택의 중간 가격)를 약 74.1% 정도 설명한다는 의미입니다. 즉, 모델이 데이터의 약 74.1%의 변동성을 설명할 수 있다는 것입니다.

- **Adjusted R-squared (수정된 결정계수)**: 0.734  
  이 값은 R-squared의 수정 버전으로, 변수의 수를 고려하여 모델의 적합도를 평가합니다. 모델이 상대적으로 잘 맞는 것을 보여줍니다.

- **F-statistic**: 108.1, **Prob(F-statistic)**: 6.72e-135  
  F-통계량은 모델의 전체 유의미성을 평가하는 값입니다. 값이 매우 크고, p-value가 매우 작기 때문에 이 회귀모델은 통계적으로 유의미하다고 할 수 있습니다.

- **AIC (Akaike Information Criterion)**: 3026, **BIC (Bayesian Information Criterion)**: 3085  
  AIC와 BIC 값은 모델 선택 시 여러 모델을 비교할 때 사용됩니다. 값이 낮을수록 모델이 더 좋다고 할 수 있지만, 두 값만으로 모델이 좋은지 나쁜지 확실히 판단하기는 어렵습니다.

### 각 변수의 해석

```
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                   MEDV   R-squared:                       0.741
Model:                            OLS   Adj. R-squared:                  0.734
Method:                 Least Squares   F-statistic:                     108.1
Date:                Thu, 10 Apr 2025   Prob (F-statistic):          6.72e-135
Time:                        12:32:24   Log-Likelihood:                -1498.8
No. Observations:                 506   AIC:                             3026.
Df Residuals:                     492   BIC:                             3085.
Df Model:                          13                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const         36.4595      5.103      7.144      0.000      26.432      46.487
CRIM          -0.1080      0.033     -3.287      0.001      -0.173      -0.043
ZN             0.0464      0.014      3.382      0.001       0.019       0.073
INDUS          0.0206      0.061      0.334      0.738      -0.100       0.141
CHAS           2.6867      0.862      3.118      0.002       0.994       4.380
NOX          -17.7666      3.820     -4.651      0.000     -25.272     -10.262
RM             3.8099      0.418      9.116      0.000       2.989       4.631
AGE            0.0007      0.013      0.052      0.958      -0.025       0.027
DIS           -1.4756      0.199     -7.398      0.000      -1.867      -1.084
RAD            0.3060      0.066      4.613      0.000       0.176       0.436
TAX           -0.0123      0.004     -3.280      0.001      -0.020      -0.005
PTRATIO       -0.9527      0.131     -7.283      0.000      -1.210      -0.696
B              0.0093      0.003      3.467      0.001       0.004       0.015
LSTAT         -0.5248      0.051    -10.347      0.000      -0.624      -0.425
==============================================================================
Omnibus:                      178.041   Durbin-Watson:                   1.078
Prob(Omnibus):                  0.000   Jarque-Bera (JB):              783.126
Skew:                           1.521   Prob(JB):                    8.84e-171
Kurtosis:                       8.281   Cond. No.                     1.51e+04
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The condition number is large, 1.51e+04. This might indicate that there are
strong multicollinearity or other numerical problems.

```


### 해석

#### 중요한 변수들:
- **CRIM (범죄율)**:  
  범죄율이 1단위 증가할 때, MEDV는 약 0.108 단위 감소합니다. p-value가 0.001로 매우 유의미하여, 범죄율이 주택 가격에 부정적인 영향을 미친다고 할 수 있습니다.

- **ZN (주거지 비율)**:  
  주거지 비율이 1단위 증가할 때, MEDV는 약 0.046 단위 증가합니다. p-value가 0.001로 유의미하여, 주거지 비율이 높을수록 주택 가격이 높아진다는 결과입니다.

- **NOX (일산화질소 농도)**:  
  일산화질소 농도가 1단위 증가할 때, MEDV는 약 17.77 단위 감소합니다. p-value가 0.000으로 매우 유의미합니다. 공기 오염이 주택 가격에 부정적인 영향을 미친다고 해석할 수 있습니다.

- **RM (평균 방 수)**:  
  방 수가 1개 증가할 때, MEDV는 약 3.81 단위 증가합니다. p-value가 0.000으로 매우 유의미합니다. 방 수가 많을수록 주택 가격이 높다는 해석이 가능합니다.

- **LSTAT (하위 계층 비율)**:  
  하위 계층 비율이 1% 증가할 때, MEDV는 약 0.525 단위 감소합니다. p-value가 0.000으로 매우 유의미합니다. 하위 계층 비율이 높을수록 주택 가격이 낮다는 결과입니다.

#### 유의미하지 않은 변수:
- **AGE (주택 연령)**:  
  주택의 연령이 1년 증가할 때, MEDV는 약 0.0007 단위 증가합니다. 그러나 p-value가 0.958로 매우 높아 이 변수는 통계적으로 유의미하지 않다고 볼 수 있습니다.

- **INDUS (상업지역 비율)**:  
  상업지역 비율이 1단위 증가할 때, MEDV는 약 0.0206 단위 증가하지만, p-value가 0.738로 매우 높아 이 변수 역시 통계적으로 유의미하지 않습니다.

### 결론

이 모델은 다양한 독립변수들이 주택의 중간 가격(MEDV)에 미치는 영향을 평가합니다. 주요 변수 중 **CRIM(범죄율)**, **NOX(일산화질소 농도)**, **RM(평균 방 수)**, **DIS(고용 센터와의 거리)**, **TAX(재산세)**, **PTRATIO(학생-교사 비율)**, **LSTAT(하위 계층 비율)** 등이 주택 가격에 유의미한 영향을 미치고 있습니다. 반면, **AGE**와 **INDUS**는 통계적으로 유의미한 영향을 미치지 않습니다.

---
## 다중 공선성(Multicollinearity)

**다중 공선성**은 회귀 분석에서 독립 변수들(피쳐) 간에 상관 관계가 매우 높을 때 발생하는 문제입니다. 이는 모델의 **계수 불안정성**, **해석의 어려움**, 그리고 **모델 성능 저하**를 초래할 수 있습니다.

### 다중 공선성의 문제점
1. **피쳐 간 상관계수가 매우 클 때 발생**:
   - 독립 변수들 간에 높은 상관 관계가 있을 경우, 중복된 정보를 다루게 되어 모델이 불안정해집니다.
   
2. **계수의 불안정성**:
   - 다중 공선성으로 인해 각 독립 변수에 대한 계수(β 값)가 불안정해지고, 데이터의 작은 변화에도 계수가 크게 달라질 수 있습니다.
   
3. **모델 해석의 어려움**:
   - 독립 변수 간의 강한 상관 관계로 인해, 개별 변수의 중요도를 파악하기 어렵고 변수 간 영향을 분리하기 어려워집니다.

4. **모델 성능 저하**:
   - 다중 공선성은 과적합(overfitting)을 유발할 수 있어 모델의 일반화 능력(새로운 데이터에 대한 예측 성능)을 떨어뜨립니다.

## 다중 공선성 해결 방법
1. **VIF(Variance Inflation Factor)**:
   - VIF를 통해 각 변수의 다중 공선성을 확인하고, VIF 값이 높은 변수를 제거하거나 변형할 수 있습니다.

2. **주성분 분석(PCA)**:
   - 주성분 분석(PCA)을 사용하여 상관 관계가 높은 변수들을 결합하여 새로운 독립 변수를 생성할 수 있습니다.

3. **정규화 기법**:
   - **Ridge 회귀**나 **Lasso 회귀**와 같은 정규화 기법을 사용하면 다중 공선성 문제를 완화할 수 있습니다.

```
# 다중 공선성 : 피쳐 간 상관계수가 너무 클 때 발생, 계수의 불안정과 해석의 어려움 발생
# 결론적으로 모델 성능 저하
from statsmodels.stats.outliers_influence import  variance_inflation_factor

vif=pd.DataFrame() # 빈 데이터프레임 생성
vif['feature']=X.columns
vif['VIF']=[
    variance_inflation_factor(X.values, idx) for idx in range(X.shape[1])
]
vif
```
![image](https://github.com/user-attachments/assets/2eeb93c1-561b-4692-b58b-36295046a1eb)

## VIF (Variance Inflation Factor) 해석

VIF(Variance Inflation Factor)는 다중 공선성을 확인하기 위해 사용됩니다. 일반적으로 VIF 값이 **10 이상**인 변수는 다중 공선성 문제가 있을 수 있습니다. 아래는 각 변수에 대한 VIF 값입니다.

| 변수      | VIF        |
|-----------|------------|
| **const** | 585.27     |
| **CRIM**  | 1.79       |
| **ZN**    | 2.30       |
| **INDUS** | 3.99       |
| **CHAS**  | 1.07       |
| **NOX**   | 4.39       |
| **RM**    | 1.93       |
| **AGE**   | 3.10       |
| **DIS**   | 3.96       |
| **RAD**   | 7.48       |
| **TAX**   | 9.01       |
| **PTRATIO**| 1.80      |
| **B**     | 1.35       |
| **LSTAT** | 2.94       |

### 참고
- vif value : 1~5이면 정상
- vif value : 5~10 약한 다음 공선성을 가지고 있다. 주의 필요
- vif value : 10이상 심한 다중 공선성, 해당 피처를 제거 필요

### 해석:
- **VIF 값이 1에 가까운 변수들**: `CRIM`, `ZN`, `CHAS`, `RM`, `PTRATIO`, `B`는 다른 변수들과 상관 관계가 적어 다중 공선성 문제가 적습니다.
- **VIF 값이 10 이상인 변수들**: `RAD`와 `TAX`는 다중 공선성 문제를 유발할 가능성이 있으며, 이를 해결하기 위해 추가적인 조치가 필요할 수 있습니다.

### 다중 공선성 해결 방법:
- VIF 값이 높은 변수는 제거하거나, **주성분 분석(PCA)** 또는 **정규화 기법**(예: **Ridge** 회귀, **Lasso** 회귀)을 사용하여 다중 공선성 문제를 완화할 수 있습니다.

---
## 다항 회귀와 과(대)적합/과소적합 이해

```
# 다항식
from sklearn.preprocessing import PolynomialFeatures

# 예제 데이터 생성 [2x2]
X=np.arange(4).reshape(2,2)
print(f'일차 단항식 계수 피쳐\n{X}')

# degree = 2인 2차 다항식으로 변환하기 위해 PolynomialFeatures를 이용해 변환
poly=PolynomialFeatures(degree=2)
poly.fit(X)
poly_ftr=poly.transform(X)
print(f'변환된 2차 다항식 계수 피쳐:\n{poly_ftr}')
```
![image](https://github.com/user-attachments/assets/145bcad5-8de5-43b4-bfc6-bc2a3b02f4d8)

---
## 보스톤 주택 데이터를 가지고 다항식으로 변환 후 처리
```
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from scipy import stats

import warnings
warnings.filterwarnings('ignore')

boston_df=pd.read_csv(
    './boston_house_prices.csv'
    ,encoding='utf-8'
    ,header=1
)


y_target=boston_df['MEDV']
X_data=boston_df.drop(['MEDV'],axis=1,inplace=False)

X_train,X_test,y_train,y_test=train_test_split(
    X_data
    ,y_target
    ,test_size=0.3
    ,random_state=0
)

lr =LinearRegression()
lr.fit(X_train,y_train) 
y_pred=lr.predict(X_test) 

mse=mean_squared_error(y_test,y_pred)
rmse=np.sqrt(mse) 


# 보스톤 주택 데이터 -> 다항식으로 변환 후 처리
# X_data : 전체 데이터
# X_data.info() : 13개의 features
from sklearn.preprocessing import PolynomialFeatures

print(f'X_data의 shape : {X_data.shape}')
poly=PolynomialFeatures(degree=2)
poly.fit(X_data)
poly_ftr=poly.transform(X_data)
print(f'X_data를 다항식 적용한 shape : {poly_ftr.shape}')
```
![image](https://github.com/user-attachments/assets/8e80ca02-6481-4021-9069-f12f18023492)

```
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from scipy import stats

import warnings
warnings.filterwarnings('ignore')

boston_df=pd.read_csv(
    './boston_house_prices.csv'
    ,encoding='utf-8'
    ,header=1
)


y_target=boston_df['MEDV']
X_data=boston_df.drop(['MEDV'],axis=1,inplace=False)

X_train,X_test,y_train,y_test=train_test_split(
    X_data
    ,y_target
    ,test_size=0.3
    ,random_state=0
)

# LinearRegression object create
lr =LinearRegression()
lr.fit(X_train,y_train) # 학습
y_pred=lr.predict(X_test) # 테스트 데이터로 예측값 출력

# 오차(원래답-예측값) : MSE
mse=mean_squared_error(y_test,y_pred)
rmse=np.sqrt(mse) # 제곱근
print('다항식 변경 전')
print(f'MSE : {mse}, RMSE : {rmse}')
print(f'Variance score : {r2_score(y_test,y_pred)}')
# 보스톤 주택 데이터 -> 다항식으로 변환 후 처리
# X_data : 전체 데이터
# X_data.info() : 13개의 features
from sklearn.preprocessing import PolynomialFeatures
# print(f'X_data의 shape : {X_data.shape}')

print('다항식 변경 후')
poly=PolynomialFeatures(degree=2)
poly.fit(X_data)
poly_ftr=poly.transform(X_data)
# print(f'X_data를 다항식 적용한 shape : {poly_ftr.shape}')

# 학습/테스트 데이터 분리
X_train_poly,X_test_poly,y_train_poly,y_test_poly=train_test_split(
    poly_ftr # 다항식으로 변환한 전체 데이터
    ,y_target # 전체 레이블
    ,test_size=0.3 
    ,random_state=156
)

lr =LinearRegression()
lr.fit(X_train_poly,y_train_poly) 
y_pred=lr.predict(X_test_poly) 

mse=mean_squared_error(y_test_poly,y_pred)
rmse=np.sqrt(mse) 
print(f'MSE : {mse}, RMSE : {rmse}')
print(f'Variance score : {r2_score(y_test_poly,y_pred)}')
```
![image](https://github.com/user-attachments/assets/1a21abdf-91a0-4c77-8b7d-e727e5480324)

```
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from scipy import stats

import warnings
warnings.filterwarnings('ignore')

boston_df=pd.read_csv(
    './boston_house_prices.csv'
    ,encoding='utf-8'
    ,header=1
)


y_target=boston_df['MEDV']
X_data=boston_df.drop(['MEDV'],axis=1,inplace=False)

X_train,X_test,y_train,y_test=train_test_split(
    X_data
    ,y_target
    ,test_size=0.3
    ,random_state=0
)

# LinearRegression object create
lr =LinearRegression()
lr.fit(X_train,y_train) # 학습
y_pred=lr.predict(X_test) # 테스트 데이터로 예측값 출력

# 오차(원래답-예측값) : MSE
mse=mean_squared_error(y_test,y_pred)
rmse=np.sqrt(mse) # 제곱근
print('다항식 변경 전')
print(f'피처의 개수 : {poly_ftr.shape[1]}')
print(f'MSE : {mse}, RMSE : {rmse}')
print(f'Variance score : {r2_score(y_test,y_pred)}')
# 보스톤 주택 데이터 -> 다항식으로 변환 후 처리
# X_data : 전체 데이터
# X_data.info() : 13개의 features
from sklearn.preprocessing import PolynomialFeatures
# print(f'X_data의 shape : {X_data.shape}')

print('다항식 변경 후')
poly=PolynomialFeatures(degree=2)
poly.fit(X_data)
poly_ftr=poly.transform(X_data)
# print(f'X_data를 다항식 적용한 shape : {poly_ftr.shape}')

print(f'피처의 개수 : {poly_ftr.shape[1]}')
# 학습/테스트 데이터 분리
X_train_poly,X_test_poly,y_train_poly,y_test_poly=train_test_split(
    poly_ftr # 다항식으로 변환한 전체 데이터
    ,y_target # 전체 레이블
    ,test_size=0.3 
    ,random_state=156
)

lr =LinearRegression()
lr.fit(X_train_poly,y_train_poly) 
y_pred=lr.predict(X_test_poly) 

mse=mean_squared_error(y_test_poly,y_pred)
rmse=np.sqrt(mse) 
print(f'MSE : {mse}, RMSE : {rmse}')
print(f'Variance score : {r2_score(y_test_poly,y_pred)}')

print('다항식 degree 5로 설정할때')
degree=5

poly=PolynomialFeatures(degree=degree)
poly.fit(X_data)
poly_ftr=poly.transform(X_data)
# print(f'X_data를 다항식 적용한 shape : {poly_ftr.shape}')

# 학습/테스트 데이터 분리
print(f'피처의 개수 : {poly_ftr.shape[1]}')
X_train_poly,X_test_poly,y_train_poly,y_test_poly=train_test_split(
    poly_ftr # 다항식으로 변환한 전체 데이터
    ,y_target # 전체 레이블
    ,test_size=0.3 
    ,random_state=156
)

lr =LinearRegression()
lr.fit(X_train_poly,y_train_poly) 
y_pred=lr.predict(X_test_poly) 

mse=mean_squared_error(y_test_poly,y_pred)
rmse=np.sqrt(mse) 
print(f'MSE : {mse}, RMSE : {rmse}')
print(f'Variance score : {r2_score(y_test_poly,y_pred)}')

print('다항식 degree 3로 설정할때')
degree=3

poly=PolynomialFeatures(degree=degree)
poly.fit(X_data)
poly_ftr=poly.transform(X_data)
# print(f'X_data를 다항식 적용한 shape : {poly_ftr.shape}')

# 학습/테스트 데이터 분리
print(f'피처의 개수 : {poly_ftr.shape[1]}')
X_train_poly,X_test_poly,y_train_poly,y_test_poly=train_test_split(
    poly_ftr # 다항식으로 변환한 전체 데이터
    ,y_target # 전체 레이블
    ,test_size=0.3 
    ,random_state=156
)

lr =LinearRegression()
lr.fit(X_train_poly,y_train_poly) 
y_pred=lr.predict(X_test_poly) 

mse=mean_squared_error(y_test_poly,y_pred)
rmse=np.sqrt(mse) 
print(f'MSE : {mse}, RMSE : {rmse}')
print(f'Variance score : {r2_score(y_test_poly,y_pred)}')
```
![image](https://github.com/user-attachments/assets/7b4edaa2-0983-420f-804a-4bafb305f4cb)

---

## 📘 규제 선형 모델 정리 - 릿지, 라쏘, 엘라스틱넷

### 📌 1. 릿지 회귀 (Ridge Regression)

#### ✅ 특징
- L2 정규화 사용 (계수의 제곱합을 페널티로 부여)
- 모든 변수를 포함하면서 **계수를 작게** 만듦
- 과적합(overfitting)을 방지
- 변수 간 **다중공선성** 문제 완화

#### ➕ 장점
- 모델의 일반화 성능 향상
- 변수 간 상관관계가 높을 때 안정적
- 구현 간단, 계산 효율적

#### ➖ 단점
- 중요하지 않은 변수도 계수가 0이 되지 않음 → **변수 선택 불가**
- 해석력이 떨어질 수 있음



### ✂️ 2. 라쏘 회귀 (Lasso Regression)

#### ✅ 특징
- L1 정규화 사용 (계수의 절댓값 합을 페널티로 부여)
- **일부 계수를 0으로 만듦** → 자동 변수 선택
- 희소(sparse) 모델 생성 가능

#### ➕ 장점
- 불필요한 변수를 제거하여 **모델 해석력 증가**
- 변수 선택 자동화
- 고차원 데이터(변수 > 샘플)에서 유리

#### ➖ 단점
- 상관관계 높은 변수들 중 일부만 선택될 수 있음 → 정보 손실 위험
- 상관관계가 큰 변수들이 많은 경우 성능 불안정


### ⚖️ 3. 엘라스틱넷 회귀 (Elastic Net)

#### ✅ 특징
- L1 + L2 정규화를 **혼합**한 방식
- 라쏘와 릿지의 장점을 결합
- 변수 선택 + 계수 축소 모두 가능

#### ➕ 장점
- 상관관계 높은 변수 그룹을 **함께 선택** 가능
- 라쏘보다 더 안정적인 성능
- 고차원 데이터나 다중공선성이 심할 때 적합

#### ➖ 단점
- 하이퍼파라미터가 2개 (\(\lambda\), \(\alpha\)) → **튜닝 필요**
- 구조가 다소 복잡할 수 있음


### 🧠 언제 어떤 모델을 쓸까?

| 상황 | 추천 모델 |
|------|-----------|
| 변수 간 상관관계가 높고 모두 유지하고 싶을 때 | **Ridge** |
| 변수 선택이 중요하고 해석력이 필요할 때 | **Lasso** |
| 변수 많고 상관관계도 높으며 안정성이 필요한 경우 | **Elastic Net** |


### 📊 정리 요약표

| 모델         | 정규화 방식 | 변수 선택 | 계수 축소 | 적합한 상황                      |
|--------------|-------------|------------|------------|----------------------------------|
| **Ridge**     | L2           | ❌         | ✅         | 변수 간 상관관계가 높은 경우       |
| **Lasso**     | L1           | ✅         | ✅         | 변수 선택이 중요한 경우           |
| **Elastic Net** | L1 + L2     | ✅         | ✅         | 고차원, 다중공선성이 있는 경우     |

```
import numpy as np
import pandas as pd
from sklearn.model_selection import  cross_val_score
from sklearn.linear_model import Ridge

import warnings
warnings.filterwarnings('ignore')

# 데이터 로딩
boston_df = pd.read_csv('./boston_house_prices.csv', encoding='utf-8', header=1)

# 타깃과 피처 분리
y_target = boston_df['MEDV']
X_data = boston_df.drop(['MEDV'], axis=1, inplace=False)

# 릿지 회귀 모델 및 교차 검증 MSE 평가
ridge = Ridge(alpha=10)
mse_scores = cross_val_score(
    ridge, X_data, y_target, scoring='neg_mean_squared_error', cv=5
)

# 5개의 평가 점수 나온다
print(f'mse_scores : {mse_scores}')
rsme_scores=np.sqrt(-1*mse_scores)
print(f'rsme_scores : {rsme_scores}')
avg_rsme=np.mean(rsme_scores)
print(f'rsme 평균 : {avg_rsme}')
```
![image](https://github.com/user-attachments/assets/55a44449-dbb0-4dca-a266-788a9527c76a)

```
import numpy as np
import pandas as pd
from sklearn.model_selection import  cross_val_score
from sklearn.linear_model import Ridge

import warnings
warnings.filterwarnings('ignore')

# 데이터 로딩
boston_df = pd.read_csv('./boston_house_prices.csv', encoding='utf-8', header=1)

# 타깃과 피처 분리
y_target = boston_df['MEDV']
X_data = boston_df.drop(['MEDV'], axis=1, inplace=False)

# 릿지 회귀 모델 및 교차 검증 MSE 평가
aplphas=[0,0.1,1,10,100]
for aplpha in aplphas:
    print('='*100)
    print(f'alpha가 {aplpha}인 경우')
    ridge = Ridge(alpha=aplpha)
    mse_scores = cross_val_score(
        ridge, X_data, y_target, scoring='neg_mean_squared_error', cv=5
    )
    # 5개의 평가 점수 나온다
    print(f'mse_scores : {mse_scores}')
    rsme_scores=np.sqrt(-1*mse_scores)
    print(f'rsme_scores : {rsme_scores}')
    avg_rsme=np.mean(rsme_scores)
    print(f'rsme 평균 : {avg_rsme}')
```
![image](https://github.com/user-attachments/assets/a0b4a32f-f570-42db-802f-7aea7d239c3e)

```
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
from scipy import stats
import warnings
warnings.filterwarnings('ignore')  #사이킷런 1.2 부터는 보스턴 주택가격 데이터가 없어진다는 warning 메시지 출력 제거


# boston 데이타셋 DataFrame 변환
bostonDF = pd.read_csv('./boston_house_prices.csv', header=1)


print('Boston 데이타셋 크기 :',bostonDF.shape)

# 컬럼명 변경
bostonDF.rename(
    columns={'MEDV':'PRICE'}
    , inplace=True
)

# 데이터프레임 안에 데이터와 레이블 분리
y_target=bostonDF['PRICE'] # 레이블 저장
X_data=bostonDF.drop(['PRICE'], axis=1, inplace=False)

# Ridge
print('='*50,'Ridge','='*50)
from sklearn.linear_model import Ridge
from sklearn.model_selection import cross_val_score

ridge=Ridge(alpha=10)
neg_mse_scores=cross_val_score(
      ridge                                     # 알고리즘
    , X_data                                    # 데이터
    , y_target                                  # 레이블
    , scoring='neg_mean_squared_error'          # 평가지표 지정
    , cv=5                                      # cv=fold cnt
)

print(f'neg_mse_scores : {neg_mse_scores}')

rmse_scores=np.sqrt(-1*neg_mse_scores)
print(f'rmse_scores : {rmse_scores}')

avg_rmse=np.mean(rmse_scores)
print(f'avg_rmse : {avg_rmse}')

# 릿지에 사용될 alpha 파라미터의 값을 정의
alphas = [0, 0.1, 1, 10, 100]


# alphas list 값을 반복하면서 alpha에 따른 평균 rmse를 구함.
for alpha in alphas :
    ridge = Ridge(alpha = alpha)
   
    # cross_val_score를 이용해 5 폴드의 평균 RMSE를 계산
    neg_mse_scores = cross_val_score(ridge, X_data, y_target, scoring="neg_mean_squared_error", cv = 5)
    avg_rmse = np.mean(np.sqrt(-1 * neg_mse_scores))
    print('alpha {0} 일 때 5 folds 의 평균 RMSE : {1:.3f} '.format(alpha, avg_rmse))

# 각 alpha에 따른 회귀 계수 값을 시각화하기 위해 5개의 열로 된 맷플롯립 축 생성  
fig , axs = plt.subplots(figsize=(18,6) , nrows=1 , ncols=5)
# 각 alpha에 따른 회귀 계수 값을 데이터로 저장하기 위한 DataFrame 생성  
coeff_df = pd.DataFrame()


# alphas 리스트 값을 차례로 입력해 회귀 계수 값 시각화 및 데이터 저장. pos는 axis의 위치 지정
for pos , alpha in enumerate(alphas) :
    ridge = Ridge(alpha = alpha)
    ridge.fit(X_data , y_target)
    # alpha에 따른 피처별 회귀 계수를 Series로 변환하고 이를 DataFrame의 컬럼으로 추가.  
    coeff = pd.Series(data=ridge.coef_ , index=X_data.columns )
    colname='alpha:'+str(alpha)
    coeff_df[colname] = coeff
    # 막대 그래프로 각 alpha 값에서의 회귀 계수를 시각화. 회귀 계수값이 높은 순으로 표현
    coeff = coeff.sort_values(ascending=False)
    axs[pos].set_title(colname)
    axs[pos].set_xlim(-3,6)
    sns.barplot(x=coeff.values , y=coeff.index, ax=axs[pos])
    

# for 문 바깥에서 맷플롯립의 show 호출 및 alpha에 따른 피처별 회귀 계수를 DataFrame으로 표시
plt.show()
```
![image](https://github.com/user-attachments/assets/e4b78f54-8a79-4181-8a87-f47180c4d9e6)

![image](https://github.com/user-attachments/assets/c427df4d-9331-4747-8119-e58532d3f9cc)

```
from sklearn.linear_model import Lasso, ElasticNet


# alpha값에 따른 회귀 모델의 폴드 평균 RMSE를 출력하고 회귀 계수값들을 DataFrame으로 반환
def get_linear_reg_eval(model_name, params=None, X_data_n=None, y_target_n=None,
                        verbose=True, return_coeff=True):
    coeff_df = pd.DataFrame()
    if verbose : print('####### ', model_name , '#######')
    for param in params:
        if model_name =='Ridge': model = Ridge(alpha=param)
        elif model_name =='Lasso': model = Lasso(alpha=param)
        elif model_name =='ElasticNet': model = ElasticNet(alpha=param, l1_ratio=0.7)
        neg_mse_scores = cross_val_score(model, X_data_n,
                                             y_target_n, scoring="neg_mean_squared_error", cv = 5)
        avg_rmse = np.mean(np.sqrt(-1 * neg_mse_scores))
        print('alpha {0}일 때 5 폴드 세트의 평균 RMSE: {1:.3f} '.format(param, avg_rmse))
        # cross_val_score는 evaluation metric만 반환하므로 모델을 다시 학습하여 회귀 계수 추출
       
        model.fit(X_data_n , y_target_n)
        if return_coeff:
            # alpha에 따른 피처별 회귀 계수를 Series로 변환하고 이를 DataFrame의 컬럼으로 추가.
            coeff = pd.Series(data=model.coef_ , index=X_data_n.columns )
            colname='alpha:'+str(param)
            coeff_df[colname] = coeff
   
    return coeff_df
# end of get_linear_regre_eval
```
```
# 라쏘에 사용될 alpha 파라미터의 값들을 정의하고 get_linear_reg_eval() 함수 호출
lasso_alphas = [ 0.07, 0.1, 0.5, 1, 3]
coeff_lasso_df =get_linear_reg_eval('Lasso', params=lasso_alphas, X_data_n=X_data, y_target_n=y_target)
```
![image](https://github.com/user-attachments/assets/4b62e483-415d-480c-807b-4f9a5d962970)

```
# 반환된 coeff_lasso_df를 첫번째 컬럼순으로 내림차순 정렬하여 회귀계수 DataFrame출력
sort_column = 'alpha:'+str(lasso_alphas[0])
coeff_lasso_df.sort_values(by=sort_column, ascending=False)
```
![image](https://github.com/user-attachments/assets/c743e08a-ecd9-431b-a0c2-4534fa3bd9fa)

```
# 엘라스틱넷에 사용될 alpha 파라미터의 값들을 정의하고 get_linear_reg_eval() 함수 호출
# l1_ratio는 0.7로 고정
elastic_alphas = [ 0.07, 0.1, 0.5, 1, 3]
coeff_elastic_df =get_linear_reg_eval('ElasticNet', params=elastic_alphas,
                                      X_data_n=X_data, y_target_n=y_target)
```
![image](https://github.com/user-attachments/assets/3438dc5b-8105-46d7-9af2-ba6df69595a6)

```
# 반환된 coeff_elastic_df를 첫번째 컬럼순으로 내림차순 정렬하여 회귀계수 DataFrame출력
sort_column = 'alpha:'+str(elastic_alphas[0])
coeff_elastic_df.sort_values(by=sort_column, ascending=False)
```
![image](https://github.com/user-attachments/assets/d7dec8ea-b28a-420b-bef5-dec3c394717b)
