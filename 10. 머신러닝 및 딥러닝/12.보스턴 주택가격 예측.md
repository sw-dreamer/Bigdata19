```
import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns

from scipy import stats

import warnings
warnings.filterwarnings('ignore')
```
```
# 보스톤 주택 가격(연속값) 예측(회귀모델)
# 데이터 로딩
boston_df=pd.read_csv(
    './boston_house_prices.csv'
    ,encoding='utf-8'
    ,header=1
)

boston_df.head()
```
![image](https://github.com/user-attachments/assets/7f014ca8-1bd8-4847-8c6f-65497276e7a5)

```
boston_df.info()
```
![image](https://github.com/user-attachments/assets/424e5e2e-0daf-492e-9cf8-4a6cef524c89)

```
boston_df.describe()
```
![image](https://github.com/user-attachments/assets/2fd74ec5-d3ec-45b1-a0e4-1f0ea629233d)

```
boston_df.columns
```
![image](https://github.com/user-attachments/assets/9e4c89a0-c924-4e24-aace-5e56273a5f31)

```
fig,axs=plt.subplots(
    figsize=(16,8)
    ,ncols=4
    ,nrows=2
)
lm_feautres=['RM','ZN','INDUS','NOX','AGE','PTRATIO','LSTAT','RAD']

for i, feature in enumerate(lm_feautres):
    row =int(i/4)
    col = i%4
    # 사본의 regplot을 이용해 산점도와 선형 회귀 직선을 함께 표현
    sns.regplot(
        x=feature
        ,y='MEDV'
        ,data=boston_df
        ,ax=axs[row][col]
    )
plt.show()
```
![image](https://github.com/user-attachments/assets/f748c32a-dbb7-4943-85e8-812366f72759)

```
# 2개의 행과 4개의 열을 가진 subplots를 이용. axs는 4x2개의 ax를 가짐.
fig, axs = plt.subplots(figsize=(16,8) , ncols=4 , nrows=2)
lm_features = ['RM','ZN','INDUS','NOX','AGE','PTRATIO','LSTAT','RAD']
for i , feature in enumerate(lm_features):
    row = int(i/4)
    col = i%4
    # 시본의 regplot을 이용해 산점도와 선형 회귀 직선을 함께 표현
    sns.regplot(x=feature , y='MEDV',data=boston_df , ax=axs[row][col])


# 이미지 파일 저장
fig1 = plt.gcf()
fig1.savefig('p322_boston.tif', format='tif', dpi=300, bbox_inches='tight')
```

```
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# 데이터 레이블 분리
y_target=boston_df['MEDV']
X_data=boston_df.drop(['MEDV'],axis=1,inplace=False)# False : 삭제한 DF를 반환

```

```
# 학습/테스트 데이터 분리
X_train,X_test,y_train,y_test=train_test_split(
    X_data
    ,y_target
    ,test_size=0.3
    ,random_state=0
)
```
```
# LinearRegression object create
lr =LinearRegression()
lr.fit(X_train,y_train) # 학습
y_pred=lr.predict(X_test) # 테스트 데이터로 예측값 출력

# 오차(원래답-예측값) : MSE
mse=mean_squared_error(y_test,y_pred)
rmse=np.sqrt(mse) # 제곱근
print(f'MSE : {mse}, RMSE : {rmse}')
print(f'Variance score : {r2_score(y_test,y_pred)}') # 모델의 설명력
```
![image](https://github.com/user-attachments/assets/d90e433c-923c-4b64-a35b-e53feece208b)

```
# 피쳐의 영향력 : 개수 값을 출력
print(f'회귀 계수 값 " {lr.coef_}')
```
![image](https://github.com/user-attachments/assets/3ff69b2f-cefb-412b-a984-8dbab38950ea)

```
coeff=pd.Series(
    data=np.round(lr.coef_,1)
    ,index=X_data.columns
)
coeff
```
![image](https://github.com/user-attachments/assets/e3023479-6964-46fc-ac79-ddfba815a9fa)

```
coeff=pd.Series(
    data=np.round(lr.coef_,1)
    ,index=X_data.columns
)
coeff.sort_values(ascending=False)
```
![image](https://github.com/user-attachments/assets/009b9f84-ccd0-405a-97d3-8ce1057e7645)

```
X_data.info()
```
![image](https://github.com/user-attachments/assets/9e6ed5f3-cf8c-4908-b400-9ff26eec2c76)

```
y_target.info()
```
![image](https://github.com/user-attachments/assets/69e1bcf0-b2c1-4acc-9b93-ac98dbc95dad)

```
pip install statsmodels
```
![image](https://github.com/user-attachments/assets/b6f8296e-102a-48eb-8ba9-156f3e73e00c)

```
# 상수항 추가
import statsmodels.api as sm

X=sm.add_constant(X_data)
X
```
![image](https://github.com/user-attachments/assets/3896f3de-5fa5-4795-8fdc-fa887f7dd90c)

```
# 상수항 추가
import statsmodels.api as sm

X=sm.add_constant(X_data)

# OLS(회구 모델)
model=sm.OLS(y_target,X)

# 학습
result=model.fit()

# 결과 출력
print(result.summary())
```
![image](https://github.com/user-attachments/assets/10a9e995-91bd-458c-8936-96fdb4d67e53)

---
## OLS 회귀 분석 결과 해석

### 주요 지표

- **R-squared (결정계수)**: 0.741  
  이 값은 독립변수들이 종속변수(MEDV, 주택의 중간 가격)를 약 74.1% 정도 설명한다는 의미입니다. 즉, 모델이 데이터의 약 74.1%의 변동성을 설명할 수 있다는 것입니다.

- **Adjusted R-squared (수정된 결정계수)**: 0.734  
  이 값은 R-squared의 수정 버전으로, 변수의 수를 고려하여 모델의 적합도를 평가합니다. 모델이 상대적으로 잘 맞는 것을 보여줍니다.

- **F-statistic**: 108.1, **Prob(F-statistic)**: 6.72e-135  
  F-통계량은 모델의 전체 유의미성을 평가하는 값입니다. 값이 매우 크고, p-value가 매우 작기 때문에 이 회귀모델은 통계적으로 유의미하다고 할 수 있습니다.

- **AIC (Akaike Information Criterion)**: 3026, **BIC (Bayesian Information Criterion)**: 3085  
  AIC와 BIC 값은 모델 선택 시 여러 모델을 비교할 때 사용됩니다. 값이 낮을수록 모델이 더 좋다고 할 수 있지만, 두 값만으로 모델이 좋은지 나쁜지 확실히 판단하기는 어렵습니다.

### 각 변수의 해석

```
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                   MEDV   R-squared:                       0.741
Model:                            OLS   Adj. R-squared:                  0.734
Method:                 Least Squares   F-statistic:                     108.1
Date:                Thu, 10 Apr 2025   Prob (F-statistic):          6.72e-135
Time:                        12:32:24   Log-Likelihood:                -1498.8
No. Observations:                 506   AIC:                             3026.
Df Residuals:                     492   BIC:                             3085.
Df Model:                          13                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const         36.4595      5.103      7.144      0.000      26.432      46.487
CRIM          -0.1080      0.033     -3.287      0.001      -0.173      -0.043
ZN             0.0464      0.014      3.382      0.001       0.019       0.073
INDUS          0.0206      0.061      0.334      0.738      -0.100       0.141
CHAS           2.6867      0.862      3.118      0.002       0.994       4.380
NOX          -17.7666      3.820     -4.651      0.000     -25.272     -10.262
RM             3.8099      0.418      9.116      0.000       2.989       4.631
AGE            0.0007      0.013      0.052      0.958      -0.025       0.027
DIS           -1.4756      0.199     -7.398      0.000      -1.867      -1.084
RAD            0.3060      0.066      4.613      0.000       0.176       0.436
TAX           -0.0123      0.004     -3.280      0.001      -0.020      -0.005
PTRATIO       -0.9527      0.131     -7.283      0.000      -1.210      -0.696
B              0.0093      0.003      3.467      0.001       0.004       0.015
LSTAT         -0.5248      0.051    -10.347      0.000      -0.624      -0.425
==============================================================================
Omnibus:                      178.041   Durbin-Watson:                   1.078
Prob(Omnibus):                  0.000   Jarque-Bera (JB):              783.126
Skew:                           1.521   Prob(JB):                    8.84e-171
Kurtosis:                       8.281   Cond. No.                     1.51e+04
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The condition number is large, 1.51e+04. This might indicate that there are
strong multicollinearity or other numerical problems.

```


### 해석

#### 중요한 변수들:
- **CRIM (범죄율)**:  
  범죄율이 1단위 증가할 때, MEDV는 약 0.108 단위 감소합니다. p-value가 0.001로 매우 유의미하여, 범죄율이 주택 가격에 부정적인 영향을 미친다고 할 수 있습니다.

- **ZN (주거지 비율)**:  
  주거지 비율이 1단위 증가할 때, MEDV는 약 0.046 단위 증가합니다. p-value가 0.001로 유의미하여, 주거지 비율이 높을수록 주택 가격이 높아진다는 결과입니다.

- **NOX (일산화질소 농도)**:  
  일산화질소 농도가 1단위 증가할 때, MEDV는 약 17.77 단위 감소합니다. p-value가 0.000으로 매우 유의미합니다. 공기 오염이 주택 가격에 부정적인 영향을 미친다고 해석할 수 있습니다.

- **RM (평균 방 수)**:  
  방 수가 1개 증가할 때, MEDV는 약 3.81 단위 증가합니다. p-value가 0.000으로 매우 유의미합니다. 방 수가 많을수록 주택 가격이 높다는 해석이 가능합니다.

- **LSTAT (하위 계층 비율)**:  
  하위 계층 비율이 1% 증가할 때, MEDV는 약 0.525 단위 감소합니다. p-value가 0.000으로 매우 유의미합니다. 하위 계층 비율이 높을수록 주택 가격이 낮다는 결과입니다.

#### 유의미하지 않은 변수:
- **AGE (주택 연령)**:  
  주택의 연령이 1년 증가할 때, MEDV는 약 0.0007 단위 증가합니다. 그러나 p-value가 0.958로 매우 높아 이 변수는 통계적으로 유의미하지 않다고 볼 수 있습니다.

- **INDUS (상업지역 비율)**:  
  상업지역 비율이 1단위 증가할 때, MEDV는 약 0.0206 단위 증가하지만, p-value가 0.738로 매우 높아 이 변수 역시 통계적으로 유의미하지 않습니다.

### 결론

이 모델은 다양한 독립변수들이 주택의 중간 가격(MEDV)에 미치는 영향을 평가합니다. 주요 변수 중 **CRIM(범죄율)**, **NOX(일산화질소 농도)**, **RM(평균 방 수)**, **DIS(고용 센터와의 거리)**, **TAX(재산세)**, **PTRATIO(학생-교사 비율)**, **LSTAT(하위 계층 비율)** 등이 주택 가격에 유의미한 영향을 미치고 있습니다. 반면, **AGE**와 **INDUS**는 통계적으로 유의미한 영향을 미치지 않습니다.

---
## 다중 공선성(Multicollinearity)

**다중 공선성**은 회귀 분석에서 독립 변수들(피쳐) 간에 상관 관계가 매우 높을 때 발생하는 문제입니다. 이는 모델의 **계수 불안정성**, **해석의 어려움**, 그리고 **모델 성능 저하**를 초래할 수 있습니다.

### 다중 공선성의 문제점
1. **피쳐 간 상관계수가 매우 클 때 발생**:
   - 독립 변수들 간에 높은 상관 관계가 있을 경우, 중복된 정보를 다루게 되어 모델이 불안정해집니다.
   
2. **계수의 불안정성**:
   - 다중 공선성으로 인해 각 독립 변수에 대한 계수(β 값)가 불안정해지고, 데이터의 작은 변화에도 계수가 크게 달라질 수 있습니다.
   
3. **모델 해석의 어려움**:
   - 독립 변수 간의 강한 상관 관계로 인해, 개별 변수의 중요도를 파악하기 어렵고 변수 간 영향을 분리하기 어려워집니다.

4. **모델 성능 저하**:
   - 다중 공선성은 과적합(overfitting)을 유발할 수 있어 모델의 일반화 능력(새로운 데이터에 대한 예측 성능)을 떨어뜨립니다.

## 다중 공선성 해결 방법
1. **VIF(Variance Inflation Factor)**:
   - VIF를 통해 각 변수의 다중 공선성을 확인하고, VIF 값이 높은 변수를 제거하거나 변형할 수 있습니다.

2. **주성분 분석(PCA)**:
   - 주성분 분석(PCA)을 사용하여 상관 관계가 높은 변수들을 결합하여 새로운 독립 변수를 생성할 수 있습니다.

3. **정규화 기법**:
   - **Ridge 회귀**나 **Lasso 회귀**와 같은 정규화 기법을 사용하면 다중 공선성 문제를 완화할 수 있습니다.

```
# 다중 공선성 : 피쳐 간 상관계수가 너무 클 때 발생, 계수의 불안정과 해석의 어려움 발생
# 결론적으로 모델 성능 저하
from statsmodels.stats.outliers_influence import  variance_inflation_factor

vif=pd.DataFrame() # 빈 데이터프레임 생성
vif['feature']=X.columns
vif['VIF']=[
    variance_inflation_factor(X.values, idx) for idx in range(X.shape[1])
]
vif
```
![image](https://github.com/user-attachments/assets/2eeb93c1-561b-4692-b58b-36295046a1eb)

## VIF (Variance Inflation Factor) 해석

VIF(Variance Inflation Factor)는 다중 공선성을 확인하기 위해 사용됩니다. 일반적으로 VIF 값이 **10 이상**인 변수는 다중 공선성 문제가 있을 수 있습니다. 아래는 각 변수에 대한 VIF 값입니다.

| 변수      | VIF        |
|-----------|------------|
| **const** | 585.27     |
| **CRIM**  | 1.79       |
| **ZN**    | 2.30       |
| **INDUS** | 3.99       |
| **CHAS**  | 1.07       |
| **NOX**   | 4.39       |
| **RM**    | 1.93       |
| **AGE**   | 3.10       |
| **DIS**   | 3.96       |
| **RAD**   | 7.48       |
| **TAX**   | 9.01       |
| **PTRATIO**| 1.80      |
| **B**     | 1.35       |
| **LSTAT** | 2.94       |

### 참고
- vif value : 1~5이면 정상
- vif value : 5~10 약한 다음 공선성을 가지고 있다. 주의 필요
- vif value : 10이상 심한 다중 공선성, 해당 피처를 제거 필요

### 해석:
- **VIF 값이 1에 가까운 변수들**: `CRIM`, `ZN`, `CHAS`, `RM`, `PTRATIO`, `B`는 다른 변수들과 상관 관계가 적어 다중 공선성 문제가 적습니다.
- **VIF 값이 10 이상인 변수들**: `RAD`와 `TAX`는 다중 공선성 문제를 유발할 가능성이 있으며, 이를 해결하기 위해 추가적인 조치가 필요할 수 있습니다.

### 다중 공선성 해결 방법:
- VIF 값이 높은 변수는 제거하거나, **주성분 분석(PCA)** 또는 **정규화 기법**(예: **Ridge** 회귀, **Lasso** 회귀)을 사용하여 다중 공선성 문제를 완화할 수 있습니다.

---
## 다항 회귀와 과(대)적합/과소적합 이해

```
# 다항식
from sklearn.preprocessing import PolynomialFeatures

# 예제 데이터 생성 [2x2]
X=np.arange(4).reshape(2,2)
print(f'일차 단항식 계수 피쳐\n{X}')

# degree = 2인 2차 다항식으로 변환하기 위해 PolynomialFeatures를 이용해 변환
poly=PolynomialFeatures(degree=2)
poly.fit(X)
poly_ftr=poly.transform(X)
print(f'변환된 2차 다항식 계수 피쳐:\n{poly_ftr}')
```
![image](https://github.com/user-attachments/assets/145bcad5-8de5-43b4-bfc6-bc2a3b02f4d8)

---
## 보스톤 주택 데이터를 가지고 다항식으로 변환 후 처리

