```
import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns

from scipy import stats

import warnings
warnings.filterwarnings('ignore')
```
```
# ë³´ìŠ¤í†¤ ì£¼íƒ ê°€ê²©(ì—°ì†ê°’) ì˜ˆì¸¡(íšŒê·€ëª¨ë¸)
# ë°ì´í„° ë¡œë”©
boston_df=pd.read_csv(
    './boston_house_prices.csv'
    ,encoding='utf-8'
    ,header=1
)

boston_df.head()
```
![image](https://github.com/user-attachments/assets/7f014ca8-1bd8-4847-8c6f-65497276e7a5)

```
boston_df.info()
```
![image](https://github.com/user-attachments/assets/424e5e2e-0daf-492e-9cf8-4a6cef524c89)

```
boston_df.describe()
```
![image](https://github.com/user-attachments/assets/2fd74ec5-d3ec-45b1-a0e4-1f0ea629233d)

```
boston_df.columns
```
![image](https://github.com/user-attachments/assets/9e4c89a0-c924-4e24-aace-5e56273a5f31)

```
fig,axs=plt.subplots(
    figsize=(16,8)
    ,ncols=4
    ,nrows=2
)
lm_feautres=['RM','ZN','INDUS','NOX','AGE','PTRATIO','LSTAT','RAD']

for i, feature in enumerate(lm_feautres):
    row =int(i/4)
    col = i%4
    # ì‚¬ë³¸ì˜ regplotì„ ì´ìš©í•´ ì‚°ì ë„ì™€ ì„ í˜• íšŒê·€ ì§ì„ ì„ í•¨ê»˜ í‘œí˜„
    sns.regplot(
        x=feature
        ,y='MEDV'
        ,data=boston_df
        ,ax=axs[row][col]
    )
plt.show()
```
![image](https://github.com/user-attachments/assets/f748c32a-dbb7-4943-85e8-812366f72759)

```
# 2ê°œì˜ í–‰ê³¼ 4ê°œì˜ ì—´ì„ ê°€ì§„ subplotsë¥¼ ì´ìš©. axsëŠ” 4x2ê°œì˜ axë¥¼ ê°€ì§.
fig, axs = plt.subplots(figsize=(16,8) , ncols=4 , nrows=2)
lm_features = ['RM','ZN','INDUS','NOX','AGE','PTRATIO','LSTAT','RAD']
for i , feature in enumerate(lm_features):
    row = int(i/4)
    col = i%4
    # ì‹œë³¸ì˜ regplotì„ ì´ìš©í•´ ì‚°ì ë„ì™€ ì„ í˜• íšŒê·€ ì§ì„ ì„ í•¨ê»˜ í‘œí˜„
    sns.regplot(x=feature , y='MEDV',data=boston_df , ax=axs[row][col])


# ì´ë¯¸ì§€ íŒŒì¼ ì €ì¥
fig1 = plt.gcf()
fig1.savefig('p322_boston.tif', format='tif', dpi=300, bbox_inches='tight')
```

```
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# ë°ì´í„° ë ˆì´ë¸” ë¶„ë¦¬
y_target=boston_df['MEDV']
X_data=boston_df.drop(['MEDV'],axis=1,inplace=False)# False : ì‚­ì œí•œ DFë¥¼ ë°˜í™˜

```

```
# í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„ë¦¬
X_train,X_test,y_train,y_test=train_test_split(
    X_data
    ,y_target
    ,test_size=0.3
    ,random_state=0
)
```
```
# LinearRegression object create
lr =LinearRegression()
lr.fit(X_train,y_train) # í•™ìŠµ
y_pred=lr.predict(X_test) # í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ì˜ˆì¸¡ê°’ ì¶œë ¥

# ì˜¤ì°¨(ì›ë˜ë‹µ-ì˜ˆì¸¡ê°’) : MSE
mse=mean_squared_error(y_test,y_pred)
rmse=np.sqrt(mse) # ì œê³±ê·¼
print(f'MSE : {mse}, RMSE : {rmse}')
print(f'Variance score : {r2_score(y_test,y_pred)}') # ëª¨ë¸ì˜ ì„¤ëª…ë ¥
```
![image](https://github.com/user-attachments/assets/d90e433c-923c-4b64-a35b-e53feece208b)

```
# í”¼ì³ì˜ ì˜í–¥ë ¥ : ê°œìˆ˜ ê°’ì„ ì¶œë ¥
print(f'íšŒê·€ ê³„ìˆ˜ ê°’ " {lr.coef_}')
```
![image](https://github.com/user-attachments/assets/3ff69b2f-cefb-412b-a984-8dbab38950ea)

```
coeff=pd.Series(
    data=np.round(lr.coef_,1)
    ,index=X_data.columns
)
coeff
```
![image](https://github.com/user-attachments/assets/e3023479-6964-46fc-ac79-ddfba815a9fa)

```
coeff=pd.Series(
    data=np.round(lr.coef_,1)
    ,index=X_data.columns
)
coeff.sort_values(ascending=False)
```
![image](https://github.com/user-attachments/assets/009b9f84-ccd0-405a-97d3-8ce1057e7645)

```
X_data.info()
```
![image](https://github.com/user-attachments/assets/9e6ed5f3-cf8c-4908-b400-9ff26eec2c76)

```
y_target.info()
```
![image](https://github.com/user-attachments/assets/69e1bcf0-b2c1-4acc-9b93-ac98dbc95dad)

```
pip install statsmodels
```
![image](https://github.com/user-attachments/assets/b6f8296e-102a-48eb-8ba9-156f3e73e00c)

```
# ìƒìˆ˜í•­ ì¶”ê°€
import statsmodels.api as sm

X=sm.add_constant(X_data)
X
```
![image](https://github.com/user-attachments/assets/3896f3de-5fa5-4795-8fdc-fa887f7dd90c)

```
# ìƒìˆ˜í•­ ì¶”ê°€
import statsmodels.api as sm

X=sm.add_constant(X_data)

# OLS(íšŒêµ¬ ëª¨ë¸)
model=sm.OLS(y_target,X)

# í•™ìŠµ
result=model.fit()

# ê²°ê³¼ ì¶œë ¥
print(result.summary())
```
![image](https://github.com/user-attachments/assets/10a9e995-91bd-458c-8936-96fdb4d67e53)

---
## OLS íšŒê·€ ë¶„ì„ ê²°ê³¼ í•´ì„

### ì£¼ìš” ì§€í‘œ

- **R-squared (ê²°ì •ê³„ìˆ˜)**: 0.741  
  ì´ ê°’ì€ ë…ë¦½ë³€ìˆ˜ë“¤ì´ ì¢…ì†ë³€ìˆ˜(MEDV, ì£¼íƒì˜ ì¤‘ê°„ ê°€ê²©)ë¥¼ ì•½ 74.1% ì •ë„ ì„¤ëª…í•œë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤. ì¦‰, ëª¨ë¸ì´ ë°ì´í„°ì˜ ì•½ 74.1%ì˜ ë³€ë™ì„±ì„ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤.

- **Adjusted R-squared (ìˆ˜ì •ëœ ê²°ì •ê³„ìˆ˜)**: 0.734  
  ì´ ê°’ì€ R-squaredì˜ ìˆ˜ì • ë²„ì „ìœ¼ë¡œ, ë³€ìˆ˜ì˜ ìˆ˜ë¥¼ ê³ ë ¤í•˜ì—¬ ëª¨ë¸ì˜ ì í•©ë„ë¥¼ í‰ê°€í•©ë‹ˆë‹¤. ëª¨ë¸ì´ ìƒëŒ€ì ìœ¼ë¡œ ì˜ ë§ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

- **F-statistic**: 108.1, **Prob(F-statistic)**: 6.72e-135  
  F-í†µê³„ëŸ‰ì€ ëª¨ë¸ì˜ ì „ì²´ ìœ ì˜ë¯¸ì„±ì„ í‰ê°€í•˜ëŠ” ê°’ì…ë‹ˆë‹¤. ê°’ì´ ë§¤ìš° í¬ê³ , p-valueê°€ ë§¤ìš° ì‘ê¸° ë•Œë¬¸ì— ì´ íšŒê·€ëª¨ë¸ì€ í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•˜ë‹¤ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

- **AIC (Akaike Information Criterion)**: 3026, **BIC (Bayesian Information Criterion)**: 3085  
  AICì™€ BIC ê°’ì€ ëª¨ë¸ ì„ íƒ ì‹œ ì—¬ëŸ¬ ëª¨ë¸ì„ ë¹„êµí•  ë•Œ ì‚¬ìš©ë©ë‹ˆë‹¤. ê°’ì´ ë‚®ì„ìˆ˜ë¡ ëª¨ë¸ì´ ë” ì¢‹ë‹¤ê³  í•  ìˆ˜ ìˆì§€ë§Œ, ë‘ ê°’ë§Œìœ¼ë¡œ ëª¨ë¸ì´ ì¢‹ì€ì§€ ë‚˜ìœì§€ í™•ì‹¤íˆ íŒë‹¨í•˜ê¸°ëŠ” ì–´ë µìŠµë‹ˆë‹¤.

### ê° ë³€ìˆ˜ì˜ í•´ì„

```
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                   MEDV   R-squared:                       0.741
Model:                            OLS   Adj. R-squared:                  0.734
Method:                 Least Squares   F-statistic:                     108.1
Date:                Thu, 10 Apr 2025   Prob (F-statistic):          6.72e-135
Time:                        12:32:24   Log-Likelihood:                -1498.8
No. Observations:                 506   AIC:                             3026.
Df Residuals:                     492   BIC:                             3085.
Df Model:                          13                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const         36.4595      5.103      7.144      0.000      26.432      46.487
CRIM          -0.1080      0.033     -3.287      0.001      -0.173      -0.043
ZN             0.0464      0.014      3.382      0.001       0.019       0.073
INDUS          0.0206      0.061      0.334      0.738      -0.100       0.141
CHAS           2.6867      0.862      3.118      0.002       0.994       4.380
NOX          -17.7666      3.820     -4.651      0.000     -25.272     -10.262
RM             3.8099      0.418      9.116      0.000       2.989       4.631
AGE            0.0007      0.013      0.052      0.958      -0.025       0.027
DIS           -1.4756      0.199     -7.398      0.000      -1.867      -1.084
RAD            0.3060      0.066      4.613      0.000       0.176       0.436
TAX           -0.0123      0.004     -3.280      0.001      -0.020      -0.005
PTRATIO       -0.9527      0.131     -7.283      0.000      -1.210      -0.696
B              0.0093      0.003      3.467      0.001       0.004       0.015
LSTAT         -0.5248      0.051    -10.347      0.000      -0.624      -0.425
==============================================================================
Omnibus:                      178.041   Durbin-Watson:                   1.078
Prob(Omnibus):                  0.000   Jarque-Bera (JB):              783.126
Skew:                           1.521   Prob(JB):                    8.84e-171
Kurtosis:                       8.281   Cond. No.                     1.51e+04
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The condition number is large, 1.51e+04. This might indicate that there are
strong multicollinearity or other numerical problems.

```


### í•´ì„

#### ì¤‘ìš”í•œ ë³€ìˆ˜ë“¤:
- **CRIM (ë²”ì£„ìœ¨)**:  
  ë²”ì£„ìœ¨ì´ 1ë‹¨ìœ„ ì¦ê°€í•  ë•Œ, MEDVëŠ” ì•½ 0.108 ë‹¨ìœ„ ê°ì†Œí•©ë‹ˆë‹¤. p-valueê°€ 0.001ë¡œ ë§¤ìš° ìœ ì˜ë¯¸í•˜ì—¬, ë²”ì£„ìœ¨ì´ ì£¼íƒ ê°€ê²©ì— ë¶€ì •ì ì¸ ì˜í–¥ì„ ë¯¸ì¹œë‹¤ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

- **ZN (ì£¼ê±°ì§€ ë¹„ìœ¨)**:  
  ì£¼ê±°ì§€ ë¹„ìœ¨ì´ 1ë‹¨ìœ„ ì¦ê°€í•  ë•Œ, MEDVëŠ” ì•½ 0.046 ë‹¨ìœ„ ì¦ê°€í•©ë‹ˆë‹¤. p-valueê°€ 0.001ë¡œ ìœ ì˜ë¯¸í•˜ì—¬, ì£¼ê±°ì§€ ë¹„ìœ¨ì´ ë†’ì„ìˆ˜ë¡ ì£¼íƒ ê°€ê²©ì´ ë†’ì•„ì§„ë‹¤ëŠ” ê²°ê³¼ì…ë‹ˆë‹¤.

- **NOX (ì¼ì‚°í™”ì§ˆì†Œ ë†ë„)**:  
  ì¼ì‚°í™”ì§ˆì†Œ ë†ë„ê°€ 1ë‹¨ìœ„ ì¦ê°€í•  ë•Œ, MEDVëŠ” ì•½ 17.77 ë‹¨ìœ„ ê°ì†Œí•©ë‹ˆë‹¤. p-valueê°€ 0.000ìœ¼ë¡œ ë§¤ìš° ìœ ì˜ë¯¸í•©ë‹ˆë‹¤. ê³µê¸° ì˜¤ì—¼ì´ ì£¼íƒ ê°€ê²©ì— ë¶€ì •ì ì¸ ì˜í–¥ì„ ë¯¸ì¹œë‹¤ê³  í•´ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

- **RM (í‰ê·  ë°© ìˆ˜)**:  
  ë°© ìˆ˜ê°€ 1ê°œ ì¦ê°€í•  ë•Œ, MEDVëŠ” ì•½ 3.81 ë‹¨ìœ„ ì¦ê°€í•©ë‹ˆë‹¤. p-valueê°€ 0.000ìœ¼ë¡œ ë§¤ìš° ìœ ì˜ë¯¸í•©ë‹ˆë‹¤. ë°© ìˆ˜ê°€ ë§ì„ìˆ˜ë¡ ì£¼íƒ ê°€ê²©ì´ ë†’ë‹¤ëŠ” í•´ì„ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.

- **LSTAT (í•˜ìœ„ ê³„ì¸µ ë¹„ìœ¨)**:  
  í•˜ìœ„ ê³„ì¸µ ë¹„ìœ¨ì´ 1% ì¦ê°€í•  ë•Œ, MEDVëŠ” ì•½ 0.525 ë‹¨ìœ„ ê°ì†Œí•©ë‹ˆë‹¤. p-valueê°€ 0.000ìœ¼ë¡œ ë§¤ìš° ìœ ì˜ë¯¸í•©ë‹ˆë‹¤. í•˜ìœ„ ê³„ì¸µ ë¹„ìœ¨ì´ ë†’ì„ìˆ˜ë¡ ì£¼íƒ ê°€ê²©ì´ ë‚®ë‹¤ëŠ” ê²°ê³¼ì…ë‹ˆë‹¤.

#### ìœ ì˜ë¯¸í•˜ì§€ ì•Šì€ ë³€ìˆ˜:
- **AGE (ì£¼íƒ ì—°ë ¹)**:  
  ì£¼íƒì˜ ì—°ë ¹ì´ 1ë…„ ì¦ê°€í•  ë•Œ, MEDVëŠ” ì•½ 0.0007 ë‹¨ìœ„ ì¦ê°€í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ p-valueê°€ 0.958ë¡œ ë§¤ìš° ë†’ì•„ ì´ ë³€ìˆ˜ëŠ” í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•˜ì§€ ì•Šë‹¤ê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

- **INDUS (ìƒì—…ì§€ì—­ ë¹„ìœ¨)**:  
  ìƒì—…ì§€ì—­ ë¹„ìœ¨ì´ 1ë‹¨ìœ„ ì¦ê°€í•  ë•Œ, MEDVëŠ” ì•½ 0.0206 ë‹¨ìœ„ ì¦ê°€í•˜ì§€ë§Œ, p-valueê°€ 0.738ë¡œ ë§¤ìš° ë†’ì•„ ì´ ë³€ìˆ˜ ì—­ì‹œ í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

### ê²°ë¡ 

ì´ ëª¨ë¸ì€ ë‹¤ì–‘í•œ ë…ë¦½ë³€ìˆ˜ë“¤ì´ ì£¼íƒì˜ ì¤‘ê°„ ê°€ê²©(MEDV)ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ í‰ê°€í•©ë‹ˆë‹¤. ì£¼ìš” ë³€ìˆ˜ ì¤‘ **CRIM(ë²”ì£„ìœ¨)**, **NOX(ì¼ì‚°í™”ì§ˆì†Œ ë†ë„)**, **RM(í‰ê·  ë°© ìˆ˜)**, **DIS(ê³ ìš© ì„¼í„°ì™€ì˜ ê±°ë¦¬)**, **TAX(ì¬ì‚°ì„¸)**, **PTRATIO(í•™ìƒ-êµì‚¬ ë¹„ìœ¨)**, **LSTAT(í•˜ìœ„ ê³„ì¸µ ë¹„ìœ¨)** ë“±ì´ ì£¼íƒ ê°€ê²©ì— ìœ ì˜ë¯¸í•œ ì˜í–¥ì„ ë¯¸ì¹˜ê³  ìˆìŠµë‹ˆë‹¤. ë°˜ë©´, **AGE**ì™€ **INDUS**ëŠ” í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•œ ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

---
## ë‹¤ì¤‘ ê³µì„ ì„±(Multicollinearity)

**ë‹¤ì¤‘ ê³µì„ ì„±**ì€ íšŒê·€ ë¶„ì„ì—ì„œ ë…ë¦½ ë³€ìˆ˜ë“¤(í”¼ì³) ê°„ì— ìƒê´€ ê´€ê³„ê°€ ë§¤ìš° ë†’ì„ ë•Œ ë°œìƒí•˜ëŠ” ë¬¸ì œì…ë‹ˆë‹¤. ì´ëŠ” ëª¨ë¸ì˜ **ê³„ìˆ˜ ë¶ˆì•ˆì •ì„±**, **í•´ì„ì˜ ì–´ë ¤ì›€**, ê·¸ë¦¬ê³  **ëª¨ë¸ ì„±ëŠ¥ ì €í•˜**ë¥¼ ì´ˆë˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ë‹¤ì¤‘ ê³µì„ ì„±ì˜ ë¬¸ì œì 
1. **í”¼ì³ ê°„ ìƒê´€ê³„ìˆ˜ê°€ ë§¤ìš° í´ ë•Œ ë°œìƒ**:
   - ë…ë¦½ ë³€ìˆ˜ë“¤ ê°„ì— ë†’ì€ ìƒê´€ ê´€ê³„ê°€ ìˆì„ ê²½ìš°, ì¤‘ë³µëœ ì •ë³´ë¥¼ ë‹¤ë£¨ê²Œ ë˜ì–´ ëª¨ë¸ì´ ë¶ˆì•ˆì •í•´ì§‘ë‹ˆë‹¤.
   
2. **ê³„ìˆ˜ì˜ ë¶ˆì•ˆì •ì„±**:
   - ë‹¤ì¤‘ ê³µì„ ì„±ìœ¼ë¡œ ì¸í•´ ê° ë…ë¦½ ë³€ìˆ˜ì— ëŒ€í•œ ê³„ìˆ˜(Î² ê°’)ê°€ ë¶ˆì•ˆì •í•´ì§€ê³ , ë°ì´í„°ì˜ ì‘ì€ ë³€í™”ì—ë„ ê³„ìˆ˜ê°€ í¬ê²Œ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
   
3. **ëª¨ë¸ í•´ì„ì˜ ì–´ë ¤ì›€**:
   - ë…ë¦½ ë³€ìˆ˜ ê°„ì˜ ê°•í•œ ìƒê´€ ê´€ê³„ë¡œ ì¸í•´, ê°œë³„ ë³€ìˆ˜ì˜ ì¤‘ìš”ë„ë¥¼ íŒŒì•…í•˜ê¸° ì–´ë µê³  ë³€ìˆ˜ ê°„ ì˜í–¥ì„ ë¶„ë¦¬í•˜ê¸° ì–´ë ¤ì›Œì§‘ë‹ˆë‹¤.

4. **ëª¨ë¸ ì„±ëŠ¥ ì €í•˜**:
   - ë‹¤ì¤‘ ê³µì„ ì„±ì€ ê³¼ì í•©(overfitting)ì„ ìœ ë°œí•  ìˆ˜ ìˆì–´ ëª¨ë¸ì˜ ì¼ë°˜í™” ëŠ¥ë ¥(ìƒˆë¡œìš´ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ ì„±ëŠ¥)ì„ ë–¨ì–´ëœ¨ë¦½ë‹ˆë‹¤.

## ë‹¤ì¤‘ ê³µì„ ì„± í•´ê²° ë°©ë²•
1. **VIF(Variance Inflation Factor)**:
   - VIFë¥¼ í†µí•´ ê° ë³€ìˆ˜ì˜ ë‹¤ì¤‘ ê³µì„ ì„±ì„ í™•ì¸í•˜ê³ , VIF ê°’ì´ ë†’ì€ ë³€ìˆ˜ë¥¼ ì œê±°í•˜ê±°ë‚˜ ë³€í˜•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

2. **ì£¼ì„±ë¶„ ë¶„ì„(PCA)**:
   - ì£¼ì„±ë¶„ ë¶„ì„(PCA)ì„ ì‚¬ìš©í•˜ì—¬ ìƒê´€ ê´€ê³„ê°€ ë†’ì€ ë³€ìˆ˜ë“¤ì„ ê²°í•©í•˜ì—¬ ìƒˆë¡œìš´ ë…ë¦½ ë³€ìˆ˜ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

3. **ì •ê·œí™” ê¸°ë²•**:
   - **Ridge íšŒê·€**ë‚˜ **Lasso íšŒê·€**ì™€ ê°™ì€ ì •ê·œí™” ê¸°ë²•ì„ ì‚¬ìš©í•˜ë©´ ë‹¤ì¤‘ ê³µì„ ì„± ë¬¸ì œë¥¼ ì™„í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```
# ë‹¤ì¤‘ ê³µì„ ì„± : í”¼ì³ ê°„ ìƒê´€ê³„ìˆ˜ê°€ ë„ˆë¬´ í´ ë•Œ ë°œìƒ, ê³„ìˆ˜ì˜ ë¶ˆì•ˆì •ê³¼ í•´ì„ì˜ ì–´ë ¤ì›€ ë°œìƒ
# ê²°ë¡ ì ìœ¼ë¡œ ëª¨ë¸ ì„±ëŠ¥ ì €í•˜
from statsmodels.stats.outliers_influence import  variance_inflation_factor

vif=pd.DataFrame() # ë¹ˆ ë°ì´í„°í”„ë ˆì„ ìƒì„±
vif['feature']=X.columns
vif['VIF']=[
    variance_inflation_factor(X.values, idx) for idx in range(X.shape[1])
]
vif
```
![image](https://github.com/user-attachments/assets/2eeb93c1-561b-4692-b58b-36295046a1eb)

## VIF (Variance Inflation Factor) í•´ì„

VIF(Variance Inflation Factor)ëŠ” ë‹¤ì¤‘ ê³µì„ ì„±ì„ í™•ì¸í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ VIF ê°’ì´ **10 ì´ìƒ**ì¸ ë³€ìˆ˜ëŠ” ë‹¤ì¤‘ ê³µì„ ì„± ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ëŠ” ê° ë³€ìˆ˜ì— ëŒ€í•œ VIF ê°’ì…ë‹ˆë‹¤.

| ë³€ìˆ˜      | VIF        |
|-----------|------------|
| **const** | 585.27     |
| **CRIM**  | 1.79       |
| **ZN**    | 2.30       |
| **INDUS** | 3.99       |
| **CHAS**  | 1.07       |
| **NOX**   | 4.39       |
| **RM**    | 1.93       |
| **AGE**   | 3.10       |
| **DIS**   | 3.96       |
| **RAD**   | 7.48       |
| **TAX**   | 9.01       |
| **PTRATIO**| 1.80      |
| **B**     | 1.35       |
| **LSTAT** | 2.94       |

### ì°¸ê³ 
- vif value : 1~5ì´ë©´ ì •ìƒ
- vif value : 5~10 ì•½í•œ ë‹¤ìŒ ê³µì„ ì„±ì„ ê°€ì§€ê³  ìˆë‹¤. ì£¼ì˜ í•„ìš”
- vif value : 10ì´ìƒ ì‹¬í•œ ë‹¤ì¤‘ ê³µì„ ì„±, í•´ë‹¹ í”¼ì²˜ë¥¼ ì œê±° í•„ìš”

### í•´ì„:
- **VIF ê°’ì´ 1ì— ê°€ê¹Œìš´ ë³€ìˆ˜ë“¤**: `CRIM`, `ZN`, `CHAS`, `RM`, `PTRATIO`, `B`ëŠ” ë‹¤ë¥¸ ë³€ìˆ˜ë“¤ê³¼ ìƒê´€ ê´€ê³„ê°€ ì ì–´ ë‹¤ì¤‘ ê³µì„ ì„± ë¬¸ì œê°€ ì ìŠµë‹ˆë‹¤.
- **VIF ê°’ì´ 10 ì´ìƒì¸ ë³€ìˆ˜ë“¤**: `RAD`ì™€ `TAX`ëŠ” ë‹¤ì¤‘ ê³µì„ ì„± ë¬¸ì œë¥¼ ìœ ë°œí•  ê°€ëŠ¥ì„±ì´ ìˆìœ¼ë©°, ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì¶”ê°€ì ì¸ ì¡°ì¹˜ê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ë‹¤ì¤‘ ê³µì„ ì„± í•´ê²° ë°©ë²•:
- VIF ê°’ì´ ë†’ì€ ë³€ìˆ˜ëŠ” ì œê±°í•˜ê±°ë‚˜, **ì£¼ì„±ë¶„ ë¶„ì„(PCA)** ë˜ëŠ” **ì •ê·œí™” ê¸°ë²•**(ì˜ˆ: **Ridge** íšŒê·€, **Lasso** íšŒê·€)ì„ ì‚¬ìš©í•˜ì—¬ ë‹¤ì¤‘ ê³µì„ ì„± ë¬¸ì œë¥¼ ì™„í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

---
## ë‹¤í•­ íšŒê·€ì™€ ê³¼(ëŒ€)ì í•©/ê³¼ì†Œì í•© ì´í•´

```
# ë‹¤í•­ì‹
from sklearn.preprocessing import PolynomialFeatures

# ì˜ˆì œ ë°ì´í„° ìƒì„± [2x2]
X=np.arange(4).reshape(2,2)
print(f'ì¼ì°¨ ë‹¨í•­ì‹ ê³„ìˆ˜ í”¼ì³\n{X}')

# degree = 2ì¸ 2ì°¨ ë‹¤í•­ì‹ìœ¼ë¡œ ë³€í™˜í•˜ê¸° ìœ„í•´ PolynomialFeaturesë¥¼ ì´ìš©í•´ ë³€í™˜
poly=PolynomialFeatures(degree=2)
poly.fit(X)
poly_ftr=poly.transform(X)
print(f'ë³€í™˜ëœ 2ì°¨ ë‹¤í•­ì‹ ê³„ìˆ˜ í”¼ì³:\n{poly_ftr}')
```
![image](https://github.com/user-attachments/assets/145bcad5-8de5-43b4-bfc6-bc2a3b02f4d8)

---
## ë³´ìŠ¤í†¤ ì£¼íƒ ë°ì´í„°ë¥¼ ê°€ì§€ê³  ë‹¤í•­ì‹ìœ¼ë¡œ ë³€í™˜ í›„ ì²˜ë¦¬
```
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from scipy import stats

import warnings
warnings.filterwarnings('ignore')

boston_df=pd.read_csv(
    './boston_house_prices.csv'
    ,encoding='utf-8'
    ,header=1
)


y_target=boston_df['MEDV']
X_data=boston_df.drop(['MEDV'],axis=1,inplace=False)

X_train,X_test,y_train,y_test=train_test_split(
    X_data
    ,y_target
    ,test_size=0.3
    ,random_state=0
)

lr =LinearRegression()
lr.fit(X_train,y_train) 
y_pred=lr.predict(X_test) 

mse=mean_squared_error(y_test,y_pred)
rmse=np.sqrt(mse) 


# ë³´ìŠ¤í†¤ ì£¼íƒ ë°ì´í„° -> ë‹¤í•­ì‹ìœ¼ë¡œ ë³€í™˜ í›„ ì²˜ë¦¬
# X_data : ì „ì²´ ë°ì´í„°
# X_data.info() : 13ê°œì˜ features
from sklearn.preprocessing import PolynomialFeatures

print(f'X_dataì˜ shape : {X_data.shape}')
poly=PolynomialFeatures(degree=2)
poly.fit(X_data)
poly_ftr=poly.transform(X_data)
print(f'X_dataë¥¼ ë‹¤í•­ì‹ ì ìš©í•œ shape : {poly_ftr.shape}')
```
![image](https://github.com/user-attachments/assets/8e80ca02-6481-4021-9069-f12f18023492)

```
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from scipy import stats

import warnings
warnings.filterwarnings('ignore')

boston_df=pd.read_csv(
    './boston_house_prices.csv'
    ,encoding='utf-8'
    ,header=1
)


y_target=boston_df['MEDV']
X_data=boston_df.drop(['MEDV'],axis=1,inplace=False)

X_train,X_test,y_train,y_test=train_test_split(
    X_data
    ,y_target
    ,test_size=0.3
    ,random_state=0
)

# LinearRegression object create
lr =LinearRegression()
lr.fit(X_train,y_train) # í•™ìŠµ
y_pred=lr.predict(X_test) # í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ì˜ˆì¸¡ê°’ ì¶œë ¥

# ì˜¤ì°¨(ì›ë˜ë‹µ-ì˜ˆì¸¡ê°’) : MSE
mse=mean_squared_error(y_test,y_pred)
rmse=np.sqrt(mse) # ì œê³±ê·¼
print('ë‹¤í•­ì‹ ë³€ê²½ ì „')
print(f'MSE : {mse}, RMSE : {rmse}')
print(f'Variance score : {r2_score(y_test,y_pred)}')
# ë³´ìŠ¤í†¤ ì£¼íƒ ë°ì´í„° -> ë‹¤í•­ì‹ìœ¼ë¡œ ë³€í™˜ í›„ ì²˜ë¦¬
# X_data : ì „ì²´ ë°ì´í„°
# X_data.info() : 13ê°œì˜ features
from sklearn.preprocessing import PolynomialFeatures
# print(f'X_dataì˜ shape : {X_data.shape}')

print('ë‹¤í•­ì‹ ë³€ê²½ í›„')
poly=PolynomialFeatures(degree=2)
poly.fit(X_data)
poly_ftr=poly.transform(X_data)
# print(f'X_dataë¥¼ ë‹¤í•­ì‹ ì ìš©í•œ shape : {poly_ftr.shape}')

# í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„ë¦¬
X_train_poly,X_test_poly,y_train_poly,y_test_poly=train_test_split(
    poly_ftr # ë‹¤í•­ì‹ìœ¼ë¡œ ë³€í™˜í•œ ì „ì²´ ë°ì´í„°
    ,y_target # ì „ì²´ ë ˆì´ë¸”
    ,test_size=0.3 
    ,random_state=156
)

lr =LinearRegression()
lr.fit(X_train_poly,y_train_poly) 
y_pred=lr.predict(X_test_poly) 

mse=mean_squared_error(y_test_poly,y_pred)
rmse=np.sqrt(mse) 
print(f'MSE : {mse}, RMSE : {rmse}')
print(f'Variance score : {r2_score(y_test_poly,y_pred)}')
```
![image](https://github.com/user-attachments/assets/1a21abdf-91a0-4c77-8b7d-e727e5480324)

```
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from scipy import stats

import warnings
warnings.filterwarnings('ignore')

boston_df=pd.read_csv(
    './boston_house_prices.csv'
    ,encoding='utf-8'
    ,header=1
)


y_target=boston_df['MEDV']
X_data=boston_df.drop(['MEDV'],axis=1,inplace=False)

X_train,X_test,y_train,y_test=train_test_split(
    X_data
    ,y_target
    ,test_size=0.3
    ,random_state=0
)

# LinearRegression object create
lr =LinearRegression()
lr.fit(X_train,y_train) # í•™ìŠµ
y_pred=lr.predict(X_test) # í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ì˜ˆì¸¡ê°’ ì¶œë ¥

# ì˜¤ì°¨(ì›ë˜ë‹µ-ì˜ˆì¸¡ê°’) : MSE
mse=mean_squared_error(y_test,y_pred)
rmse=np.sqrt(mse) # ì œê³±ê·¼
print('ë‹¤í•­ì‹ ë³€ê²½ ì „')
print(f'í”¼ì²˜ì˜ ê°œìˆ˜ : {poly_ftr.shape[1]}')
print(f'MSE : {mse}, RMSE : {rmse}')
print(f'Variance score : {r2_score(y_test,y_pred)}')
# ë³´ìŠ¤í†¤ ì£¼íƒ ë°ì´í„° -> ë‹¤í•­ì‹ìœ¼ë¡œ ë³€í™˜ í›„ ì²˜ë¦¬
# X_data : ì „ì²´ ë°ì´í„°
# X_data.info() : 13ê°œì˜ features
from sklearn.preprocessing import PolynomialFeatures
# print(f'X_dataì˜ shape : {X_data.shape}')

print('ë‹¤í•­ì‹ ë³€ê²½ í›„')
poly=PolynomialFeatures(degree=2)
poly.fit(X_data)
poly_ftr=poly.transform(X_data)
# print(f'X_dataë¥¼ ë‹¤í•­ì‹ ì ìš©í•œ shape : {poly_ftr.shape}')

print(f'í”¼ì²˜ì˜ ê°œìˆ˜ : {poly_ftr.shape[1]}')
# í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„ë¦¬
X_train_poly,X_test_poly,y_train_poly,y_test_poly=train_test_split(
    poly_ftr # ë‹¤í•­ì‹ìœ¼ë¡œ ë³€í™˜í•œ ì „ì²´ ë°ì´í„°
    ,y_target # ì „ì²´ ë ˆì´ë¸”
    ,test_size=0.3 
    ,random_state=156
)

lr =LinearRegression()
lr.fit(X_train_poly,y_train_poly) 
y_pred=lr.predict(X_test_poly) 

mse=mean_squared_error(y_test_poly,y_pred)
rmse=np.sqrt(mse) 
print(f'MSE : {mse}, RMSE : {rmse}')
print(f'Variance score : {r2_score(y_test_poly,y_pred)}')

print('ë‹¤í•­ì‹ degree 5ë¡œ ì„¤ì •í• ë•Œ')
degree=5

poly=PolynomialFeatures(degree=degree)
poly.fit(X_data)
poly_ftr=poly.transform(X_data)
# print(f'X_dataë¥¼ ë‹¤í•­ì‹ ì ìš©í•œ shape : {poly_ftr.shape}')

# í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„ë¦¬
print(f'í”¼ì²˜ì˜ ê°œìˆ˜ : {poly_ftr.shape[1]}')
X_train_poly,X_test_poly,y_train_poly,y_test_poly=train_test_split(
    poly_ftr # ë‹¤í•­ì‹ìœ¼ë¡œ ë³€í™˜í•œ ì „ì²´ ë°ì´í„°
    ,y_target # ì „ì²´ ë ˆì´ë¸”
    ,test_size=0.3 
    ,random_state=156
)

lr =LinearRegression()
lr.fit(X_train_poly,y_train_poly) 
y_pred=lr.predict(X_test_poly) 

mse=mean_squared_error(y_test_poly,y_pred)
rmse=np.sqrt(mse) 
print(f'MSE : {mse}, RMSE : {rmse}')
print(f'Variance score : {r2_score(y_test_poly,y_pred)}')

print('ë‹¤í•­ì‹ degree 3ë¡œ ì„¤ì •í• ë•Œ')
degree=3

poly=PolynomialFeatures(degree=degree)
poly.fit(X_data)
poly_ftr=poly.transform(X_data)
# print(f'X_dataë¥¼ ë‹¤í•­ì‹ ì ìš©í•œ shape : {poly_ftr.shape}')

# í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„ë¦¬
print(f'í”¼ì²˜ì˜ ê°œìˆ˜ : {poly_ftr.shape[1]}')
X_train_poly,X_test_poly,y_train_poly,y_test_poly=train_test_split(
    poly_ftr # ë‹¤í•­ì‹ìœ¼ë¡œ ë³€í™˜í•œ ì „ì²´ ë°ì´í„°
    ,y_target # ì „ì²´ ë ˆì´ë¸”
    ,test_size=0.3 
    ,random_state=156
)

lr =LinearRegression()
lr.fit(X_train_poly,y_train_poly) 
y_pred=lr.predict(X_test_poly) 

mse=mean_squared_error(y_test_poly,y_pred)
rmse=np.sqrt(mse) 
print(f'MSE : {mse}, RMSE : {rmse}')
print(f'Variance score : {r2_score(y_test_poly,y_pred)}')
```
![image](https://github.com/user-attachments/assets/7b4edaa2-0983-420f-804a-4bafb305f4cb)

---

## ğŸ“˜ ê·œì œ ì„ í˜• ëª¨ë¸ ì •ë¦¬ - ë¦¿ì§€, ë¼ì˜, ì—˜ë¼ìŠ¤í‹±ë„·

### ğŸ“Œ 1. ë¦¿ì§€ íšŒê·€ (Ridge Regression)

#### âœ… íŠ¹ì§•
- L2 ì •ê·œí™” ì‚¬ìš© (ê³„ìˆ˜ì˜ ì œê³±í•©ì„ í˜ë„í‹°ë¡œ ë¶€ì—¬)
- ëª¨ë“  ë³€ìˆ˜ë¥¼ í¬í•¨í•˜ë©´ì„œ **ê³„ìˆ˜ë¥¼ ì‘ê²Œ** ë§Œë“¦
- ê³¼ì í•©(overfitting)ì„ ë°©ì§€
- ë³€ìˆ˜ ê°„ **ë‹¤ì¤‘ê³µì„ ì„±** ë¬¸ì œ ì™„í™”

#### â• ì¥ì 
- ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ
- ë³€ìˆ˜ ê°„ ìƒê´€ê´€ê³„ê°€ ë†’ì„ ë•Œ ì•ˆì •ì 
- êµ¬í˜„ ê°„ë‹¨, ê³„ì‚° íš¨ìœ¨ì 

#### â– ë‹¨ì 
- ì¤‘ìš”í•˜ì§€ ì•Šì€ ë³€ìˆ˜ë„ ê³„ìˆ˜ê°€ 0ì´ ë˜ì§€ ì•ŠìŒ â†’ **ë³€ìˆ˜ ì„ íƒ ë¶ˆê°€**
- í•´ì„ë ¥ì´ ë–¨ì–´ì§ˆ ìˆ˜ ìˆìŒ



### âœ‚ï¸ 2. ë¼ì˜ íšŒê·€ (Lasso Regression)

#### âœ… íŠ¹ì§•
- L1 ì •ê·œí™” ì‚¬ìš© (ê³„ìˆ˜ì˜ ì ˆëŒ“ê°’ í•©ì„ í˜ë„í‹°ë¡œ ë¶€ì—¬)
- **ì¼ë¶€ ê³„ìˆ˜ë¥¼ 0ìœ¼ë¡œ ë§Œë“¦** â†’ ìë™ ë³€ìˆ˜ ì„ íƒ
- í¬ì†Œ(sparse) ëª¨ë¸ ìƒì„± ê°€ëŠ¥

#### â• ì¥ì 
- ë¶ˆí•„ìš”í•œ ë³€ìˆ˜ë¥¼ ì œê±°í•˜ì—¬ **ëª¨ë¸ í•´ì„ë ¥ ì¦ê°€**
- ë³€ìˆ˜ ì„ íƒ ìë™í™”
- ê³ ì°¨ì› ë°ì´í„°(ë³€ìˆ˜ > ìƒ˜í”Œ)ì—ì„œ ìœ ë¦¬

#### â– ë‹¨ì 
- ìƒê´€ê´€ê³„ ë†’ì€ ë³€ìˆ˜ë“¤ ì¤‘ ì¼ë¶€ë§Œ ì„ íƒë  ìˆ˜ ìˆìŒ â†’ ì •ë³´ ì†ì‹¤ ìœ„í—˜
- ìƒê´€ê´€ê³„ê°€ í° ë³€ìˆ˜ë“¤ì´ ë§ì€ ê²½ìš° ì„±ëŠ¥ ë¶ˆì•ˆì •


### âš–ï¸ 3. ì—˜ë¼ìŠ¤í‹±ë„· íšŒê·€ (Elastic Net)

#### âœ… íŠ¹ì§•
- L1 + L2 ì •ê·œí™”ë¥¼ **í˜¼í•©**í•œ ë°©ì‹
- ë¼ì˜ì™€ ë¦¿ì§€ì˜ ì¥ì ì„ ê²°í•©
- ë³€ìˆ˜ ì„ íƒ + ê³„ìˆ˜ ì¶•ì†Œ ëª¨ë‘ ê°€ëŠ¥

#### â• ì¥ì 
- ìƒê´€ê´€ê³„ ë†’ì€ ë³€ìˆ˜ ê·¸ë£¹ì„ **í•¨ê»˜ ì„ íƒ** ê°€ëŠ¥
- ë¼ì˜ë³´ë‹¤ ë” ì•ˆì •ì ì¸ ì„±ëŠ¥
- ê³ ì°¨ì› ë°ì´í„°ë‚˜ ë‹¤ì¤‘ê³µì„ ì„±ì´ ì‹¬í•  ë•Œ ì í•©

#### â– ë‹¨ì 
- í•˜ì´í¼íŒŒë¼ë¯¸í„°ê°€ 2ê°œ (\(\lambda\), \(\alpha\)) â†’ **íŠœë‹ í•„ìš”**
- êµ¬ì¡°ê°€ ë‹¤ì†Œ ë³µì¡í•  ìˆ˜ ìˆìŒ


### ğŸ§  ì–¸ì œ ì–´ë–¤ ëª¨ë¸ì„ ì“¸ê¹Œ?

| ìƒí™© | ì¶”ì²œ ëª¨ë¸ |
|------|-----------|
| ë³€ìˆ˜ ê°„ ìƒê´€ê´€ê³„ê°€ ë†’ê³  ëª¨ë‘ ìœ ì§€í•˜ê³  ì‹¶ì„ ë•Œ | **Ridge** |
| ë³€ìˆ˜ ì„ íƒì´ ì¤‘ìš”í•˜ê³  í•´ì„ë ¥ì´ í•„ìš”í•  ë•Œ | **Lasso** |
| ë³€ìˆ˜ ë§ê³  ìƒê´€ê´€ê³„ë„ ë†’ìœ¼ë©° ì•ˆì •ì„±ì´ í•„ìš”í•œ ê²½ìš° | **Elastic Net** |


### ğŸ“Š ì •ë¦¬ ìš”ì•½í‘œ

| ëª¨ë¸         | ì •ê·œí™” ë°©ì‹ | ë³€ìˆ˜ ì„ íƒ | ê³„ìˆ˜ ì¶•ì†Œ | ì í•©í•œ ìƒí™©                      |
|--------------|-------------|------------|------------|----------------------------------|
| **Ridge**     | L2           | âŒ         | âœ…         | ë³€ìˆ˜ ê°„ ìƒê´€ê´€ê³„ê°€ ë†’ì€ ê²½ìš°       |
| **Lasso**     | L1           | âœ…         | âœ…         | ë³€ìˆ˜ ì„ íƒì´ ì¤‘ìš”í•œ ê²½ìš°           |
| **Elastic Net** | L1 + L2     | âœ…         | âœ…         | ê³ ì°¨ì›, ë‹¤ì¤‘ê³µì„ ì„±ì´ ìˆëŠ” ê²½ìš°     |

```
import numpy as np
import pandas as pd
from sklearn.model_selection import  cross_val_score
from sklearn.linear_model import Ridge

import warnings
warnings.filterwarnings('ignore')

# ë°ì´í„° ë¡œë”©
boston_df = pd.read_csv('./boston_house_prices.csv', encoding='utf-8', header=1)

# íƒ€ê¹ƒê³¼ í”¼ì²˜ ë¶„ë¦¬
y_target = boston_df['MEDV']
X_data = boston_df.drop(['MEDV'], axis=1, inplace=False)

# ë¦¿ì§€ íšŒê·€ ëª¨ë¸ ë° êµì°¨ ê²€ì¦ MSE í‰ê°€
ridge = Ridge(alpha=10)
mse_scores = cross_val_score(
    ridge, X_data, y_target, scoring='neg_mean_squared_error', cv=5
)

# 5ê°œì˜ í‰ê°€ ì ìˆ˜ ë‚˜ì˜¨ë‹¤
print(f'mse_scores : {mse_scores}')
rsme_scores=np.sqrt(-1*mse_scores)
print(f'rsme_scores : {rsme_scores}')
avg_rsme=np.mean(rsme_scores)
print(f'rsme í‰ê·  : {avg_rsme}')
```
![image](https://github.com/user-attachments/assets/55a44449-dbb0-4dca-a266-788a9527c76a)

```
import numpy as np
import pandas as pd
from sklearn.model_selection import  cross_val_score
from sklearn.linear_model import Ridge

import warnings
warnings.filterwarnings('ignore')

# ë°ì´í„° ë¡œë”©
boston_df = pd.read_csv('./boston_house_prices.csv', encoding='utf-8', header=1)

# íƒ€ê¹ƒê³¼ í”¼ì²˜ ë¶„ë¦¬
y_target = boston_df['MEDV']
X_data = boston_df.drop(['MEDV'], axis=1, inplace=False)

# ë¦¿ì§€ íšŒê·€ ëª¨ë¸ ë° êµì°¨ ê²€ì¦ MSE í‰ê°€
aplphas=[0,0.1,1,10,100]
for aplpha in aplphas:
    print('='*100)
    print(f'alphaê°€ {aplpha}ì¸ ê²½ìš°')
    ridge = Ridge(alpha=aplpha)
    mse_scores = cross_val_score(
        ridge, X_data, y_target, scoring='neg_mean_squared_error', cv=5
    )
    # 5ê°œì˜ í‰ê°€ ì ìˆ˜ ë‚˜ì˜¨ë‹¤
    print(f'mse_scores : {mse_scores}')
    rsme_scores=np.sqrt(-1*mse_scores)
    print(f'rsme_scores : {rsme_scores}')
    avg_rsme=np.mean(rsme_scores)
    print(f'rsme í‰ê·  : {avg_rsme}')
```
![image](https://github.com/user-attachments/assets/a0b4a32f-f570-42db-802f-7aea7d239c3e)

```
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
from scipy import stats
import warnings
warnings.filterwarnings('ignore')  #ì‚¬ì´í‚·ëŸ° 1.2 ë¶€í„°ëŠ” ë³´ìŠ¤í„´ ì£¼íƒê°€ê²© ë°ì´í„°ê°€ ì—†ì–´ì§„ë‹¤ëŠ” warning ë©”ì‹œì§€ ì¶œë ¥ ì œê±°


# boston ë°ì´íƒ€ì…‹ DataFrame ë³€í™˜
bostonDF = pd.read_csv('./boston_house_prices.csv', header=1)


print('Boston ë°ì´íƒ€ì…‹ í¬ê¸° :',bostonDF.shape)

# ì»¬ëŸ¼ëª… ë³€ê²½
bostonDF.rename(
    columns={'MEDV':'PRICE'}
    , inplace=True
)

# ë°ì´í„°í”„ë ˆì„ ì•ˆì— ë°ì´í„°ì™€ ë ˆì´ë¸” ë¶„ë¦¬
y_target=bostonDF['PRICE'] # ë ˆì´ë¸” ì €ì¥
X_data=bostonDF.drop(['PRICE'], axis=1, inplace=False)

# Ridge
print('='*50,'Ridge','='*50)
from sklearn.linear_model import Ridge
from sklearn.model_selection import cross_val_score

ridge=Ridge(alpha=10)
neg_mse_scores=cross_val_score(
      ridge                                     # ì•Œê³ ë¦¬ì¦˜
    , X_data                                    # ë°ì´í„°
    , y_target                                  # ë ˆì´ë¸”
    , scoring='neg_mean_squared_error'          # í‰ê°€ì§€í‘œ ì§€ì •
    , cv=5                                      # cv=fold cnt
)

print(f'neg_mse_scores : {neg_mse_scores}')

rmse_scores=np.sqrt(-1*neg_mse_scores)
print(f'rmse_scores : {rmse_scores}')

avg_rmse=np.mean(rmse_scores)
print(f'avg_rmse : {avg_rmse}')

# ë¦¿ì§€ì— ì‚¬ìš©ë  alpha íŒŒë¼ë¯¸í„°ì˜ ê°’ì„ ì •ì˜
alphas = [0, 0.1, 1, 10, 100]


# alphas list ê°’ì„ ë°˜ë³µí•˜ë©´ì„œ alphaì— ë”°ë¥¸ í‰ê·  rmseë¥¼ êµ¬í•¨.
for alpha in alphas :
    ridge = Ridge(alpha = alpha)
   
    # cross_val_scoreë¥¼ ì´ìš©í•´ 5 í´ë“œì˜ í‰ê·  RMSEë¥¼ ê³„ì‚°
    neg_mse_scores = cross_val_score(ridge, X_data, y_target, scoring="neg_mean_squared_error", cv = 5)
    avg_rmse = np.mean(np.sqrt(-1 * neg_mse_scores))
    print('alpha {0} ì¼ ë•Œ 5 folds ì˜ í‰ê·  RMSE : {1:.3f} '.format(alpha, avg_rmse))

# ê° alphaì— ë”°ë¥¸ íšŒê·€ ê³„ìˆ˜ ê°’ì„ ì‹œê°í™”í•˜ê¸° ìœ„í•´ 5ê°œì˜ ì—´ë¡œ ëœ ë§·í”Œë¡¯ë¦½ ì¶• ìƒì„±  
fig , axs = plt.subplots(figsize=(18,6) , nrows=1 , ncols=5)
# ê° alphaì— ë”°ë¥¸ íšŒê·€ ê³„ìˆ˜ ê°’ì„ ë°ì´í„°ë¡œ ì €ì¥í•˜ê¸° ìœ„í•œ DataFrame ìƒì„±  
coeff_df = pd.DataFrame()


# alphas ë¦¬ìŠ¤íŠ¸ ê°’ì„ ì°¨ë¡€ë¡œ ì…ë ¥í•´ íšŒê·€ ê³„ìˆ˜ ê°’ ì‹œê°í™” ë° ë°ì´í„° ì €ì¥. posëŠ” axisì˜ ìœ„ì¹˜ ì§€ì •
for pos , alpha in enumerate(alphas) :
    ridge = Ridge(alpha = alpha)
    ridge.fit(X_data , y_target)
    # alphaì— ë”°ë¥¸ í”¼ì²˜ë³„ íšŒê·€ ê³„ìˆ˜ë¥¼ Seriesë¡œ ë³€í™˜í•˜ê³  ì´ë¥¼ DataFrameì˜ ì»¬ëŸ¼ìœ¼ë¡œ ì¶”ê°€.  
    coeff = pd.Series(data=ridge.coef_ , index=X_data.columns )
    colname='alpha:'+str(alpha)
    coeff_df[colname] = coeff
    # ë§‰ëŒ€ ê·¸ë˜í”„ë¡œ ê° alpha ê°’ì—ì„œì˜ íšŒê·€ ê³„ìˆ˜ë¥¼ ì‹œê°í™”. íšŒê·€ ê³„ìˆ˜ê°’ì´ ë†’ì€ ìˆœìœ¼ë¡œ í‘œí˜„
    coeff = coeff.sort_values(ascending=False)
    axs[pos].set_title(colname)
    axs[pos].set_xlim(-3,6)
    sns.barplot(x=coeff.values , y=coeff.index, ax=axs[pos])
    

# for ë¬¸ ë°”ê¹¥ì—ì„œ ë§·í”Œë¡¯ë¦½ì˜ show í˜¸ì¶œ ë° alphaì— ë”°ë¥¸ í”¼ì²˜ë³„ íšŒê·€ ê³„ìˆ˜ë¥¼ DataFrameìœ¼ë¡œ í‘œì‹œ
plt.show()
```
![image](https://github.com/user-attachments/assets/e4b78f54-8a79-4181-8a87-f47180c4d9e6)

![image](https://github.com/user-attachments/assets/c427df4d-9331-4747-8119-e58532d3f9cc)

```
from sklearn.linear_model import Lasso, ElasticNet


# alphaê°’ì— ë”°ë¥¸ íšŒê·€ ëª¨ë¸ì˜ í´ë“œ í‰ê·  RMSEë¥¼ ì¶œë ¥í•˜ê³  íšŒê·€ ê³„ìˆ˜ê°’ë“¤ì„ DataFrameìœ¼ë¡œ ë°˜í™˜
def get_linear_reg_eval(model_name, params=None, X_data_n=None, y_target_n=None,
                        verbose=True, return_coeff=True):
    coeff_df = pd.DataFrame()
    if verbose : print('####### ', model_name , '#######')
    for param in params:
        if model_name =='Ridge': model = Ridge(alpha=param)
        elif model_name =='Lasso': model = Lasso(alpha=param)
        elif model_name =='ElasticNet': model = ElasticNet(alpha=param, l1_ratio=0.7)
        neg_mse_scores = cross_val_score(model, X_data_n,
                                             y_target_n, scoring="neg_mean_squared_error", cv = 5)
        avg_rmse = np.mean(np.sqrt(-1 * neg_mse_scores))
        print('alpha {0}ì¼ ë•Œ 5 í´ë“œ ì„¸íŠ¸ì˜ í‰ê·  RMSE: {1:.3f} '.format(param, avg_rmse))
        # cross_val_scoreëŠ” evaluation metricë§Œ ë°˜í™˜í•˜ë¯€ë¡œ ëª¨ë¸ì„ ë‹¤ì‹œ í•™ìŠµí•˜ì—¬ íšŒê·€ ê³„ìˆ˜ ì¶”ì¶œ
       
        model.fit(X_data_n , y_target_n)
        if return_coeff:
            # alphaì— ë”°ë¥¸ í”¼ì²˜ë³„ íšŒê·€ ê³„ìˆ˜ë¥¼ Seriesë¡œ ë³€í™˜í•˜ê³  ì´ë¥¼ DataFrameì˜ ì»¬ëŸ¼ìœ¼ë¡œ ì¶”ê°€.
            coeff = pd.Series(data=model.coef_ , index=X_data_n.columns )
            colname='alpha:'+str(param)
            coeff_df[colname] = coeff
   
    return coeff_df
# end of get_linear_regre_eval
```
```
# ë¼ì˜ì— ì‚¬ìš©ë  alpha íŒŒë¼ë¯¸í„°ì˜ ê°’ë“¤ì„ ì •ì˜í•˜ê³  get_linear_reg_eval() í•¨ìˆ˜ í˜¸ì¶œ
lasso_alphas = [ 0.07, 0.1, 0.5, 1, 3]
coeff_lasso_df =get_linear_reg_eval('Lasso', params=lasso_alphas, X_data_n=X_data, y_target_n=y_target)
```
![image](https://github.com/user-attachments/assets/4b62e483-415d-480c-807b-4f9a5d962970)

```
# ë°˜í™˜ëœ coeff_lasso_dfë¥¼ ì²«ë²ˆì§¸ ì»¬ëŸ¼ìˆœìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬í•˜ì—¬ íšŒê·€ê³„ìˆ˜ DataFrameì¶œë ¥
sort_column = 'alpha:'+str(lasso_alphas[0])
coeff_lasso_df.sort_values(by=sort_column, ascending=False)
```
![image](https://github.com/user-attachments/assets/c743e08a-ecd9-431b-a0c2-4534fa3bd9fa)

```
# ì—˜ë¼ìŠ¤í‹±ë„·ì— ì‚¬ìš©ë  alpha íŒŒë¼ë¯¸í„°ì˜ ê°’ë“¤ì„ ì •ì˜í•˜ê³  get_linear_reg_eval() í•¨ìˆ˜ í˜¸ì¶œ
# l1_ratioëŠ” 0.7ë¡œ ê³ ì •
elastic_alphas = [ 0.07, 0.1, 0.5, 1, 3]
coeff_elastic_df =get_linear_reg_eval('ElasticNet', params=elastic_alphas,
                                      X_data_n=X_data, y_target_n=y_target)
```
![image](https://github.com/user-attachments/assets/3438dc5b-8105-46d7-9af2-ba6df69595a6)

```
# ë°˜í™˜ëœ coeff_elastic_dfë¥¼ ì²«ë²ˆì§¸ ì»¬ëŸ¼ìˆœìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬í•˜ì—¬ íšŒê·€ê³„ìˆ˜ DataFrameì¶œë ¥
sort_column = 'alpha:'+str(elastic_alphas[0])
coeff_elastic_df.sort_values(by=sort_column, ascending=False)
```
![image](https://github.com/user-attachments/assets/d7dec8ea-b28a-420b-bef5-dec3c394717b)

```
# ì „ì²˜ë¦¬
from sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures

# methodëŠ” í‘œì¤€ ì •ê·œ ë¶„í¬ ë³€í™˜(Standard), ìµœëŒ€ê°’/ìµœì†Œê°’ ì •ê·œí™”(MinMax), ë¡œê·¸ë³€í™˜(Log) ê²°ì •
# p_degreeëŠ” ë‹¤í–¥ì‹ íŠ¹ì„±ì„ ì¶”ê°€í•  ë•Œ ì ìš©. p_degreeëŠ” 2ì´ìƒ ë¶€ì—¬í•˜ì§€ ì•ŠìŒ.
def get_scaled_data(method='None', p_degree=None, input_data=None):
    if method == 'Standard':
        scaled_data = StandardScaler().fit_transform(input_data)
    elif method == 'MinMax':
        scaled_data = MinMaxScaler().fit_transform(input_data)
    elif method == 'Log':
        scaled_data = np.log1p(input_data)
    else:
        scaled_data = input_data


    if p_degree != None:
        scaled_data = PolynomialFeatures(degree=p_degree,
                                         include_bias=False).fit_transform(scaled_data)
   
    return scaled_data
```
```
# Ridgeì˜ alphaê°’ì„ ë‹¤ë¥´ê²Œ ì ìš©í•˜ê³  ë‹¤ì–‘í•œ ë°ì´í„° ë³€í™˜ë°©ë²•ì— ë”°ë¥¸ RMSE ì¶”ì¶œ.
alphas = [0.1, 1, 10, 100]
#ë³€í™˜ ë°©ë²•ì€ ëª¨ë‘ 6ê°œ, ì›ë³¸ ê·¸ëŒ€ë¡œ, í‘œì¤€ì •ê·œë¶„í¬, í‘œì¤€ì •ê·œë¶„í¬+ë‹¤í•­ì‹ íŠ¹ì„±
# ìµœëŒ€/ìµœì†Œ ì •ê·œí™”, ìµœëŒ€/ìµœì†Œ ì •ê·œí™”+ë‹¤í•­ì‹ íŠ¹ì„±, ë¡œê·¸ë³€í™˜
scale_methods=[(None, None), ('Standard', None), ('Standard', 2),
               ('MinMax', None), ('MinMax', 2), ('Log', None)]
for scale_method in scale_methods:
    X_data_scaled = get_scaled_data(method=scale_method[0], p_degree=scale_method[1],
                                    input_data=X_data)
    print(X_data_scaled.shape, X_data.shape)
    print('\n## ë³€í™˜ ìœ í˜•:{0}, Polynomial Degree:{1}'.format(scale_method[0], scale_method[1]))
    get_linear_reg_eval('Ridge', params=alphas, X_data_n=X_data_scaled,
                        y_target_n=y_target, verbose=False, return_coeff=False)
```
```
(506, 13) (506, 13)

## ë³€í™˜ ìœ í˜•:None, Polynomial Degree:None
alpha 0.1ì¼ ë•Œ 5 í´ë“œ ì„¸íŠ¸ì˜ í‰ê·  RMSE: 5.788 
alpha 1ì¼ ë•Œ 5 í´ë“œ ì„¸íŠ¸ì˜ í‰ê·  RMSE: 5.653 
alpha 10ì¼ ë•Œ 5 í´ë“œ ì„¸íŠ¸ì˜ í‰ê·  RMSE: 5.518 
alpha 100ì¼ ë•Œ 5 í´ë“œ ì„¸íŠ¸ì˜ í‰ê·  RMSE: 5.330 
(506, 13) (506, 13)

## ë³€í™˜ ìœ í˜•:Standard, Polynomial Degree:None
alpha 0.1ì¼ ë•Œ 5 í´ë“œ ì„¸íŠ¸ì˜ í‰ê·  RMSE: 5.826 
alpha 1ì¼ ë•Œ 5 í´ë“œ ì„¸íŠ¸ì˜ í‰ê·  RMSE: 5.803 
alpha 10ì¼ ë•Œ 5 í´ë“œ ì„¸íŠ¸ì˜ í‰ê·  RMSE: 5.637 
alpha 100ì¼ ë•Œ 5 í´ë“œ ì„¸íŠ¸ì˜ í‰ê·  RMSE: 5.421 
(506, 104) (506, 13)

## ë³€í™˜ ìœ í˜•:Standard, Polynomial Degree:2
alpha 0.1ì¼ ë•Œ 5 í´ë“œ ì„¸íŠ¸ì˜ í‰ê·  RMSE: 8.827 
alpha 1ì¼ ë•Œ 5 í´ë“œ ì„¸íŠ¸ì˜ í‰ê·  RMSE: 6.871 
alpha 10ì¼ ë•Œ 5 í´ë“œ ì„¸íŠ¸ì˜ í‰ê·  RMSE: 5.485 
alpha 100ì¼ ë•Œ 5 í´ë“œ ì„¸íŠ¸ì˜ í‰ê·  RMSE: 4.634 
(506, 13) (506, 13)

## ë³€í™˜ ìœ í˜•:MinMax, Polynomial Degree:None
alpha 0.1ì¼ ë•Œ 5 í´ë“œ ì„¸íŠ¸ì˜ í‰ê·  RMSE: 5.764 
alpha 1ì¼ ë•Œ 5 í´ë“œ ì„¸íŠ¸ì˜ í‰ê·  RMSE: 5.465 
alpha 10ì¼ ë•Œ 5 í´ë“œ ì„¸íŠ¸ì˜ í‰ê·  RMSE: 5.754 
alpha 100ì¼ ë•Œ 5 í´ë“œ ì„¸íŠ¸ì˜ í‰ê·  RMSE: 7.635 
(506, 104) (506, 13)

## ë³€í™˜ ìœ í˜•:MinMax, Polynomial Degree:2
alpha 0.1ì¼ ë•Œ 5 í´ë“œ ì„¸íŠ¸ì˜ í‰ê·  RMSE: 5.298 
alpha 1ì¼ ë•Œ 5 í´ë“œ ì„¸íŠ¸ì˜ í‰ê·  RMSE: 4.323 
alpha 10ì¼ ë•Œ 5 í´ë“œ ì„¸íŠ¸ì˜ í‰ê·  RMSE: 5.185 
alpha 100ì¼ ë•Œ 5 í´ë“œ ì„¸íŠ¸ì˜ í‰ê·  RMSE: 6.538 
(506, 13) (506, 13)

## ë³€í™˜ ìœ í˜•:Log, Polynomial Degree:None
alpha 0.1ì¼ ë•Œ 5 í´ë“œ ì„¸íŠ¸ì˜ í‰ê·  RMSE: 4.770 
alpha 1ì¼ ë•Œ 5 í´ë“œ ì„¸íŠ¸ì˜ í‰ê·  RMSE: 4.676 
alpha 10ì¼ ë•Œ 5 í´ë“œ ì„¸íŠ¸ì˜ í‰ê·  RMSE: 4.836 
alpha 100ì¼ ë•Œ 5 í´ë“œ ì„¸íŠ¸ì˜ í‰ê·  RMSE: 6.241 
```
