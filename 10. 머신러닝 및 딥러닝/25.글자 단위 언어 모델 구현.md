# 글자 단위 언어 모델 구현

```
import torch
import torch.nn as nn
import pandas as pd
import numpy as np
```
```
# 데이터 로딩
# 파일 읽기
with open('./1268-0.txt','r',encoding='utf-8') as fp:
    text = fp.read()

print(text)
```
![image](https://github.com/user-attachments/assets/92f64326-d248-49c6-9db7-b3c2ea052d05)

```
start_idx=text.find('THE MYSTERIOUS ISLAND')
print(start_idx)
```
![image](https://github.com/user-attachments/assets/6a3379cb-5729-4b0f-9147-094ef0510868)

```
end_idx=text.find('End of the Project')
print(end_idx)
```
![image](https://github.com/user-attachments/assets/11134ea9-53fc-4481-9ba2-0479468e07f7)

```
text=text[start_idx:end_idx]
print(text)
```
![image](https://github.com/user-attachments/assets/eac9db17-03f6-40cf-8442-cc811a89c11c)

```
char_set=set(text)
print(char_set)
```
![image](https://github.com/user-attachments/assets/61a406b0-d24e-4e88-b950-348c838d7397)

```
len(char_set)
```
![image](https://github.com/user-attachments/assets/5a5e0fd6-d879-4069-952f-78fecfd30081)

```
print(len(text))
```
![image](https://github.com/user-attachments/assets/b1693be3-0eba-4941-8619-437c98efc3f0)

```
# 인코딩 : 문자 => 숫자, 디코딩 : 숫자 => 문자
# 인코딩
# char_set 정렬
chars_sorted=sorted(char_set)
print(chars_sorted)
```
![image](https://github.com/user-attachments/assets/1caa1427-926f-463f-a40d-594ce7096c90)

```
# 한자씩 인코딩 -> 숫자
char2int={ch:i for i, ch in enumerate(chars_sorted)}
print(char2int)
```
![image](https://github.com/user-attachments/assets/a6f26a63-58f7-4ad5-9d4a-a3717db399e5)
```
# 한 숫자씩 디코딩 => 문자
char_array=np.array(chars_sorted)
print(char_array)
```
![image](https://github.com/user-attachments/assets/0295af6f-2565-463e-b56c-a681916c9244)

```
# text 인코딩
text_encode=np.array(
    [char2int[ch] for ch in text]
    ,dtype=np.int32
)
# 앞에서 10개
print(text[:10])
print(text_encode[:10])
```
![image](https://github.com/user-attachments/assets/fa6ba8e0-2132-47c4-97bd-7c3697e5c9f1)

```
print(text_encode.shape)
```
![image](https://github.com/user-attachments/assets/f7ca81ab-8446-4dc0-9fee-d615b7d0cf72)

```
# 디코딩
print(text_encode[:10])
print(char_array[text_encode[:10]])
```
![image](https://github.com/user-attachments/assets/419d77b9-689e-43cc-b61d-c95ba2437e7a)

```
for ex in text_encode[:10]:
    print('{} -> {}'.format(ex,char_array[ex]))
```
![image](https://github.com/user-attachments/assets/0e86b483-639d-4afd-b657-1a26d6ed29e5)

```
#데이터셋 : 데이터+레이블
from torch.utils.data import Dataset
seq_length=40
chunk_size=seq_length+1
text_chunks = [
    text_encode[i:i+chunk_size] for i in range(len(text_encode)-chunk_size+1)
]
print(text_chunks)
```
![image](https://github.com/user-attachments/assets/85c48aaa-b313-41f2-ba29-5a4aded484ec)

```
len(text_encode)-chunk_size+1
```
![image](https://github.com/user-attachments/assets/ffcff031-3df2-4288-89d8-e19addb67d1f)
```
len(text_encode)
```
![image](https://github.com/user-attachments/assets/b9e4d15f-8d58-4be4-a26e-b25093156c3d)

```
text_chunks[:1]
```
![image](https://github.com/user-attachments/assets/429e3060-7f58-48ba-ac93-5fb02b20b1b1)

```
# 데이터셋 (데이터+렝블) 생성 클래스
class TextDataset(Dataset):
    def __init__(self, text_chunks):
        super().__init__()
        self.text_chunks = text_chunks
        
    def __len__(self):
        return len(self.text_chunks)
    
    def __getitem__(self, index):
        text_chunk = torch.tensor(self.text_chunks[index], dtype=torch.long)
        return text_chunk[:-1], text_chunk[1:]
```
```
seq_dataset=TextDataset(text_chunks=text_chunks)
```
```
for i, (seq, target) in enumerate(seq_dataset):
    print('입력데이터 : ', ''.join(char_array[seq]))  
    print('레이블 : ', ''.join(char_array[target])) 
    if i == 0:
        break
```
![image](https://github.com/user-attachments/assets/0f10a301-8853-46cb-b99f-af24c2883aaa)

```
for i, (seq, target) in enumerate(seq_dataset):
    print('입력데이터 : ',repr(''.join(char_array[seq])))
    print('레이블 : ',repr(''.join(char_array[target])))
    if i==0:
        break
```
![image](https://github.com/user-attachments/assets/20821e3c-1884-4838-a178-ef18d48d5e75)

```
# DataLoader
from torch.utils.data import DataLoader
batch_size=64
torch.manual_seed(1)

seq_dl=DataLoader(
    seq_dataset
    ,batch_size=batch_size
    ,shuffle=True
    ,drop_last=True
)
```
```
import torch.nn as nn

device=torch.device('cuda:0')

class RNN(nn.Module):
    def __init__(self, vocab_size, embed_dim, rnn_hidden_size):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.rnn_hidden_size = rnn_hidden_size
        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size,
                           batch_first=True)
        self.fc = nn.Linear(rnn_hidden_size, vocab_size)


    def forward(self, x, hidden, cell):
        out = self.embedding(x).unsqueeze(1)
        out, (hidden, cell) = self.rnn(out, (hidden, cell))
        out = self.fc(out).reshape(out.size(0), -1)
        return out, hidden, cell


    def init_hidden(self, batch_size):
        hidden = torch.zeros(1, batch_size, self.rnn_hidden_size)
        cell = torch.zeros(1, batch_size, self.rnn_hidden_size)
        return hidden.to(device), cell.to(device)
```
```
vocab_size=len(char_array)
embed_size=256
rnn_hidden_size=512
torch.manual_seed(1)
model=RNN(
    vocab_size=vocab_size
    ,embed_dim=embed_size
    ,rnn_hidden_size=rnn_hidden_size
)
#GPU
model=model.to(device=device)
# 신곙망 구성 확인
model
```
![image](https://github.com/user-attachments/assets/7043e415-7f87-4e55-b34d-cb2038d6bf94)

```
# 손실 함수로 다중 분류에 적합한 CrossEntropyLoss 사용
loss_fn = nn.CrossEntropyLoss()

# 모델 파라미터를 학습할 옵티마이저로 Adam 선택, 학습률은 0.005로 설정
optimizer = torch.optim.Adam(
    model.parameters(),
    lr=0.005
)

# 전체 학습 에폭 수 설정
num_epochs = 10000

# 랜덤 시드 고정 (재현 가능성 확보)
torch.manual_seed(1)

# 학습 루프 시작
for epoch in range(num_epochs):
    # 에폭마다 초기 hidden, cell 상태를 초기화
    hidden, cell = model.init_hidden(batch_size=batch_size)
    
    # 시퀀스 데이터(batch)와 타겟(batch)을 하나 가져옴
    seq_batch, target_batch = next(iter(seq_dl))  # 입력 데이터, 정답 레이블
    seq_batch = seq_batch.to(device)              # 입력 데이터를 GPU/CPU로 이동
    target_batch = target_batch.to(device)        # 레이블도 GPU/CPU로 이동

    optimizer.zero_grad()  # 이전 그래디언트 초기화
    loss = 0               # 누적 손실 초기화

    # 시퀀스 길이만큼 루프를 돌며 모델에 한 타임스텝씩 입력
    for c in range(seq_length):
        # 현재 시점 c의 배치 데이터를 모델에 입력
        # # seq_batch[:, c]는 [batch_size, input_size] 형태로, 모든 배치에 대해 시점 c의 입력 데이터를 가져옴
        # # hidden, cell은 이전 시점의 LSTM hidden state와 cell state
        # # 모델은 현재 입력과 이전 hidden, cell 상태를 바탕으로 현재 시점의 출력(pred), 다음 hidden, cell 상태를 반환
        pred, hidden, cell = model(seq_batch[:, c], hidden, cell)
        
        # 현재 시점 c의 예측값(pred)과 실제 정답(target_batch[:, c])을 손실 함수에 넣어 손실 계산
        # # target_batch[:, c]는 [batch_size] 형태이며, 각 샘플의 정답 클래스 인덱스를 나타냄
        # # pred는 [batch_size, num_classes] 형태이며, 각 클래스에 대한 logit 값을 포함
        # CrossEntropyLoss는 내부적으로 softmax를 적용한 후, 정답 클래스에 대한 확률의 음의 로그를 계산함
        loss += loss_fn(pred, target_batch[:, c])


    # 역전파 수행
    loss.backward()

    # 파라미터 업데이트
    optimizer.step()

    # 평균 손실 계산 (시퀀스 길이로 나눔)
    loss = loss.item() / seq_length

    # 일정 간격으로 학습 진행 상태 출력
    if epoch % 500 == 0:
        print(f'에포크 : {epoch}, 손실 : {loss:.4f}')

```
![image](https://github.com/user-attachments/assets/c8178d3f-71a5-4e15-908e-1688eb078e57)

```
from torch.distributions.categorical import Categorical


def sample(model, starting_str,
           len_generated_text=500,
           scale_factor=1.0):


    encoded_input = torch.tensor([char2int[s] for s in starting_str])
    encoded_input = torch.reshape(encoded_input, (1, -1))


    generated_str = starting_str


    model.eval()
    hidden, cell = model.init_hidden(1)
    hidden = hidden.to('cpu')
    cell = cell.to('cpu')
    for c in range(len(starting_str)-1):
        _, hidden, cell = model(encoded_input[:, c].view(1), hidden, cell)


    last_char = encoded_input[:, -1]
    for i in range(len_generated_text):
        logits, hidden, cell = model(last_char.view(1), hidden, cell)
        logits = torch.squeeze(logits, 0)
        scaled_logits = logits * scale_factor
        m = Categorical(logits=scaled_logits)
        last_char = m.sample()
        generated_str += str(char_array[last_char])


    return generated_str


torch.manual_seed(1)
model.to('cpu')
print(sample(model, starting_str='The island'))
```
![image](https://github.com/user-attachments/assets/365c6e18-11e8-467b-aa05-569b9186bf86)

```
from torch.distributions.categorical import Categorical


def sample(model, starting_str,
           len_generated_text=500,
           scale_factor=1.0):


    encoded_input = torch.tensor([char2int[s] for s in starting_str])
    encoded_input = torch.reshape(encoded_input, (1, -1))


    generated_str = starting_str


    model.eval()
    hidden, cell = model.init_hidden(1)
    hidden = hidden.to('cpu')
    cell = cell.to('cpu')
    for c in range(len(starting_str)-1):
        _, hidden, cell = model(encoded_input[:, c].view(1), hidden, cell)


    last_char = encoded_input[:, -1]
    for i in range(len_generated_text):
        logits, hidden, cell = model(last_char.view(1), hidden, cell)
        logits = torch.squeeze(logits, 0)
        scaled_logits = logits * scale_factor
        m = Categorical(logits=scaled_logits)
        last_char = m.sample()
        generated_str += str(char_array[last_char])


    return generated_str


torch.manual_seed(1)
model.to('cpu')
print(sample(model, starting_str='The island',scale_factor=2.0))
```
![image](https://github.com/user-attachments/assets/53510422-348a-4488-9cda-8c09b3c59420)
