# GPT2를 사용하여 새로운 텍스트 생성

```
%pip install transformers
```
![image](https://github.com/user-attachments/assets/ea8ed49b-56c4-4068-985d-1f3694dce684)

```
from transformers import pipeline, set_seed

generator = pipeline( # 생성하는 오브젝트 생성
    'text-generation' # 머할것인지 지정
    ,model='gpt2' # 사용할 모델 지정
)

set_seed(123) # 실행할 때마다 데이터가 바뀌는것을 방지

# generator 실행
generator(
    'Hey reader, today is '
    , max_length=100
    , num_return_sequences=3 # 답변의 갯수
)
```
```
Device set to use cuda:0
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[{'generated_text': "Hey reader, today is the final day (no blogpost) of my\xa0 I'm working on an upcoming project with you guys. Today is the last day of school, so I'm taking part in a lecture about computer science today. The first of what will be a series of blog posts will focus on today's topic, the topic which will lead me through the development of the webkit.\nThis blogpost will focus mostly on the concepts and terminology which are important to most web"},
 {'generated_text': 'Hey reader, today is the 50th anniversary of the death of a single person who I thought would have been very unusual to write: Mike Cernovich. I am writing to offer my deepest condolences in advance to the family and friends of Mike, the man whom I wrote about at my very own blog last August and who is at our core a fan of all things video games, in his late teens for the first time with his family, and now for his friends. I knew from'},
 {'generated_text': 'Hey reader, today is Sunday.After my last post here on Aesop we are going down to the woods to get some quality woods. We are planning the day for our tour, but will be picking up lots of extra supplies if it runs out. We decided to make an initial hike and head back to our campground to help get ready for a big picnic lunch.\nAfter spending some time playing around in what is now called the Wild Horse State Forest, we came across'}]
```
```
from transformers import pipeline, set_seed

generator = pipeline( # 생성하는 오브젝트 생성
    'text-generation' # 머할것인지 지정
    ,model='gpt2' # 사용할 모델 지정
)

set_seed(123) # 실행할 때마다 데이터가 바뀌는것을 방지

# generator 실행
generator(
    '딥러닝 어떻게 배워야 하니?'
    , max_length=100
    , num_return_sequences=3 # 답변의 갯수
)
```
![image](https://github.com/user-attachments/assets/6b18f8ec-3184-4c05-8700-0615c35480d6)

```
from transformers import pipeline, set_seed
import torch

# 1. GPT-3 모델 사용 (실제로는 GPT-3 크기의 모델인 gpt-j-6B 사용)
#model_name = "EleutherAI/gpt-j-6B"  # OpenAI의 GPT-3는 Hugging Face에서 직접 사용할 수 없어 대체 모델 사용

# 리소스가 제한된 환경에서는 더 작은 모델을 사용할 수 있음
model_name = "EleutherAI/gpt-neo-1.3B"  # 더 작은 모델 옵션

# 2. 한글 처리를 위한 토크나이저 설정
generator = pipeline(
    'text-generation',
    model=model_name,
    tokenizer=model_name,
    device=0 if torch.cuda.is_available() else -1  # GPU 사용 가능시 활용
)

set_seed(42)

# 한글 프롬프트로 테스트
result = generator(
    "트랜스포머에 대해 자세한 설명 부탁해?",
    max_length=500,
    num_return_sequences=3,
    do_sample=True,
    top_k=50,
    top_p=0.95,
    temperature=0.8
)

for i, text in enumerate(result):
    print(f"결과 {i+1}: {text['generated_text']}")
```
![image](https://github.com/user-attachments/assets/f076c943-ad4f-4342-b2d4-afa56e0edb54)
```
Device set to use cuda:0
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
결과 1: 트랜스포머에 대해 자세한 설명 부탁해?
오늘날 금액면 저지해야합니다
저는 세계의 목적으로 표현되는 영화가 있습니다.
저는 인기가 무엇에 대해 자세한 가치를 포함할 수 있게 합니다

Japanese: 
自分の言葉が素早く自分が言ってくれることを
忘れないと思っているのです
言葉が言えないのでも
ある日の言葉を続けているのです
それは人間の言葉です
これがあなたの言葉ですか？
あなたに言えばいいのですか？
あなたはどう言っていますか？
あなたはどう言っていますか？
あなたがこんなに言えるようになっていますか？
この言葉を言っています
言うべきやすいのです
この言葉を言っていますか？
こん
결과 2: 트랜스포머에 대해 자세한 설명 부탁해?
처음으로 시작해서 양파를 예정
생각해보세요
저희는 해보죠
처음으로 시작해보세요
이제 시작해서
저희는 앞으로 시작하고 싶었던
거예요

English: 
so, now I can just get that
out of the way.
But how to do it?
And here's the idea:
I don't really want to
have to put any code
into this,
so I'm going to
use these
little pieces of code
that you can
just get into a file
called "main.c"
and just put
those in there.
And they're called
variables.
So let me just write
out some of these.
Let me write one here.
This is called the
"int" variable.
It's the first
variable and
it's going to be
a number here.
It's called "i"
because it's the integer,
so that's what it's
going to be.
And it's going to be
the number of
the first character
in the string.
So what I'm going to
do is I'm going
to say, if I
ever want to
put that into
the main program,
then I'm going
to say, "int i = 0;

Korean: 
이제 저희는 시작해보세요
여
결과 3: 트랜스포머에 대해 자세한 설명 부탁해? 그런데 동안 가장 중요한 사람이 이미 여러분에게 의미하는 것을 확인해 주셔서 감사합니다.

영상에서 예약을 보고 싶어 한 팀을 잘 알리는 방식 화면처럼 보게되기 때문에 그래서 처음으로 이미 이어보겠습니다.

알고리즘 팀 인사에서 관심을 하는 방식으로 이미 하는 것은 예약을 잘 받아야 한다는 것을 알 수 있습니다. 영상에서 알고리즘 팀을 하는 것으로 관심입니다.

폴리스트에서
```
