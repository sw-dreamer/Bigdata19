# Santander Customer Satisfaction

데이터 : [캐글 산탄데로 고객 만족 예측](https://www.kaggle.com/competitions/santander-customer-satisfaction/overview)

클래스 레이블 명은 TARGET이며 이 값이 1이면 불만을 가진 고객, 0이면 만족한 고객

---
## 데이터 전처리

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib
import warnings
warnings.filterwarnings('ignore')

cust_df = pd.read_csv(
    './santander-customer-satisfaction/train.csv'
    ,encoding='latin-1'
)

print(f'dataset shape : {cust_df.shape}')
cust_df.head()
```
![image](https://github.com/user-attachments/assets/2eb0b5c0-1549-4903-998f-d527ea13a187)

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib
import warnings
warnings.filterwarnings('ignore')

cust_df = pd.read_csv(
    './santander-customer-satisfaction/train.csv'
    ,encoding='latin-1'
)

print(cust_df['TARGET'].value_counts())

unsatisfied_cnt=cust_df[cust_df['TARGET']==1].TARGET.count()
total_cnt=cust_df.TARGET.count()
print(f'unsatisfied 비율은 : {unsatisfied_cnt/total_cnt:.2f}')
```
![image](https://github.com/user-attachments/assets/b755a97d-90a9-4284-8597-01eca6e296c5)

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib
import warnings
warnings.filterwarnings('ignore')

cust_df = pd.read_csv(
    './santander-customer-satisfaction/train.csv'
    ,encoding='latin-1'
)

unsatisfied_cnt=cust_df[cust_df['TARGET']==1].TARGET.count()
total_cnt=cust_df.TARGET.count()

cust_df.describe()
```
![image](https://github.com/user-attachments/assets/9571e95f-a091-4aa8-9dfe-82a2aa6f8a95)

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib
import warnings
warnings.filterwarnings('ignore')

cust_df = pd.read_csv(
    './santander-customer-satisfaction/train.csv'
    ,encoding='latin-1'
)

unsatisfied_cnt=cust_df[cust_df['TARGET']==1].TARGET.count()
total_cnt=cust_df.TARGET.count()

print(f"cust_df['var3']의 min값 : {cust_df['var3'].min()}")
print(f"cust_df['var3']의 max값 : {cust_df['var3'].max()}")
print(f"cust_df['var3']의 mean값 : {cust_df['var3'].mean()}")
print(f"cust_df['var3']의 std값 : {cust_df['var3'].std()}")
print(f"cust_df['var3']의 var(분산)값 : {cust_df['var3'].var()}")

invalid_values_count = (cust_df['var3'] == -999999).sum()
print(f"'var3'에서 -999999인 값의 개수: {invalid_values_count}")
```
![image](https://github.com/user-attachments/assets/f22a77f2-4da6-43a2-a3c9-8f9c6665cf67)

> var3 칼럼의 경우 min 값이 -999999입니다.
> var3은 숫자 형이고, 다른 값에 비해 -999999은 너무 편차가 심하므로 -999999를 다른 값으로 변환하여 처리하겠습니다.

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib
import warnings
warnings.filterwarnings('ignore')

cust_df = pd.read_csv(
    './santander-customer-satisfaction/train.csv'
    ,encoding='latin-1'
)

# 데이터 전처리 : -999999 값을 제일 많은 2로 변환
cust_df['var3'].replace(-999999,2,inplace=True)

# ID feature은 일련번호로 의미가 없으므로 삭제를 한다
cust_df.drop(
    'ID'
    ,axis=1
    ,inplace=True
)

cust_df.describe()

```
![image](https://github.com/user-attachments/assets/a3e4d140-b385-4ef9-a844-0726309093ac)

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib
import warnings
warnings.filterwarnings('ignore')

cust_df = pd.read_csv(
    './santander-customer-satisfaction/train.csv'
    ,encoding='latin-1'
)

# 데이터 전처리 : -999999 값을 제일 많은 2로 변환
cust_df['var3'].replace(-999999,2,inplace=True)

# ID feature은 일련번호로 의미가 없으므로 삭제를 한다
cust_df.drop(
    'ID'
    ,axis=1
    ,inplace=True
)

# 데이터 / 레이블 분리
X_features=cust_df.iloc[:,:-1]
y_labels=cust_df.loc[:,'TARGET']


print(f'전체 데이터 구조 : {cust_df.shape}')
print(f'데이터 구조 : {X_features.shape}')
print(f'타겟 데이터 구조 : {y_labels.shape}')
```
![image](https://github.com/user-attachments/assets/8533d19b-0165-4cc9-8cf4-583e77ea0ef5)

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib
import warnings
warnings.filterwarnings('ignore')

cust_df = pd.read_csv(
    './santander-customer-satisfaction/train.csv'
    ,encoding='latin-1'
)

# 데이터 전처리 : -999999 값을 제일 많은 2로 변환
cust_df['var3'].replace(-999999,2,inplace=True)

# ID feature은 일련번호로 의미가 없으므로 삭제를 한다
cust_df.drop(
    'ID'
    ,axis=1
    ,inplace=True
)

# 데이터 / 레이블 분리
X_features=cust_df.iloc[:,:-1]
y_labels=cust_df.loc[:,'TARGET']

# 학습/테스트 데이터 분리
from sklearn.model_selection import train_test_split

X_train,X_test,y_train,y_test=train_test_split(
    X_features
    ,y_labels
    ,test_size=0.2
    ,random_state=0
    ,stratify=y_labels 
)

# 학습/테스트 레이블의 균형성 확인
# 학습/테스트 레이블의 개수 저장
train_cnt=y_train.count() # 학습데이터 레이블의 개수
test_cnt=y_test.count() # 테스트 데이터 레이블의 개수

print(f'학습 세트 Shape : {X_train.shape}, 테스트 세트 Shape : {X_test.shape}')
print('='*50)
print(f'학습 세트 레이블 값 분포 비율\n{y_train.value_counts()/train_cnt}')
print('='*50)
print(f'테스트 세트 레이블 값 분포 비율\n{y_test.value_counts()/test_cnt}')
```
![image](https://github.com/user-attachments/assets/4b43fdda-2d5f-4193-a261-81e74a2b605c)

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib
import warnings
warnings.filterwarnings('ignore')

cust_df = pd.read_csv(
    './santander-customer-satisfaction/train.csv'
    ,encoding='latin-1'
)

# 데이터 전처리 : -999999 값을 제일 많은 2로 변환
cust_df['var3'].replace(-999999,2,inplace=True)

# ID feature은 일련번호로 의미가 없으므로 삭제를 한다
cust_df.drop(
    'ID'
    ,axis=1
    ,inplace=True
)

# 데이터 / 레이블 분리
X_features=cust_df.iloc[:,:-1]
y_labels=cust_df.loc[:,'TARGET']

# 학습/테스트 데이터 분리
from sklearn.model_selection import train_test_split

X_train,X_test,y_train,y_test=train_test_split(
    X_features
    ,y_labels
    ,test_size=0.2
    ,random_state=0
    ,stratify=y_labels 
)

# 학습/테스트 레이블의 균형성 확인
# 학습/테스트 레이블의 개수 저장
train_cnt=y_train.count() # 학습데이터 레이블의 개수
test_cnt=y_test.count() # 테스트 데이터 레이블의 개수

X_tr,X_val,y_tr,y_val=train_test_split(
    X_train
    ,y_train
    ,test_size=0.3
    ,random_state=0
)

from xgboost import XGBClassifier
from sklearn.metrics import roc_auc_score

xgb_clf=XGBClassifier(
    n_estimators=500
    ,learning_rate=0.05
    ,early_stopping=100
    ,eval_metric='auc'
    ,random_state=156
)

xgb_clf.fit(
    X_tr
    ,y_tr
    ,eval_set=[
        (
            X_val,y_val
        )
    ]
)
```
![image](https://github.com/user-attachments/assets/ce4d1c1a-cd4f-4c3e-9b78-d2e6601caf64)

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib
import warnings
warnings.filterwarnings('ignore')

cust_df = pd.read_csv(
    './santander-customer-satisfaction/train.csv'
    ,encoding='latin-1'
)

# 데이터 전처리 : -999999 값을 제일 많은 2로 변환
cust_df['var3'].replace(-999999,2,inplace=True)

# ID feature은 일련번호로 의미가 없으므로 삭제를 한다
cust_df.drop(
    'ID'
    ,axis=1
    ,inplace=True
)

# 데이터 / 레이블 분리
X_features=cust_df.iloc[:,:-1]
y_labels=cust_df.loc[:,'TARGET']

# 학습/테스트 데이터 분리
from sklearn.model_selection import train_test_split

X_train,X_test,y_train,y_test=train_test_split(
    X_features
    ,y_labels
    ,test_size=0.2
    ,random_state=0
    ,stratify=y_labels 
)

# 학습/테스트 레이블의 균형성 확인
# 학습/테스트 레이블의 개수 저장
train_cnt=y_train.count() # 학습데이터 레이블의 개수
test_cnt=y_test.count() # 테스트 데이터 레이블의 개수

X_tr,X_val,y_tr,y_val=train_test_split(
    X_train
    ,y_train
    ,test_size=0.3
    ,random_state=0
)

from xgboost import XGBClassifier
from sklearn.metrics import roc_auc_score

xgb_clf=XGBClassifier(
    n_estimators=500
    ,learning_rate=0.05
    ,early_stopping=100
    ,eval_metric='auc'
    ,random_state=156
)

xgb_clf.fit(
    X_tr
    ,y_tr
    ,eval_set=[
        (
            X_val,y_val
        )
    ]
)

xgb_roc_score=roc_auc_score(y_test,xgb_clf.predict_proba(X_test)[:,1])
print(f'Roc Auc : {xgb_roc_score:.4f}')
```
![image](https://github.com/user-attachments/assets/4470f2a9-7996-4d5a-ba3e-d7208abf58af)

```
# gpu 버전
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib
import warnings
warnings.filterwarnings('ignore')

cust_df = pd.read_csv(
    './santander-customer-satisfaction/train.csv'
    ,encoding='latin-1'
)

# 데이터 전처리 : -999999 값을 제일 많은 2로 변환
cust_df['var3'].replace(-999999,2,inplace=True)

# ID feature은 일련번호로 의미가 없으므로 삭제를 한다
cust_df.drop(
    'ID'
    ,axis=1
    ,inplace=True
)

# 데이터 / 레이블 분리
X_features=cust_df.iloc[:,:-1]
y_labels=cust_df.loc[:,'TARGET']

# 학습/테스트 데이터 분리
from sklearn.model_selection import train_test_split

X_train,X_test,y_train,y_test=train_test_split(
    X_features
    ,y_labels
    ,test_size=0.2
    ,random_state=0
    ,stratify=y_labels 
)

# 학습/테스트 레이블의 균형성 확인
# 학습/테스트 레이블의 개수 저장
train_cnt=y_train.count() # 학습데이터 레이블의 개수
test_cnt=y_test.count() # 테스트 데이터 레이블의 개수

X_tr,X_val,y_tr,y_val=train_test_split(
    X_train
    ,y_train
    ,test_size=0.3
    ,random_state=0
)

from xgboost import XGBClassifier
from sklearn.metrics import roc_auc_score

xgb_clf_gpu = XGBClassifier(
    n_estimators=500,
    learning_rate=0.05,
    early_stopping_rounds=100,
    eval_metric='auc',
    random_state=156,
    tree_method='gpu_hist',  # Use GPU for training
    gpu_id=0  # Specify GPU device ID, if you have multiple GPUs
)


# Fit the model on training data
xgb_clf_gpu.fit(
    X_tr,
    y_tr,
    eval_set=[(X_val, y_val)]
)

# Evaluate model with ROC AUC score on test data
xgb_roc_score_gpu = roc_auc_score(y_test, xgb_clf_gpu.predict_proba(X_test)[:, 1])
print(f'GPU Roc Auc: {xgb_roc_score_gpu:.4f}')
```
![image](https://github.com/user-attachments/assets/93ae2dba-be8b-4a9f-adc6-8f9b8487f00c)

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib
import warnings
warnings.filterwarnings('ignore')

cust_df = pd.read_csv(
    './santander-customer-satisfaction/train.csv'
    ,encoding='latin-1'
)

# 데이터 전처리 : -999999 값을 제일 많은 2로 변환
cust_df['var3'].replace(-999999,2,inplace=True)

# ID feature은 일련번호로 의미가 없으므로 삭제를 한다
cust_df.drop(
    'ID'
    ,axis=1
    ,inplace=True
)

# 데이터 / 레이블 분리
X_features=cust_df.iloc[:,:-1]
y_labels=cust_df.loc[:,'TARGET']

# 학습/테스트 데이터 분리
from sklearn.model_selection import train_test_split

X_train,X_test,y_train,y_test=train_test_split(
    X_features
    ,y_labels
    ,test_size=0.2
    ,random_state=0
    ,stratify=y_labels 
)

# 학습/테스트 레이블의 균형성 확인
# 학습/테스트 레이블의 개수 저장
train_cnt=y_train.count() # 학습데이터 레이블의 개수
test_cnt=y_test.count() # 테스트 데이터 레이블의 개수

X_tr,X_val,y_tr,y_val=train_test_split(
    X_train
    ,y_train
    ,test_size=0.3
    ,random_state=0
)

from xgboost import XGBClassifier
from sklearn.metrics import roc_auc_score

xgb_clf=XGBClassifier(
    n_estimators=500
    ,learning_rate=0.05
    ,early_stopping=100
    ,eval_metric='auc'
    ,random_state=156
)

xgb_clf.fit(
    X_tr
    ,y_tr
    ,eval_set=[
        (
            X_val,y_val
        )
    ]
)

xgb_roc_score=roc_auc_score(y_test,xgb_clf.predict_proba(X_test)[:,1])

# hyperopt, KFold 사용한 튜닝
from hyperopt import hp


# max_depth는 5에서 15까지 1간격으로, min_child_weight는 1에서 6까지 1간격으로
# colsample_bytree는 0.5에서 0.95사이, learning_rate는 0.01에서 0.2사이 정규 분포된 값으로 검색.


xgb_search_space = {
    'max_depth': hp.quniform('max_depth', 5, 15, 1),
    'min_child_weight': hp.quniform('min_child_weight', 1, 6, 1),
    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 0.95),
    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2)
}

from sklearn.model_selection import KFold
from sklearn.metrics import roc_auc_score


# 목적 함수 설정.
# 추후 fmin()에서 입력된 search_space값으로 XGBClassifier 교차 검증 학습 후 -1* roc_auc 평균 값을 반환.  
def objective_func(search_space):
    xgb_clf = XGBClassifier(
        n_estimators=100, max_depth=int(search_space['max_depth'])
        ,min_child_weight=int(search_space['min_child_weight'])
        ,colsample_bytree=search_space['colsample_bytree']
        ,learning_rate=search_space['learning_rate']
        ,early_stopping_rounds=30
        , eval_metric='auc'
    )
    # 3개 k-fold 방식으로 평가된 roc_auc 지표를 담는 list
    roc_auc_list= []
   
    # 3개 k-fold방식 적용
    kf = KFold(n_splits=3)
    # X_train을 다시 학습과 검증용 데이터로 분리
    for tr_index, val_index in kf.split(X_train): # X_train : 6만건
        # kf.split(X_train)으로 추출된 학습과 검증 index값으로 학습과 검증 데이터 세트 분리
        X_tr, y_tr = X_train.iloc[tr_index], y_train.iloc[tr_index]
        X_val, y_val = X_train.iloc[val_index], y_train.iloc[val_index]
        # early stopping은 30회로 설정하고 추출된 학습과 검증 데이터로 XGBClassifier 학습 수행.
        xgb_clf.fit(X_tr, y_tr,eval_set=[(X_tr, y_tr), (X_val, y_val)])
   
        # 1로 예측한 확률값 추출후 roc auc 계산하고 평균 roc auc 계산을 위해 list에 결과값 담음.
        score = roc_auc_score(y_val, xgb_clf.predict_proba(X_val)[:, 1])
        roc_auc_list.append(score)
       
    # 3개 k-fold로 계산된 roc_auc값의 평균값을 반환하되,
    # HyperOpt는 목적함수의 최소값을 위한 입력값을 찾으므로 -1을 곱한 뒤 반환.
    return -1 * np.mean(roc_auc_list)

from hyperopt import fmin, tpe, Trials


trials = Trials()


# fmin()함수를 호출. max_evals지정된 횟수만큼 반복 후 목적함수의 최소값을 가지는 최적 입력값 추출.
best = fmin(
    fn=objective_func
    ,space=xgb_search_space
    ,algo=tpe.suggest
    ,max_evals=50 # 최대 반복 횟수를 지정합니다.
    , trials=trials
    , rstate=np.random.default_rng(seed=30)
)
print('best:', best)
```
![image](https://github.com/user-attachments/assets/6c3f2b0f-9933-4049-91c0-f78d0d10c96c)

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib
import warnings
warnings.filterwarnings('ignore')

cust_df = pd.read_csv(
    './santander-customer-satisfaction/train.csv'
    ,encoding='latin-1'
)

# 데이터 전처리 : -999999 값을 제일 많은 2로 변환
cust_df['var3'].replace(-999999,2,inplace=True)

# ID feature은 일련번호로 의미가 없으므로 삭제를 한다
cust_df.drop(
    'ID'
    ,axis=1
    ,inplace=True
)

# 데이터 / 레이블 분리
X_features=cust_df.iloc[:,:-1]
y_labels=cust_df.loc[:,'TARGET']

# 학습/테스트 데이터 분리
from sklearn.model_selection import train_test_split

X_train,X_test,y_train,y_test=train_test_split(
    X_features
    ,y_labels
    ,test_size=0.2
    ,random_state=0
    ,stratify=y_labels 
)

# 학습/테스트 레이블의 균형성 확인
# 학습/테스트 레이블의 개수 저장
train_cnt=y_train.count() # 학습데이터 레이블의 개수
test_cnt=y_test.count() # 테스트 데이터 레이블의 개수

X_tr,X_val,y_tr,y_val=train_test_split(
    X_train
    ,y_train
    ,test_size=0.3
    ,random_state=0
)

from xgboost import XGBClassifier
from sklearn.metrics import roc_auc_score

xgb_clf=XGBClassifier(
    n_estimators=500
    ,learning_rate=0.05
    ,early_stopping=100
    ,eval_metric='auc'
    ,random_state=156
)

xgb_clf.fit(
    X_tr
    ,y_tr
    ,eval_set=[
        (
            X_val,y_val
        )
    ]
)

xgb_roc_score=roc_auc_score(y_test,xgb_clf.predict_proba(X_test)[:,1])

# hyperopt, KFold 사용한 튜닝
from hyperopt import hp


# max_depth는 5에서 15까지 1간격으로, min_child_weight는 1에서 6까지 1간격으로
# colsample_bytree는 0.5에서 0.95사이, learning_rate는 0.01에서 0.2사이 정규 분포된 값으로 검색.


xgb_search_space = {
    'max_depth': hp.quniform('max_depth', 5, 15, 1),
    'min_child_weight': hp.quniform('min_child_weight', 1, 6, 1),
    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 0.95),
    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2)
}

from sklearn.model_selection import KFold
from sklearn.metrics import roc_auc_score


# 목적 함수 설정.
# 추후 fmin()에서 입력된 search_space값으로 XGBClassifier 교차 검증 학습 후 -1* roc_auc 평균 값을 반환.  
def objective_func(search_space):
    xgb_clf = XGBClassifier(
        n_estimators=100, max_depth=int(search_space['max_depth'])
        ,min_child_weight=int(search_space['min_child_weight'])
        ,colsample_bytree=search_space['colsample_bytree']
        ,learning_rate=search_space['learning_rate']
        ,early_stopping_rounds=30
        , eval_metric='auc'
        ,tree_method='gpu_hist'
        ,gpu_id=0
    )
    # 3개 k-fold 방식으로 평가된 roc_auc 지표를 담는 list
    roc_auc_list= []
   
    # 3개 k-fold방식 적용
    kf = KFold(n_splits=3)
    # X_train을 다시 학습과 검증용 데이터로 분리
    for tr_index, val_index in kf.split(X_train): # X_train : 6만건
        # kf.split(X_train)으로 추출된 학습과 검증 index값으로 학습과 검증 데이터 세트 분리
        X_tr, y_tr = X_train.iloc[tr_index], y_train.iloc[tr_index]
        X_val, y_val = X_train.iloc[val_index], y_train.iloc[val_index]
        # early stopping은 30회로 설정하고 추출된 학습과 검증 데이터로 XGBClassifier 학습 수행.
        xgb_clf.fit(X_tr, y_tr,eval_set=[(X_tr, y_tr), (X_val, y_val)])
   
        # 1로 예측한 확률값 추출후 roc auc 계산하고 평균 roc auc 계산을 위해 list에 결과값 담음.
        score = roc_auc_score(y_val, xgb_clf.predict_proba(X_val)[:, 1])
        roc_auc_list.append(score)
       
    # 3개 k-fold로 계산된 roc_auc값의 평균값을 반환하되,
    # HyperOpt는 목적함수의 최소값을 위한 입력값을 찾으므로 -1을 곱한 뒤 반환.
    return -1 * np.mean(roc_auc_list)

from hyperopt import fmin, tpe, Trials


trials = Trials()


# fmin()함수를 호출. max_evals지정된 횟수만큼 반복 후 목적함수의 최소값을 가지는 최적 입력값 추출.
best = fmin(
    fn=objective_func
    ,space=xgb_search_space
    ,algo=tpe.suggest
    ,max_evals=50 # 최대 반복 횟수를 지정합니다.
    , trials=trials
    , rstate=np.random.default_rng(seed=30)
)


print('best:', best)
```
![image](https://github.com/user-attachments/assets/1402d805-25fe-4628-a48d-922e7a26f308)

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib
import warnings
warnings.filterwarnings('ignore')

cust_df = pd.read_csv(
    './santander-customer-satisfaction/train.csv'
    ,encoding='latin-1'
)

# 데이터 전처리 : -999999 값을 제일 많은 2로 변환
cust_df['var3'].replace(-999999,2,inplace=True)

# ID feature은 일련번호로 의미가 없으므로 삭제를 한다
cust_df.drop(
    'ID'
    ,axis=1
    ,inplace=True
)

# 데이터 / 레이블 분리
X_features = cust_df.iloc[:, :-1]
y_labels = cust_df.loc[:, 'TARGET']

# 학습/테스트 데이터 분리
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X_features
    , y_labels
    , test_size=0.2
    , random_state=0
    , stratify=y_labels
)

# 학습/테스트 레이블의 균형성 확인
# 학습/테스트 레이블의 개수 저장
train_cnt = y_train.count()  # 학습데이터 레이블의 개수
test_cnt = y_test.count()  # 테스트 데이터 레이블의 개수

X_tr, X_val, y_tr, y_val = train_test_split(
    X_train
    , y_train
    , test_size=0.3
    , random_state=0
)

from xgboost import XGBClassifier
from sklearn.metrics import roc_auc_score

xgb_clf = XGBClassifier(
    n_estimators=500
    , learning_rate=0.05
    , early_stopping_rounds=100
    , eval_metric='auc'
    , random_state=156
    , tree_method='gpu_hist'  # GPU version
    , gpu_id=0  # GPU index (use 0 for the first GPU)
)

xgb_clf.fit(
    X_tr
    , y_tr
    , eval_set=[
        (
            X_val, y_val
        )
    ]
)

xgb_roc_score = roc_auc_score(y_test, xgb_clf.predict_proba(X_test)[:, 1])

# hyperopt, KFold 사용한 튜닝
from hyperopt import hp

# max_depth는 5에서 15까지 1간격으로, min_child_weight는 1에서 6까지 1간격으로
# colsample_bytree는 0.5에서 0.95사이, learning_rate는 0.01에서 0.2사이 정규 분포된 값으로 검색.

xgb_search_space = {
    'max_depth': hp.quniform('max_depth', 5, 15, 1),
    'min_child_weight': hp.quniform('min_child_weight', 1, 6, 1),
    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 0.95),
    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2)
}

from sklearn.model_selection import KFold
from sklearn.metrics import roc_auc_score


# 목적 함수 설정.
# 추후 fmin()에서 입력된 search_space값으로 XGBClassifier 교차 검증 학습 후 -1* roc_auc 평균 값을 반환.
def objective_func(search_space):
    xgb_clf = XGBClassifier(
        n_estimators=100, max_depth=int(search_space['max_depth'])
        , min_child_weight=int(search_space['min_child_weight'])
        , colsample_bytree=search_space['colsample_bytree']
        , learning_rate=search_space['learning_rate']
        , early_stopping_rounds=30
        , eval_metric='auc'
        , tree_method='gpu_hist'  # GPU version
        , gpu_id=0  # GPU index (use 0 for the first GPU)
        , n_jobs=-1
    )
    # 3개 k-fold 방식으로 평가된 roc_auc 지표를 담는 list
    roc_auc_list = []
   
    # 3개 k-fold방식 적용
    kf = KFold(n_splits=3)
    # X_train을 다시 학습과 검증용 데이터로 분리
    for tr_index, val_index in kf.split(X_train):  # X_train : 6만건
        # kf.split(X_train)으로 추출된 학습과 검증 index값으로 학습과 검증 데이터 세트 분리
        X_tr, y_tr = X_train.iloc[tr_index], y_train.iloc[tr_index]
        X_val, y_val = X_train.iloc[val_index], y_train.iloc[val_index]
        # early stopping은 30회로 설정하고 추출된 학습과 검증 데이터로 XGBClassifier 학습 수행.
        xgb_clf.fit(X_tr, y_tr, eval_set=[(X_tr, y_tr), (X_val, y_val)])
   
        # 1로 예측한 확률값 추출후 roc auc 계산하고 평균 roc auc 계산을 위해 list에 결과값 담음.
        score = roc_auc_score(y_val, xgb_clf.predict_proba(X_val)[:, 1])
        roc_auc_list.append(score)
       
    # 3개 k-fold로 계산된 roc_auc값의 평균값을 반환하되,
    # HyperOpt는 목적함수의 최소값을 위한 입력값을 찾으므로 -1을 곱한 뒤 반환.
    return -1 * np.mean(roc_auc_list)

from hyperopt import fmin, tpe, Trials

trials = Trials()

# fmin()함수를 호출. max_evals지정된 횟수만큼 반복 후 목적함수의 최소값을 가지는 최적 입력값 추출.
best = fmin(
    fn=objective_func
    , space=xgb_search_space
    , algo=tpe.suggest
    , max_evals=50  # 최대 반복 횟수를 지정합니다.
    , trials=trials
    , rstate=np.random.default_rng(seed=30)
)

# n_estimators를 500증가 후 최적으로 찾은 하이퍼 파라미터를 기반으로 학습과 예측 수행.
# evaluation metric을 auc로, early stopping은 100 으로 설정하고 학습 수행. 
xgb_clf = XGBClassifier(n_estimators=500, learning_rate=round(best['learning_rate'], 5),
                        max_depth=int(best['max_depth']), min_child_weight=int(best['min_child_weight']), 
                        colsample_bytree=round(best['colsample_bytree'], 5)
                        , n_jobs=-1
                        ,early_stopping=100
                        , tree_method='gpu_hist'  # GPU version
                        ,eval_metric="auc"
                        , gpu_id=0  # GPU index
                       )


xgb_clf.fit(X_tr, y_tr, 
            eval_set=[(X_tr, y_tr), (X_val, y_val)])

xgb_roc_score = roc_auc_score(y_test, xgb_clf.predict_proba(X_test)[:, 1])
print('ROC AUC: {0:.4f}'.format(xgb_roc_score))
```
![image](https://github.com/user-attachments/assets/f7729300-85be-4c14-8287-b06eba77990f)

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib
import warnings
warnings.filterwarnings('ignore')

cust_df = pd.read_csv(
    './santander-customer-satisfaction/train.csv'
    ,encoding='latin-1'
)

# 데이터 전처리 : -999999 값을 제일 많은 2로 변환
cust_df['var3'].replace(-999999,2,inplace=True)

# ID feature은 일련번호로 의미가 없으므로 삭제를 한다
cust_df.drop(
    'ID'
    ,axis=1
    ,inplace=True
)

# 데이터 / 레이블 분리
X_features = cust_df.iloc[:, :-1]
y_labels = cust_df.loc[:, 'TARGET']

# 학습/테스트 데이터 분리
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X_features
    , y_labels
    , test_size=0.2
    , random_state=0
    , stratify=y_labels
)

# 학습/테스트 레이블의 균형성 확인
# 학습/테스트 레이블의 개수 저장
train_cnt = y_train.count()  # 학습데이터 레이블의 개수
test_cnt = y_test.count()  # 테스트 데이터 레이블의 개수

X_tr, X_val, y_tr, y_val = train_test_split(
    X_train
    , y_train
    , test_size=0.3
    , random_state=0
)

from xgboost import XGBClassifier
from sklearn.metrics import roc_auc_score

xgb_clf = XGBClassifier(
    n_estimators=500
    , learning_rate=0.05
    , early_stopping_rounds=100
    , eval_metric='auc'
    , random_state=156
    , tree_method='gpu_hist'  # GPU version
    , gpu_id=0  # GPU index (use 0 for the first GPU)
)

xgb_clf.fit(
    X_tr
    , y_tr
    , eval_set=[
        (
            X_val, y_val
        )
    ]
)

xgb_roc_score = roc_auc_score(y_test, xgb_clf.predict_proba(X_test)[:, 1])

# hyperopt, KFold 사용한 튜닝
from hyperopt import hp

# max_depth는 5에서 15까지 1간격으로, min_child_weight는 1에서 6까지 1간격으로
# colsample_bytree는 0.5에서 0.95사이, learning_rate는 0.01에서 0.2사이 정규 분포된 값으로 검색.

xgb_search_space = {
    'max_depth': hp.quniform('max_depth', 5, 15, 1),
    'min_child_weight': hp.quniform('min_child_weight', 1, 6, 1),
    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 0.95),
    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2)
}

from sklearn.model_selection import KFold
from sklearn.metrics import roc_auc_score


# 목적 함수 설정.
# 추후 fmin()에서 입력된 search_space값으로 XGBClassifier 교차 검증 학습 후 -1* roc_auc 평균 값을 반환.
def objective_func(search_space):
    xgb_clf = XGBClassifier(
        n_estimators=100, max_depth=int(search_space['max_depth'])
        , min_child_weight=int(search_space['min_child_weight'])
        , colsample_bytree=search_space['colsample_bytree']
        , learning_rate=search_space['learning_rate']
        , early_stopping_rounds=30
        , eval_metric='auc'
        , tree_method='gpu_hist'  # GPU version
        , gpu_id=0  # GPU index (use 0 for the first GPU)
        , n_jobs=-1
    )
    # 3개 k-fold 방식으로 평가된 roc_auc 지표를 담는 list
    roc_auc_list = []
   
    # 3개 k-fold방식 적용
    kf = KFold(n_splits=3)
    # X_train을 다시 학습과 검증용 데이터로 분리
    for tr_index, val_index in kf.split(X_train):  # X_train : 6만건
        # kf.split(X_train)으로 추출된 학습과 검증 index값으로 학습과 검증 데이터 세트 분리
        X_tr, y_tr = X_train.iloc[tr_index], y_train.iloc[tr_index]
        X_val, y_val = X_train.iloc[val_index], y_train.iloc[val_index]
        # early stopping은 30회로 설정하고 추출된 학습과 검증 데이터로 XGBClassifier 학습 수행.
        xgb_clf.fit(X_tr, y_tr, eval_set=[(X_tr, y_tr), (X_val, y_val)])
   
        # 1로 예측한 확률값 추출후 roc auc 계산하고 평균 roc auc 계산을 위해 list에 결과값 담음.
        score = roc_auc_score(y_val, xgb_clf.predict_proba(X_val)[:, 1])
        roc_auc_list.append(score)
       
    # 3개 k-fold로 계산된 roc_auc값의 평균값을 반환하되,
    # HyperOpt는 목적함수의 최소값을 위한 입력값을 찾으므로 -1을 곱한 뒤 반환.
    return -1 * np.mean(roc_auc_list)

from hyperopt import fmin, tpe, Trials

trials = Trials()

# fmin()함수를 호출. max_evals지정된 횟수만큼 반복 후 목적함수의 최소값을 가지는 최적 입력값 추출.
best = fmin(
    fn=objective_func
    , space=xgb_search_space
    , algo=tpe.suggest
    , max_evals=50  # 최대 반복 횟수를 지정합니다.
    , trials=trials
    , rstate=np.random.default_rng(seed=30)
)

# n_estimators를 500증가 후 최적으로 찾은 하이퍼 파라미터를 기반으로 학습과 예측 수행.
# evaluation metric을 auc로, early stopping은 100 으로 설정하고 학습 수행. 
xgb_clf = XGBClassifier(n_estimators=500, learning_rate=round(best['learning_rate'], 5),
                        max_depth=int(best['max_depth']), min_child_weight=int(best['min_child_weight']), 
                        colsample_bytree=round(best['colsample_bytree'], 5)
                        , n_jobs=-1
                        ,early_stopping=100
                        , tree_method='gpu_hist'  # GPU version
                        ,eval_metric="auc"
                        , gpu_id=0  # GPU index
                       )


xgb_clf.fit(X_tr, y_tr, 
            eval_set=[(X_tr, y_tr), (X_val, y_val)])

xgb_roc_score = roc_auc_score(y_test, xgb_clf.predict_proba(X_test)[:, 1])

from xgboost import plot_importance

fig,ax =plt.subplots(
    1
    ,1
    ,figsize=(10,8)
)
plot_importance(xgb_clf,ax=ax,max_num_features=20,height=0.4)
plt.show()
```
![image](https://github.com/user-attachments/assets/2d0f7cab-20eb-4a81-b9eb-191d8b403162)

---
## LightGBM 모델 학습과 하이퍼 파라미터 튜닝(GPU버전)

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib
import warnings
from sklearn.model_selection import train_test_split
from lightgbm import LGBMClassifier
from sklearn.metrics import roc_auc_score

warnings.filterwarnings('ignore')

# 데이터 로드
cust_df = pd.read_csv('./santander-customer-satisfaction/train.csv', encoding='latin-1')

# 데이터 전처리 : -999999 값을 제일 많은 2로 변환
cust_df['var3'].replace(-999999, 2, inplace=True)

# ID feature은 일련번호로 의미가 없으므로 삭제
cust_df.drop('ID', axis=1, inplace=True)

# 데이터 / 레이블 분리
X_features = cust_df.iloc[:, :-1]
y_labels = cust_df.loc[:, 'TARGET']

# 학습/테스트 데이터 분리
X_train, X_test, y_train, y_test = train_test_split(X_features, y_labels, test_size=0.2, random_state=0, stratify=y_labels)

# 학습/테스트 레이블의 균형성 확인
train_cnt = y_train.count()  # 학습 데이터 레이블의 개수
test_cnt = y_test.count()    # 테스트 데이터 레이블의 개수

X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.3, random_state=0)

# LightGBM 모델 학습과 하이퍼 파라미터 튜닝 (GPU 버전 사용)
lgbm_clf = LGBMClassifier(
    n_estimators=500,
    device='gpu',  # GPU 사용을 설정
    boosting_type='gbdt',  # GPU로 GBDT 부스터 사용
    gpu_platform_id=0,  # 사용하려는 GPU의 platform ID
    gpu_device_id=0,  # 사용할 GPU의 device ID (기본값은 0)
    early_stopping_round=100
)

eval_set = [
    (X_tr, y_tr),
    (X_val, y_val)
]

# 모델 학습
lgbm_clf.fit(
    X_tr, y_tr,
    eval_metric='auc',
    eval_set=eval_set
)

# ROC AUC 성능 평가
lgbm_roc_score = roc_auc_score(
    y_test,
    lgbm_clf.predict_proba(X_test)[:, 1]
)

print(f'ROC AUC : {lgbm_roc_score}')

```
![image](https://github.com/user-attachments/assets/d4576062-4600-4105-a9b6-2aa006821cc2)

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib
import warnings
from sklearn.model_selection import train_test_split
from lightgbm import LGBMClassifier
from sklearn.metrics import roc_auc_score

warnings.filterwarnings('ignore')

# 데이터 로드
cust_df = pd.read_csv('./santander-customer-satisfaction/train.csv', encoding='latin-1')

# 데이터 전처리 : -999999 값을 제일 많은 2로 변환
cust_df['var3'].replace(-999999, 2, inplace=True)

# ID feature은 일련번호로 의미가 없으므로 삭제
cust_df.drop('ID', axis=1, inplace=True)

# 데이터 / 레이블 분리
X_features = cust_df.iloc[:, :-1]
y_labels = cust_df.loc[:, 'TARGET']

# 학습/테스트 데이터 분리
X_train, X_test, y_train, y_test = train_test_split(X_features, y_labels, test_size=0.2, random_state=0)

# 학습/테스트 레이블의 균형성 확인
train_cnt = y_train.count()  # 학습 데이터 레이블의 개수
test_cnt = y_test.count()    # 테스트 데이터 레이블의 개수

X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.3, random_state=0)

# LightGBM 모델 학습과 하이퍼 파라미터 튜닝 (GPU 버전 사용)
lgbm_clf = LGBMClassifier(
    n_estimators=500,
    device='gpu',  # GPU 사용을 설정
    boosting_type='gbdt',  # GPU로 GBDT 부스터 사용
    gpu_platform_id=0,  # 사용하려는 GPU의 platform ID
    gpu_device_id=0,  # 사용할 GPU의 device ID (기본값은 0)
    early_stopping_round=100
)

eval_set = [
    (X_tr, y_tr),
    (X_val, y_val)
]

# 모델 학습
lgbm_clf.fit(
    X_tr, y_tr,
    eval_metric='auc',
    eval_set=eval_set
)

# ROC AUC 성능 평가
lgbm_roc_score = roc_auc_score(
    y_test,
    lgbm_clf.predict_proba(X_test)[:, 1]
)

print(f'ROC AUC : {lgbm_roc_score}')
```
![image](https://github.com/user-attachments/assets/4dfc9dab-453c-46e9-b12c-7af9ea8cdc0f)

```
import numpy as np
import pandas as pd
from hyperopt import hp
import matplotlib.pyplot as plt
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from hyperopt import STATUS_OK
from hyperopt import fmin, tpe, Trials
from sklearn.model_selection import cross_val_score
from xgboost import XGBClassifier
import warnings
from lightgbm import LGBMClassifier
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.model_selection import KFold

warnings.filterwarnings('ignore')

# 데이터 준비
dataset = load_breast_cancer()
X_features = dataset.data
y_label = dataset.target
cancer_dr = pd.DataFrame(data=X_features, columns=dataset.feature_names)
cancer_dr['target'] = y_label

X_features = cancer_dr.iloc[:, :-1]
y_label = cancer_dr.iloc[:, -1]

X_train, X_test, y_train, y_test = train_test_split(X_features, y_label, train_size=0.8, random_state=156)
X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=156)

# LightGBM GPU 버전 설정
lgbm_search_space = {
    'num_leaves': hp.quniform('num_leaves', 32, 64, 1),
    'max_depth': hp.quniform('max_depth', 100, 160, 1),
    'min_child_samples': hp.quniform('min_child_samples', 60, 100, 1),
    'subsample': hp.uniform('subsample', 0.7, 1),
    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2)
}

def objective_func_lgbm(search_space):
    lgbm_clf = LGBMClassifier(
        n_estimators=500,
        num_leaves=int(search_space['num_leaves']),
        max_depth=int(search_space['max_depth']),
        min_child_samples=int(search_space['min_child_samples']),
        subsample=search_space['subsample'],
        learning_rate=search_space['learning_rate'],
        device='gpu',  # GPU 사용 설정
        gpu_device_id=0,  # GPU 0번 사용
        early_stopping_round=10
    )

    roc_auc_list = []
    kf = KFold(n_splits=3)
    for tr_index, val_index in kf.split(X_train):
        X_tr_fold, y_tr_fold = X_train.iloc[tr_index], y_train.iloc[tr_index]
        X_val_fold, y_val_fold = X_train.iloc[val_index], y_train.iloc[val_index]
        lgbm_clf.fit(X_tr_fold, y_tr_fold, eval_set=[(X_tr_fold, y_tr_fold), (X_val_fold, y_val_fold)])
        score = roc_auc_score(y_val_fold, lgbm_clf.predict_proba(X_val_fold)[:, 1])
        roc_auc_list.append(score)

    return -1 * np.mean(roc_auc_list)

trials_lgbm = Trials()

best_lgbm = fmin(
    fn=objective_func_lgbm,
    space=lgbm_search_space,
    algo=tpe.suggest,
    max_evals=50,
    trials=trials_lgbm,
    rstate=np.random.default_rng(seed=30)

)

print("Best LightGBM Hyperparameters:", best_lgbm)

# 최적 하이퍼 파라미터로 LightGBM 학습
lgbm_clf = LGBMClassifier(
    n_estimators=500,
    num_leaves=int(best_lgbm['num_leaves']),
    max_depth=int(best_lgbm['max_depth']),
    min_child_samples=int(best_lgbm['min_child_samples']),
    subsample=best_lgbm['subsample'],
    learning_rate=best_lgbm['learning_rate'],
    device='gpu',  # GPU 사용 설정
    gpu_device_id=0,  # GPU 0번 사용
    early_stopping_round=10,
    n_jobs=-1
)

lgbm_clf.fit(X_tr, y_tr, eval_metric="auc", eval_set=[(X_tr, y_tr), (X_val, y_val)])
lgbm_roc_score = roc_auc_score(y_test, lgbm_clf.predict_proba(X_test)[:, 1])
print("LightGBM ROC AUC Score: {0:.4f}".format(lgbm_roc_score))
```
![image](https://github.com/user-attachments/assets/db2307b6-c1ec-43e2-9a9b-29ff3cebf214)
