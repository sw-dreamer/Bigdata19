# Santander Customer Satisfaction

데이터 : [캐글 산탄데로 고객 만족 예측](https://www.kaggle.com/competitions/santander-customer-satisfaction/overview)

클래스 레이블 명은 TARGET이며 이 값이 1이면 불만을 가진 고객, 0이면 만족한 고객

---
## 데이터 전처리

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib
import warnings
warnings.filterwarnings('ignore')

cust_df = pd.read_csv(
    './santander-customer-satisfaction/train.csv'
    ,encoding='latin-1'
)

print(f'dataset shape : {cust_df.shape}')
cust_df.head()
```
![image](https://github.com/user-attachments/assets/2eb0b5c0-1549-4903-998f-d527ea13a187)

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib
import warnings
warnings.filterwarnings('ignore')

cust_df = pd.read_csv(
    './santander-customer-satisfaction/train.csv'
    ,encoding='latin-1'
)

print(cust_df['TARGET'].value_counts())

unsatisfied_cnt=cust_df[cust_df['TARGET']==1].TARGET.count()
total_cnt=cust_df.TARGET.count()
print(f'unsatisfied 비율은 : {unsatisfied_cnt/total_cnt:.2f}')
```
![image](https://github.com/user-attachments/assets/b755a97d-90a9-4284-8597-01eca6e296c5)

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib
import warnings
warnings.filterwarnings('ignore')

cust_df = pd.read_csv(
    './santander-customer-satisfaction/train.csv'
    ,encoding='latin-1'
)

unsatisfied_cnt=cust_df[cust_df['TARGET']==1].TARGET.count()
total_cnt=cust_df.TARGET.count()

cust_df.describe()
```
![image](https://github.com/user-attachments/assets/9571e95f-a091-4aa8-9dfe-82a2aa6f8a95)

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib
import warnings
warnings.filterwarnings('ignore')

cust_df = pd.read_csv(
    './santander-customer-satisfaction/train.csv'
    ,encoding='latin-1'
)

unsatisfied_cnt=cust_df[cust_df['TARGET']==1].TARGET.count()
total_cnt=cust_df.TARGET.count()

print(f"cust_df['var3']의 min값 : {cust_df['var3'].min()}")
print(f"cust_df['var3']의 max값 : {cust_df['var3'].max()}")
print(f"cust_df['var3']의 mean값 : {cust_df['var3'].mean()}")
print(f"cust_df['var3']의 std값 : {cust_df['var3'].std()}")
print(f"cust_df['var3']의 var(분산)값 : {cust_df['var3'].var()}")

invalid_values_count = (cust_df['var3'] == -999999).sum()
print(f"'var3'에서 -999999인 값의 개수: {invalid_values_count}")
```
![image](https://github.com/user-attachments/assets/f22a77f2-4da6-43a2-a3c9-8f9c6665cf67)

> var3 칼럼의 경우 min 값이 -999999입니다.
> var3은 숫자 형이고, 다른 값에 비해 -999999은 너무 편차가 심하므로 -999999를 다른 값으로 변환하여 처리하겠습니다.

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib
import warnings
warnings.filterwarnings('ignore')

cust_df = pd.read_csv(
    './santander-customer-satisfaction/train.csv'
    ,encoding='latin-1'
)

# 데이터 전처리 : -999999 값을 제일 많은 2로 변환
cust_df['var3'].replace(-999999,2,inplace=True)

# ID feature은 일련번호로 의미가 없으므로 삭제를 한다
cust_df.drop(
    'ID'
    ,axis=1
    ,inplace=True
)

cust_df.describe()

```
![image](https://github.com/user-attachments/assets/a3e4d140-b385-4ef9-a844-0726309093ac)

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib
import warnings
warnings.filterwarnings('ignore')

cust_df = pd.read_csv(
    './santander-customer-satisfaction/train.csv'
    ,encoding='latin-1'
)

# 데이터 전처리 : -999999 값을 제일 많은 2로 변환
cust_df['var3'].replace(-999999,2,inplace=True)

# ID feature은 일련번호로 의미가 없으므로 삭제를 한다
cust_df.drop(
    'ID'
    ,axis=1
    ,inplace=True
)

# 데이터 / 레이블 분리
X_features=cust_df.iloc[:,:-1]
y_labels=cust_df.loc[:,'TARGET']


print(f'전체 데이터 구조 : {cust_df.shape}')
print(f'데이터 구조 : {X_features.shape}')
print(f'타겟 데이터 구조 : {y_labels.shape}')
```
![image](https://github.com/user-attachments/assets/8533d19b-0165-4cc9-8cf4-583e77ea0ef5)

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib
import warnings
warnings.filterwarnings('ignore')

cust_df = pd.read_csv(
    './santander-customer-satisfaction/train.csv'
    ,encoding='latin-1'
)

# 데이터 전처리 : -999999 값을 제일 많은 2로 변환
cust_df['var3'].replace(-999999,2,inplace=True)

# ID feature은 일련번호로 의미가 없으므로 삭제를 한다
cust_df.drop(
    'ID'
    ,axis=1
    ,inplace=True
)

# 데이터 / 레이블 분리
X_features=cust_df.iloc[:,:-1]
y_labels=cust_df.loc[:,'TARGET']

# 학습/테스트 데이터 분리
from sklearn.model_selection import train_test_split

X_train,X_test,y_train,y_test=train_test_split(
    X_features
    ,y_labels
    ,test_size=0.2
    ,random_state=0
    ,stratify=y_labels 
)

# 학습/테스트 레이블의 균형성 확인
# 학습/테스트 레이블의 개수 저장
train_cnt=y_train.count() # 학습데이터 레이블의 개수
test_cnt=y_test.count() # 테스트 데이터 레이블의 개수

print(f'학습 세트 Shape : {X_train.shape}, 테스트 세트 Shape : {X_test.shape}')
print('='*50)
print(f'학습 세트 레이블 값 분포 비율\n{y_train.value_counts()/train_cnt}')
print('='*50)
print(f'테스트 세트 레이블 값 분포 비율\n{y_test.value_counts()/test_cnt}')
```
![image](https://github.com/user-attachments/assets/4b43fdda-2d5f-4193-a261-81e74a2b605c)

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib
import warnings
warnings.filterwarnings('ignore')

cust_df = pd.read_csv(
    './santander-customer-satisfaction/train.csv'
    ,encoding='latin-1'
)

# 데이터 전처리 : -999999 값을 제일 많은 2로 변환
cust_df['var3'].replace(-999999,2,inplace=True)

# ID feature은 일련번호로 의미가 없으므로 삭제를 한다
cust_df.drop(
    'ID'
    ,axis=1
    ,inplace=True
)

# 데이터 / 레이블 분리
X_features=cust_df.iloc[:,:-1]
y_labels=cust_df.loc[:,'TARGET']

# 학습/테스트 데이터 분리
from sklearn.model_selection import train_test_split

X_train,X_test,y_train,y_test=train_test_split(
    X_features
    ,y_labels
    ,test_size=0.2
    ,random_state=0
    ,stratify=y_labels 
)

# 학습/테스트 레이블의 균형성 확인
# 학습/테스트 레이블의 개수 저장
train_cnt=y_train.count() # 학습데이터 레이블의 개수
test_cnt=y_test.count() # 테스트 데이터 레이블의 개수

X_tr,X_val,y_tr,y_val=train_test_split(
    X_train
    ,y_train
    ,test_size=0.3
    ,random_state=0
)

from xgboost import XGBClassifier
from sklearn.metrics import roc_auc_score

xgb_clf=XGBClassifier(
    n_estimators=500
    ,learning_rate=0.05
    ,early_stopping=100
    ,eval_metric='auc'
    ,random_state=156
)

xgb_clf.fit(
    X_tr
    ,y_tr
    ,eval_set=[
        (
            X_val,y_val
        )
    ]
)
```
![image](https://github.com/user-attachments/assets/ce4d1c1a-cd4f-4c3e-9b78-d2e6601caf64)

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib
import warnings
warnings.filterwarnings('ignore')

cust_df = pd.read_csv(
    './santander-customer-satisfaction/train.csv'
    ,encoding='latin-1'
)

# 데이터 전처리 : -999999 값을 제일 많은 2로 변환
cust_df['var3'].replace(-999999,2,inplace=True)

# ID feature은 일련번호로 의미가 없으므로 삭제를 한다
cust_df.drop(
    'ID'
    ,axis=1
    ,inplace=True
)

# 데이터 / 레이블 분리
X_features=cust_df.iloc[:,:-1]
y_labels=cust_df.loc[:,'TARGET']

# 학습/테스트 데이터 분리
from sklearn.model_selection import train_test_split

X_train,X_test,y_train,y_test=train_test_split(
    X_features
    ,y_labels
    ,test_size=0.2
    ,random_state=0
    ,stratify=y_labels 
)

# 학습/테스트 레이블의 균형성 확인
# 학습/테스트 레이블의 개수 저장
train_cnt=y_train.count() # 학습데이터 레이블의 개수
test_cnt=y_test.count() # 테스트 데이터 레이블의 개수

X_tr,X_val,y_tr,y_val=train_test_split(
    X_train
    ,y_train
    ,test_size=0.3
    ,random_state=0
)

from xgboost import XGBClassifier
from sklearn.metrics import roc_auc_score

xgb_clf=XGBClassifier(
    n_estimators=500
    ,learning_rate=0.05
    ,early_stopping=100
    ,eval_metric='auc'
    ,random_state=156
)

xgb_clf.fit(
    X_tr
    ,y_tr
    ,eval_set=[
        (
            X_val,y_val
        )
    ]
)

xgb_roc_score=roc_auc_score(y_test,xgb_clf.predict_proba(X_test)[:,1])
print(f'Roc Auc : {xgb_roc_score:.4f}')
```
![image](https://github.com/user-attachments/assets/4470f2a9-7996-4d5a-ba3e-d7208abf58af)

```
# gpu 버전
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib
import warnings
warnings.filterwarnings('ignore')

cust_df = pd.read_csv(
    './santander-customer-satisfaction/train.csv'
    ,encoding='latin-1'
)

# 데이터 전처리 : -999999 값을 제일 많은 2로 변환
cust_df['var3'].replace(-999999,2,inplace=True)

# ID feature은 일련번호로 의미가 없으므로 삭제를 한다
cust_df.drop(
    'ID'
    ,axis=1
    ,inplace=True
)

# 데이터 / 레이블 분리
X_features=cust_df.iloc[:,:-1]
y_labels=cust_df.loc[:,'TARGET']

# 학습/테스트 데이터 분리
from sklearn.model_selection import train_test_split

X_train,X_test,y_train,y_test=train_test_split(
    X_features
    ,y_labels
    ,test_size=0.2
    ,random_state=0
    ,stratify=y_labels 
)

# 학습/테스트 레이블의 균형성 확인
# 학습/테스트 레이블의 개수 저장
train_cnt=y_train.count() # 학습데이터 레이블의 개수
test_cnt=y_test.count() # 테스트 데이터 레이블의 개수

X_tr,X_val,y_tr,y_val=train_test_split(
    X_train
    ,y_train
    ,test_size=0.3
    ,random_state=0
)

from xgboost import XGBClassifier
from sklearn.metrics import roc_auc_score

xgb_clf_gpu = XGBClassifier(
    n_estimators=500,
    learning_rate=0.05,
    early_stopping_rounds=100,
    eval_metric='auc',
    random_state=156,
    tree_method='gpu_hist',  # Use GPU for training
    gpu_id=0  # Specify GPU device ID, if you have multiple GPUs
)


# Fit the model on training data
xgb_clf_gpu.fit(
    X_tr,
    y_tr,
    eval_set=[(X_val, y_val)]
)

# Evaluate model with ROC AUC score on test data
xgb_roc_score_gpu = roc_auc_score(y_test, xgb_clf_gpu.predict_proba(X_test)[:, 1])
print(f'GPU Roc Auc: {xgb_roc_score_gpu:.4f}')
```
![image](https://github.com/user-attachments/assets/93ae2dba-be8b-4a9f-adc6-8f9b8487f00c)

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib
import warnings
warnings.filterwarnings('ignore')

cust_df = pd.read_csv(
    './santander-customer-satisfaction/train.csv'
    ,encoding='latin-1'
)

# 데이터 전처리 : -999999 값을 제일 많은 2로 변환
cust_df['var3'].replace(-999999,2,inplace=True)

# ID feature은 일련번호로 의미가 없으므로 삭제를 한다
cust_df.drop(
    'ID'
    ,axis=1
    ,inplace=True
)

# 데이터 / 레이블 분리
X_features=cust_df.iloc[:,:-1]
y_labels=cust_df.loc[:,'TARGET']

# 학습/테스트 데이터 분리
from sklearn.model_selection import train_test_split

X_train,X_test,y_train,y_test=train_test_split(
    X_features
    ,y_labels
    ,test_size=0.2
    ,random_state=0
    ,stratify=y_labels 
)

# 학습/테스트 레이블의 균형성 확인
# 학습/테스트 레이블의 개수 저장
train_cnt=y_train.count() # 학습데이터 레이블의 개수
test_cnt=y_test.count() # 테스트 데이터 레이블의 개수

X_tr,X_val,y_tr,y_val=train_test_split(
    X_train
    ,y_train
    ,test_size=0.3
    ,random_state=0
)

from xgboost import XGBClassifier
from sklearn.metrics import roc_auc_score

xgb_clf=XGBClassifier(
    n_estimators=500
    ,learning_rate=0.05
    ,early_stopping=100
    ,eval_metric='auc'
    ,random_state=156
)

xgb_clf.fit(
    X_tr
    ,y_tr
    ,eval_set=[
        (
            X_val,y_val
        )
    ]
)

xgb_roc_score=roc_auc_score(y_test,xgb_clf.predict_proba(X_test)[:,1])

# hyperopt, KFold 사용한 튜닝
from hyperopt import hp


# max_depth는 5에서 15까지 1간격으로, min_child_weight는 1에서 6까지 1간격으로
# colsample_bytree는 0.5에서 0.95사이, learning_rate는 0.01에서 0.2사이 정규 분포된 값으로 검색.


xgb_search_space = {
    'max_depth': hp.quniform('max_depth', 5, 15, 1),
    'min_child_weight': hp.quniform('min_child_weight', 1, 6, 1),
    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 0.95),
    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2)
}

from sklearn.model_selection import KFold
from sklearn.metrics import roc_auc_score


# 목적 함수 설정.
# 추후 fmin()에서 입력된 search_space값으로 XGBClassifier 교차 검증 학습 후 -1* roc_auc 평균 값을 반환.  
def objective_func(search_space):
    xgb_clf = XGBClassifier(
        n_estimators=100, max_depth=int(search_space['max_depth'])
        ,min_child_weight=int(search_space['min_child_weight'])
        ,colsample_bytree=search_space['colsample_bytree']
        ,learning_rate=search_space['learning_rate']
        ,early_stopping_rounds=30
        , eval_metric='auc'
    )
    # 3개 k-fold 방식으로 평가된 roc_auc 지표를 담는 list
    roc_auc_list= []
   
    # 3개 k-fold방식 적용
    kf = KFold(n_splits=3)
    # X_train을 다시 학습과 검증용 데이터로 분리
    for tr_index, val_index in kf.split(X_train): # X_train : 6만건
        # kf.split(X_train)으로 추출된 학습과 검증 index값으로 학습과 검증 데이터 세트 분리
        X_tr, y_tr = X_train.iloc[tr_index], y_train.iloc[tr_index]
        X_val, y_val = X_train.iloc[val_index], y_train.iloc[val_index]
        # early stopping은 30회로 설정하고 추출된 학습과 검증 데이터로 XGBClassifier 학습 수행.
        xgb_clf.fit(X_tr, y_tr,eval_set=[(X_tr, y_tr), (X_val, y_val)])
   
        # 1로 예측한 확률값 추출후 roc auc 계산하고 평균 roc auc 계산을 위해 list에 결과값 담음.
        score = roc_auc_score(y_val, xgb_clf.predict_proba(X_val)[:, 1])
        roc_auc_list.append(score)
       
    # 3개 k-fold로 계산된 roc_auc값의 평균값을 반환하되,
    # HyperOpt는 목적함수의 최소값을 위한 입력값을 찾으므로 -1을 곱한 뒤 반환.
    return -1 * np.mean(roc_auc_list)

from hyperopt import fmin, tpe, Trials


trials = Trials()


# fmin()함수를 호출. max_evals지정된 횟수만큼 반복 후 목적함수의 최소값을 가지는 최적 입력값 추출.
best = fmin(
    fn=objective_func
    ,space=xgb_search_space
    ,algo=tpe.suggest
    ,max_evals=50 # 최대 반복 횟수를 지정합니다.
    , trials=trials
    , rstate=np.random.default_rng(seed=30)
)
print('best:', best)
```
![image](https://github.com/user-attachments/assets/6c3f2b0f-9933-4049-91c0-f78d0d10c96c)

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib
import warnings
warnings.filterwarnings('ignore')

cust_df = pd.read_csv(
    './santander-customer-satisfaction/train.csv'
    ,encoding='latin-1'
)

# 데이터 전처리 : -999999 값을 제일 많은 2로 변환
cust_df['var3'].replace(-999999,2,inplace=True)

# ID feature은 일련번호로 의미가 없으므로 삭제를 한다
cust_df.drop(
    'ID'
    ,axis=1
    ,inplace=True
)

# 데이터 / 레이블 분리
X_features=cust_df.iloc[:,:-1]
y_labels=cust_df.loc[:,'TARGET']

# 학습/테스트 데이터 분리
from sklearn.model_selection import train_test_split

X_train,X_test,y_train,y_test=train_test_split(
    X_features
    ,y_labels
    ,test_size=0.2
    ,random_state=0
    ,stratify=y_labels 
)

# 학습/테스트 레이블의 균형성 확인
# 학습/테스트 레이블의 개수 저장
train_cnt=y_train.count() # 학습데이터 레이블의 개수
test_cnt=y_test.count() # 테스트 데이터 레이블의 개수

X_tr,X_val,y_tr,y_val=train_test_split(
    X_train
    ,y_train
    ,test_size=0.3
    ,random_state=0
)

from xgboost import XGBClassifier
from sklearn.metrics import roc_auc_score

xgb_clf=XGBClassifier(
    n_estimators=500
    ,learning_rate=0.05
    ,early_stopping=100
    ,eval_metric='auc'
    ,random_state=156
)

xgb_clf.fit(
    X_tr
    ,y_tr
    ,eval_set=[
        (
            X_val,y_val
        )
    ]
)

xgb_roc_score=roc_auc_score(y_test,xgb_clf.predict_proba(X_test)[:,1])

# hyperopt, KFold 사용한 튜닝
from hyperopt import hp


# max_depth는 5에서 15까지 1간격으로, min_child_weight는 1에서 6까지 1간격으로
# colsample_bytree는 0.5에서 0.95사이, learning_rate는 0.01에서 0.2사이 정규 분포된 값으로 검색.


xgb_search_space = {
    'max_depth': hp.quniform('max_depth', 5, 15, 1),
    'min_child_weight': hp.quniform('min_child_weight', 1, 6, 1),
    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 0.95),
    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2)
}

from sklearn.model_selection import KFold
from sklearn.metrics import roc_auc_score


# 목적 함수 설정.
# 추후 fmin()에서 입력된 search_space값으로 XGBClassifier 교차 검증 학습 후 -1* roc_auc 평균 값을 반환.  
def objective_func(search_space):
    xgb_clf = XGBClassifier(
        n_estimators=100, max_depth=int(search_space['max_depth'])
        ,min_child_weight=int(search_space['min_child_weight'])
        ,colsample_bytree=search_space['colsample_bytree']
        ,learning_rate=search_space['learning_rate']
        ,early_stopping_rounds=30
        , eval_metric='auc'
        ,tree_method='gpu_hist'
        ,gpu_id=0
    )
    # 3개 k-fold 방식으로 평가된 roc_auc 지표를 담는 list
    roc_auc_list= []
   
    # 3개 k-fold방식 적용
    kf = KFold(n_splits=3)
    # X_train을 다시 학습과 검증용 데이터로 분리
    for tr_index, val_index in kf.split(X_train): # X_train : 6만건
        # kf.split(X_train)으로 추출된 학습과 검증 index값으로 학습과 검증 데이터 세트 분리
        X_tr, y_tr = X_train.iloc[tr_index], y_train.iloc[tr_index]
        X_val, y_val = X_train.iloc[val_index], y_train.iloc[val_index]
        # early stopping은 30회로 설정하고 추출된 학습과 검증 데이터로 XGBClassifier 학습 수행.
        xgb_clf.fit(X_tr, y_tr,eval_set=[(X_tr, y_tr), (X_val, y_val)])
   
        # 1로 예측한 확률값 추출후 roc auc 계산하고 평균 roc auc 계산을 위해 list에 결과값 담음.
        score = roc_auc_score(y_val, xgb_clf.predict_proba(X_val)[:, 1])
        roc_auc_list.append(score)
       
    # 3개 k-fold로 계산된 roc_auc값의 평균값을 반환하되,
    # HyperOpt는 목적함수의 최소값을 위한 입력값을 찾으므로 -1을 곱한 뒤 반환.
    return -1 * np.mean(roc_auc_list)

from hyperopt import fmin, tpe, Trials


trials = Trials()


# fmin()함수를 호출. max_evals지정된 횟수만큼 반복 후 목적함수의 최소값을 가지는 최적 입력값 추출.
best = fmin(
    fn=objective_func
    ,space=xgb_search_space
    ,algo=tpe.suggest
    ,max_evals=50 # 최대 반복 횟수를 지정합니다.
    , trials=trials
    , rstate=np.random.default_rng(seed=30)
)


print('best:', best)
```
![image](https://github.com/user-attachments/assets/1402d805-25fe-4628-a48d-922e7a26f308)
