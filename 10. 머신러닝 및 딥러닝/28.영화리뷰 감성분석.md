
```
%pip install transformers datasets accelerate evaluate
```
![image](https://github.com/user-attachments/assets/6c490665-c112-46e3-a40c-6ced8a19c902)

```
# 영화리뷰 데이터 로드
import gzip
import shutil
import time

import pandas as pd
import requests
import torch
import torch.nn.functional as F
import torchtext

import transformers
from transformers import DistilBertTokenizerFast # 토크나이저저
from transformers import DistilBertForSequenceClassification # 분류 모델
```
```
torch.backends.cudnn.deterministic = True # PyTorch에서 CUDA를 사용하는 딥러닝 모델의 연산 결과를 "결정론적(deterministic)"으로 만들지 여부를 설정하는 옵션입니다.
RANDOM_SEED = 123
torch.manual_seed(RANDOM_SEED)
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

NUM_EPOCHS = 3
```
```
url = "https://github.com/rasbt/machine-learning-book/raw/main/ch08/movie_data.csv.gz"
filename = url.split("/")[-1]

with open(filename, "wb") as f:
    r = requests.get(url)
    f.write(r.content)

with gzip.open('movie_data.csv.gz', 'rb') as f_in:
    with open('movie_data.csv', 'wb') as f_out:
        shutil.copyfileobj(f_in, f_out)
```
![image](https://github.com/user-attachments/assets/63b0155f-5313-4632-86bd-70826cfb5ef4)

```
df = pd.read_csv('movie_data.csv')
df.head()
```
![image](https://github.com/user-attachments/assets/068a2527-ac30-4cec-b555-f557f8f418bf)

```
train_texts = df.iloc[:35000]['review'].values
train_labels = df.iloc[:35000]['sentiment'].values

valid_texts = df.iloc[35000:40000]['review'].values
valid_labels = df.iloc[35000:40000]['sentiment'].values

test_texts = df.iloc[40000:]['review'].values
test_labels = df.iloc[40000:]['sentiment'].values
```
```
# 토큰나이저 오브젝트 생성
tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased') # uncased는 대소문자 무시
```
![image](https://github.com/user-attachments/assets/d7105f2f-890a-4ddd-a56e-f2611251b8e2)

```
train_encodings = tokenizer(list(train_texts), truncation=True, padding=True)
valid_encodings = tokenizer(list(valid_texts), truncation=True, padding=True)
test_encodings = tokenizer(list(test_texts), truncation=True, padding=True)
```
```
# Dataset = text + mask + label
class IMDbDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings # encoding text
        self.labels = labels # 답


    def __getitem__(self, idx): # 꺼내주는 메소드드
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item


    def __len__(self): # 길이 반환
        return len(self.labels)

train_dataset = IMDbDataset(train_encodings, train_labels)
valid_dataset = IMDbDataset(valid_encodings, valid_labels)
test_dataset = IMDbDataset(test_encodings, test_labels)
```
```
# DataLoader
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)
valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=16, shuffle=False)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=False)
```
```
# BERT 계열의 분류 모델 생성
model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')
model.to(DEVICE)
model.train() # 학습모드로 설정

optim = torch.optim.Adam(model.parameters(), lr=5e-5)
```
```
# 평가 -> 정확도
def compute_accuracy(model, data_loader, device):
    with torch.no_grad():
        correct_pred, num_examples = 0, 0


        for batch_idx, batch in enumerate(data_loader):


        ### 데이터 준비
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)
            outputs = model(input_ids, attention_mask=attention_mask)
            logits = outputs['logits']
            predicted_labels = torch.argmax(logits, 1)
            num_examples += labels.size(0)
            correct_pred += (predicted_labels == labels).sum()


        return correct_pred.float()/num_examples * 100
```
```
#학습 검증 테스트
start_time = time.time()
for epoch in range(NUM_EPOCHS): # 3번 반복복
    model.train() # 학습 모드 설정
    for batch_idx, batch in enumerate(train_loader): # 학습 데이터 학습
        ### 데이터 준비
        input_ids = batch['input_ids'].to(DEVICE)
        attention_mask = batch['attention_mask'].to(DEVICE)
        labels = batch['labels'].to(DEVICE)
        ### 정방향
        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
        loss, logits = outputs['loss'], outputs['logits']
        ### 역방향
        optim.zero_grad()
        loss.backward()
        optim.step()
        ### 로깅
        if not batch_idx % 250:
            print (f'에포크: {epoch+1:04d}/{NUM_EPOCHS:04d} | '
                   f'배치 {batch_idx:04d}/{len(train_loader):04d} | '
                   f'손실: {loss:.4f}')
    model.eval() # 검증모드 설정정
    with torch.set_grad_enabled(False):
        print(f'훈련 정확도: '
              f'{compute_accuracy(model, train_loader, DEVICE):.2f}%'
              f'\n검증 정확도: '
              f'{compute_accuracy(model, valid_loader, DEVICE):.2f}%')


    print(f'소요 시간: {(time.time() - start_time)/60:.2f} min')


print(f'총 훈련 시간: {(time.time() - start_time)/60:.2f} min')
print(f'테스트 정확도: {compute_accuracy(model, test_loader, DEVICE):.2f}%')
```
