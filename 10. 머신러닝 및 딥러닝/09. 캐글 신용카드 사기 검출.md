# 캐글 신용카드 사기 검출기 검출

데이터 : [캐글 신용카드 사기 검출기 검출](https://www.kaggle.com/competitions/credit-card-fraud-prediction)


```
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline

import warnings
warnings.filterwarnings('ignore')

card_df=pd.read_csv(
    './creditcard.csv'
    ,encoding='utf-8'
)

card_df.head()
```
![image](https://github.com/user-attachments/assets/8bb53d08-b998-4bc2-b97d-e3d8ae03f23a)

```
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline

import warnings
warnings.filterwarnings('ignore')

card_df=pd.read_csv(
    './creditcard.csv'
    ,encoding='utf-8'
)

from sklearn.model_selection import train_test_split

def get_preprocessed_df(df=None):
    df_copy = df.copy()
    df_copy.drop('Time',axis=1,inplace=True)
    return df_copy

def get_train_test_dataset(df=None):
    if df is None: 
        return '데이터베이스가 존재하지 않습니다'
    else:
        df_copy = get_preprocessed_df(df)
        X_features = df_copy.iloc[:, :-1]
        y_target = df_copy.iloc[:, -1]
        X_train, X_test, y_train, y_test = train_test_split(
            X_features,
            y_target,
            test_size=0.3,
            random_state=0,
            stratify=y_target
        )
        return X_train, X_test, y_train, y_test


X_train,X_test,y_train,y_test=get_train_test_dataset(card_df)

print('='*50)
print(f'학습 데이터 레이블 값 비율\n{y_train.value_counts()/y_train.shape[0]*100}')
print('='*50)
print(f'테스트 데이터 레이블 값 비율\n{y_test.value_counts()/y_test.shape[0]*100}')
```
![image](https://github.com/user-attachments/assets/328661f9-668d-4d1e-87de-b61dc8347596)

---
## 원본 데이터 가공 없이 모델 학습, 일반화 성능 확인

```
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline

import warnings
warnings.filterwarnings('ignore')

card_df=pd.read_csv(
    './creditcard.csv'
    ,encoding='utf-8'
)

from sklearn.model_selection import train_test_split

def get_preprocessed_df(df=None):
    df_copy = df.copy()
    df_copy.drop('Time',axis=1,inplace=True)
    return df_copy

def get_train_test_dataset(df=None):
    if df is None:  
        return '데이터베이스가 존재하지 않습니다'
    else:
        df_copy = get_preprocessed_df(df)
        X_features = df_copy.iloc[:, :-1]
        y_target = df_copy.iloc[:, -1]
        X_train, X_test, y_train, y_test = train_test_split(
            X_features,
            y_target,
            test_size=0.3,
            random_state=0,
            stratify=y_target
        )
        return X_train, X_test, y_train, y_test

X_train,X_test,y_train,y_test=get_train_test_dataset(card_df)

# print('='*50)
# print(f'학습 데이터 레이블 값 비율\n{y_train.value_counts()/y_train.shape[0]*100}')
# print('='*50)
# print(f'테스트 데이터 레이블 값 비율\n{y_test.value_counts()/y_test.shape[0]*100}')


# 모델 생성 성능 평가 : get_clf_eval(원래답, 예측값, 예측확률)
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.metrics import precision_score, recall_score
from sklearn.metrics import f1_score, roc_auc_score


def get_clf_eval(y_test, pred=None, pred_proba=None):
    confusion = confusion_matrix( y_test, pred)
    accuracy = accuracy_score(y_test , pred) # 정확도 점수
    precision = precision_score(y_test , pred) # 정밀도 점수
    recall = recall_score(y_test , pred) # 재현율 점수
    f1 = f1_score(y_test,pred) # 정밀도, 재현율 조화평균 값
    # ROC-AUC 추가
    roc_auc = roc_auc_score(y_test, pred_proba) # AUC 점수 : 불균형 데이터 셋에서 필요
    print('오차 행렬')
    print(confusion)
    # ROC-AUC print 추가
    print('정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f},\
    F1: {3:.4f}, AUC:{4:.4f}'.format(accuracy, precision, recall, f1, roc_auc))

from sklearn.linear_model import LogisticRegression

lr_clf =LogisticRegression(max_iter=1000)
lr_clf.fit(X_train,y_train)
lr_pred=lr_clf.predict(X_test)
lr_pred_proba=lr_clf.predict_proba(X_test)[:,1]

get_clf_eval(y_test,lr_pred,lr_pred_proba)

```
![image](https://github.com/user-attachments/assets/8977c25e-5025-4751-8a22-1c4ce2a3ef3d)

```
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline

import warnings
warnings.filterwarnings('ignore')

card_df=pd.read_csv(
    './creditcard.csv'
    ,encoding='utf-8'
)

from sklearn.model_selection import train_test_split

def get_preprocessed_df(df=None):
    df_copy = df.copy()
    df_copy.drop('Time',axis=1,inplace=True)
    return df_copy

def get_train_test_dataset(df=None):
    if df is None:  
        return '데이터베이스가 존재하지 않습니다'
    else:
        df_copy = get_preprocessed_df(df)
        X_features = df_copy.iloc[:, :-1]
        y_target = df_copy.iloc[:, -1]
        X_train, X_test, y_train, y_test = train_test_split(
            X_features,
            y_target,
            test_size=0.3,
            random_state=0,
            stratify=y_target
        )
        return X_train, X_test, y_train, y_test

X_train,X_test,y_train,y_test=get_train_test_dataset(card_df)

# print('='*50)
# print(f'학습 데이터 레이블 값 비율\n{y_train.value_counts()/y_train.shape[0]*100}')
# print('='*50)
# print(f'테스트 데이터 레이블 값 비율\n{y_test.value_counts()/y_test.shape[0]*100}')

from sklearn.linear_model import LogisticRegression

lr_clf =LogisticRegression(max_iter=1000)
lr_clf.fit(X_train,y_train)
lr_pred=lr_clf.predict(X_test)
lr_pred_proba=lr_clf.predict_proba(X_test)[:,1]


# 모델 생성 성능 평가 : get_clf_eval(원래답, 예측값, 예측확률)
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.metrics import precision_score, recall_score
from sklearn.metrics import f1_score, roc_auc_score


def get_clf_eval(y_test, pred=None, pred_proba=None):
    confusion = confusion_matrix( y_test, pred)
    accuracy = accuracy_score(y_test , pred) # 정확도 점수
    precision = precision_score(y_test , pred) # 정밀도 점수
    recall = recall_score(y_test , pred) # 재현율 점수
    f1 = f1_score(y_test,pred) # 정밀도, 재현율 조화평균 값
    # ROC-AUC 추가
    roc_auc = roc_auc_score(y_test, pred_proba) # AUC 점수 : 불균형 데이터 셋에서 필요
    print('오차 행렬')
    print(confusion)
    # ROC-AUC print 추가
    print('정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f}, F1: {3:.4f}, AUC:{4:.4f}'.format(accuracy, precision, recall, f1, roc_auc))

def get_model_train_eval(
    model                   # 분류 알고리즘
    ,ftr_train=None         # 학습 데이터
    ,ftr_test=None          # 테스트 데이터
    ,tgt_train=None         # 학습 데이터 레이블
    ,tgt_test=None          # 테스트 데이터 레이블
    ):
    model.fit(ftr_train,tgt_train)
    pred = model.predict(ftr_test)
    pred_proba=model.predict_proba(ftr_test)[:,1] # 예측 확률
    get_clf_eval(tgt_test,pred,pred_proba)

from lightgbm import LGBMClassifier

lgbm_clf = LGBMClassifier(
    n_estimators=1000
    ,num_leaves=64
    #, n_jobs=-1 # cpu 일때만 사용
    ,device='gpu' # gpu 일때만 사용
    ,boosting_from_average=False # 극도로 불균형한 레이블일 경우 False를 줘야 함
)

get_model_train_eval(lgbm_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)
```

![image](https://github.com/user-attachments/assets/bb4b5e60-f3bb-4bd1-9f6c-72c5f373acd5)

```
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline

import warnings
warnings.filterwarnings('ignore')

card_df=pd.read_csv(
    './creditcard.csv'
    ,encoding='utf-8'
)

from sklearn.model_selection import train_test_split

def get_preprocessed_df(df=None):
    df_copy = df.copy()
    df_copy.drop('Time',axis=1,inplace=True)
    return df_copy

def get_train_test_dataset(df=None):
    if df is None:  
        return '데이터베이스가 존재하지 않습니다'
    else:
        df_copy = get_preprocessed_df(df)
        X_features = df_copy.iloc[:, :-1]
        y_target = df_copy.iloc[:, -1]
        X_train, X_test, y_train, y_test = train_test_split(
            X_features,
            y_target,
            test_size=0.3,
            random_state=0,
            stratify=y_target
        )
        return X_train, X_test, y_train, y_test

X_train,X_test,y_train,y_test=get_train_test_dataset(card_df)

# print('='*50)
# print(f'학습 데이터 레이블 값 비율\n{y_train.value_counts()/y_train.shape[0]*100}')
# print('='*50)
# print(f'테스트 데이터 레이블 값 비율\n{y_test.value_counts()/y_test.shape[0]*100}')

from sklearn.linear_model import LogisticRegression

lr_clf =LogisticRegression(max_iter=1000)
lr_clf.fit(X_train,y_train)
lr_pred=lr_clf.predict(X_test)
lr_pred_proba=lr_clf.predict_proba(X_test)[:,1]


# 모델 생성 성능 평가 : get_clf_eval(원래답, 예측값, 예측확률)
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.metrics import precision_score, recall_score
from sklearn.metrics import f1_score, roc_auc_score


def get_clf_eval(y_test, pred=None, pred_proba=None):
    confusion = confusion_matrix( y_test, pred)
    accuracy = accuracy_score(y_test , pred) # 정확도 점수
    precision = precision_score(y_test , pred) # 정밀도 점수
    recall = recall_score(y_test , pred) # 재현율 점수
    f1 = f1_score(y_test,pred) # 정밀도, 재현율 조화평균 값
    # ROC-AUC 추가
    roc_auc = roc_auc_score(y_test, pred_proba) # AUC 점수 : 불균형 데이터 셋에서 필요
    print('오차 행렬')
    print(confusion)
    # ROC-AUC print 추가
    print('정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f}, F1: {3:.4f}, AUC:{4:.4f}'.format(accuracy, precision, recall, f1, roc_auc))

def get_model_train_eval(
    model                   # 분류 알고리즘
    ,ftr_train=None         # 학습 데이터
    ,ftr_test=None          # 테스트 데이터
    ,tgt_train=None         # 학습 데이터 레이블
    ,tgt_test=None          # 테스트 데이터 레이블
    ):
    model.fit(ftr_train,tgt_train)
    pred = model.predict(ftr_test)
    pred_proba=model.predict_proba(ftr_test)[:,1] # 예측 확률
    get_clf_eval(tgt_test,pred,pred_proba)


from lightgbm import LGBMClassifier

lgbm_clf = LGBMClassifier(
    n_estimators=1000
    ,num_leaves=64
    #, n_jobs=-1 # cpu 일때만 사용
    ,device='gpu' # gpu 일때만 사용
    ,boosting_from_average=False # 극도로 불균형한 레이블일 경우 False를 줘야 함
)

# lightGBM
get_model_train_eval(lgbm_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)

# LogisticRegression
get_model_train_eval(lr_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)
```
![image](https://github.com/user-attachments/assets/ac09dbe7-edea-40e5-8c34-0d8363bd1d5c)

---
## 데이터 분포 확인, 변환후 모델 학습/예측/평가

```
# 특정 피쳐(금액)의 데이터 분포 확인
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline

import warnings
warnings.filterwarnings('ignore')

import seaborn as sns

card_df=pd.read_csv(
    './creditcard.csv'
    ,encoding='utf-8'
)

plt.figure(figsize=(8,4))
plt.xticks(
    range(0,30000,1000)
    ,rotation=60
)
sns.histplot(card_df['Amount'],bins=100,kde=True)
plt.show()
```
![image](https://github.com/user-attachments/assets/b32af0f7-1e8b-4743-bd6b-f3629af769b7)

---

```
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline  

import warnings
warnings.filterwarnings('ignore')  

import seaborn as sns

# CSV 파일을 읽어와 DataFrame으로 저장
card_df = pd.read_csv(
    './creditcard.csv',  # 신용카드 거래 데이터 파일 경로
    encoding='utf-8'  # UTF-8 인코딩으로 파일을 읽음
)

# 외도(Skewness) 값 확인: 데이터의 비대칭성을 확인하기 위함
# 비대칭 데이터의 경우 로그 변환 등으로 정규성을 확보할 필요가 있음

# 첨도(Kurtosis) 값 확인: 극단값(outlier)의 존재 여부를 판단하기 위함
# 양의 첨도: 분포의 꼬리가 두껍고, 중앙이 뾰족 -> 극단값이 자주 발생
#           -> 극단값 처리가 필요하며, 적절한 모델 선택이 중요
# 음의 첨도: 분포의 꼬리가 얇고, 중앙이 평평 -> 극단값이 적게 발생
# 중간 첨도: 정규분포에 가까움 (외도와 첨도가 0에 가까움)

from scipy.stats import skew  # 외도 계산 함수
from scipy.stats import kurtosis  # 첨도 계산 함수

# 'Amount' 열의 외도(Skewness) 값 출력
print(f"외도 : {skew(card_df['Amount'])}")
print(f"첨도 : {kurtosis(card_df['Amount'])}")
```
![image](https://github.com/user-attachments/assets/9fc70ec9-5e36-4415-800c-3abcccc2cccc)
