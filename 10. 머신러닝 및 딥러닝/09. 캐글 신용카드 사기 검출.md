# ìºê¸€ ì‹ ìš©ì¹´ë“œ ì‚¬ê¸° ê²€ì¶œê¸° ê²€ì¶œ

ë°ì´í„° : [ìºê¸€ ì‹ ìš©ì¹´ë“œ ì‚¬ê¸° ê²€ì¶œê¸° ê²€ì¶œ](https://www.kaggle.com/competitions/credit-card-fraud-prediction)


```
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline

import warnings
warnings.filterwarnings('ignore')

card_df=pd.read_csv(
    './creditcard.csv'
    ,encoding='utf-8'
)

card_df.head()
```
![image](https://github.com/user-attachments/assets/8bb53d08-b998-4bc2-b97d-e3d8ae03f23a)

```
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline

import warnings
warnings.filterwarnings('ignore')

card_df=pd.read_csv(
    './creditcard.csv'
    ,encoding='utf-8'
)

from sklearn.model_selection import train_test_split

def get_preprocessed_df(df=None):
    df_copy = df.copy()
    df_copy.drop('Time',axis=1,inplace=True)
    return df_copy

def get_train_test_dataset(df=None):
    if df is None: 
        return 'ë°ì´í„°ë² ì´ìŠ¤ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤'
    else:
        df_copy = get_preprocessed_df(df)
        X_features = df_copy.iloc[:, :-1]
        y_target = df_copy.iloc[:, -1]
        X_train, X_test, y_train, y_test = train_test_split(
            X_features,
            y_target,
            test_size=0.3,
            random_state=0,
            stratify=y_target
        )
        return X_train, X_test, y_train, y_test


X_train,X_test,y_train,y_test=get_train_test_dataset(card_df)

print('='*50)
print(f'í•™ìŠµ ë°ì´í„° ë ˆì´ë¸” ê°’ ë¹„ìœ¨\n{y_train.value_counts()/y_train.shape[0]*100}')
print('='*50)
print(f'í…ŒìŠ¤íŠ¸ ë°ì´í„° ë ˆì´ë¸” ê°’ ë¹„ìœ¨\n{y_test.value_counts()/y_test.shape[0]*100}')
```
![image](https://github.com/user-attachments/assets/328661f9-668d-4d1e-87de-b61dc8347596)

---
## ì›ë³¸ ë°ì´í„° ê°€ê³µ ì—†ì´ ëª¨ë¸ í•™ìŠµ, ì¼ë°˜í™” ì„±ëŠ¥ í™•ì¸

```
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline

import warnings
warnings.filterwarnings('ignore')

card_df=pd.read_csv(
    './creditcard.csv'
    ,encoding='utf-8'
)

from sklearn.model_selection import train_test_split

def get_preprocessed_df(df=None):
    df_copy = df.copy()
    df_copy.drop('Time',axis=1,inplace=True)
    return df_copy

def get_train_test_dataset(df=None):
    if df is None:  
        return 'ë°ì´í„°ë² ì´ìŠ¤ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤'
    else:
        df_copy = get_preprocessed_df(df)
        X_features = df_copy.iloc[:, :-1]
        y_target = df_copy.iloc[:, -1]
        X_train, X_test, y_train, y_test = train_test_split(
            X_features,
            y_target,
            test_size=0.3,
            random_state=0,
            stratify=y_target
        )
        return X_train, X_test, y_train, y_test

X_train,X_test,y_train,y_test=get_train_test_dataset(card_df)

# print('='*50)
# print(f'í•™ìŠµ ë°ì´í„° ë ˆì´ë¸” ê°’ ë¹„ìœ¨\n{y_train.value_counts()/y_train.shape[0]*100}')
# print('='*50)
# print(f'í…ŒìŠ¤íŠ¸ ë°ì´í„° ë ˆì´ë¸” ê°’ ë¹„ìœ¨\n{y_test.value_counts()/y_test.shape[0]*100}')


# ëª¨ë¸ ìƒì„± ì„±ëŠ¥ í‰ê°€ : get_clf_eval(ì›ë˜ë‹µ, ì˜ˆì¸¡ê°’, ì˜ˆì¸¡í™•ë¥ )
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.metrics import precision_score, recall_score
from sklearn.metrics import f1_score, roc_auc_score


def get_clf_eval(y_test, pred=None, pred_proba=None):
    confusion = confusion_matrix( y_test, pred)
    accuracy = accuracy_score(y_test , pred) # ì •í™•ë„ ì ìˆ˜
    precision = precision_score(y_test , pred) # ì •ë°€ë„ ì ìˆ˜
    recall = recall_score(y_test , pred) # ì¬í˜„ìœ¨ ì ìˆ˜
    f1 = f1_score(y_test,pred) # ì •ë°€ë„, ì¬í˜„ìœ¨ ì¡°í™”í‰ê·  ê°’
    # ROC-AUC ì¶”ê°€
    roc_auc = roc_auc_score(y_test, pred_proba) # AUC ì ìˆ˜ : ë¶ˆê· í˜• ë°ì´í„° ì…‹ì—ì„œ í•„ìš”
    print('ì˜¤ì°¨ í–‰ë ¬')
    print(confusion)
    # ROC-AUC print ì¶”ê°€
    print('ì •í™•ë„: {0:.4f}, ì •ë°€ë„: {1:.4f}, ì¬í˜„ìœ¨: {2:.4f},\
    F1: {3:.4f}, AUC:{4:.4f}'.format(accuracy, precision, recall, f1, roc_auc))

from sklearn.linear_model import LogisticRegression

lr_clf =LogisticRegression(max_iter=1000)
lr_clf.fit(X_train,y_train)
lr_pred=lr_clf.predict(X_test)
lr_pred_proba=lr_clf.predict_proba(X_test)[:,1]

get_clf_eval(y_test,lr_pred,lr_pred_proba)

```
![image](https://github.com/user-attachments/assets/8977c25e-5025-4751-8a22-1c4ce2a3ef3d)

```
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline

import warnings
warnings.filterwarnings('ignore')

card_df=pd.read_csv(
    './creditcard.csv'
    ,encoding='utf-8'
)

from sklearn.model_selection import train_test_split

def get_preprocessed_df(df=None):
    df_copy = df.copy()
    df_copy.drop('Time',axis=1,inplace=True)
    return df_copy

def get_train_test_dataset(df=None):
    if df is None:  
        return 'ë°ì´í„°ë² ì´ìŠ¤ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤'
    else:
        df_copy = get_preprocessed_df(df)
        X_features = df_copy.iloc[:, :-1]
        y_target = df_copy.iloc[:, -1]
        X_train, X_test, y_train, y_test = train_test_split(
            X_features,
            y_target,
            test_size=0.3,
            random_state=0,
            stratify=y_target
        )
        return X_train, X_test, y_train, y_test

X_train,X_test,y_train,y_test=get_train_test_dataset(card_df)

# print('='*50)
# print(f'í•™ìŠµ ë°ì´í„° ë ˆì´ë¸” ê°’ ë¹„ìœ¨\n{y_train.value_counts()/y_train.shape[0]*100}')
# print('='*50)
# print(f'í…ŒìŠ¤íŠ¸ ë°ì´í„° ë ˆì´ë¸” ê°’ ë¹„ìœ¨\n{y_test.value_counts()/y_test.shape[0]*100}')

from sklearn.linear_model import LogisticRegression

lr_clf =LogisticRegression(max_iter=1000)
lr_clf.fit(X_train,y_train)
lr_pred=lr_clf.predict(X_test)
lr_pred_proba=lr_clf.predict_proba(X_test)[:,1]


# ëª¨ë¸ ìƒì„± ì„±ëŠ¥ í‰ê°€ : get_clf_eval(ì›ë˜ë‹µ, ì˜ˆì¸¡ê°’, ì˜ˆì¸¡í™•ë¥ )
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.metrics import precision_score, recall_score
from sklearn.metrics import f1_score, roc_auc_score


def get_clf_eval(y_test, pred=None, pred_proba=None):
    confusion = confusion_matrix( y_test, pred)
    accuracy = accuracy_score(y_test , pred) # ì •í™•ë„ ì ìˆ˜
    precision = precision_score(y_test , pred) # ì •ë°€ë„ ì ìˆ˜
    recall = recall_score(y_test , pred) # ì¬í˜„ìœ¨ ì ìˆ˜
    f1 = f1_score(y_test,pred) # ì •ë°€ë„, ì¬í˜„ìœ¨ ì¡°í™”í‰ê·  ê°’
    # ROC-AUC ì¶”ê°€
    roc_auc = roc_auc_score(y_test, pred_proba) # AUC ì ìˆ˜ : ë¶ˆê· í˜• ë°ì´í„° ì…‹ì—ì„œ í•„ìš”
    print('ì˜¤ì°¨ í–‰ë ¬')
    print(confusion)
    # ROC-AUC print ì¶”ê°€
    print('ì •í™•ë„: {0:.4f}, ì •ë°€ë„: {1:.4f}, ì¬í˜„ìœ¨: {2:.4f}, F1: {3:.4f}, AUC:{4:.4f}'.format(accuracy, precision, recall, f1, roc_auc))

def get_model_train_eval(
    model                   # ë¶„ë¥˜ ì•Œê³ ë¦¬ì¦˜
    ,ftr_train=None         # í•™ìŠµ ë°ì´í„°
    ,ftr_test=None          # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    ,tgt_train=None         # í•™ìŠµ ë°ì´í„° ë ˆì´ë¸”
    ,tgt_test=None          # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë ˆì´ë¸”
    ):
    model.fit(ftr_train,tgt_train)
    pred = model.predict(ftr_test)
    pred_proba=model.predict_proba(ftr_test)[:,1] # ì˜ˆì¸¡ í™•ë¥ 
    get_clf_eval(tgt_test,pred,pred_proba)

from lightgbm import LGBMClassifier

lgbm_clf = LGBMClassifier(
    n_estimators=1000
    ,num_leaves=64
    #, n_jobs=-1 # cpu ì¼ë•Œë§Œ ì‚¬ìš©
    ,device='gpu' # gpu ì¼ë•Œë§Œ ì‚¬ìš©
    ,boosting_from_average=False # ê·¹ë„ë¡œ ë¶ˆê· í˜•í•œ ë ˆì´ë¸”ì¼ ê²½ìš° Falseë¥¼ ì¤˜ì•¼ í•¨
)

get_model_train_eval(lgbm_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)
```

![image](https://github.com/user-attachments/assets/bb4b5e60-f3bb-4bd1-9f6c-72c5f373acd5)

```
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline

import warnings
warnings.filterwarnings('ignore')

card_df=pd.read_csv(
    './creditcard.csv'
    ,encoding='utf-8'
)

from sklearn.model_selection import train_test_split

def get_preprocessed_df(df=None):
    df_copy = df.copy()
    df_copy.drop('Time',axis=1,inplace=True)
    return df_copy

def get_train_test_dataset(df=None):
    if df is None:  
        return 'ë°ì´í„°ë² ì´ìŠ¤ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤'
    else:
        df_copy = get_preprocessed_df(df)
        X_features = df_copy.iloc[:, :-1]
        y_target = df_copy.iloc[:, -1]
        X_train, X_test, y_train, y_test = train_test_split(
            X_features,
            y_target,
            test_size=0.3,
            random_state=0,
            stratify=y_target
        )
        return X_train, X_test, y_train, y_test

X_train,X_test,y_train,y_test=get_train_test_dataset(card_df)

# print('='*50)
# print(f'í•™ìŠµ ë°ì´í„° ë ˆì´ë¸” ê°’ ë¹„ìœ¨\n{y_train.value_counts()/y_train.shape[0]*100}')
# print('='*50)
# print(f'í…ŒìŠ¤íŠ¸ ë°ì´í„° ë ˆì´ë¸” ê°’ ë¹„ìœ¨\n{y_test.value_counts()/y_test.shape[0]*100}')

from sklearn.linear_model import LogisticRegression

lr_clf =LogisticRegression(max_iter=1000)
lr_clf.fit(X_train,y_train)
lr_pred=lr_clf.predict(X_test)
lr_pred_proba=lr_clf.predict_proba(X_test)[:,1]


# ëª¨ë¸ ìƒì„± ì„±ëŠ¥ í‰ê°€ : get_clf_eval(ì›ë˜ë‹µ, ì˜ˆì¸¡ê°’, ì˜ˆì¸¡í™•ë¥ )
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.metrics import precision_score, recall_score
from sklearn.metrics import f1_score, roc_auc_score


def get_clf_eval(y_test, pred=None, pred_proba=None):
    confusion = confusion_matrix( y_test, pred)
    accuracy = accuracy_score(y_test , pred) # ì •í™•ë„ ì ìˆ˜
    precision = precision_score(y_test , pred) # ì •ë°€ë„ ì ìˆ˜
    recall = recall_score(y_test , pred) # ì¬í˜„ìœ¨ ì ìˆ˜
    f1 = f1_score(y_test,pred) # ì •ë°€ë„, ì¬í˜„ìœ¨ ì¡°í™”í‰ê·  ê°’
    # ROC-AUC ì¶”ê°€
    roc_auc = roc_auc_score(y_test, pred_proba) # AUC ì ìˆ˜ : ë¶ˆê· í˜• ë°ì´í„° ì…‹ì—ì„œ í•„ìš”
    print('ì˜¤ì°¨ í–‰ë ¬')
    print(confusion)
    # ROC-AUC print ì¶”ê°€
    print('ì •í™•ë„: {0:.4f}, ì •ë°€ë„: {1:.4f}, ì¬í˜„ìœ¨: {2:.4f}, F1: {3:.4f}, AUC:{4:.4f}'.format(accuracy, precision, recall, f1, roc_auc))

def get_model_train_eval(
    model                   # ë¶„ë¥˜ ì•Œê³ ë¦¬ì¦˜
    ,ftr_train=None         # í•™ìŠµ ë°ì´í„°
    ,ftr_test=None          # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    ,tgt_train=None         # í•™ìŠµ ë°ì´í„° ë ˆì´ë¸”
    ,tgt_test=None          # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë ˆì´ë¸”
    ):
    model.fit(ftr_train,tgt_train)
    pred = model.predict(ftr_test)
    pred_proba=model.predict_proba(ftr_test)[:,1] # ì˜ˆì¸¡ í™•ë¥ 
    get_clf_eval(tgt_test,pred,pred_proba)


from lightgbm import LGBMClassifier

lgbm_clf = LGBMClassifier(
    n_estimators=1000
    ,num_leaves=64
    #, n_jobs=-1 # cpu ì¼ë•Œë§Œ ì‚¬ìš©
    ,device='gpu' # gpu ì¼ë•Œë§Œ ì‚¬ìš©
    ,boosting_from_average=False # ê·¹ë„ë¡œ ë¶ˆê· í˜•í•œ ë ˆì´ë¸”ì¼ ê²½ìš° Falseë¥¼ ì¤˜ì•¼ í•¨
)

# lightGBM
get_model_train_eval(lgbm_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)

# LogisticRegression
get_model_train_eval(lr_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)
```
![image](https://github.com/user-attachments/assets/ac09dbe7-edea-40e5-8c34-0d8363bd1d5c)

---
## ë°ì´í„° ë¶„í¬ í™•ì¸, ë³€í™˜í›„ ëª¨ë¸ í•™ìŠµ/ì˜ˆì¸¡/í‰ê°€

```
# íŠ¹ì • í”¼ì³(ê¸ˆì•¡)ì˜ ë°ì´í„° ë¶„í¬ í™•ì¸
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline

import warnings
warnings.filterwarnings('ignore')

import seaborn as sns

card_df=pd.read_csv(
    './creditcard.csv'
    ,encoding='utf-8'
)

plt.figure(figsize=(8,4))
plt.xticks(
    range(0,30000,1000)
    ,rotation=60
)
sns.histplot(card_df['Amount'],bins=100,kde=True)
plt.show()
```
![image](https://github.com/user-attachments/assets/b32af0f7-1e8b-4743-bd6b-f3629af769b7)

---

```
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline  

import warnings
warnings.filterwarnings('ignore')  

import seaborn as sns

# CSV íŒŒì¼ì„ ì½ì–´ì™€ DataFrameìœ¼ë¡œ ì €ì¥
card_df = pd.read_csv(
    './creditcard.csv',  # ì‹ ìš©ì¹´ë“œ ê±°ë˜ ë°ì´í„° íŒŒì¼ ê²½ë¡œ
    encoding='utf-8'  # UTF-8 ì¸ì½”ë”©ìœ¼ë¡œ íŒŒì¼ì„ ì½ìŒ
)

# ì™¸ë„(Skewness) ê°’ í™•ì¸: ë°ì´í„°ì˜ ë¹„ëŒ€ì¹­ì„±ì„ í™•ì¸í•˜ê¸° ìœ„í•¨
# ë¹„ëŒ€ì¹­ ë°ì´í„°ì˜ ê²½ìš° ë¡œê·¸ ë³€í™˜ ë“±ìœ¼ë¡œ ì •ê·œì„±ì„ í™•ë³´í•  í•„ìš”ê°€ ìˆìŒ

# ì²¨ë„(Kurtosis) ê°’ í™•ì¸: ê·¹ë‹¨ê°’(outlier)ì˜ ì¡´ì¬ ì—¬ë¶€ë¥¼ íŒë‹¨í•˜ê¸° ìœ„í•¨
# ì–‘ì˜ ì²¨ë„: ë¶„í¬ì˜ ê¼¬ë¦¬ê°€ ë‘ê»ê³ , ì¤‘ì•™ì´ ë¾°ì¡± -> ê·¹ë‹¨ê°’ì´ ìì£¼ ë°œìƒ
#           -> ê·¹ë‹¨ê°’ ì²˜ë¦¬ê°€ í•„ìš”í•˜ë©°, ì ì ˆí•œ ëª¨ë¸ ì„ íƒì´ ì¤‘ìš”
# ìŒì˜ ì²¨ë„: ë¶„í¬ì˜ ê¼¬ë¦¬ê°€ ì–‡ê³ , ì¤‘ì•™ì´ í‰í‰ -> ê·¹ë‹¨ê°’ì´ ì ê²Œ ë°œìƒ
# ì¤‘ê°„ ì²¨ë„: ì •ê·œë¶„í¬ì— ê°€ê¹Œì›€ (ì™¸ë„ì™€ ì²¨ë„ê°€ 0ì— ê°€ê¹Œì›€)

from scipy.stats import skew  # ì™¸ë„ ê³„ì‚° í•¨ìˆ˜
from scipy.stats import kurtosis  # ì²¨ë„ ê³„ì‚° í•¨ìˆ˜

# 'Amount' ì—´ì˜ ì™¸ë„(Skewness) ê°’ ì¶œë ¥
print(f"ì™¸ë„ : {skew(card_df['Amount'])}")
print(f"ì²¨ë„ : {kurtosis(card_df['Amount'])}")
```
![image](https://github.com/user-attachments/assets/9fc70ec9-5e36-4415-800c-3abcccc2cccc)

---
## ğŸ“Š ëª¨ìˆ˜ì  vs ë¹„ëª¨ìˆ˜ì  ê²€ì • ë°©ë²• ì •ë¦¬

### 1. t-test (ë…ë¦½í‘œë³¸ t-ê²€ì •, ëŒ€ì‘í‘œë³¸ t-ê²€ì •)
#### ğŸ”¹ íŠ¹ì§•
- ë‘ ê·¸ë£¹ì˜ **í‰ê·  ì°¨ì´**ë¥¼ ë¹„êµí•˜ëŠ” **ëª¨ìˆ˜ì  ê²€ì •**  
- ë°ì´í„°ê°€ **ì •ê·œì„±ì„ ë§Œì¡±í•´ì•¼** í•¨  
- **ëŒ€ì‘í‘œë³¸ t-ê²€ì •**: ê°™ì€ ì§‘ë‹¨ì˜ ì‚¬ì „-ì‚¬í›„ ë°ì´í„°ë¥¼ ë¹„êµ  
- **ë…ë¦½í‘œë³¸ t-ê²€ì •**: ì„œë¡œ ë‹¤ë¥¸ ë‘ ì§‘ë‹¨ì„ ë¹„êµ  

#### âœ… ì¥ì 
- í•´ì„ì´ ì§ê´€ì ì´ë©° ë„ë¦¬ ì‚¬ìš©ë¨  
- ê²€ì •ë ¥ì´ ê°•í•˜ì—¬ ë¹„êµì  ì ì€ í‘œë³¸ì—ì„œë„ ì‹ ë¢°ì„± í™•ë³´ ê°€ëŠ¥  

#### âŒ ë‹¨ì 
- ì •ê·œì„±ì„ ë§Œì¡±í•˜ì§€ ì•Šìœ¼ë©´ ì‚¬ìš©í•˜ê¸° ì–´ë ¤ì›€  
- ë“±ë¶„ì‚°ì„±ì„ ê°€ì •í•´ì•¼ í•¨ (ë“±ë¶„ì‚°ì´ ë‹¤ë¥´ë©´ Welchâ€™s t-test ì‚¬ìš©)  

---

### 2. Mann-Whitney U test (ìœŒì½•ìŠ¨ ìˆœìœ„í•© ê²€ì •)
#### ğŸ”¹ íŠ¹ì§•
- ë‘ ê·¸ë£¹ì˜ **ì¤‘ìœ„ìˆ˜ ì°¨ì´**ë¥¼ ë¹„êµí•˜ëŠ” **ë¹„ëª¨ìˆ˜ ê²€ì •**  
- ë°ì´í„°ì˜ í¬ê¸° ìˆœì„œ(ìˆœìœ„)ë¥¼ ì´ìš©í•˜ì—¬ ë¹„êµ  
- ì •ê·œì„±ì„ ë§Œì¡±í•˜ì§€ ì•Šì•„ë„ ì‚¬ìš© ê°€ëŠ¥  

#### âœ… ì¥ì 
- ì •ê·œì„± ê°€ì •ì´ í•„ìš” ì—†ìŒ â†’ ì†Œê·œëª¨ ìƒ˜í”Œì—ì„œë„ í™œìš© ê°€ëŠ¥  
- ì´ìƒì¹˜(outlier)ì— ëœ ë¯¼ê°í•¨  

#### âŒ ë‹¨ì 
- í‰ê· ì´ ì•„ë‹Œ **ì¤‘ìœ„ìˆ˜ ë¹„êµ**ì´ë¯€ë¡œ í‰ê·  ì°¨ì´ë¥¼ í™•ì¸í•˜ëŠ” ë° í•œê³„ê°€ ìˆìŒ  
- í‘œë³¸ í¬ê¸°ê°€ í´ ê²½ìš° ê²€ì •ë ¥ì´ ë‚®ì•„ì§ˆ ìˆ˜ ìˆìŒ  

---

### 3. ANOVA (ë¶„ì‚° ë¶„ì„, Analysis of Variance)
#### ğŸ”¹ íŠ¹ì§•
- ì„¸ ê·¸ë£¹ ì´ìƒì˜ **í‰ê·  ì°¨ì´**ë¥¼ ë¹„êµí•˜ëŠ” **ëª¨ìˆ˜ì  ê²€ì •**  
- ì •ê·œì„±ê³¼ ë“±ë¶„ì‚°ì„±ì„ ë§Œì¡±í•´ì•¼ í•¨  
- ì§‘ë‹¨ ê°„ ë¶„ì‚°ê³¼ ì§‘ë‹¨ ë‚´ ë¶„ì‚°ì„ ì´ìš©í•˜ì—¬ ì°¨ì´ë¥¼ ë¶„ì„  

#### âœ… ì¥ì 
- ì„¸ ê·¸ë£¹ ì´ìƒì„ ë™ì‹œì— ë¹„êµí•  ìˆ˜ ìˆì–´ **ë‹¤ì¤‘ ë¹„êµ ë¬¸ì œ ë°©ì§€**  
- ê·¸ë£¹ ê°„ ì°¨ì´ë¥¼ ë¶„ì„í•˜ëŠ” ë° íš¨ê³¼ì   

#### âŒ ë‹¨ì 
- ì •ê·œì„±ê³¼ ë“±ë¶„ì‚°ì„±ì„ ë§Œì¡±í•˜ì§€ ì•Šìœ¼ë©´ ë¶€ì ì ˆí•¨  
- ì°¨ì´ê°€ ìˆë‹¤ê³ ë§Œ ì•Œë ¤ì£¼ë©°, **ì–´ëŠ ê·¸ë£¹ ê°„ ì°¨ì´ê°€ ìˆëŠ”ì§€ ì¶”ê°€ ë¶„ì„(Tukey HSD ë“±)ì´ í•„ìš”**  

---

### 4. Kruskal-Wallis test (í¬ë£¨ìŠ¤ì¹¼-ì™ˆë¦¬ìŠ¤ ê²€ì •)
#### ğŸ”¹ íŠ¹ì§•
- ì„¸ ê·¸ë£¹ ì´ìƒì˜ **ì¤‘ìœ„ìˆ˜ ì°¨ì´**ë¥¼ ë¹„êµí•˜ëŠ” **ë¹„ëª¨ìˆ˜ ê²€ì •**  
- ì •ê·œì„±ì„ ë§Œì¡±í•˜ì§€ ì•Šì•„ë„ ì‚¬ìš© ê°€ëŠ¥  
- ë°ì´í„°ì˜ ìˆœìœ„ë¥¼ ì´ìš©í•˜ì—¬ ë¹„êµ  

#### âœ… ì¥ì 
- ì •ê·œì„±ì„ ê°€ì •í•  í•„ìš” ì—†ìŒ  
- ì´ìƒì¹˜ì— ëœ ë¯¼ê°í•˜ì—¬ ë‹¤ì–‘í•œ ë°ì´í„°ì— ì ìš© ê°€ëŠ¥  

#### âŒ ë‹¨ì 
- í‰ê· ì´ ì•„ë‹Œ **ì¤‘ìœ„ìˆ˜ë¥¼ ë¹„êµ**í•˜ë¯€ë¡œ ëª¨ìˆ˜ì  ë°©ë²•ë³´ë‹¤ ëœ ê°•ë ¥í•¨  
- ê·¸ë£¹ ê°„ ì–´ëŠ ë¶€ë¶„ì—ì„œ ì°¨ì´ê°€ ë‚˜ëŠ”ì§€ ì¶”ê°€ ë¶„ì„ í•„ìš”  

---

### ğŸ“Œ ìš”ì•½ ì •ë¦¬

| ë¹„êµ ëŒ€ìƒ | ëª¨ìˆ˜ì  ë°©ë²• | ë¹„ëª¨ìˆ˜ì  ë°©ë²• |
|-----------|------------|--------------|
| **ë‘ ê·¸ë£¹ ë¹„êµ (í‰ê· )** | t-test | Mann-Whitney U test (Wilcoxon rank-sum test) |
| **ì„¸ ê·¸ë£¹ ì´ìƒ ë¹„êµ (í‰ê· )** | ANOVA | Kruskal-Wallis test |


| ê²€ì • ë°©ë²• | íŠ¹ì§• | ì¥ì  | ë‹¨ì  |
|----------|------|------|------|
| **t-test** | ë‘ ê·¸ë£¹ì˜ í‰ê·  ë¹„êµ (ëª¨ìˆ˜) | ê²€ì •ë ¥ì´ ê°•í•¨, ì§ê´€ì  í•´ì„ ê°€ëŠ¥ | ì •ê·œì„± í•„ìš”, ë“±ë¶„ì‚° ê°€ì • í•„ìš” |
| **Mann-Whitney U test** | ë‘ ê·¸ë£¹ì˜ ì¤‘ìœ„ìˆ˜ ë¹„êµ (ë¹„ëª¨ìˆ˜) | ì •ê·œì„± í•„ìš” ì—†ìŒ, ì´ìƒì¹˜ì— ê°•í•¨ | í‰ê·  ë¹„êµ ë¶ˆê°€, í° í‘œë³¸ì—ì„œ ê²€ì •ë ¥ ë‚®ìŒ |
| **ANOVA** | ì„¸ ê·¸ë£¹ ì´ìƒì˜ í‰ê·  ë¹„êµ (ëª¨ìˆ˜) | ë‹¤ì¤‘ ë¹„êµ ê°€ëŠ¥, íš¨ê³¼ì ì¸ ê·¸ë£¹ ì°¨ì´ ë¶„ì„ | ì •ê·œì„±Â·ë“±ë¶„ì‚° í•„ìš”, ì¶”ê°€ ë¶„ì„ í•„ìš” |
| **Kruskal-Wallis test** | ì„¸ ê·¸ë£¹ ì´ìƒì˜ ì¤‘ìœ„ìˆ˜ ë¹„êµ (ë¹„ëª¨ìˆ˜) | ì •ê·œì„± í•„ìš” ì—†ìŒ, ì´ìƒì¹˜ì— ê°•í•¨ | í‰ê·  ë¹„êµ ë¶ˆê°€, ì¶”ê°€ ë¶„ì„ í•„ìš” |

---
## ğŸ“Š ì •ê·œì„± íŒë‹¨ ë°©ë²• ì •ë¦¬

### **1. í†µê³„ì  ê²€ì • ë°©ë²•**
ì •ê·œì„±ì„ í™•ì¸í•˜ê¸° ìœ„í•œ ë‹¤ì–‘í•œ **ìœ ì˜í™•ë¥  ê¸°ë°˜ ê²€ì • ë°©ë²•**ì…ë‹ˆë‹¤.

#### ğŸ“Œ **1) Shapiro-Wilk Test**
- **íŠ¹ì§•**: ê°€ì¥ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” ì •ê·œì„± ê²€ì • ë°©ë²•  
- **ì‚¬ìš© ì¡°ê±´**: ìƒ˜í”Œ í¬ê¸°ê°€ ì‘ì„ ë•Œ(50 ì´í•˜) ì í•©  

âœ… **ì¥ì **  
- ì†Œê·œëª¨ ìƒ˜í”Œ(â‰¤50ê°œ)ì— ëŒ€í•´ ë†’ì€ ê²€ì •ë ¥ ì œê³µ  
- ë¹„êµì  ì •í™•í•œ ì •ê·œì„± ê²€ì • ê°€ëŠ¥  

âŒ **ë‹¨ì **  
- ìƒ˜í”Œ í¬ê¸°ê°€ ì»¤ì§€ë©´ ê²€ì •ë ¥ì´ ë„ˆë¬´ ê°•í•´ì ¸ ì •ê·œì„±ì„ ì‰½ê²Œ ê¸°ê°í•  ìˆ˜ ìˆìŒ  
- ì´ìƒì¹˜(outlier)ì— ë¯¼ê°  

---

#### ğŸ“Œ **2) Kolmogorov-Smirnov Test (K-S Test)**
- **íŠ¹ì§•**: ì •ê·œë¶„í¬ì™€ ë°ì´í„° ë¶„í¬ë¥¼ ë¹„êµí•˜ì—¬ ì •ê·œì„± ê²€ì •  
- **ì‚¬ìš© ì¡°ê±´**: ëŒ€ê·œëª¨ ìƒ˜í”Œì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•˜ì§€ë§Œ, ì •í™•ì„±ì´ ë‚®ìŒ  

âœ… **ì¥ì **  
- ìƒ˜í”Œ í¬ê¸°ì— ìƒê´€ì—†ì´ ì ìš© ê°€ëŠ¥  
- ë¶„í¬ì˜ ì°¨ì´ë¥¼ ì§ì ‘ ë¹„êµí•˜ëŠ” ë°©ì‹  

âŒ **ë‹¨ì **  
- ê²€ì •ë ¥ì´ ë‚®ì•„ ì •ê·œì„±ì„ ì •í™•íˆ íŒë³„í•˜ì§€ ëª»í•  ìˆ˜ ìˆìŒ  
- ìƒ˜í”Œ í¬ê¸°ê°€ í¬ë©´ ì •ê·œì„±ì„ ê¸°ê°í•  ê°€ëŠ¥ì„±ì´ ì»¤ì§  

---

#### ğŸ“Œ **3) Anderson-Darling Test**
- **íŠ¹ì§•**: Kolmogorov-Smirnov Testë³´ë‹¤ ì •ë°€í•œ ì •ê·œì„± ê²€ì • ë°©ë²•  
- **ì‚¬ìš© ì¡°ê±´**: ìƒ˜í”Œ í¬ê¸°ì— ìƒê´€ì—†ì´ í™œìš© ê°€ëŠ¥  

âœ… **ì¥ì **  
- ìƒ˜í”Œ í¬ê¸°ì— í¬ê²Œ ì˜í–¥ì„ ë°›ì§€ ì•ŠìŒ  
- ë¶„í¬ì˜ ê¼¬ë¦¬ ë¶€ë¶„ê¹Œì§€ ê³ ë ¤í•˜ì—¬ ì •ê·œì„±ì„ íŒë‹¨  

âŒ **ë‹¨ì **  
- Shapiro-Wilk Testë³´ë‹¤ ëœ ì§ê´€ì   
- íŠ¹ì •í•œ ìƒí™©ì—ì„œ p-value í•´ì„ì´ ì–´ë ¤ìš¸ ìˆ˜ ìˆìŒ  

---

#### ğŸ“Œ **4) Jarque-Bera Test**
- **íŠ¹ì§•**: ë°ì´í„°ì˜ **ì™œë„(skewness)ì™€ ì²¨ë„(kurtosis)** ë¥¼ ì´ìš©í•œ ì •ê·œì„± ê²€ì •  
- **ì‚¬ìš© ì¡°ê±´**: ëŒ€ê·œëª¨ ìƒ˜í”Œì—ì„œ ì‚¬ìš©  

âœ… **ì¥ì **  
- í° ìƒ˜í”Œì—ì„œë„ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ê²°ê³¼ ì œê³µ  
- ì™œë„ì™€ ì²¨ë„ë¥¼ í•¨ê»˜ ê³ ë ¤í•˜ì—¬ ì •ê·œì„± ê²€ì • ê°€ëŠ¥  

âŒ **ë‹¨ì **  
- ì†Œê·œëª¨ ìƒ˜í”Œì—ì„œëŠ” ê²€ì •ë ¥ì´ ë‚®ì•„ ë¹„íš¨ìœ¨ì   
- ì™œë„ì™€ ì²¨ë„ê°€ ê·¹ë‹¨ì ì¸ ê²½ìš° ì •ê·œì„±ì„ ì •í™•íˆ íŒë‹¨í•˜ê¸° ì–´ë ¤ì›€  

---

### **2. ì‹œê°ì  ë¶„ì„ ë°©ë²•**
ì •ê·œì„±ì„ **ì§ê´€ì ìœ¼ë¡œ í™•ì¸**í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.

#### ğŸ“Œ **1) íˆìŠ¤í† ê·¸ë¨ (Histogram)**
- **íŠ¹ì§•**: ë°ì´í„°ì˜ ë¶„í¬ê°€ ì¢…ëª¨ì–‘(ì •ê·œë¶„í¬)ì¸ì§€ í™•ì¸  

âœ… **ì¥ì **  
- ì •ê·œì„±ì„ ë¹ ë¥´ê²Œ ì‹œê°ì ìœ¼ë¡œ í™•ì¸ ê°€ëŠ¥  
- ì§ê´€ì ì¸ í•´ì„ ê°€ëŠ¥  

âŒ **ë‹¨ì **  
- ìƒ˜í”Œ í¬ê¸°ê°€ ì‘ìœ¼ë©´ ë¶€ì •í™•í•  ìˆ˜ ìˆìŒ  
- ìœ¡ì•ˆ íŒë‹¨ì´ë¯€ë¡œ ì£¼ê´€ì ì¼ ìˆ˜ ìˆìŒ  

---

#### ğŸ“Œ **2) Q-Q Plot (Quantile-Quantile Plot)**
- **íŠ¹ì§•**: ë°ì´í„°ì˜ ë¶„ìœ„ìˆ˜ë¥¼ ì •ê·œë¶„í¬ì˜ ë¶„ìœ„ìˆ˜ì™€ ë¹„êµ  

âœ… **ì¥ì **  
- ì •ê·œì„±ì„ ì‹œê°ì ìœ¼ë¡œ ì‰½ê²Œ íŒë‹¨ ê°€ëŠ¥  
- ì •ê·œì„± ì—¬ë¶€ë¿ë§Œ ì•„ë‹ˆë¼ ë°ì´í„° ë¶„í¬ì˜ íŠ¹ì„±(ì™œë„, ì´ìƒì¹˜)ë„ í™•ì¸ ê°€ëŠ¥  

âŒ **ë‹¨ì **  
- í•´ì„ì´ ë‹¤ì†Œ ì–´ë ¤ìš¸ ìˆ˜ ìˆìŒ  
- ìœ¡ì•ˆ íŒë‹¨ì´ë¯€ë¡œ ì •í™•í•œ ê²€ì • ë°©ë²•ì€ ì•„ë‹˜  

---

#### ğŸ“Œ **3) P-P Plot (Probability-Probability Plot)**
- **íŠ¹ì§•**: í‘œë³¸ ë°ì´í„°ì˜ ëˆ„ì ë¶„í¬ì™€ ì •ê·œë¶„í¬ì˜ ëˆ„ì ë¶„í¬ë¥¼ ë¹„êµ  

âœ… **ì¥ì **  
- Q-Q Plotê³¼ ìœ ì‚¬í•˜ì§€ë§Œ ëˆ„ì ë¶„í¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ê¸° ë•Œë¬¸ì— ì •ê·œì„± ê²€ì •ì— ë” ìœ ë¦¬  

âŒ **ë‹¨ì **  
- ë°ì´í„° ìˆ˜ê°€ ë§ì•„ì§ˆìˆ˜ë¡ í•´ì„ì´ ì–´ë ¤ì›Œì§ˆ ìˆ˜ ìˆìŒ  

---

#### ğŸ“Œ **4) Box Plot (ìƒì ê·¸ë¦¼)**
- **íŠ¹ì§•**: ë°ì´í„°ì˜ ëŒ€ì¹­ì„±ê³¼ ì´ìƒì¹˜(outlier)ë¥¼ í™•ì¸  

âœ… **ì¥ì **  
- ë°ì´í„°ì˜ ë¶„í¬ë¥¼ ì§ê´€ì ìœ¼ë¡œ ë³¼ ìˆ˜ ìˆìŒ  
- ì´ìƒì¹˜ê°€ ìˆëŠ”ì§€ ì‰½ê²Œ í™•ì¸ ê°€ëŠ¥  

âŒ **ë‹¨ì **  
- ì •ê·œì„±ì„ ì§ì ‘ì ìœ¼ë¡œ ê²€ì •í•  ìˆ˜ ì—†ìŒ  
- ìƒ˜í”Œ í¬ê¸°ê°€ ì‘ìœ¼ë©´ ì™œê³¡ë  ê°€ëŠ¥ì„±ì´ ìˆìŒ  

---

### **ğŸ“Œ ì •ê·œì„± íŒë‹¨ ë°©ë²• ë¹„êµí‘œ**

| ë°©ë²• | íŠ¹ì§• | ì¥ì  | ë‹¨ì  |
|------|------|------|------|
| **Shapiro-Wilk Test** | ê°€ì¥ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” ì •ê·œì„± ê²€ì • | ì†Œê·œëª¨ ìƒ˜í”Œ(â‰¤50)ì—ì„œ ë†’ì€ ê²€ì •ë ¥ | ëŒ€ê·œëª¨ ìƒ˜í”Œì—ì„œ ê³¼ë„í•˜ê²Œ ì •ê·œì„±ì„ ê¸°ê°í•  ê°€ëŠ¥ì„± ìˆìŒ |
| **Kolmogorov-Smirnov Test** | ì •ê·œë¶„í¬ì™€ ë°ì´í„° ë¶„í¬ë¥¼ ë¹„êµ | ìƒ˜í”Œ í¬ê¸°ì— ê´€ê³„ì—†ì´ ì ìš© ê°€ëŠ¥ | ê²€ì •ë ¥ì´ ë‚®ì•„ ì •ê·œì„±ì„ ì •í™•íˆ íŒë³„í•˜ê¸° ì–´ë ¤ì›€ |
| **Anderson-Darling Test** | K-S Testë³´ë‹¤ ì •ë°€í•œ ì •ê·œì„± ê²€ì • | ìƒ˜í”Œ í¬ê¸° ì˜í–¥ì„ ëœ ë°›ìŒ | í•´ì„ì´ ë‹¤ì†Œ ì–´ë ¤ìš¸ ìˆ˜ ìˆìŒ |
| **Jarque-Bera Test** | ì™œë„ì™€ ì²¨ë„ë¥¼ ì´ìš©í•œ ì •ê·œì„± ê²€ì • | ëŒ€ê·œëª¨ ìƒ˜í”Œì—ì„œ ì‹ ë¢°ì„± ë†’ìŒ | ì†Œê·œëª¨ ìƒ˜í”Œì—ì„œëŠ” ê²€ì •ë ¥ì´ ë‚®ìŒ |
| **íˆìŠ¤í† ê·¸ë¨** | ë°ì´í„° ë¶„í¬ê°€ ì •ê·œì„±ì„ ë”°ë¥´ëŠ”ì§€ í™•ì¸ | ì§ê´€ì ì¸ í•´ì„ ê°€ëŠ¥ | ìƒ˜í”Œ í¬ê¸°ê°€ ì‘ìœ¼ë©´ ë¶€ì •í™•í•  ìˆ˜ ìˆìŒ |
| **Q-Q Plot** | ë°ì´í„° ë¶„ìœ„ìˆ˜ì™€ ì •ê·œë¶„í¬ ë¶„ìœ„ìˆ˜ë¥¼ ë¹„êµ | ì™œë„, ì´ìƒì¹˜ ë“±ì„ í•œëˆˆì— íŒŒì•… ê°€ëŠ¥ | í•´ì„ì´ ë‹¤ì†Œ ì–´ë ¤ìš¸ ìˆ˜ ìˆìŒ |
| **P-P Plot** | ëˆ„ì ë¶„í¬ë¥¼ ë¹„êµí•˜ì—¬ ì •ê·œì„± ê²€ì • | Q-Q Plotë³´ë‹¤ ì •ê·œì„± ê²€ì •ì— ìœ ë¦¬ | ë°ì´í„° ìˆ˜ê°€ ë§ì•„ì§€ë©´ í•´ì„ ì–´ë ¤ì›€ |
| **Box Plot** | ë°ì´í„°ì˜ ëŒ€ì¹­ì„±ê³¼ ì´ìƒì¹˜ í™•ì¸ | ì´ìƒì¹˜ë¥¼ ì‰½ê²Œ ë°œê²¬ ê°€ëŠ¥ | ì •ê·œì„±ì„ ì§ì ‘ ê²€ì •í•  ìˆ˜ ì—†ìŒ |

---

### **âœ… ì •ê·œì„± íŒë‹¨ íë¦„**
1ï¸âƒ£ **ì‹œê°ì  ë°©ë²•(Q-Q Plot, íˆìŠ¤í† ê·¸ë¨ ë“±)ìœ¼ë¡œ ëŒ€ëµì ì¸ ì •ê·œì„± í™•ì¸**  
2ï¸âƒ£ **í†µê³„ì  ê²€ì •(Shapiro-Wilk Test, K-S Test ë“±) ìˆ˜í–‰**  
3ï¸âƒ£ **p-value â‰¥ 0.05** â†’ ì •ê·œì„± ë§Œì¡± (ëª¨ìˆ˜ ê²€ì • ê°€ëŠ¥)  
4ï¸âƒ£ **p-value < 0.05** â†’ ì •ê·œì„± ë¶ˆë§Œì¡± (ë¹„ëª¨ìˆ˜ ê²€ì • í•„ìš”)  

ğŸ“Œ **ì •ê·œì„± ê²€ì •ì€ ë°ì´í„° íŠ¹ì„±ì— ë”°ë¼ ì ì ˆí•œ ë°©ë²•ì„ ì„ íƒí•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤!** ğŸš€

---
## ğŸ“Œ ì •ê·œì„± íŒë‹¨ ë°©ë²•

### 1ï¸âƒ£ í†µê³„ì  ê²€ì • ë°©ë²• (ìœ ì˜í™•ë¥  ê¸°ë°˜)

| ë°©ë²• | ì„¤ëª… | ì‚¬ìš© ì¡°ê±´ |
|------|------|----------|
| **Shapiro-Wilk Test** | ê°€ì¥ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” ì •ê·œì„± ê²€ì • ë°©ë²• | ìƒ˜í”Œ í¬ê¸°ê°€ 50 ì´í•˜ì¼ ë•Œ ì í•© |
| **Kolmogorov-Smirnov Test** | ì •ê·œë¶„í¬ì™€ ë°ì´í„° ë¶„í¬ë¥¼ ë¹„êµ | ìƒ˜í”Œ í¬ê¸°ê°€ í¬ë©´ ê²€ì •ë ¥ì´ ë–¨ì–´ì§ |
| **Anderson-Darling Test** | Kolmogorov-Smirnov Testë³´ë‹¤ ì •ë°€í•œ ê²€ì • | ìƒ˜í”Œ í¬ê¸°ì— ìƒê´€ì—†ì´ í™œìš© ê°€ëŠ¥ |
| **Jarque-Bera Test** | ì™œë„(skewness)ì™€ ì²¨ë„(kurtosis)ë¥¼ ì´ìš©í•œ ì •ê·œì„± ê²€ì • | ëŒ€ê·œëª¨ ìƒ˜í”Œì—ì„œ ë§ì´ ì‚¬ìš©ë¨ |

ğŸ’¡ **ìœ ì˜í™•ë¥ (p-value)ì´ 0.05ë³´ë‹¤ ì‘ìœ¼ë©´ â†’ ì •ê·œì„±ì„ ë§Œì¡±í•˜ì§€ ì•ŠìŒ(ê·€ë¬´ê°€ì„¤ ê¸°ê°)**

---

### 2ï¸âƒ£ ì‹œê°ì  ë¶„ì„ ë°©ë²•

| ë°©ë²• | ì„¤ëª… |
|------|------|
| **íˆìŠ¤í† ê·¸ë¨ (Histogram)** | ë°ì´í„°ê°€ ì¢…ëª¨ì–‘(ì •ê·œë¶„í¬)ì¸ì§€ í™•ì¸ |
| **Q-Q Plot (Quantile-Quantile Plot)** | ë°ì´í„°ê°€ ì •ê·œë¶„í¬ì˜ ë¶„ìœ„ìˆ˜ì™€ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸ |
| **P-P Plot (Probability-Probability Plot)** | í‘œë³¸ì˜ ëˆ„ì ë¶„í¬ì™€ ì •ê·œë¶„í¬ì˜ ëˆ„ì ë¶„í¬ë¥¼ ë¹„êµ |
| **Box Plot (ìƒì ê·¸ë¦¼)** | ëŒ€ì¹­ì„±ì´ ìˆëŠ”ì§€, ì´ìƒì¹˜(outlier)ê°€ ë§ì€ì§€ í™•ì¸ |

ğŸ’¡ **ë°ì´í„°ê°€ ì¢…ëª¨ì–‘ì„ ë ê³ , Q-Q Plotì´ ì§ì„ ì— ê°€ê¹Œìš°ë©´ ì •ê·œì„±ì„ ë§Œì¡±**

---

```
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline  

import warnings
warnings.filterwarnings('ignore')  

import seaborn as sns

# CSV íŒŒì¼ì„ ì½ì–´ì™€ DataFrameìœ¼ë¡œ ì €ì¥  
card_df = pd.read_csv(
    './creditcard.csv',  # ì‹ ìš©ì¹´ë“œ ê±°ë˜ ë°ì´í„° íŒŒì¼ ê²½ë¡œ
    encoding='utf-8'  # UTF-8 ì¸ì½”ë”©ìœ¼ë¡œ íŒŒì¼ì„ ì½ìŒ
)

from scipy.stats import shapiro

stat,p=shapiro(card_df['Amount'].values)
print(f'í†µê³„ëŸ‰ : {stat}')
print(f'p-value : {p}')
```
![image](https://github.com/user-attachments/assets/d2ed69b7-01f6-429c-b367-dd3533a28da8)

```
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline  

import warnings
warnings.filterwarnings('ignore')  

import seaborn as sns

# CSV íŒŒì¼ì„ ì½ì–´ì™€ DataFrameìœ¼ë¡œ ì €ì¥  
card_df = pd.read_csv(
    './creditcard.csv',  # ì‹ ìš©ì¹´ë“œ ê±°ë˜ ë°ì´í„° íŒŒì¼ ê²½ë¡œ
    encoding='utf-8'  # UTF-8 ì¸ì½”ë”©ìœ¼ë¡œ íŒŒì¼ì„ ì½ìŒ
)

from scipy.stats import shapiro

stat,p=shapiro(card_df['Amount'].values)

print(f'í†µê³„ëŸ‰ : {stat}')
print(f'p-value : {p}')

if p>0.05:
    print('ì •ê·œì„±ì„ ë§Œì¡±')
else:
    print('ì •ê·œì„± ì—†ìŒ')
```
![image](https://github.com/user-attachments/assets/f0ca2035-1dd7-44b2-9b53-8aa7041bb485)

```
# ì •ê·œì„± ìˆëŠ” ë°ì´í„° ì‹œê°í™”
import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as stats

data=np.random.normal(loc=0,  scale=1,size=1000)

# QQ plot ìƒì„±
stats.probplot(data,dist='norm',plot=plt)
plt.grid()
plt.show()
```
![image](https://github.com/user-attachments/assets/91ddba78-a759-4bf2-824a-21bbf4882c49)

```
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline  

import warnings
warnings.filterwarnings('ignore')  

import seaborn as sns

# CSV íŒŒì¼ì„ ì½ì–´ì™€ DataFrameìœ¼ë¡œ ì €ì¥  
card_df = pd.read_csv(
    './creditcard.csv',  # ì‹ ìš©ì¹´ë“œ ê±°ë˜ ë°ì´í„° íŒŒì¼ ê²½ë¡œ
    encoding='utf-8'  # UTF-8 ì¸ì½”ë”©ìœ¼ë¡œ íŒŒì¼ì„ ì½ìŒ
)

import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as stats

data=np.random.normal(loc=0,  scale=1,size=1000)

# QQ plot ìƒì„±
stats.probplot(card_df['Amount'].values,dist='norm',plot=plt)
plt.grid()
plt.show()
```
![image](https://github.com/user-attachments/assets/21ffed58-bd3a-404a-a2b7-a9f6c999101b)

```
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline  

import warnings
warnings.filterwarnings('ignore')  

import seaborn as sns

# CSV íŒŒì¼ì„ ì½ì–´ì™€ DataFrameìœ¼ë¡œ ì €ì¥  
card_df = pd.read_csv(
    './creditcard.csv',  # ì‹ ìš©ì¹´ë“œ ê±°ë˜ ë°ì´í„° íŒŒì¼ ê²½ë¡œ
    encoding='utf-8'  # UTF-8 ì¸ì½”ë”©ìœ¼ë¡œ íŒŒì¼ì„ ì½ìŒ
)

# standarization : ë°ì´í„° ì •ê·œë¶„í¬ í˜•íƒœë¡œ ë³€í™˜

from sklearn.preprocessing import StandardScaler

# ê¸°ì¡´ì˜ Amount featureì˜ í‰ê· ,í‘œì¤€í¸ì°¨, ì™¸ë„, ì²¨ë„
avg_amount=card_df['Amount'].mean()
std_amount=card_df['Amount'].std()

from scipy.stats import skew  # ì™¸ë„ ê³„ì‚° í•¨ìˆ˜
from scipy.stats import kurtosis  # ì²¨ë„ ê³„ì‚° í•¨ìˆ˜
skew_amount=skew(card_df['Amount'])
kurtosis_amount=kurtosis(card_df['Amount'])

# StandardScaler ì ìš©í•œ Amount featureì˜ í‰ê· ,í‘œì¤€í¸ì°¨, ì™¸ë„, ì²¨ë„
scaler=StandardScaler()
ss_amount=scaler.fit_transform(card_df['Amount'].values.reshape(-1,1))

print('ê¸°ì¡´ ë°ì´í„° Amount ì»¬ëŸ¼ì˜ í‰ê·  ,í‘œì¤€í¸ì°¨ ,ì™¸ë„ ,ì²¨ë„')
print(f'{avg_amount} / {std_amount} / {skew_amount} / {kurtosis_amount}')
print('='*100)
print('í‘œì¤€í™”í•œ ë°ì´í„° Amount ì»¬ëŸ¼ì˜ í‰ê·  ,í‘œì¤€í¸ì°¨ ,ì™¸ë„ ,ì²¨ë„')
print(f'{ss_amount.mean()} / {ss_amount.std()} / {skew(ss_amount)} / {kurtosis(ss_amount)}')
```
![image](https://github.com/user-attachments/assets/0c268337-ed99-4ad8-920b-8a62d9003aa8)

```
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline

import warnings
warnings.filterwarnings('ignore')

card_df=pd.read_csv(
    './creditcard.csv'
    ,encoding='utf-8'
)

from sklearn.model_selection import train_test_split

def get_preprocessed_df(df=None):
    df_copy = df.copy()
    df_copy.drop('Time',axis=1,inplace=True)
    return df_copy

def get_train_test_dataset(df=None):
    if df is None:  
        return 'ë°ì´í„°ë² ì´ìŠ¤ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤'
    else:
        df_copy = get_preprocessed_df(df)
        X_features = df_copy.iloc[:, :-1]
        y_target = df_copy.iloc[:, -1]
        X_train, X_test, y_train, y_test = train_test_split(
            X_features,
            y_target,
            test_size=0.3,
            random_state=0,
            stratify=y_target
        )
        return X_train, X_test, y_train, y_test

X_train,X_test,y_train,y_test=get_train_test_dataset(card_df)

# print('='*50)
# print(f'í•™ìŠµ ë°ì´í„° ë ˆì´ë¸” ê°’ ë¹„ìœ¨\n{y_train.value_counts()/y_train.shape[0]*100}')
# print('='*50)
# print(f'í…ŒìŠ¤íŠ¸ ë°ì´í„° ë ˆì´ë¸” ê°’ ë¹„ìœ¨\n{y_test.value_counts()/y_test.shape[0]*100}')

from sklearn.linear_model import LogisticRegression

lr_clf =LogisticRegression(max_iter=1000)
lr_clf.fit(X_train,y_train)
lr_pred=lr_clf.predict(X_test)
lr_pred_proba=lr_clf.predict_proba(X_test)[:,1]


# ëª¨ë¸ ìƒì„± ì„±ëŠ¥ í‰ê°€ : get_clf_eval(ì›ë˜ë‹µ, ì˜ˆì¸¡ê°’, ì˜ˆì¸¡í™•ë¥ )
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.metrics import precision_score, recall_score
from sklearn.metrics import f1_score, roc_auc_score


def get_clf_eval(y_test, pred=None, pred_proba=None):
    confusion = confusion_matrix( y_test, pred)
    accuracy = accuracy_score(y_test , pred) # ì •í™•ë„ ì ìˆ˜
    precision = precision_score(y_test , pred) # ì •ë°€ë„ ì ìˆ˜
    recall = recall_score(y_test , pred) # ì¬í˜„ìœ¨ ì ìˆ˜
    f1 = f1_score(y_test,pred) # ì •ë°€ë„, ì¬í˜„ìœ¨ ì¡°í™”í‰ê·  ê°’
    # ROC-AUC ì¶”ê°€
    roc_auc = roc_auc_score(y_test, pred_proba) # AUC ì ìˆ˜ : ë¶ˆê· í˜• ë°ì´í„° ì…‹ì—ì„œ í•„ìš”
    print('ì˜¤ì°¨ í–‰ë ¬')
    print(confusion)
    # ROC-AUC print ì¶”ê°€
    print('ì •í™•ë„: {0:.4f}, ì •ë°€ë„: {1:.4f}, ì¬í˜„ìœ¨: {2:.4f}, F1: {3:.4f}, AUC:{4:.4f}'.format(accuracy, precision, recall, f1, roc_auc))

def get_model_train_eval(
    model                   # ë¶„ë¥˜ ì•Œê³ ë¦¬ì¦˜
    ,ftr_train=None         # í•™ìŠµ ë°ì´í„°
    ,ftr_test=None          # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    ,tgt_train=None         # í•™ìŠµ ë°ì´í„° ë ˆì´ë¸”
    ,tgt_test=None          # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë ˆì´ë¸”
    ):
    model.fit(ftr_train,tgt_train)
    pred = model.predict(ftr_test)
    pred_proba=model.predict_proba(ftr_test)[:,1] # ì˜ˆì¸¡ í™•ë¥ 
    get_clf_eval(tgt_test,pred,pred_proba)


from lightgbm import LGBMClassifier

lgbm_clf = LGBMClassifier(
    n_estimators=1000
    ,num_leaves=64
    #, n_jobs=-1 # cpu ì¼ë•Œë§Œ ì‚¬ìš©
    ,device='gpu' # gpu ì¼ë•Œë§Œ ì‚¬ìš©
    ,boosting_from_average=False # ê·¹ë„ë¡œ ë¶ˆê· í˜•í•œ ë ˆì´ë¸”ì¼ ê²½ìš° Falseë¥¼ ì¤˜ì•¼ í•¨
)

print('StandarScaler ì²˜ë¦¬ ì „')
print('lightGBM')
# lightGBM
get_model_train_eval(lgbm_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)
print('='*100)
print('LogisticRegression')
# LogisticRegression
get_model_train_eval(lr_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)
print('='*100)
print('StandarScaler ì²˜ë¦¬ í›„')
# ë°ì´í„°ë¥¼ StandarScaler ì²˜ë¦¬ í›„ ì„±ëŠ¥ í‰ê°€
from sklearn.preprocessing import StandardScaler

def get_preprocessed_df(df=None):
    df_copy = df.copy()
    scaler=StandardScaler()
    ss_amount=scaler.fit_transform(df_copy['Amount'].values.reshape(-1,1))
    df_copy.insert(0,'Amount_Scaled',ss_amount)
    df_copy.drop(['Time','Amount'],axis=1,inplace=True)
    return df_copy # ì „ì²˜ë¦¬ ëœ ë°ì´í„°í”„ë ˆì„ ë°˜í™˜

# ì›ë³¸ ë°ì´í„°í”„ë ˆì„ : card_df
X_train,X_test,y_train,y_test=get_train_test_dataset(card_df)

print('ë¡œì§€ìŠ¤í‹± íšŒê·€ ì˜ˆì¸¡ ì„±ëŠ¥')
get_model_train_eval(lr_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)
print('='*100)
print('lightGBM ì˜ˆì¸¡ ì„±ëŠ¥')
get_model_train_eval(lgbm_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)
```
```
StandarScaler ì²˜ë¦¬ ì „
lightGBM
ì˜¤ì°¨ í–‰ë ¬
[[85243    52]
 [   69    79]]
ì •í™•ë„: 0.9986, ì •ë°€ë„: 0.6031, ì¬í˜„ìœ¨: 0.5338, F1: 0.5663, AUC:0.7665
====================================================================================================
LogisticRegression
ì˜¤ì°¨ í–‰ë ¬
[[85280    15]
 [   54    94]]
ì •í™•ë„: 0.9992, ì •ë°€ë„: 0.8624, ì¬í˜„ìœ¨: 0.6351, F1: 0.7315, AUC:0.9727
====================================================================================================
StandarScaler ì²˜ë¦¬ í›„
ë¡œì§€ìŠ¤í‹± íšŒê·€ ì˜ˆì¸¡ ì„±ëŠ¥
ì˜¤ì°¨ í–‰ë ¬
[[85281    14]
 [   55    93]]
ì •í™•ë„: 0.9992, ì •ë°€ë„: 0.8692, ì¬í˜„ìœ¨: 0.6284, F1: 0.7294, AUC:0.9706
====================================================================================================
lightGBM ì˜ˆì¸¡ ì„±ëŠ¥
ì˜¤ì°¨ í–‰ë ¬
[[85258    37]
 [   62    86]]
ì •í™•ë„: 0.9988, ì •ë°€ë„: 0.6992, ì¬í˜„ìœ¨: 0.5811, F1: 0.6347, AUC:0.7903
```

```
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline  

import warnings
warnings.filterwarnings('ignore')  

import seaborn as sns

# CSV íŒŒì¼ì„ ì½ì–´ì™€ DataFrameìœ¼ë¡œ ì €ì¥  
card_df = pd.read_csv(
    './creditcard.csv',  # ì‹ ìš©ì¹´ë“œ ê±°ë˜ ë°ì´í„° íŒŒì¼ ê²½ë¡œ
    encoding='utf-8'  # UTF-8 ì¸ì½”ë”©ìœ¼ë¡œ íŒŒì¼ì„ ì½ìŒ
)

# ê¸°ì¡´ì˜ Amount featureì˜ í‰ê· , í‘œì¤€í¸ì°¨, ì™¸ë„, ì²¨ë„
avg_amount = card_df['Amount'].mean()
std_amount = card_df['Amount'].std()

from scipy.stats import skew, kurtosis  # ì™¸ë„, ì²¨ë„ ê³„ì‚° í•¨ìˆ˜

skew_amount = skew(card_df['Amount'])
kurtosis_amount = kurtosis(card_df['Amount'])

# StandardScaler ì ìš©í•œ Amount featureì˜ í‰ê· ,í‘œì¤€í¸ì°¨, ì™¸ë„, ì²¨ë„
from sklearn.preprocessing import StandardScaler
scaler=StandardScaler()
ss_amount=scaler.fit_transform(card_df['Amount'].values.reshape(-1,1))


# ë¡œê·¸ ë³€í™˜ ì ìš©
log_amount = np.log1p(card_df['Amount'])


# ë¡œê·¸ ë³€í™˜ í›„ í‰ê· , í‘œì¤€í¸ì°¨, ì™¸ë„, ì²¨ë„
log_avg_amount = log_amount.mean()
log_std_amount = log_amount.std()
log_skew_amount = skew(log_amount)
log_kurtosis_amount = kurtosis(log_amount)

print('='*100)
print('ê¸°ì¡´ ë°ì´í„° Amount ì»¬ëŸ¼ì˜ í‰ê·  ,í‘œì¤€í¸ì°¨ ,ì™¸ë„ ,ì²¨ë„')
print(f'{avg_amount} / {std_amount} / {skew_amount} / {kurtosis_amount}')

print('='*100)
print('logí™”í•œ ë°ì´í„° Amount ì»¬ëŸ¼ì˜ í‰ê·  ,í‘œì¤€í¸ì°¨ ,ì™¸ë„ ,ì²¨ë„')
print(f'{ss_amount.mean()} / {ss_amount.std()} / {skew(ss_amount)} / {kurtosis(ss_amount)}')
print('='*100)
print('ë¡œê·¸ ë³€í™˜í•œ ë°ì´í„° Amount ì»¬ëŸ¼ì˜ í‰ê·  ,í‘œì¤€í¸ì°¨ ,ì™¸ë„ ,ì²¨ë„')
print(f'{log_avg_amount} / {log_std_amount} / {log_skew_amount} / {log_kurtosis_amount}')
print('='*100)
```
![image](https://github.com/user-attachments/assets/e974af5e-6c00-4131-aa3c-4a9e7a398ae2)

```
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline

import warnings
warnings.filterwarnings('ignore')

card_df=pd.read_csv(
    './creditcard.csv'
    ,encoding='utf-8'
)

from sklearn.model_selection import train_test_split

def get_preprocessed_df(df=None):
    df_copy = df.copy()
    df_copy.drop('Time',axis=1,inplace=True)
    return df_copy

def get_train_test_dataset(df=None):
    if df is None:  
        return 'ë°ì´í„°ë² ì´ìŠ¤ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤'
    else:
        df_copy = get_preprocessed_df(df)
        X_features = df_copy.iloc[:, :-1]
        y_target = df_copy.iloc[:, -1]
        X_train, X_test, y_train, y_test = train_test_split(
            X_features,
            y_target,
            test_size=0.3,
            random_state=0,
            stratify=y_target
        )
        return X_train, X_test, y_train, y_test

X_train,X_test,y_train,y_test=get_train_test_dataset(card_df)

# print('='*50)
# print(f'í•™ìŠµ ë°ì´í„° ë ˆì´ë¸” ê°’ ë¹„ìœ¨\n{y_train.value_counts()/y_train.shape[0]*100}')
# print('='*50)
# print(f'í…ŒìŠ¤íŠ¸ ë°ì´í„° ë ˆì´ë¸” ê°’ ë¹„ìœ¨\n{y_test.value_counts()/y_test.shape[0]*100}')

from sklearn.linear_model import LogisticRegression

lr_clf =LogisticRegression(max_iter=1000)
lr_clf.fit(X_train,y_train)
lr_pred=lr_clf.predict(X_test)
lr_pred_proba=lr_clf.predict_proba(X_test)[:,1]


# ëª¨ë¸ ìƒì„± ì„±ëŠ¥ í‰ê°€ : get_clf_eval(ì›ë˜ë‹µ, ì˜ˆì¸¡ê°’, ì˜ˆì¸¡í™•ë¥ )
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.metrics import precision_score, recall_score
from sklearn.metrics import f1_score, roc_auc_score


def get_clf_eval(y_test, pred=None, pred_proba=None):
    confusion = confusion_matrix( y_test, pred)
    accuracy = accuracy_score(y_test , pred) # ì •í™•ë„ ì ìˆ˜
    precision = precision_score(y_test , pred) # ì •ë°€ë„ ì ìˆ˜
    recall = recall_score(y_test , pred) # ì¬í˜„ìœ¨ ì ìˆ˜
    f1 = f1_score(y_test,pred) # ì •ë°€ë„, ì¬í˜„ìœ¨ ì¡°í™”í‰ê·  ê°’
    # ROC-AUC ì¶”ê°€
    roc_auc = roc_auc_score(y_test, pred_proba) # AUC ì ìˆ˜ : ë¶ˆê· í˜• ë°ì´í„° ì…‹ì—ì„œ í•„ìš”
    print('ì˜¤ì°¨ í–‰ë ¬')
    print(confusion)
    # ROC-AUC print ì¶”ê°€
    print('ì •í™•ë„: {0:.4f}, ì •ë°€ë„: {1:.4f}, ì¬í˜„ìœ¨: {2:.4f}, F1: {3:.4f}, AUC:{4:.4f}'.format(accuracy, precision, recall, f1, roc_auc))

def get_model_train_eval(
    model                   # ë¶„ë¥˜ ì•Œê³ ë¦¬ì¦˜
    ,ftr_train=None         # í•™ìŠµ ë°ì´í„°
    ,ftr_test=None          # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    ,tgt_train=None         # í•™ìŠµ ë°ì´í„° ë ˆì´ë¸”
    ,tgt_test=None          # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë ˆì´ë¸”
    ):
    model.fit(ftr_train,tgt_train)
    pred = model.predict(ftr_test)
    pred_proba=model.predict_proba(ftr_test)[:,1] # ì˜ˆì¸¡ í™•ë¥ 
    get_clf_eval(tgt_test,pred,pred_proba)


from lightgbm import LGBMClassifier

lgbm_clf = LGBMClassifier(
    n_estimators=1000
    ,num_leaves=64
    #, n_jobs=-1 # cpu ì¼ë•Œë§Œ ì‚¬ìš©
    ,device='gpu' # gpu ì¼ë•Œë§Œ ì‚¬ìš©
    ,boosting_from_average=False # ê·¹ë„ë¡œ ë¶ˆê· í˜•í•œ ë ˆì´ë¸”ì¼ ê²½ìš° Falseë¥¼ ì¤˜ì•¼ í•¨
)

print('StandarScaler ì²˜ë¦¬ ì „')
print('lightGBM')
# lightGBM
get_model_train_eval(lgbm_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)
print('='*100)
print('LogisticRegression')
# LogisticRegression
get_model_train_eval(lr_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)
print('='*100)
print('StandarScaler ì²˜ë¦¬ í›„')

# ë°ì´í„°ë¥¼ StandarScaler ì²˜ë¦¬ í›„ ì„±ëŠ¥ í‰ê°€
from sklearn.preprocessing import StandardScaler

def get_preprocessed_df(df=None):
    df_copy = df.copy()
    scaler=StandardScaler()
    ss_amount=scaler.fit_transform(df_copy['Amount'].values.reshape(-1,1))
    df_copy.insert(0,'Amount_Scaled',ss_amount)
    df_copy.drop(['Time','Amount'],axis=1,inplace=True)
    return df_copy # ì „ì²˜ë¦¬ ëœ ë°ì´í„°í”„ë ˆì„ ë°˜í™˜

# ì›ë³¸ ë°ì´í„°í”„ë ˆì„ : card_df
X_train,X_test,y_train,y_test=get_train_test_dataset(card_df)


print('ë¡œì§€ìŠ¤í‹± íšŒê·€ ì˜ˆì¸¡ ì„±ëŠ¥')
get_model_train_eval(lr_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)
print('='*100)
print('lightGBM ì˜ˆì¸¡ ì„±ëŠ¥')
get_model_train_eval(lgbm_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)

print('='*100)
print('log ì²˜ë¦¬ í›„')
print('='*100)

# ë°ì´í„°ë¥¼ log ì²˜ë¦¬ í›„ ì„±ëŠ¥ í‰ê°€

def get_preprocessed_df(df=None):
    df_copy=df.copy()
    amount_n = np.log1p(df_copy['Amount'])
    df_copy.insert(0,'Amount_Scaled', ss_amount)
    df_copy.drop(['Time','Amount'], axis=1, inplace=True)
    
    return df_copy # ì „ì²˜ë¦¬ëœ ë°ì´í„° í”„ë ˆì„ ë°˜í™˜ 

X_train,X_test,y_train,y_test=get_train_test_dataset(card_df)


print('ë¡œì§€ìŠ¤í‹± íšŒê·€ ì˜ˆì¸¡ ì„±ëŠ¥')
get_model_train_eval(lr_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)
print('='*100)
print('lightGBM ì˜ˆì¸¡ ì„±ëŠ¥')
get_model_train_eval(lgbm_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)
```
```
StandarScaler ì²˜ë¦¬ ì „
lightGBM
ì˜¤ì°¨ í–‰ë ¬
[[85226    69]
 [  124    24]]
ì •í™•ë„: 0.9977, ì •ë°€ë„: 0.2581, ì¬í˜„ìœ¨: 0.1622, F1: 0.1992, AUC:0.5806
====================================================================================================
LogisticRegression
ì˜¤ì°¨ í–‰ë ¬
[[85280    15]
 [   54    94]]
ì •í™•ë„: 0.9992, ì •ë°€ë„: 0.8624, ì¬í˜„ìœ¨: 0.6351, F1: 0.7315, AUC:0.9727
====================================================================================================
StandarScaler ì²˜ë¦¬ í›„
ë¡œì§€ìŠ¤í‹± íšŒê·€ ì˜ˆì¸¡ ì„±ëŠ¥
ì˜¤ì°¨ í–‰ë ¬
[[85281    14]
 [   55    93]]
ì •í™•ë„: 0.9992, ì •ë°€ë„: 0.8692, ì¬í˜„ìœ¨: 0.6284, F1: 0.7294, AUC:0.9706
====================================================================================================
lightGBM ì˜ˆì¸¡ ì„±ëŠ¥
ì˜¤ì°¨ í–‰ë ¬
[[85224    71]
 [  127    21]]
ì •í™•ë„: 0.9977, ì •ë°€ë„: 0.2283, ì¬í˜„ìœ¨: 0.1419, F1: 0.1750, AUC:0.5705
====================================================================================================
log ì²˜ë¦¬ í›„
====================================================================================================
ë¡œì§€ìŠ¤í‹± íšŒê·€ ì˜ˆì¸¡ ì„±ëŠ¥
ì˜¤ì°¨ í–‰ë ¬
[[85281    14]
 [   55    93]]
ì •í™•ë„: 0.9992, ì •ë°€ë„: 0.8692, ì¬í˜„ìœ¨: 0.6284, F1: 0.7294, AUC:0.9706
====================================================================================================
lightGBM ì˜ˆì¸¡ ì„±ëŠ¥
ì˜¤ì°¨ í–‰ë ¬
[[85224    71]
 [  148     0]]
ì •í™•ë„: 0.9974, ì •ë°€ë„: 0.0000, ì¬í˜„ìœ¨: 0.0000, F1: 0.0000, AUC:0.4995
```
---
## ì´ìƒì¹˜ ì œê±°
```
# ì´ìƒì¹˜ ê²€ì¶œ/ì œê±°
# ìƒê´€ê³„ìˆ˜ í™•ì¸
import seaborn as sns

plt.Figure(
    figsize=(9,9)
)
corr=card_df.corr()

sns.heatmap(
    corr
    ,cmap='RdBu'
)
plt.show()
```
![image](https://github.com/user-attachments/assets/ae083d39-50f1-41db-b49b-0de1a43ed207)

```
fraud=card_df[card_df['Class']==1]['V14']
fraud
```
![image](https://github.com/user-attachments/assets/232fc48d-c8ef-4be7-839b-1455b6b8439e)

```
fraud=card_df[card_df['Class']==1]['V14']
fraud.values
```
![image](https://github.com/user-attachments/assets/5d51d979-54ae-46f2-8c41-b6c6b27dde13)

```
q1= np.percentile(card_df[card_df['Class']==1]['V14'].values,25)
q3= np.percentile(card_df[card_df['Class']==1]['V14'].values,75)

print(q1)
print(q3)
iqr=q3-q1
print(f'IQR value : {iqr}')
```
![image](https://github.com/user-attachments/assets/32d7492a-4a2b-44ba-9931-ee42dc97a87a)

```
# ì´ìƒì¹˜ ë°ì´í„°ì— ëŒ€í•œ ì¸ë±ìŠ¤ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜

# df=card_df ëŒ€ì… ì˜ˆì •
# column='V14
def get_outlier(df=None,column=None,weight=1.5):
    # ì‚¬ê¸°(class==1)ì— í•´ë‹¹í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì¶”ì¶œ, ì‚¬ë¶„ìœ„(Q1,Q3) ì§€ì ì„ êµ¬í•¨
    fraud=df[df['Class']==1][column]
    q_25=np.percentile(fraud.values,25) # Q1 ìœ„ì¹˜ ê°’
    q_75=np.percentile(fraud.values,75) # Q3 ìœ„ì¹˜ ê°’
    # IQR êµ¬í•˜ê³ , 1.5 ê³±í•˜ê¸°
    iqr = q_75 - q_25
    iqr_weight=iqr*weight
    low_value=q_25-iqr_weight # min
    high_value=q_75+iqr_weight # max
    
    # ìµœëŒ€ê°’ ë³´ë‹¤ í¬ê³ , ìµœì†Œê°’ ë³´ë‹¤ ì‘ì€ ì¸ë±ìŠ¤ ë°˜í™˜
    outlier_index=fraud[(fraud<low_value) | (fraud>high_value)].index
    return outlier_index

outlier_index=get_outlier(df=card_df,column='V14')
print(f'ì´ìƒì¹˜ ë°ì´í„°ì˜ ì¸ë±ìŠ¤ : {outlier_index}')
```
![image](https://github.com/user-attachments/assets/9ddf8564-eebe-4686-bbad-66d1d7f1519b)

```
# get_preprocessed_df() ì— ì´ìƒì¹˜ ì œê±° í•˜ëŠ” ê¸°ëŠ¥ ì¶”ê°€
# ë¡œê·¸ë³€í™˜, ì´ìƒì¹˜ ì œê±° ì¶”ê°€
def get_preprocessed_df(df=None):
    df_copy=df.copy()
    # Amount featureë§Œ ë¡œê·¸ ë³€í™˜
    amount_n=np.log1p(df_copy['Amount'])
    # ë¡œê·¸ë³€í™˜í•œ í”¼ì²˜ë¥¼ ë°ì´í„° í”„ë ˆì„ì— ì‚½ì…(insert)
    df_copy.drop(['Time','Amount'],axis=1,inplace=True)
    df_copy.insert(0, 'Amount_scaled', amount_n)
    # ì´ìƒì¹˜ ë°ì´í„° ì œê±°
    outlier_index=get_outlier(df=df_copy, column='V14', weight=1.5)
    df_copy.drop(outlier_index, axis=0, inplace=True)
    return df_copy

X_train,X_test,y_train,y_test=get_train_test_dataset(df=card_df)
print('###ë¡œì§€ìŠ¤í‹± íšŒê·€###')
get_model_train_eval(
    lr_clf
    ,ftr_train=X_train
    ,ftr_test=X_test
    ,tgt_train=y_train
    ,tgt_test=y_test
)

print('###lightgbm ì˜ˆì¸¡ ì„±ëŠ¥###')
get_model_train_eval(
    lgbm_clf
    ,ftr_train=X_train
    ,ftr_test=X_test
    ,tgt_train=y_train
    ,tgt_test=y_test
)
```
![image](https://github.com/user-attachments/assets/43c1db71-a663-499f-a2c0-b7e8122ebb50)

---
# ë‹¤ë³€ëŸ‰ ì´ìƒì¹˜ íƒì§€ ë°©ë²•

ë‹¤ë³€ëŸ‰ ì´ìƒì¹˜ íƒì§€ì—ëŠ” ë‹¤ì–‘í•œ ë°©ë²•ì´ ìˆìœ¼ë©°, ê° ë°©ë²•ì€ ë°ì´í„°ì˜ íŠ¹ì„±ì— ë”°ë¼ íš¨ê³¼ì ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ëŠ” ì—¬ëŸ¬ ê°€ì§€ ì´ìƒì¹˜ íƒì§€ ë°©ë²•ì„ ì„¤ëª…í•œ ë‚´ìš©ì…ë‹ˆë‹¤.

---

## 1. Mahalanobis Distance (ë§ˆí• ë¼ë…¸ë¹„ìŠ¤ ê±°ë¦¬)
**Mahalanobis Distance**ëŠ” ê° ë°ì´í„° í¬ì¸íŠ¸ê°€ í‰ê· ê°’ì—ì„œ ì–¼ë§ˆë‚˜ ë²—ì–´ë‚˜ ìˆëŠ”ì§€ë¥¼ ì¸¡ì •í•˜ëŠ” ê±°ë¦¬ì…ë‹ˆë‹¤. ì´ ë°©ì‹ì€ **ê³µë¶„ì‚° í–‰ë ¬**ì„ ê³ ë ¤í•˜ì—¬ ë‹¤ë³€ëŸ‰ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê¸° ë•Œë¬¸ì—, ë³€ìˆ˜ë“¤ ê°„ì˜ ìƒê´€ê´€ê³„ë¥¼ ë°˜ì˜í•œ ì´ìƒì¹˜ íƒì§€ì— íš¨ê³¼ì ì…ë‹ˆë‹¤.

- **ê³µì‹**:
  
  $$
  \[
  D_M(x) = \sqrt{(x - \mu)^T S^{-1} (x - \mu)}
  \]
  $$
  
  ì—¬ê¸°ì„œ, \(x\)ëŠ” ê´€ì¸¡ê°’, \(\mu\)ëŠ” í‰ê· , \(S^{-1}\)ëŠ” ê³µë¶„ì‚° í–‰ë ¬ì˜ ì—­í–‰ë ¬ì…ë‹ˆë‹¤.
  
- **íŠ¹ì§•**:
  - **ë‹¤ë³€ëŸ‰** ë°ì´í„°ì—ì„œ ì´ìƒì¹˜ë¥¼ ì°¾ëŠ” ë° ìœ ìš©.
  - ê³ ì°¨ì› ë°ì´í„°ë¥¼ ë‹¤ë£° ë•Œ, ë³€ìˆ˜ ê°„ì˜ ìƒê´€ê´€ê³„ë¥¼ ê³ ë ¤í•˜ì—¬ ì´ìƒì¹˜ë¥¼ íƒì§€í•  ìˆ˜ ìˆìŒ.
  
- **ì ìš©**:
  - ë°ì´í„°ì˜ **ê³µë¶„ì‚°**ì„ ì´ìš©í•´ ê° ì ì´ í‰ê· ì—ì„œ ì–¼ë§ˆë‚˜ ë²—ì–´ë‚¬ëŠ”ì§€ ê³„ì‚°.
  - ë§ˆí• ë¼ë…¸ë¹„ìŠ¤ ê±°ë¦¬ê°€ í° ê°’ì´ ì´ìƒì¹˜ë¡œ ê°„ì£¼ë©ë‹ˆë‹¤.
  
- **ë‹¨ì **:
  - ë°ì´í„°ê°€ **ì •ê·œ ë¶„í¬**ë¥¼ ë”°ë¥¸ë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.
  - ê³µë¶„ì‚° í–‰ë ¬ ê³„ì‚° ì‹œ, **ê³ ì°¨ì›** ë°ì´í„°ì—ì„œ ê³„ì‚°ì´ ì–´ë ¤ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

---

## 2. Isolation Forest (ì´ìƒì¹˜ íƒì§€ë¥¼ ìœ„í•œ ëœë¤ í¬ë ˆìŠ¤íŠ¸)
**Isolation Forest**ëŠ” **ì•™ìƒë¸” ê¸°ë°˜ì˜ ì´ìƒì¹˜ íƒì§€ ë°©ë²•**ìœ¼ë¡œ, íŠ¸ë¦¬ êµ¬ì¡°ë¥¼ ì´ìš©í•˜ì—¬ ë°ì´í„°ì˜ ì´ìƒì¹˜ë¥¼ êµ¬ë³„í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ë‹¤ë¥¸ ë°©ë²•ë“¤ë³´ë‹¤ **ê³ ì°¨ì› ë°ì´í„°ì—ì„œ íš¨ê³¼ì **ì…ë‹ˆë‹¤.

- **ì‘ë™ ì›ë¦¬**:
  - ë°ì´í„°ë¥¼ ë¬´ì‘ìœ„ë¡œ ë¶„í• í•˜ì—¬ íŠ¸ë¦¬ êµ¬ì¡°ë¥¼ ë§Œë“­ë‹ˆë‹¤. ê° ë°ì´í„° í¬ì¸íŠ¸ê°€ íŠ¸ë¦¬ì—ì„œ ë¶„ë¦¬ë˜ëŠ” **ì†ë„**ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì´ìƒì¹˜ë¥¼ íƒì§€í•©ë‹ˆë‹¤.
  - ì •ìƒ ë°ì´í„°ëŠ” ì—¬ëŸ¬ ë²ˆ ë¶„í• ì´ í•„ìš”í•˜ì§€ë§Œ, ì´ìƒì¹˜ëŠ” ëª‡ ë²ˆë§Œ ë¶„í• ë˜ë©´ ì‰½ê²Œ ë¶„ë¦¬ë©ë‹ˆë‹¤.
  
- **íŠ¹ì§•**:
  - ë°ì´í„°ê°€ **ë‹¤ì–‘í•œ ì°¨ì›**ì„ ê°€ì§ˆ ë•Œ íš¨ê³¼ì .
  - ë¹ ë¥¸ ê³„ì‚° ì†ë„ë¥¼ ìë‘í•©ë‹ˆë‹¤.
  - ë¹„ì§€ë„ í•™ìŠµ ë°©ì‹ìœ¼ë¡œ, **ë ˆì´ë¸”ì´ ì—†ëŠ” ë°ì´í„°**ì—ë„ ì ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.

- **ì ìš©**:
  - ëœë¤ íŠ¸ë¦¬ ë°©ì‹ìœ¼ë¡œ ë°ì´í„°ë¥¼ ë¶„ë¦¬í•˜ë©°, ì´ìƒì¹˜ëŠ” **ë¶„ë¦¬ë˜ê¸° ì‰½ê³ , ë¶„í•  íšŸìˆ˜ê°€ ì ìŠµë‹ˆë‹¤**.
  - íŠ¸ë¦¬ì˜ ë¶„í• ì´ ì ì€ ë°ì´í„° í¬ì¸íŠ¸ë¥¼ ì´ìƒì¹˜ë¡œ ê°„ì£¼í•©ë‹ˆë‹¤.

- **ë‹¨ì **:
  - ì¼ë¶€ ë°ì´í„°ì—ì„œëŠ” **ê³¼ì í•©**ì„ ìœ ë°œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
  - íŠ¸ë¦¬ì˜ ê¹Šì´ê°€ ë„ˆë¬´ ê¹Šì–´ì§€ë©´, ê³„ì‚° ë¹„ìš©ì´ ì»¤ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

---

## 3. Local Outlier Factor (LOF, ë¡œì»¬ ì´ìƒì¹˜ íŒ©í„°)
**Local Outlier Factor**ëŠ” ë°ì´í„°ì˜ **ë°€ë„**ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì´ìƒì¹˜ë¥¼ íƒì§€í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ë°€ë„ê°€ ë‚®ì€ ì§€ì—­ì— ìœ„ì¹˜í•œ ë°ì´í„° í¬ì¸íŠ¸ë¥¼ ì´ìƒì¹˜ë¡œ ê°„ì£¼í•©ë‹ˆë‹¤. íŠ¹íˆ **ì§€ì—­ì  ì´ìƒì¹˜(local outlier)**ë¥¼ ì˜ íƒì§€í•˜ëŠ” ë° ìœ ìš©í•©ë‹ˆë‹¤.

- **ì‘ë™ ì›ë¦¬**:
  - ê° ë°ì´í„° í¬ì¸íŠ¸ì— ëŒ€í•´ **ê·¼ì ‘ ì´ì›ƒ**ì„ ì°¾ê³ , ê·¸ ì´ì›ƒë“¤ê³¼ì˜ **ë°€ë„ ì°¨ì´**ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
  - ë°€ë„ê°€ ë‚®ì€ ì ì„ ì´ìƒì¹˜ë¡œ íŒë‹¨í•©ë‹ˆë‹¤.
  
- **íŠ¹ì§•**:
  - **ë°€ë„ê°€ ë‚®ì€ ì§€ì—­**ì— ìœ„ì¹˜í•œ ë°ì´í„°ë¥¼ ì´ìƒì¹˜ë¡œ ê°„ì£¼í•©ë‹ˆë‹¤.
  - ì´ìƒì¹˜ê°€ **ë‹¤ì–‘í•œ ë°€ë„ë¥¼ ê°€ì§„ ë°ì´í„°ì— ìœ íš¨**í•©ë‹ˆë‹¤.
  
- **ì ìš©**:
  - ê·¼ì²˜ ì´ì›ƒë“¤ê³¼ ë¹„êµí•˜ì—¬, íŠ¹ì • ë°ì´í„° í¬ì¸íŠ¸ê°€ ì–¼ë§ˆë‚˜ ë°€ë„ê°€ ë‚®ì€ì§€ ê³„ì‚°í•©ë‹ˆë‹¤.
  - **ê·¼ì ‘ ì´ì›ƒì´ ë°€ë„ê°€ ë‚®ì€ ê²½ìš°** ì´ìƒì¹˜ë¡œ ê°„ì£¼ë©ë‹ˆë‹¤.

- **ë‹¨ì **:
  - **ê³ ì°¨ì› ë°ì´í„°**ì—ì„œëŠ” íš¨ê³¼ê°€ ë–¨ì–´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
  - **ì´ì›ƒ ê°œìˆ˜**ì— ë¯¼ê°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

---

## 4. DBSCAN (Density-Based Spatial Clustering of Applications with Noise)
**DBSCAN**ì€ **ë°€ë„ ê¸°ë°˜ì˜ í´ëŸ¬ìŠ¤í„°ë§** ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ, ì´ìƒì¹˜ íƒì§€ì—ì„œë„ í™œìš©ë©ë‹ˆë‹¤. DBSCANì€ ë°ì´í„°ê°€ ë°€ì§‘ëœ ì§€ì—­ì„ **í´ëŸ¬ìŠ¤í„°**ë¡œ ë¶„ë¥˜í•˜ê³ , ë°€ë„ê°€ ë‚®ì€ ì§€ì—­ì„ **ì´ìƒì¹˜**ë¡œ ê°„ì£¼í•©ë‹ˆë‹¤.

- **ì‘ë™ ì›ë¦¬**:
  - **í•µì‹¬ ê°ì²´(core points)**ì™€ **ê²½ê³„ ê°ì²´(border points)**, **ì´ìƒì¹˜(noise points)**ë¡œ ë°ì´í„°ë¥¼ ë¶„ë¥˜í•©ë‹ˆë‹¤.
  - ë°€ë„ê°€ ë‚®ì€ ë°ì´í„° í¬ì¸íŠ¸ëŠ” **ì´ìƒì¹˜**ë¡œ ê°„ì£¼ë©ë‹ˆë‹¤.
  
- **íŠ¹ì§•**:
  - **ë¹„ì„ í˜•ì **ì¸ í´ëŸ¬ìŠ¤í„°ë§ì„ ì§€ì›í•˜ë©°, ì´ìƒì¹˜ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ êµ¬ë³„í•©ë‹ˆë‹¤.
  - **í´ëŸ¬ìŠ¤í„°ì˜ ë°€ë„**ê°€ ì¤‘ìš”í•œ ê¸°ì¤€ì…ë‹ˆë‹¤.
  
- **ì ìš©**:
  - ë°€ë„ê°€ ë‚®ì€ ì˜ì—­ì— ìˆëŠ” ë°ì´í„° í¬ì¸íŠ¸ë¥¼ **ì´ìƒì¹˜**ë¡œ íŒë‹¨í•©ë‹ˆë‹¤.
  - ë‹¤ì–‘í•œ ë°€ë„ í™˜ê²½ì—ì„œë„ ì˜ ì‘ë™í•©ë‹ˆë‹¤.
  
- **ë‹¨ì **:
  - **ê³ ì°¨ì›** ë°ì´í„°ì—ì„œëŠ” ì„±ëŠ¥ì´ ë–¨ì–´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
  - **í•µì‹¬ ê°ì²´ì™€ ì´ì›ƒ**ì˜ ê¸°ì¤€ì¸ **Epsilon** ê°’ì„ ì„¤ì •í•˜ëŠ” ë° ë¯¼ê°í•©ë‹ˆë‹¤.

---

## 5. ì‹œê³„ì—´ ë°ì´í„°ì¼ ê²½ìš°, Moving Average (MA) + Threshold
ì‹œê³„ì—´ ë°ì´í„°ì—ì„œ ì´ìƒì¹˜ë¥¼ íƒì§€í•˜ê¸° ìœ„í•œ ê°„ë‹¨í•œ ë°©ë²•ì€ **ì´ë™ í‰ê·  (Moving Average, MA)**ê³¼ **ì„ê³„ê°’ (Threshold)**ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ì‹œê°„ì— ë”°ë¥¸ ë³€í™” íŒ¨í„´ì„ ê³ ë ¤í•˜ì—¬ ì´ìƒì¹˜ë¥¼ íƒì§€í•©ë‹ˆë‹¤.

- **ì‘ë™ ì›ë¦¬**:
  - **ì´ë™ í‰ê· **ì„ ê³„ì‚°í•˜ì—¬, ë°ì´í„°ì˜ ì •ìƒ ë²”ìœ„(í‰ê·  ë²”ìœ„)ë¥¼ ì¶”ì •í•©ë‹ˆë‹¤.
  - **ê¸°ì¤€ ì„ê³„ê°’(threshold)**ì„ ì„¤ì •í•˜ì—¬, í‰ê· ìœ¼ë¡œë¶€í„° ë²—ì–´ë‚˜ëŠ” ì ì„ ì´ìƒì¹˜ë¡œ ê°„ì£¼í•©ë‹ˆë‹¤.

- **íŠ¹ì§•**:
  - ì‹œê³„ì—´ ë°ì´í„°ì˜ **í‰ê·  íë¦„**ì„ ì¶”ì í•˜ê³ , ê¸‰ê²©í•œ ë³€í™”ë¥¼ **ì´ìƒì¹˜**ë¡œ ì‹ë³„í•©ë‹ˆë‹¤.
  - **ë‹¨ê¸° ë³€í™”**ì— ë¯¼ê°í•˜ê²Œ ë°˜ì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
  
- **ì ìš©**:
  - ì‹œê°„ì˜ íë¦„ì— ë”°ë¼ ë³€í•˜ëŠ” **ì´ë™ í‰ê· **ê³¼ ì´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì„¤ì •í•œ ì„ê³„ê°’ì„ ë¹„êµí•˜ì—¬ ì´ìƒì¹˜ë¥¼ íƒì§€í•©ë‹ˆë‹¤.
  - ì˜ˆë¥¼ ë“¤ì–´, í˜„ì¬ ê°’ì´ **ì´ë™ í‰ê·  + threshold**ë¥¼ ë²—ì–´ë‚˜ëŠ” ê²½ìš° ì´ìƒì¹˜ë¡œ ê°„ì£¼ë©ë‹ˆë‹¤.
  
- **ë‹¨ì **:
  - **ë™ì  í™˜ê²½**ì—ì„œëŠ” ê¸°ì¤€ì„ ì„¤ì •í•˜ê¸° ì–´ë ¤ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
  - **ê¸´ ì‹œê°„ ë²”ìœ„**ì—ì„œëŠ” ê³¼ê±° ë°ì´í„°ì˜ ë³€í™”ê°€ ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

---

# ì´ìƒì¹˜ íƒì§€ ë°©ë²• ìš”ì•½

| **ë°©ë²•**                      | **ì£¼ìš” íŠ¹ì§•**                                                | **ì í•©í•œ ìƒí™©**                                               |
|-------------------------------|------------------------------------------------------------|---------------------------------------------------------------|
| **Mahalanobis Distance**       | ê³µë¶„ì‚°ì„ ê³ ë ¤í•œ ë‹¤ë³€ëŸ‰ ì´ìƒì¹˜ íƒì§€                         | ë³€ìˆ˜ ê°„ ìƒê´€ ê´€ê³„ê°€ ì¤‘ìš”í•œ ê²½ìš°                                 |
| **Isolation Forest**           | ëœë¤ íŠ¸ë¦¬ ê¸°ë°˜ì˜ ì´ìƒì¹˜ íƒì§€ (ì•™ìƒë¸”)                       | ê³ ì°¨ì› ë°ì´í„°ì—ì„œ íš¨ê³¼ì , ë¹ ë¥¸ ê³„ì‚°                              |
| **Local Outlier Factor (LOF)** | ë°€ë„ ê¸°ë°˜ ì´ìƒì¹˜ íƒì§€, ì§€ì—­ì  ì´ìƒì¹˜ íƒì§€                   | ë°€ë„ê°€ ë‹¤ë¥¸ ë°ì´í„°ì—ì„œ ì´ìƒì¹˜ íƒì§€ì— ìœ ìš©                      |
| **DBSCAN**                     | ë°€ë„ ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§, ì´ìƒì¹˜ íƒì§€                          | ë¹„ì„ í˜• ë¶„í¬ë‚˜ í´ëŸ¬ìŠ¤í„°ë§ì„ ê¸°ë°˜ìœ¼ë¡œ ì´ìƒì¹˜ íƒì§€                |
| **Moving Average + Threshold** | ì‹œê³„ì—´ ë°ì´í„°ì—ì„œ í‰ê· ê³¼ ì„ê³„ê°’ì„ ê¸°ë°˜ìœ¼ë¡œ ì´ìƒì¹˜ íƒì§€    | ì‹œê³„ì—´ ë°ì´í„°ì—ì„œ ê¸‰ê²©í•œ ë³€í™” ê°ì§€ì— ìœ ìš©                      |

ê° ë°©ë²•ì€ íŠ¹ì • ìƒí™©ì—ì„œ ë” íš¨ê³¼ì ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ë°ì´í„°ì˜ íŠ¹ì„±ê³¼ ë¶„ì„í•˜ë ¤ëŠ” ë¬¸ì œì— ë”°ë¼ ì í•©í•œ ë°©ë²•ì„ ì„ íƒí•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.

---
## ì‹œê³„ì—´ ë°ì´í„° ì´ìƒì¹˜ íƒì§€: Moving Average + Threshold

### 1. ê°œìš”
ì‹œê³„ì—´ ë°ì´í„°ì—ì„œ **ì´ìƒ ë°ì´í„° íƒì§€(Anomaly Detection)**ëŠ” ë°ì´í„°ì—ì„œ ì˜ˆìƒì¹˜ ëª»í•œ ë³€ë™ì´ë‚˜ íŒ¨í„´ì„ ì‹ë³„í•˜ëŠ” ì¤‘ìš”í•œ ì‘ì—…ì…ë‹ˆë‹¤. ê·¸ ì¤‘ í•˜ë‚˜ì¸ **Moving Average + Threshold** ë°©ë²•ì€ ì´ë™ í‰ê· ì„ ì‚¬ìš©í•˜ì—¬ ì‹œê³„ì—´ì˜ ì „ë°˜ì ì¸ ì¶”ì„¸ë¥¼ íŒŒì•…í•˜ê³ , ì´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì´ìƒì¹˜ë¥¼ íƒì§€í•˜ëŠ” ê¸°ë²•ì…ë‹ˆë‹¤.

### 2. Moving Average (ì´ë™ í‰ê· )
ì´ë™ í‰ê· ì€ ì‹œê³„ì—´ ë°ì´í„°ì˜ **ìµœê·¼ ê°’ë“¤ì„ í‰ê· ë‚´ì–´** ë°ì´í„°ì˜ ë³€ë™ì„±ì„ ì¤„ì´ê³ , ì¶”ì„¸ë¥¼ íŒŒì•…í•˜ëŠ” ë° ìœ ìš©í•œ ë°©ë²•ì…ë‹ˆë‹¤. ì´ë™ í‰ê· ì—ëŠ” ì—¬ëŸ¬ ì¢…ë¥˜ê°€ ìˆì§€ë§Œ, ê°€ì¥ ë§ì´ ì‚¬ìš©ë˜ëŠ” ë‘ ê°€ì§€ëŠ”:

- **ë‹¨ìˆœ ì´ë™ í‰ê· (SMA, Simple Moving Average)**: ì£¼ì–´ì§„ ê¸°ê°„ ë™ì•ˆì˜ ë°ì´í„° ê°’ë“¤ì˜ **ë‹¨ìˆœ í‰ê· **ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
- **ì§€ìˆ˜ ê°€ì¤‘ ì´ë™ í‰ê· (EMA, Exponential Moving Average)**: ìµœì‹  ë°ì´í„°ì— ë” ë§ì€ ê°€ì¤‘ì¹˜ë¥¼ ë‘ì–´ í‰ê· ì„ ê³„ì‚°í•©ë‹ˆë‹¤.

#### ì´ë™ í‰ê·  ê³„ì‚° ê³µì‹ (SMA):

$$
SMA(t) = \frac{1}{n} \sum_{i=t-n+1}^{t} x_i
$$

ì—¬ê¸°ì„œ,
- `x_i`ëŠ” ë°ì´í„° í¬ì¸íŠ¸,
- `n`ì€ ì´ë™ í‰ê· ì˜ ìœˆë„ìš° í¬ê¸°,
- `t`ëŠ” í˜„ì¬ ì‹œê°„ ì¸ë±ìŠ¤ì…ë‹ˆë‹¤.

### 3. Threshold (ì„ê³„ê°’ ì„¤ì •)
ì´ë™ í‰ê· ì„ ê³„ì‚°í•œ í›„, ë°ì´í„°ë¥¼ ê¸°ì¤€ê°’(ì´ë™ í‰ê· )ê³¼ ë¹„êµí•˜ì—¬ ì´ìƒì¹˜ë¥¼ íƒì§€í•©ë‹ˆë‹¤. ì´ìƒì¹˜ë¥¼ íƒì§€í•˜ëŠ” ë°©ë²•ì€ ì¼ë°˜ì ìœ¼ë¡œ **í‘œì¤€í¸ì°¨**ë¥¼ ì´ìš©í•´ **ì„ê³„ê°’(threshold)**ì„ ì„¤ì •í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.

#### ì˜ˆì‹œ:
- **ìƒí•œì„ ** = ì´ë™ í‰ê·  + 2 * í‘œì¤€í¸ì°¨
- **í•˜í•œì„ ** = ì´ë™ í‰ê·  - 2 * í‘œì¤€í¸ì°¨

ì´ ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ëŠ” ê°’ì€ ì´ìƒì¹˜ë¡œ ê°„ì£¼ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### 4. Moving Average + Thresholdì„ í™œìš©í•œ ì´ìƒì¹˜ íƒì§€ ë°©ë²•

#### 1) ì´ë™ í‰ê·  ê³„ì‚°:
- ë¨¼ì €, ì‹œê³„ì—´ ë°ì´í„°ì—ì„œ ì¼ì • ê¸°ê°„(ì˜ˆ: 10ì¼)ì˜ ì´ë™ í‰ê· ì„ ê³„ì‚°í•©ë‹ˆë‹¤.

#### 2) í‘œì¤€í¸ì°¨ ê³„ì‚°:
- ì´ë™ í‰ê· ì„ ê¸°ì¤€ìœ¼ë¡œ, í•´ë‹¹ ê¸°ê°„ì˜ **í‘œì¤€í¸ì°¨**ë¥¼ êµ¬í•©ë‹ˆë‹¤. í‘œì¤€í¸ì°¨ëŠ” ë°ì´í„°ì˜ ë¶„í¬ë¥¼ ë‚˜íƒ€ë‚´ë©°, ê°’ì´ í¬ê²Œ ë²—ì–´ë‚  ê²½ìš° ì´ìƒì¹˜ë¡œ ê°„ì£¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

#### 3) ì„ê³„ê°’ ì„¤ì •:
- ì´ë™ í‰ê· ê³¼ í‘œì¤€í¸ì°¨ë¥¼ ê¸°ë°˜ìœ¼ë¡œ **ìƒí•œì„ **ê³¼ **í•˜í•œì„ **ì„ ì„¤ì •í•©ë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ "ì´ë™ í‰ê·  Â± 2ë°° í‘œì¤€í¸ì°¨"ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.

#### 4) ì´ìƒì¹˜ íƒì§€:
- ê° ë°ì´í„° í¬ì¸íŠ¸ê°€ ì´ë™ í‰ê· ì—ì„œ ë²—ì–´ë‚œ ì •ë„ë¥¼ ê³„ì‚°í•˜ê³ , ì„¤ì •ëœ ì„ê³„ê°’ì„ ì´ˆê³¼í•˜ë©´ ì´ìƒì¹˜ë¡œ íŒë‹¨í•©ë‹ˆë‹¤.

### 5. ì˜ˆì‹œ

ì´ ê¸°ë²•ì„ ì‹œê°ì ìœ¼ë¡œ ì„¤ëª…í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

- **ì‹œê³„ì—´ ë°ì´í„°**: ì£¼ì–´ì§„ ì‹œê°„ì— ë”°ë¼ ë³€í™”í•˜ëŠ” ë°ì´í„°ì…ë‹ˆë‹¤.
- **ì´ë™ í‰ê· **: ì¼ì • ê¸°ê°„ ë™ì•ˆì˜ í‰ê· ì„ ê³„ì‚°í•˜ì—¬ ë°ì´í„°ì˜ ë³€ë™ì„±ì„ ì¤„ì…ë‹ˆë‹¤.
- **ìƒí•œì„ /í•˜í•œì„ **: ì´ë™ í‰ê·  Â± 2ë°° í‘œì¤€í¸ì°¨ë¡œ ì„¤ì •ëœ ìƒí•œì„ ê³¼ í•˜í•œì„ ìœ¼ë¡œ ì´ìƒì¹˜ ë²”ìœ„ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
- **ì´ìƒì¹˜**: ì´ ë²”ìœ„ë¥¼ ë²—ì–´ë‚œ ê°’ë“¤ì´ ì´ìƒì¹˜ë¡œ íƒì§€ë©ë‹ˆë‹¤.

#### ì‹œê°í™”:
1. ì‹œê³„ì—´ ë°ì´í„°ì™€ ì´ë™ í‰ê· ì„ í”Œë¡œíŒ…í•©ë‹ˆë‹¤.
2. ìƒí•œì„ ê³¼ í•˜í•œì„ ì„ í”Œë¡œíŒ…í•©ë‹ˆë‹¤.
3. ì´ë™ í‰ê· ì„ ë²—ì–´ë‚œ ê°’ì„ ë¹¨ê°„ ì ìœ¼ë¡œ í‘œì‹œí•˜ì—¬ ì´ìƒì¹˜ë¥¼ ê°•ì¡°í•©ë‹ˆë‹¤.

### 6. ì¥ì  ë° í™œìš©
- **ì¥ì **: ì´ë™ í‰ê· ê³¼ í‘œì¤€í¸ì°¨ë¥¼ ì‚¬ìš©í•˜ë©´ ì‹œê³„ì—´ ë°ì´í„°ì˜ **ì „ë°˜ì ì¸ ì¶”ì„¸**ë¥¼ íŒŒì•…í•˜ê³ , **ë‹¨ê¸°ì ì¸ ë³€ë™**ì„ ë¬´ì‹œí•  ìˆ˜ ìˆì–´ ì´ìƒì¹˜ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ íƒì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- **í™œìš© ë¶„ì•¼**:
  - **ê¸ˆìœµ**: ì£¼ì‹ ê°€ê²© ë³€ë™ ë¶„ì„, ê±°ë˜ ì‹ í˜¸ íƒì§€
  - **IoT**: ì„¼ì„œ ë°ì´í„°ì˜ ì´ìƒ íƒì§€
  - **ì›¹ ë¶„ì„**: íŠ¸ë˜í”½ íŒ¨í„´ì—ì„œ ë¹„ì •ìƒì ì¸ ë³€í™” íƒì§€

### 7. ê²°ë¡ 
**Moving Average + Threshold** ê¸°ë²•ì€ ì‹œê³„ì—´ ë°ì´í„°ì—ì„œ ì´ìƒì¹˜ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ íƒì§€í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ì´ ê¸°ë²•ì„ í™œìš©í•˜ì—¬ ë°ì´í„°ì˜ ì¶”ì„¸ë¥¼ íŒŒì•…í•˜ê³ , ì¤‘ìš”í•œ ì´ìƒ ë³€í™”ë¥¼ ì¡°ê¸°ì— íƒì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

---

```
# over sampling
from imblearn.over_sampling import SMOTE

smote=SMOTE(random_state=0)
X_train_over,y_train_over=smote.fit_resample(X_train,y_train)

print(f'SMOTE ì ìš© ì „ í•™ìŠµìš© í”¼ì³/ë ˆì´ë¸”ì˜ êµ¬ì¡° : {X_train.shape,y_train.shape}')
print(f'SMOTE ì ìš© í›„ í•™ìŠµìš© í”¼ì³/ë ˆì´ë¸”ì˜ êµ¬ì¡° : {X_train_over.shape,y_train_over.shape}')
print(f'SMOTE ì ìš© ì „ ë ˆì´ë¸” ê°’ ë¶„í¬ : {pd.Series(y_train).value_counts()}')
print(f'SMOTE ì ìš© í›„ ë ˆì´ë¸” ê°’ ë¶„í¬ : {pd.Series(y_train_over).value_counts()}')
```
![image](https://github.com/user-attachments/assets/d2402ae9-996c-4b9c-9882-608fe0631163)

```
# ë¡œì§€ìŠ¤í‹± íšŒê·€ì˜ ì„±ëŠ¥ í‰ê°€
lr_clf=LogisticRegression(max_iter=1000)

get_model_train_eval(
    lr_clf
    ,ftr_train=X_train_over
    ,ftr_test=X_test
    ,tgt_train=y_train_over
    ,tgt_test=y_test
)

```
![image](https://github.com/user-attachments/assets/3d28f591-3e8d-4685-b0bd-835702d0c08b)

```
# lightgbm ì„±ëŠ¥ í‰ê°€

get_model_train_eval(
    lgbm_clf
    ,ftr_train=X_train_over
    ,ftr_test=X_test
    ,tgt_train=y_train_over
    ,tgt_test=y_test
)

```
![image](https://github.com/user-attachments/assets/cf2520f4-096b-4b83-b9c5-9d1cca54c2bf)
