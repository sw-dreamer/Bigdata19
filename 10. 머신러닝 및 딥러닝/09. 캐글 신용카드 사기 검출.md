# ìºê¸€ ì‹ ìš©ì¹´ë“œ ì‚¬ê¸° ê²€ì¶œê¸° ê²€ì¶œ

ë°ì´í„° : [ìºê¸€ ì‹ ìš©ì¹´ë“œ ì‚¬ê¸° ê²€ì¶œê¸° ê²€ì¶œ](https://www.kaggle.com/competitions/credit-card-fraud-prediction)


```
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline

import warnings
warnings.filterwarnings('ignore')

card_df=pd.read_csv(
    './creditcard.csv'
    ,encoding='utf-8'
)

card_df.head()
```
![image](https://github.com/user-attachments/assets/8bb53d08-b998-4bc2-b97d-e3d8ae03f23a)

```
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline

import warnings
warnings.filterwarnings('ignore')

card_df=pd.read_csv(
    './creditcard.csv'
    ,encoding='utf-8'
)

from sklearn.model_selection import train_test_split

def get_preprocessed_df(df=None):
    df_copy = df.copy()
    df_copy.drop('Time',axis=1,inplace=True)
    return df_copy

def get_train_test_dataset(df=None):
    if df is None: 
        return 'ë°ì´í„°ë² ì´ìŠ¤ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤'
    else:
        df_copy = get_preprocessed_df(df)
        X_features = df_copy.iloc[:, :-1]
        y_target = df_copy.iloc[:, -1]
        X_train, X_test, y_train, y_test = train_test_split(
            X_features,
            y_target,
            test_size=0.3,
            random_state=0,
            stratify=y_target
        )
        return X_train, X_test, y_train, y_test


X_train,X_test,y_train,y_test=get_train_test_dataset(card_df)

print('='*50)
print(f'í•™ìŠµ ë°ì´í„° ë ˆì´ë¸” ê°’ ë¹„ìœ¨\n{y_train.value_counts()/y_train.shape[0]*100}')
print('='*50)
print(f'í…ŒìŠ¤íŠ¸ ë°ì´í„° ë ˆì´ë¸” ê°’ ë¹„ìœ¨\n{y_test.value_counts()/y_test.shape[0]*100}')
```
![image](https://github.com/user-attachments/assets/328661f9-668d-4d1e-87de-b61dc8347596)

---
## ì›ë³¸ ë°ì´í„° ê°€ê³µ ì—†ì´ ëª¨ë¸ í•™ìŠµ, ì¼ë°˜í™” ì„±ëŠ¥ í™•ì¸

```
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline

import warnings
warnings.filterwarnings('ignore')

card_df=pd.read_csv(
    './creditcard.csv'
    ,encoding='utf-8'
)

from sklearn.model_selection import train_test_split

def get_preprocessed_df(df=None):
    df_copy = df.copy()
    df_copy.drop('Time',axis=1,inplace=True)
    return df_copy

def get_train_test_dataset(df=None):
    if df is None:  
        return 'ë°ì´í„°ë² ì´ìŠ¤ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤'
    else:
        df_copy = get_preprocessed_df(df)
        X_features = df_copy.iloc[:, :-1]
        y_target = df_copy.iloc[:, -1]
        X_train, X_test, y_train, y_test = train_test_split(
            X_features,
            y_target,
            test_size=0.3,
            random_state=0,
            stratify=y_target
        )
        return X_train, X_test, y_train, y_test

X_train,X_test,y_train,y_test=get_train_test_dataset(card_df)

# print('='*50)
# print(f'í•™ìŠµ ë°ì´í„° ë ˆì´ë¸” ê°’ ë¹„ìœ¨\n{y_train.value_counts()/y_train.shape[0]*100}')
# print('='*50)
# print(f'í…ŒìŠ¤íŠ¸ ë°ì´í„° ë ˆì´ë¸” ê°’ ë¹„ìœ¨\n{y_test.value_counts()/y_test.shape[0]*100}')


# ëª¨ë¸ ìƒì„± ì„±ëŠ¥ í‰ê°€ : get_clf_eval(ì›ë˜ë‹µ, ì˜ˆì¸¡ê°’, ì˜ˆì¸¡í™•ë¥ )
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.metrics import precision_score, recall_score
from sklearn.metrics import f1_score, roc_auc_score


def get_clf_eval(y_test, pred=None, pred_proba=None):
    confusion = confusion_matrix( y_test, pred)
    accuracy = accuracy_score(y_test , pred) # ì •í™•ë„ ì ìˆ˜
    precision = precision_score(y_test , pred) # ì •ë°€ë„ ì ìˆ˜
    recall = recall_score(y_test , pred) # ì¬í˜„ìœ¨ ì ìˆ˜
    f1 = f1_score(y_test,pred) # ì •ë°€ë„, ì¬í˜„ìœ¨ ì¡°í™”í‰ê·  ê°’
    # ROC-AUC ì¶”ê°€
    roc_auc = roc_auc_score(y_test, pred_proba) # AUC ì ìˆ˜ : ë¶ˆê· í˜• ë°ì´í„° ì…‹ì—ì„œ í•„ìš”
    print('ì˜¤ì°¨ í–‰ë ¬')
    print(confusion)
    # ROC-AUC print ì¶”ê°€
    print('ì •í™•ë„: {0:.4f}, ì •ë°€ë„: {1:.4f}, ì¬í˜„ìœ¨: {2:.4f},\
    F1: {3:.4f}, AUC:{4:.4f}'.format(accuracy, precision, recall, f1, roc_auc))

from sklearn.linear_model import LogisticRegression

lr_clf =LogisticRegression(max_iter=1000)
lr_clf.fit(X_train,y_train)
lr_pred=lr_clf.predict(X_test)
lr_pred_proba=lr_clf.predict_proba(X_test)[:,1]

get_clf_eval(y_test,lr_pred,lr_pred_proba)

```
![image](https://github.com/user-attachments/assets/8977c25e-5025-4751-8a22-1c4ce2a3ef3d)

```
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline

import warnings
warnings.filterwarnings('ignore')

card_df=pd.read_csv(
    './creditcard.csv'
    ,encoding='utf-8'
)

from sklearn.model_selection import train_test_split

def get_preprocessed_df(df=None):
    df_copy = df.copy()
    df_copy.drop('Time',axis=1,inplace=True)
    return df_copy

def get_train_test_dataset(df=None):
    if df is None:  
        return 'ë°ì´í„°ë² ì´ìŠ¤ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤'
    else:
        df_copy = get_preprocessed_df(df)
        X_features = df_copy.iloc[:, :-1]
        y_target = df_copy.iloc[:, -1]
        X_train, X_test, y_train, y_test = train_test_split(
            X_features,
            y_target,
            test_size=0.3,
            random_state=0,
            stratify=y_target
        )
        return X_train, X_test, y_train, y_test

X_train,X_test,y_train,y_test=get_train_test_dataset(card_df)

# print('='*50)
# print(f'í•™ìŠµ ë°ì´í„° ë ˆì´ë¸” ê°’ ë¹„ìœ¨\n{y_train.value_counts()/y_train.shape[0]*100}')
# print('='*50)
# print(f'í…ŒìŠ¤íŠ¸ ë°ì´í„° ë ˆì´ë¸” ê°’ ë¹„ìœ¨\n{y_test.value_counts()/y_test.shape[0]*100}')

from sklearn.linear_model import LogisticRegression

lr_clf =LogisticRegression(max_iter=1000)
lr_clf.fit(X_train,y_train)
lr_pred=lr_clf.predict(X_test)
lr_pred_proba=lr_clf.predict_proba(X_test)[:,1]


# ëª¨ë¸ ìƒì„± ì„±ëŠ¥ í‰ê°€ : get_clf_eval(ì›ë˜ë‹µ, ì˜ˆì¸¡ê°’, ì˜ˆì¸¡í™•ë¥ )
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.metrics import precision_score, recall_score
from sklearn.metrics import f1_score, roc_auc_score


def get_clf_eval(y_test, pred=None, pred_proba=None):
    confusion = confusion_matrix( y_test, pred)
    accuracy = accuracy_score(y_test , pred) # ì •í™•ë„ ì ìˆ˜
    precision = precision_score(y_test , pred) # ì •ë°€ë„ ì ìˆ˜
    recall = recall_score(y_test , pred) # ì¬í˜„ìœ¨ ì ìˆ˜
    f1 = f1_score(y_test,pred) # ì •ë°€ë„, ì¬í˜„ìœ¨ ì¡°í™”í‰ê·  ê°’
    # ROC-AUC ì¶”ê°€
    roc_auc = roc_auc_score(y_test, pred_proba) # AUC ì ìˆ˜ : ë¶ˆê· í˜• ë°ì´í„° ì…‹ì—ì„œ í•„ìš”
    print('ì˜¤ì°¨ í–‰ë ¬')
    print(confusion)
    # ROC-AUC print ì¶”ê°€
    print('ì •í™•ë„: {0:.4f}, ì •ë°€ë„: {1:.4f}, ì¬í˜„ìœ¨: {2:.4f}, F1: {3:.4f}, AUC:{4:.4f}'.format(accuracy, precision, recall, f1, roc_auc))

def get_model_train_eval(
    model                   # ë¶„ë¥˜ ì•Œê³ ë¦¬ì¦˜
    ,ftr_train=None         # í•™ìŠµ ë°ì´í„°
    ,ftr_test=None          # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    ,tgt_train=None         # í•™ìŠµ ë°ì´í„° ë ˆì´ë¸”
    ,tgt_test=None          # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë ˆì´ë¸”
    ):
    model.fit(ftr_train,tgt_train)
    pred = model.predict(ftr_test)
    pred_proba=model.predict_proba(ftr_test)[:,1] # ì˜ˆì¸¡ í™•ë¥ 
    get_clf_eval(tgt_test,pred,pred_proba)

from lightgbm import LGBMClassifier

lgbm_clf = LGBMClassifier(
    n_estimators=1000
    ,num_leaves=64
    #, n_jobs=-1 # cpu ì¼ë•Œë§Œ ì‚¬ìš©
    ,device='gpu' # gpu ì¼ë•Œë§Œ ì‚¬ìš©
    ,boosting_from_average=False # ê·¹ë„ë¡œ ë¶ˆê· í˜•í•œ ë ˆì´ë¸”ì¼ ê²½ìš° Falseë¥¼ ì¤˜ì•¼ í•¨
)

get_model_train_eval(lgbm_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)
```

![image](https://github.com/user-attachments/assets/bb4b5e60-f3bb-4bd1-9f6c-72c5f373acd5)

```
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline

import warnings
warnings.filterwarnings('ignore')

card_df=pd.read_csv(
    './creditcard.csv'
    ,encoding='utf-8'
)

from sklearn.model_selection import train_test_split

def get_preprocessed_df(df=None):
    df_copy = df.copy()
    df_copy.drop('Time',axis=1,inplace=True)
    return df_copy

def get_train_test_dataset(df=None):
    if df is None:  
        return 'ë°ì´í„°ë² ì´ìŠ¤ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤'
    else:
        df_copy = get_preprocessed_df(df)
        X_features = df_copy.iloc[:, :-1]
        y_target = df_copy.iloc[:, -1]
        X_train, X_test, y_train, y_test = train_test_split(
            X_features,
            y_target,
            test_size=0.3,
            random_state=0,
            stratify=y_target
        )
        return X_train, X_test, y_train, y_test

X_train,X_test,y_train,y_test=get_train_test_dataset(card_df)

# print('='*50)
# print(f'í•™ìŠµ ë°ì´í„° ë ˆì´ë¸” ê°’ ë¹„ìœ¨\n{y_train.value_counts()/y_train.shape[0]*100}')
# print('='*50)
# print(f'í…ŒìŠ¤íŠ¸ ë°ì´í„° ë ˆì´ë¸” ê°’ ë¹„ìœ¨\n{y_test.value_counts()/y_test.shape[0]*100}')

from sklearn.linear_model import LogisticRegression

lr_clf =LogisticRegression(max_iter=1000)
lr_clf.fit(X_train,y_train)
lr_pred=lr_clf.predict(X_test)
lr_pred_proba=lr_clf.predict_proba(X_test)[:,1]


# ëª¨ë¸ ìƒì„± ì„±ëŠ¥ í‰ê°€ : get_clf_eval(ì›ë˜ë‹µ, ì˜ˆì¸¡ê°’, ì˜ˆì¸¡í™•ë¥ )
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.metrics import precision_score, recall_score
from sklearn.metrics import f1_score, roc_auc_score


def get_clf_eval(y_test, pred=None, pred_proba=None):
    confusion = confusion_matrix( y_test, pred)
    accuracy = accuracy_score(y_test , pred) # ì •í™•ë„ ì ìˆ˜
    precision = precision_score(y_test , pred) # ì •ë°€ë„ ì ìˆ˜
    recall = recall_score(y_test , pred) # ì¬í˜„ìœ¨ ì ìˆ˜
    f1 = f1_score(y_test,pred) # ì •ë°€ë„, ì¬í˜„ìœ¨ ì¡°í™”í‰ê·  ê°’
    # ROC-AUC ì¶”ê°€
    roc_auc = roc_auc_score(y_test, pred_proba) # AUC ì ìˆ˜ : ë¶ˆê· í˜• ë°ì´í„° ì…‹ì—ì„œ í•„ìš”
    print('ì˜¤ì°¨ í–‰ë ¬')
    print(confusion)
    # ROC-AUC print ì¶”ê°€
    print('ì •í™•ë„: {0:.4f}, ì •ë°€ë„: {1:.4f}, ì¬í˜„ìœ¨: {2:.4f}, F1: {3:.4f}, AUC:{4:.4f}'.format(accuracy, precision, recall, f1, roc_auc))

def get_model_train_eval(
    model                   # ë¶„ë¥˜ ì•Œê³ ë¦¬ì¦˜
    ,ftr_train=None         # í•™ìŠµ ë°ì´í„°
    ,ftr_test=None          # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    ,tgt_train=None         # í•™ìŠµ ë°ì´í„° ë ˆì´ë¸”
    ,tgt_test=None          # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë ˆì´ë¸”
    ):
    model.fit(ftr_train,tgt_train)
    pred = model.predict(ftr_test)
    pred_proba=model.predict_proba(ftr_test)[:,1] # ì˜ˆì¸¡ í™•ë¥ 
    get_clf_eval(tgt_test,pred,pred_proba)


from lightgbm import LGBMClassifier

lgbm_clf = LGBMClassifier(
    n_estimators=1000
    ,num_leaves=64
    #, n_jobs=-1 # cpu ì¼ë•Œë§Œ ì‚¬ìš©
    ,device='gpu' # gpu ì¼ë•Œë§Œ ì‚¬ìš©
    ,boosting_from_average=False # ê·¹ë„ë¡œ ë¶ˆê· í˜•í•œ ë ˆì´ë¸”ì¼ ê²½ìš° Falseë¥¼ ì¤˜ì•¼ í•¨
)

# lightGBM
get_model_train_eval(lgbm_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)

# LogisticRegression
get_model_train_eval(lr_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)
```
![image](https://github.com/user-attachments/assets/ac09dbe7-edea-40e5-8c34-0d8363bd1d5c)

---
## ë°ì´í„° ë¶„í¬ í™•ì¸, ë³€í™˜í›„ ëª¨ë¸ í•™ìŠµ/ì˜ˆì¸¡/í‰ê°€

```
# íŠ¹ì • í”¼ì³(ê¸ˆì•¡)ì˜ ë°ì´í„° ë¶„í¬ í™•ì¸
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline

import warnings
warnings.filterwarnings('ignore')

import seaborn as sns

card_df=pd.read_csv(
    './creditcard.csv'
    ,encoding='utf-8'
)

plt.figure(figsize=(8,4))
plt.xticks(
    range(0,30000,1000)
    ,rotation=60
)
sns.histplot(card_df['Amount'],bins=100,kde=True)
plt.show()
```
![image](https://github.com/user-attachments/assets/b32af0f7-1e8b-4743-bd6b-f3629af769b7)

---

```
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline  

import warnings
warnings.filterwarnings('ignore')  

import seaborn as sns

# CSV íŒŒì¼ì„ ì½ì–´ì™€ DataFrameìœ¼ë¡œ ì €ì¥
card_df = pd.read_csv(
    './creditcard.csv',  # ì‹ ìš©ì¹´ë“œ ê±°ë˜ ë°ì´í„° íŒŒì¼ ê²½ë¡œ
    encoding='utf-8'  # UTF-8 ì¸ì½”ë”©ìœ¼ë¡œ íŒŒì¼ì„ ì½ìŒ
)

# ì™¸ë„(Skewness) ê°’ í™•ì¸: ë°ì´í„°ì˜ ë¹„ëŒ€ì¹­ì„±ì„ í™•ì¸í•˜ê¸° ìœ„í•¨
# ë¹„ëŒ€ì¹­ ë°ì´í„°ì˜ ê²½ìš° ë¡œê·¸ ë³€í™˜ ë“±ìœ¼ë¡œ ì •ê·œì„±ì„ í™•ë³´í•  í•„ìš”ê°€ ìˆìŒ

# ì²¨ë„(Kurtosis) ê°’ í™•ì¸: ê·¹ë‹¨ê°’(outlier)ì˜ ì¡´ì¬ ì—¬ë¶€ë¥¼ íŒë‹¨í•˜ê¸° ìœ„í•¨
# ì–‘ì˜ ì²¨ë„: ë¶„í¬ì˜ ê¼¬ë¦¬ê°€ ë‘ê»ê³ , ì¤‘ì•™ì´ ë¾°ì¡± -> ê·¹ë‹¨ê°’ì´ ìì£¼ ë°œìƒ
#           -> ê·¹ë‹¨ê°’ ì²˜ë¦¬ê°€ í•„ìš”í•˜ë©°, ì ì ˆí•œ ëª¨ë¸ ì„ íƒì´ ì¤‘ìš”
# ìŒì˜ ì²¨ë„: ë¶„í¬ì˜ ê¼¬ë¦¬ê°€ ì–‡ê³ , ì¤‘ì•™ì´ í‰í‰ -> ê·¹ë‹¨ê°’ì´ ì ê²Œ ë°œìƒ
# ì¤‘ê°„ ì²¨ë„: ì •ê·œë¶„í¬ì— ê°€ê¹Œì›€ (ì™¸ë„ì™€ ì²¨ë„ê°€ 0ì— ê°€ê¹Œì›€)

from scipy.stats import skew  # ì™¸ë„ ê³„ì‚° í•¨ìˆ˜
from scipy.stats import kurtosis  # ì²¨ë„ ê³„ì‚° í•¨ìˆ˜

# 'Amount' ì—´ì˜ ì™¸ë„(Skewness) ê°’ ì¶œë ¥
print(f"ì™¸ë„ : {skew(card_df['Amount'])}")
print(f"ì²¨ë„ : {kurtosis(card_df['Amount'])}")
```
![image](https://github.com/user-attachments/assets/9fc70ec9-5e36-4415-800c-3abcccc2cccc)

---
## ğŸ“Š ëª¨ìˆ˜ì  vs ë¹„ëª¨ìˆ˜ì  ê²€ì • ë°©ë²• ì •ë¦¬

### 1. t-test (ë…ë¦½í‘œë³¸ t-ê²€ì •, ëŒ€ì‘í‘œë³¸ t-ê²€ì •)
#### ğŸ”¹ íŠ¹ì§•
- ë‘ ê·¸ë£¹ì˜ **í‰ê·  ì°¨ì´**ë¥¼ ë¹„êµí•˜ëŠ” **ëª¨ìˆ˜ì  ê²€ì •**  
- ë°ì´í„°ê°€ **ì •ê·œì„±ì„ ë§Œì¡±í•´ì•¼** í•¨  
- **ëŒ€ì‘í‘œë³¸ t-ê²€ì •**: ê°™ì€ ì§‘ë‹¨ì˜ ì‚¬ì „-ì‚¬í›„ ë°ì´í„°ë¥¼ ë¹„êµ  
- **ë…ë¦½í‘œë³¸ t-ê²€ì •**: ì„œë¡œ ë‹¤ë¥¸ ë‘ ì§‘ë‹¨ì„ ë¹„êµ  

#### âœ… ì¥ì 
- í•´ì„ì´ ì§ê´€ì ì´ë©° ë„ë¦¬ ì‚¬ìš©ë¨  
- ê²€ì •ë ¥ì´ ê°•í•˜ì—¬ ë¹„êµì  ì ì€ í‘œë³¸ì—ì„œë„ ì‹ ë¢°ì„± í™•ë³´ ê°€ëŠ¥  

#### âŒ ë‹¨ì 
- ì •ê·œì„±ì„ ë§Œì¡±í•˜ì§€ ì•Šìœ¼ë©´ ì‚¬ìš©í•˜ê¸° ì–´ë ¤ì›€  
- ë“±ë¶„ì‚°ì„±ì„ ê°€ì •í•´ì•¼ í•¨ (ë“±ë¶„ì‚°ì´ ë‹¤ë¥´ë©´ Welchâ€™s t-test ì‚¬ìš©)  

---

### 2. Mann-Whitney U test (ìœŒì½•ìŠ¨ ìˆœìœ„í•© ê²€ì •)
#### ğŸ”¹ íŠ¹ì§•
- ë‘ ê·¸ë£¹ì˜ **ì¤‘ìœ„ìˆ˜ ì°¨ì´**ë¥¼ ë¹„êµí•˜ëŠ” **ë¹„ëª¨ìˆ˜ ê²€ì •**  
- ë°ì´í„°ì˜ í¬ê¸° ìˆœì„œ(ìˆœìœ„)ë¥¼ ì´ìš©í•˜ì—¬ ë¹„êµ  
- ì •ê·œì„±ì„ ë§Œì¡±í•˜ì§€ ì•Šì•„ë„ ì‚¬ìš© ê°€ëŠ¥  

#### âœ… ì¥ì 
- ì •ê·œì„± ê°€ì •ì´ í•„ìš” ì—†ìŒ â†’ ì†Œê·œëª¨ ìƒ˜í”Œì—ì„œë„ í™œìš© ê°€ëŠ¥  
- ì´ìƒì¹˜(outlier)ì— ëœ ë¯¼ê°í•¨  

#### âŒ ë‹¨ì 
- í‰ê· ì´ ì•„ë‹Œ **ì¤‘ìœ„ìˆ˜ ë¹„êµ**ì´ë¯€ë¡œ í‰ê·  ì°¨ì´ë¥¼ í™•ì¸í•˜ëŠ” ë° í•œê³„ê°€ ìˆìŒ  
- í‘œë³¸ í¬ê¸°ê°€ í´ ê²½ìš° ê²€ì •ë ¥ì´ ë‚®ì•„ì§ˆ ìˆ˜ ìˆìŒ  

---

### 3. ANOVA (ë¶„ì‚° ë¶„ì„, Analysis of Variance)
#### ğŸ”¹ íŠ¹ì§•
- ì„¸ ê·¸ë£¹ ì´ìƒì˜ **í‰ê·  ì°¨ì´**ë¥¼ ë¹„êµí•˜ëŠ” **ëª¨ìˆ˜ì  ê²€ì •**  
- ì •ê·œì„±ê³¼ ë“±ë¶„ì‚°ì„±ì„ ë§Œì¡±í•´ì•¼ í•¨  
- ì§‘ë‹¨ ê°„ ë¶„ì‚°ê³¼ ì§‘ë‹¨ ë‚´ ë¶„ì‚°ì„ ì´ìš©í•˜ì—¬ ì°¨ì´ë¥¼ ë¶„ì„  

#### âœ… ì¥ì 
- ì„¸ ê·¸ë£¹ ì´ìƒì„ ë™ì‹œì— ë¹„êµí•  ìˆ˜ ìˆì–´ **ë‹¤ì¤‘ ë¹„êµ ë¬¸ì œ ë°©ì§€**  
- ê·¸ë£¹ ê°„ ì°¨ì´ë¥¼ ë¶„ì„í•˜ëŠ” ë° íš¨ê³¼ì   

#### âŒ ë‹¨ì 
- ì •ê·œì„±ê³¼ ë“±ë¶„ì‚°ì„±ì„ ë§Œì¡±í•˜ì§€ ì•Šìœ¼ë©´ ë¶€ì ì ˆí•¨  
- ì°¨ì´ê°€ ìˆë‹¤ê³ ë§Œ ì•Œë ¤ì£¼ë©°, **ì–´ëŠ ê·¸ë£¹ ê°„ ì°¨ì´ê°€ ìˆëŠ”ì§€ ì¶”ê°€ ë¶„ì„(Tukey HSD ë“±)ì´ í•„ìš”**  

---

### 4. Kruskal-Wallis test (í¬ë£¨ìŠ¤ì¹¼-ì™ˆë¦¬ìŠ¤ ê²€ì •)
#### ğŸ”¹ íŠ¹ì§•
- ì„¸ ê·¸ë£¹ ì´ìƒì˜ **ì¤‘ìœ„ìˆ˜ ì°¨ì´**ë¥¼ ë¹„êµí•˜ëŠ” **ë¹„ëª¨ìˆ˜ ê²€ì •**  
- ì •ê·œì„±ì„ ë§Œì¡±í•˜ì§€ ì•Šì•„ë„ ì‚¬ìš© ê°€ëŠ¥  
- ë°ì´í„°ì˜ ìˆœìœ„ë¥¼ ì´ìš©í•˜ì—¬ ë¹„êµ  

#### âœ… ì¥ì 
- ì •ê·œì„±ì„ ê°€ì •í•  í•„ìš” ì—†ìŒ  
- ì´ìƒì¹˜ì— ëœ ë¯¼ê°í•˜ì—¬ ë‹¤ì–‘í•œ ë°ì´í„°ì— ì ìš© ê°€ëŠ¥  

#### âŒ ë‹¨ì 
- í‰ê· ì´ ì•„ë‹Œ **ì¤‘ìœ„ìˆ˜ë¥¼ ë¹„êµ**í•˜ë¯€ë¡œ ëª¨ìˆ˜ì  ë°©ë²•ë³´ë‹¤ ëœ ê°•ë ¥í•¨  
- ê·¸ë£¹ ê°„ ì–´ëŠ ë¶€ë¶„ì—ì„œ ì°¨ì´ê°€ ë‚˜ëŠ”ì§€ ì¶”ê°€ ë¶„ì„ í•„ìš”  

---

### ğŸ“Œ ìš”ì•½ ì •ë¦¬

| ë¹„êµ ëŒ€ìƒ | ëª¨ìˆ˜ì  ë°©ë²• | ë¹„ëª¨ìˆ˜ì  ë°©ë²• |
|-----------|------------|--------------|
| **ë‘ ê·¸ë£¹ ë¹„êµ (í‰ê· )** | t-test | Mann-Whitney U test (Wilcoxon rank-sum test) |
| **ì„¸ ê·¸ë£¹ ì´ìƒ ë¹„êµ (í‰ê· )** | ANOVA | Kruskal-Wallis test |


| ê²€ì • ë°©ë²• | íŠ¹ì§• | ì¥ì  | ë‹¨ì  |
|----------|------|------|------|
| **t-test** | ë‘ ê·¸ë£¹ì˜ í‰ê·  ë¹„êµ (ëª¨ìˆ˜) | ê²€ì •ë ¥ì´ ê°•í•¨, ì§ê´€ì  í•´ì„ ê°€ëŠ¥ | ì •ê·œì„± í•„ìš”, ë“±ë¶„ì‚° ê°€ì • í•„ìš” |
| **Mann-Whitney U test** | ë‘ ê·¸ë£¹ì˜ ì¤‘ìœ„ìˆ˜ ë¹„êµ (ë¹„ëª¨ìˆ˜) | ì •ê·œì„± í•„ìš” ì—†ìŒ, ì´ìƒì¹˜ì— ê°•í•¨ | í‰ê·  ë¹„êµ ë¶ˆê°€, í° í‘œë³¸ì—ì„œ ê²€ì •ë ¥ ë‚®ìŒ |
| **ANOVA** | ì„¸ ê·¸ë£¹ ì´ìƒì˜ í‰ê·  ë¹„êµ (ëª¨ìˆ˜) | ë‹¤ì¤‘ ë¹„êµ ê°€ëŠ¥, íš¨ê³¼ì ì¸ ê·¸ë£¹ ì°¨ì´ ë¶„ì„ | ì •ê·œì„±Â·ë“±ë¶„ì‚° í•„ìš”, ì¶”ê°€ ë¶„ì„ í•„ìš” |
| **Kruskal-Wallis test** | ì„¸ ê·¸ë£¹ ì´ìƒì˜ ì¤‘ìœ„ìˆ˜ ë¹„êµ (ë¹„ëª¨ìˆ˜) | ì •ê·œì„± í•„ìš” ì—†ìŒ, ì´ìƒì¹˜ì— ê°•í•¨ | í‰ê·  ë¹„êµ ë¶ˆê°€, ì¶”ê°€ ë¶„ì„ í•„ìš” |

---
## ğŸ“Š ì •ê·œì„± íŒë‹¨ ë°©ë²• ì •ë¦¬

### **1. í†µê³„ì  ê²€ì • ë°©ë²•**
ì •ê·œì„±ì„ í™•ì¸í•˜ê¸° ìœ„í•œ ë‹¤ì–‘í•œ **ìœ ì˜í™•ë¥  ê¸°ë°˜ ê²€ì • ë°©ë²•**ì…ë‹ˆë‹¤.

#### ğŸ“Œ **1) Shapiro-Wilk Test**
- **íŠ¹ì§•**: ê°€ì¥ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” ì •ê·œì„± ê²€ì • ë°©ë²•  
- **ì‚¬ìš© ì¡°ê±´**: ìƒ˜í”Œ í¬ê¸°ê°€ ì‘ì„ ë•Œ(50 ì´í•˜) ì í•©  

âœ… **ì¥ì **  
- ì†Œê·œëª¨ ìƒ˜í”Œ(â‰¤50ê°œ)ì— ëŒ€í•´ ë†’ì€ ê²€ì •ë ¥ ì œê³µ  
- ë¹„êµì  ì •í™•í•œ ì •ê·œì„± ê²€ì • ê°€ëŠ¥  

âŒ **ë‹¨ì **  
- ìƒ˜í”Œ í¬ê¸°ê°€ ì»¤ì§€ë©´ ê²€ì •ë ¥ì´ ë„ˆë¬´ ê°•í•´ì ¸ ì •ê·œì„±ì„ ì‰½ê²Œ ê¸°ê°í•  ìˆ˜ ìˆìŒ  
- ì´ìƒì¹˜(outlier)ì— ë¯¼ê°  

---

#### ğŸ“Œ **2) Kolmogorov-Smirnov Test (K-S Test)**
- **íŠ¹ì§•**: ì •ê·œë¶„í¬ì™€ ë°ì´í„° ë¶„í¬ë¥¼ ë¹„êµí•˜ì—¬ ì •ê·œì„± ê²€ì •  
- **ì‚¬ìš© ì¡°ê±´**: ëŒ€ê·œëª¨ ìƒ˜í”Œì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•˜ì§€ë§Œ, ì •í™•ì„±ì´ ë‚®ìŒ  

âœ… **ì¥ì **  
- ìƒ˜í”Œ í¬ê¸°ì— ìƒê´€ì—†ì´ ì ìš© ê°€ëŠ¥  
- ë¶„í¬ì˜ ì°¨ì´ë¥¼ ì§ì ‘ ë¹„êµí•˜ëŠ” ë°©ì‹  

âŒ **ë‹¨ì **  
- ê²€ì •ë ¥ì´ ë‚®ì•„ ì •ê·œì„±ì„ ì •í™•íˆ íŒë³„í•˜ì§€ ëª»í•  ìˆ˜ ìˆìŒ  
- ìƒ˜í”Œ í¬ê¸°ê°€ í¬ë©´ ì •ê·œì„±ì„ ê¸°ê°í•  ê°€ëŠ¥ì„±ì´ ì»¤ì§  

---

#### ğŸ“Œ **3) Anderson-Darling Test**
- **íŠ¹ì§•**: Kolmogorov-Smirnov Testë³´ë‹¤ ì •ë°€í•œ ì •ê·œì„± ê²€ì • ë°©ë²•  
- **ì‚¬ìš© ì¡°ê±´**: ìƒ˜í”Œ í¬ê¸°ì— ìƒê´€ì—†ì´ í™œìš© ê°€ëŠ¥  

âœ… **ì¥ì **  
- ìƒ˜í”Œ í¬ê¸°ì— í¬ê²Œ ì˜í–¥ì„ ë°›ì§€ ì•ŠìŒ  
- ë¶„í¬ì˜ ê¼¬ë¦¬ ë¶€ë¶„ê¹Œì§€ ê³ ë ¤í•˜ì—¬ ì •ê·œì„±ì„ íŒë‹¨  

âŒ **ë‹¨ì **  
- Shapiro-Wilk Testë³´ë‹¤ ëœ ì§ê´€ì   
- íŠ¹ì •í•œ ìƒí™©ì—ì„œ p-value í•´ì„ì´ ì–´ë ¤ìš¸ ìˆ˜ ìˆìŒ  

---

#### ğŸ“Œ **4) Jarque-Bera Test**
- **íŠ¹ì§•**: ë°ì´í„°ì˜ **ì™œë„(skewness)ì™€ ì²¨ë„(kurtosis)** ë¥¼ ì´ìš©í•œ ì •ê·œì„± ê²€ì •  
- **ì‚¬ìš© ì¡°ê±´**: ëŒ€ê·œëª¨ ìƒ˜í”Œì—ì„œ ì‚¬ìš©  

âœ… **ì¥ì **  
- í° ìƒ˜í”Œì—ì„œë„ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ê²°ê³¼ ì œê³µ  
- ì™œë„ì™€ ì²¨ë„ë¥¼ í•¨ê»˜ ê³ ë ¤í•˜ì—¬ ì •ê·œì„± ê²€ì • ê°€ëŠ¥  

âŒ **ë‹¨ì **  
- ì†Œê·œëª¨ ìƒ˜í”Œì—ì„œëŠ” ê²€ì •ë ¥ì´ ë‚®ì•„ ë¹„íš¨ìœ¨ì   
- ì™œë„ì™€ ì²¨ë„ê°€ ê·¹ë‹¨ì ì¸ ê²½ìš° ì •ê·œì„±ì„ ì •í™•íˆ íŒë‹¨í•˜ê¸° ì–´ë ¤ì›€  

---

### **2. ì‹œê°ì  ë¶„ì„ ë°©ë²•**
ì •ê·œì„±ì„ **ì§ê´€ì ìœ¼ë¡œ í™•ì¸**í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.

#### ğŸ“Œ **1) íˆìŠ¤í† ê·¸ë¨ (Histogram)**
- **íŠ¹ì§•**: ë°ì´í„°ì˜ ë¶„í¬ê°€ ì¢…ëª¨ì–‘(ì •ê·œë¶„í¬)ì¸ì§€ í™•ì¸  

âœ… **ì¥ì **  
- ì •ê·œì„±ì„ ë¹ ë¥´ê²Œ ì‹œê°ì ìœ¼ë¡œ í™•ì¸ ê°€ëŠ¥  
- ì§ê´€ì ì¸ í•´ì„ ê°€ëŠ¥  

âŒ **ë‹¨ì **  
- ìƒ˜í”Œ í¬ê¸°ê°€ ì‘ìœ¼ë©´ ë¶€ì •í™•í•  ìˆ˜ ìˆìŒ  
- ìœ¡ì•ˆ íŒë‹¨ì´ë¯€ë¡œ ì£¼ê´€ì ì¼ ìˆ˜ ìˆìŒ  

---

#### ğŸ“Œ **2) Q-Q Plot (Quantile-Quantile Plot)**
- **íŠ¹ì§•**: ë°ì´í„°ì˜ ë¶„ìœ„ìˆ˜ë¥¼ ì •ê·œë¶„í¬ì˜ ë¶„ìœ„ìˆ˜ì™€ ë¹„êµ  

âœ… **ì¥ì **  
- ì •ê·œì„±ì„ ì‹œê°ì ìœ¼ë¡œ ì‰½ê²Œ íŒë‹¨ ê°€ëŠ¥  
- ì •ê·œì„± ì—¬ë¶€ë¿ë§Œ ì•„ë‹ˆë¼ ë°ì´í„° ë¶„í¬ì˜ íŠ¹ì„±(ì™œë„, ì´ìƒì¹˜)ë„ í™•ì¸ ê°€ëŠ¥  

âŒ **ë‹¨ì **  
- í•´ì„ì´ ë‹¤ì†Œ ì–´ë ¤ìš¸ ìˆ˜ ìˆìŒ  
- ìœ¡ì•ˆ íŒë‹¨ì´ë¯€ë¡œ ì •í™•í•œ ê²€ì • ë°©ë²•ì€ ì•„ë‹˜  

---

#### ğŸ“Œ **3) P-P Plot (Probability-Probability Plot)**
- **íŠ¹ì§•**: í‘œë³¸ ë°ì´í„°ì˜ ëˆ„ì ë¶„í¬ì™€ ì •ê·œë¶„í¬ì˜ ëˆ„ì ë¶„í¬ë¥¼ ë¹„êµ  

âœ… **ì¥ì **  
- Q-Q Plotê³¼ ìœ ì‚¬í•˜ì§€ë§Œ ëˆ„ì ë¶„í¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ê¸° ë•Œë¬¸ì— ì •ê·œì„± ê²€ì •ì— ë” ìœ ë¦¬  

âŒ **ë‹¨ì **  
- ë°ì´í„° ìˆ˜ê°€ ë§ì•„ì§ˆìˆ˜ë¡ í•´ì„ì´ ì–´ë ¤ì›Œì§ˆ ìˆ˜ ìˆìŒ  

---

#### ğŸ“Œ **4) Box Plot (ìƒì ê·¸ë¦¼)**
- **íŠ¹ì§•**: ë°ì´í„°ì˜ ëŒ€ì¹­ì„±ê³¼ ì´ìƒì¹˜(outlier)ë¥¼ í™•ì¸  

âœ… **ì¥ì **  
- ë°ì´í„°ì˜ ë¶„í¬ë¥¼ ì§ê´€ì ìœ¼ë¡œ ë³¼ ìˆ˜ ìˆìŒ  
- ì´ìƒì¹˜ê°€ ìˆëŠ”ì§€ ì‰½ê²Œ í™•ì¸ ê°€ëŠ¥  

âŒ **ë‹¨ì **  
- ì •ê·œì„±ì„ ì§ì ‘ì ìœ¼ë¡œ ê²€ì •í•  ìˆ˜ ì—†ìŒ  
- ìƒ˜í”Œ í¬ê¸°ê°€ ì‘ìœ¼ë©´ ì™œê³¡ë  ê°€ëŠ¥ì„±ì´ ìˆìŒ  

---

### **ğŸ“Œ ì •ê·œì„± íŒë‹¨ ë°©ë²• ë¹„êµí‘œ**

| ë°©ë²• | íŠ¹ì§• | ì¥ì  | ë‹¨ì  |
|------|------|------|------|
| **Shapiro-Wilk Test** | ê°€ì¥ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” ì •ê·œì„± ê²€ì • | ì†Œê·œëª¨ ìƒ˜í”Œ(â‰¤50)ì—ì„œ ë†’ì€ ê²€ì •ë ¥ | ëŒ€ê·œëª¨ ìƒ˜í”Œì—ì„œ ê³¼ë„í•˜ê²Œ ì •ê·œì„±ì„ ê¸°ê°í•  ê°€ëŠ¥ì„± ìˆìŒ |
| **Kolmogorov-Smirnov Test** | ì •ê·œë¶„í¬ì™€ ë°ì´í„° ë¶„í¬ë¥¼ ë¹„êµ | ìƒ˜í”Œ í¬ê¸°ì— ê´€ê³„ì—†ì´ ì ìš© ê°€ëŠ¥ | ê²€ì •ë ¥ì´ ë‚®ì•„ ì •ê·œì„±ì„ ì •í™•íˆ íŒë³„í•˜ê¸° ì–´ë ¤ì›€ |
| **Anderson-Darling Test** | K-S Testë³´ë‹¤ ì •ë°€í•œ ì •ê·œì„± ê²€ì • | ìƒ˜í”Œ í¬ê¸° ì˜í–¥ì„ ëœ ë°›ìŒ | í•´ì„ì´ ë‹¤ì†Œ ì–´ë ¤ìš¸ ìˆ˜ ìˆìŒ |
| **Jarque-Bera Test** | ì™œë„ì™€ ì²¨ë„ë¥¼ ì´ìš©í•œ ì •ê·œì„± ê²€ì • | ëŒ€ê·œëª¨ ìƒ˜í”Œì—ì„œ ì‹ ë¢°ì„± ë†’ìŒ | ì†Œê·œëª¨ ìƒ˜í”Œì—ì„œëŠ” ê²€ì •ë ¥ì´ ë‚®ìŒ |
| **íˆìŠ¤í† ê·¸ë¨** | ë°ì´í„° ë¶„í¬ê°€ ì •ê·œì„±ì„ ë”°ë¥´ëŠ”ì§€ í™•ì¸ | ì§ê´€ì ì¸ í•´ì„ ê°€ëŠ¥ | ìƒ˜í”Œ í¬ê¸°ê°€ ì‘ìœ¼ë©´ ë¶€ì •í™•í•  ìˆ˜ ìˆìŒ |
| **Q-Q Plot** | ë°ì´í„° ë¶„ìœ„ìˆ˜ì™€ ì •ê·œë¶„í¬ ë¶„ìœ„ìˆ˜ë¥¼ ë¹„êµ | ì™œë„, ì´ìƒì¹˜ ë“±ì„ í•œëˆˆì— íŒŒì•… ê°€ëŠ¥ | í•´ì„ì´ ë‹¤ì†Œ ì–´ë ¤ìš¸ ìˆ˜ ìˆìŒ |
| **P-P Plot** | ëˆ„ì ë¶„í¬ë¥¼ ë¹„êµí•˜ì—¬ ì •ê·œì„± ê²€ì • | Q-Q Plotë³´ë‹¤ ì •ê·œì„± ê²€ì •ì— ìœ ë¦¬ | ë°ì´í„° ìˆ˜ê°€ ë§ì•„ì§€ë©´ í•´ì„ ì–´ë ¤ì›€ |
| **Box Plot** | ë°ì´í„°ì˜ ëŒ€ì¹­ì„±ê³¼ ì´ìƒì¹˜ í™•ì¸ | ì´ìƒì¹˜ë¥¼ ì‰½ê²Œ ë°œê²¬ ê°€ëŠ¥ | ì •ê·œì„±ì„ ì§ì ‘ ê²€ì •í•  ìˆ˜ ì—†ìŒ |

---

### **âœ… ì •ê·œì„± íŒë‹¨ íë¦„**
1ï¸âƒ£ **ì‹œê°ì  ë°©ë²•(Q-Q Plot, íˆìŠ¤í† ê·¸ë¨ ë“±)ìœ¼ë¡œ ëŒ€ëµì ì¸ ì •ê·œì„± í™•ì¸**  
2ï¸âƒ£ **í†µê³„ì  ê²€ì •(Shapiro-Wilk Test, K-S Test ë“±) ìˆ˜í–‰**  
3ï¸âƒ£ **p-value â‰¥ 0.05** â†’ ì •ê·œì„± ë§Œì¡± (ëª¨ìˆ˜ ê²€ì • ê°€ëŠ¥)  
4ï¸âƒ£ **p-value < 0.05** â†’ ì •ê·œì„± ë¶ˆë§Œì¡± (ë¹„ëª¨ìˆ˜ ê²€ì • í•„ìš”)  

ğŸ“Œ **ì •ê·œì„± ê²€ì •ì€ ë°ì´í„° íŠ¹ì„±ì— ë”°ë¼ ì ì ˆí•œ ë°©ë²•ì„ ì„ íƒí•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤!** ğŸš€

---
## ğŸ“Œ ì •ê·œì„± íŒë‹¨ ë°©ë²•

### 1ï¸âƒ£ í†µê³„ì  ê²€ì • ë°©ë²• (ìœ ì˜í™•ë¥  ê¸°ë°˜)

| ë°©ë²• | ì„¤ëª… | ì‚¬ìš© ì¡°ê±´ |
|------|------|----------|
| **Shapiro-Wilk Test** | ê°€ì¥ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” ì •ê·œì„± ê²€ì • ë°©ë²• | ìƒ˜í”Œ í¬ê¸°ê°€ 50 ì´í•˜ì¼ ë•Œ ì í•© |
| **Kolmogorov-Smirnov Test** | ì •ê·œë¶„í¬ì™€ ë°ì´í„° ë¶„í¬ë¥¼ ë¹„êµ | ìƒ˜í”Œ í¬ê¸°ê°€ í¬ë©´ ê²€ì •ë ¥ì´ ë–¨ì–´ì§ |
| **Anderson-Darling Test** | Kolmogorov-Smirnov Testë³´ë‹¤ ì •ë°€í•œ ê²€ì • | ìƒ˜í”Œ í¬ê¸°ì— ìƒê´€ì—†ì´ í™œìš© ê°€ëŠ¥ |
| **Jarque-Bera Test** | ì™œë„(skewness)ì™€ ì²¨ë„(kurtosis)ë¥¼ ì´ìš©í•œ ì •ê·œì„± ê²€ì • | ëŒ€ê·œëª¨ ìƒ˜í”Œì—ì„œ ë§ì´ ì‚¬ìš©ë¨ |

ğŸ’¡ **ìœ ì˜í™•ë¥ (p-value)ì´ 0.05ë³´ë‹¤ ì‘ìœ¼ë©´ â†’ ì •ê·œì„±ì„ ë§Œì¡±í•˜ì§€ ì•ŠìŒ(ê·€ë¬´ê°€ì„¤ ê¸°ê°)**

---

### 2ï¸âƒ£ ì‹œê°ì  ë¶„ì„ ë°©ë²•

| ë°©ë²• | ì„¤ëª… |
|------|------|
| **íˆìŠ¤í† ê·¸ë¨ (Histogram)** | ë°ì´í„°ê°€ ì¢…ëª¨ì–‘(ì •ê·œë¶„í¬)ì¸ì§€ í™•ì¸ |
| **Q-Q Plot (Quantile-Quantile Plot)** | ë°ì´í„°ê°€ ì •ê·œë¶„í¬ì˜ ë¶„ìœ„ìˆ˜ì™€ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸ |
| **P-P Plot (Probability-Probability Plot)** | í‘œë³¸ì˜ ëˆ„ì ë¶„í¬ì™€ ì •ê·œë¶„í¬ì˜ ëˆ„ì ë¶„í¬ë¥¼ ë¹„êµ |
| **Box Plot (ìƒì ê·¸ë¦¼)** | ëŒ€ì¹­ì„±ì´ ìˆëŠ”ì§€, ì´ìƒì¹˜(outlier)ê°€ ë§ì€ì§€ í™•ì¸ |

ğŸ’¡ **ë°ì´í„°ê°€ ì¢…ëª¨ì–‘ì„ ë ê³ , Q-Q Plotì´ ì§ì„ ì— ê°€ê¹Œìš°ë©´ ì •ê·œì„±ì„ ë§Œì¡±**

---

```
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline  

import warnings
warnings.filterwarnings('ignore')  

import seaborn as sns

# CSV íŒŒì¼ì„ ì½ì–´ì™€ DataFrameìœ¼ë¡œ ì €ì¥  
card_df = pd.read_csv(
    './creditcard.csv',  # ì‹ ìš©ì¹´ë“œ ê±°ë˜ ë°ì´í„° íŒŒì¼ ê²½ë¡œ
    encoding='utf-8'  # UTF-8 ì¸ì½”ë”©ìœ¼ë¡œ íŒŒì¼ì„ ì½ìŒ
)

from scipy.stats import shapiro

stat,p=shapiro(card_df['Amount'].values)
print(f'í†µê³„ëŸ‰ : {stat}')
print(f'p-value : {p}')
```
![image](https://github.com/user-attachments/assets/d2ed69b7-01f6-429c-b367-dd3533a28da8)

```
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline  

import warnings
warnings.filterwarnings('ignore')  

import seaborn as sns

# CSV íŒŒì¼ì„ ì½ì–´ì™€ DataFrameìœ¼ë¡œ ì €ì¥  
card_df = pd.read_csv(
    './creditcard.csv',  # ì‹ ìš©ì¹´ë“œ ê±°ë˜ ë°ì´í„° íŒŒì¼ ê²½ë¡œ
    encoding='utf-8'  # UTF-8 ì¸ì½”ë”©ìœ¼ë¡œ íŒŒì¼ì„ ì½ìŒ
)

from scipy.stats import shapiro

stat,p=shapiro(card_df['Amount'].values)

print(f'í†µê³„ëŸ‰ : {stat}')
print(f'p-value : {p}')

if p>0.05:
    print('ì •ê·œì„±ì„ ë§Œì¡±')
else:
    print('ì •ê·œì„± ì—†ìŒ')
```
![image](https://github.com/user-attachments/assets/f0ca2035-1dd7-44b2-9b53-8aa7041bb485)

```
# ì •ê·œì„± ìˆëŠ” ë°ì´í„° ì‹œê°í™”
import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as stats

data=np.random.normal(loc=0,  scale=1,size=1000)

# QQ plot ìƒì„±
stats.probplot(data,dist='norm',plot=plt)
plt.grid()
plt.show()
```
![image](https://github.com/user-attachments/assets/91ddba78-a759-4bf2-824a-21bbf4882c49)

```
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline  

import warnings
warnings.filterwarnings('ignore')  

import seaborn as sns

# CSV íŒŒì¼ì„ ì½ì–´ì™€ DataFrameìœ¼ë¡œ ì €ì¥  
card_df = pd.read_csv(
    './creditcard.csv',  # ì‹ ìš©ì¹´ë“œ ê±°ë˜ ë°ì´í„° íŒŒì¼ ê²½ë¡œ
    encoding='utf-8'  # UTF-8 ì¸ì½”ë”©ìœ¼ë¡œ íŒŒì¼ì„ ì½ìŒ
)

import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as stats

data=np.random.normal(loc=0,  scale=1,size=1000)

# QQ plot ìƒì„±
stats.probplot(card_df['Amount'].values,dist='norm',plot=plt)
plt.grid()
plt.show()
```
![image](https://github.com/user-attachments/assets/21ffed58-bd3a-404a-a2b7-a9f6c999101b)

```
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline  

import warnings
warnings.filterwarnings('ignore')  

import seaborn as sns

# CSV íŒŒì¼ì„ ì½ì–´ì™€ DataFrameìœ¼ë¡œ ì €ì¥  
card_df = pd.read_csv(
    './creditcard.csv',  # ì‹ ìš©ì¹´ë“œ ê±°ë˜ ë°ì´í„° íŒŒì¼ ê²½ë¡œ
    encoding='utf-8'  # UTF-8 ì¸ì½”ë”©ìœ¼ë¡œ íŒŒì¼ì„ ì½ìŒ
)

# standarization : ë°ì´í„° ì •ê·œë¶„í¬ í˜•íƒœë¡œ ë³€í™˜

from sklearn.preprocessing import StandardScaler

# ê¸°ì¡´ì˜ Amount featureì˜ í‰ê· ,í‘œì¤€í¸ì°¨, ì™¸ë„, ì²¨ë„
avg_amount=card_df['Amount'].mean()
std_amount=card_df['Amount'].std()

from scipy.stats import skew  # ì™¸ë„ ê³„ì‚° í•¨ìˆ˜
from scipy.stats import kurtosis  # ì²¨ë„ ê³„ì‚° í•¨ìˆ˜
skew_amount=skew(card_df['Amount'])
kurtosis_amount=kurtosis(card_df['Amount'])

# StandardScaler ì ìš©í•œ Amount featureì˜ í‰ê· ,í‘œì¤€í¸ì°¨, ì™¸ë„, ì²¨ë„
scaler=StandardScaler()
ss_amount=scaler.fit_transform(card_df['Amount'].values.reshape(-1,1))

print('ê¸°ì¡´ ë°ì´í„° Amount ì»¬ëŸ¼ì˜ í‰ê·  ,í‘œì¤€í¸ì°¨ ,ì™¸ë„ ,ì²¨ë„')
print(f'{avg_amount} / {std_amount} / {skew_amount} / {kurtosis_amount}')
print('='*100)
print('í‘œì¤€í™”í•œ ë°ì´í„° Amount ì»¬ëŸ¼ì˜ í‰ê·  ,í‘œì¤€í¸ì°¨ ,ì™¸ë„ ,ì²¨ë„')
print(f'{ss_amount.mean()} / {ss_amount.std()} / {skew(ss_amount)} / {kurtosis(ss_amount)}')
```
![image](https://github.com/user-attachments/assets/0c268337-ed99-4ad8-920b-8a62d9003aa8)

```
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline

import warnings
warnings.filterwarnings('ignore')

card_df=pd.read_csv(
    './creditcard.csv'
    ,encoding='utf-8'
)

from sklearn.model_selection import train_test_split

def get_preprocessed_df(df=None):
    df_copy = df.copy()
    df_copy.drop('Time',axis=1,inplace=True)
    return df_copy

def get_train_test_dataset(df=None):
    if df is None:  
        return 'ë°ì´í„°ë² ì´ìŠ¤ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤'
    else:
        df_copy = get_preprocessed_df(df)
        X_features = df_copy.iloc[:, :-1]
        y_target = df_copy.iloc[:, -1]
        X_train, X_test, y_train, y_test = train_test_split(
            X_features,
            y_target,
            test_size=0.3,
            random_state=0,
            stratify=y_target
        )
        return X_train, X_test, y_train, y_test

X_train,X_test,y_train,y_test=get_train_test_dataset(card_df)

# print('='*50)
# print(f'í•™ìŠµ ë°ì´í„° ë ˆì´ë¸” ê°’ ë¹„ìœ¨\n{y_train.value_counts()/y_train.shape[0]*100}')
# print('='*50)
# print(f'í…ŒìŠ¤íŠ¸ ë°ì´í„° ë ˆì´ë¸” ê°’ ë¹„ìœ¨\n{y_test.value_counts()/y_test.shape[0]*100}')

from sklearn.linear_model import LogisticRegression

lr_clf =LogisticRegression(max_iter=1000)
lr_clf.fit(X_train,y_train)
lr_pred=lr_clf.predict(X_test)
lr_pred_proba=lr_clf.predict_proba(X_test)[:,1]


# ëª¨ë¸ ìƒì„± ì„±ëŠ¥ í‰ê°€ : get_clf_eval(ì›ë˜ë‹µ, ì˜ˆì¸¡ê°’, ì˜ˆì¸¡í™•ë¥ )
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.metrics import precision_score, recall_score
from sklearn.metrics import f1_score, roc_auc_score


def get_clf_eval(y_test, pred=None, pred_proba=None):
    confusion = confusion_matrix( y_test, pred)
    accuracy = accuracy_score(y_test , pred) # ì •í™•ë„ ì ìˆ˜
    precision = precision_score(y_test , pred) # ì •ë°€ë„ ì ìˆ˜
    recall = recall_score(y_test , pred) # ì¬í˜„ìœ¨ ì ìˆ˜
    f1 = f1_score(y_test,pred) # ì •ë°€ë„, ì¬í˜„ìœ¨ ì¡°í™”í‰ê·  ê°’
    # ROC-AUC ì¶”ê°€
    roc_auc = roc_auc_score(y_test, pred_proba) # AUC ì ìˆ˜ : ë¶ˆê· í˜• ë°ì´í„° ì…‹ì—ì„œ í•„ìš”
    print('ì˜¤ì°¨ í–‰ë ¬')
    print(confusion)
    # ROC-AUC print ì¶”ê°€
    print('ì •í™•ë„: {0:.4f}, ì •ë°€ë„: {1:.4f}, ì¬í˜„ìœ¨: {2:.4f}, F1: {3:.4f}, AUC:{4:.4f}'.format(accuracy, precision, recall, f1, roc_auc))

def get_model_train_eval(
    model                   # ë¶„ë¥˜ ì•Œê³ ë¦¬ì¦˜
    ,ftr_train=None         # í•™ìŠµ ë°ì´í„°
    ,ftr_test=None          # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    ,tgt_train=None         # í•™ìŠµ ë°ì´í„° ë ˆì´ë¸”
    ,tgt_test=None          # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë ˆì´ë¸”
    ):
    model.fit(ftr_train,tgt_train)
    pred = model.predict(ftr_test)
    pred_proba=model.predict_proba(ftr_test)[:,1] # ì˜ˆì¸¡ í™•ë¥ 
    get_clf_eval(tgt_test,pred,pred_proba)


from lightgbm import LGBMClassifier

lgbm_clf = LGBMClassifier(
    n_estimators=1000
    ,num_leaves=64
    #, n_jobs=-1 # cpu ì¼ë•Œë§Œ ì‚¬ìš©
    ,device='gpu' # gpu ì¼ë•Œë§Œ ì‚¬ìš©
    ,boosting_from_average=False # ê·¹ë„ë¡œ ë¶ˆê· í˜•í•œ ë ˆì´ë¸”ì¼ ê²½ìš° Falseë¥¼ ì¤˜ì•¼ í•¨
)

print('StandarScaler ì²˜ë¦¬ ì „')
print('lightGBM')
# lightGBM
get_model_train_eval(lgbm_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)
print('='*100)
print('LogisticRegression')
# LogisticRegression
get_model_train_eval(lr_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)
print('='*100)
print('StandarScaler ì²˜ë¦¬ í›„')
# ë°ì´í„°ë¥¼ StandarScaler ì²˜ë¦¬ í›„ ì„±ëŠ¥ í‰ê°€
from sklearn.preprocessing import StandardScaler

def get_preprocessed_df(df=None):
    df_copy = df.copy()
    scaler=StandardScaler()
    ss_amount=scaler.fit_transform(df_copy['Amount'].values.reshape(-1,1))
    df_copy.insert(0,'Amount_Scaled',ss_amount)
    df_copy.drop(['Time','Amount'],axis=1,inplace=True)
    return df_copy # ì „ì²˜ë¦¬ ëœ ë°ì´í„°í”„ë ˆì„ ë°˜í™˜

# ì›ë³¸ ë°ì´í„°í”„ë ˆì„ : card_df
X_train,X_test,y_train,y_test=get_train_test_dataset(card_df)

print('ë¡œì§€ìŠ¤í‹± íšŒê·€ ì˜ˆì¸¡ ì„±ëŠ¥')
get_model_train_eval(lr_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)
print('='*100)
print('lightGBM ì˜ˆì¸¡ ì„±ëŠ¥')
get_model_train_eval(lgbm_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)
```
