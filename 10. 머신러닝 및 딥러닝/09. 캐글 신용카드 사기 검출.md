# 캐글 신용카드 사기 검출기 검출

데이터 : [캐글 신용카드 사기 검출기 검출](https://www.kaggle.com/competitions/credit-card-fraud-prediction)


```
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline

import warnings
warnings.filterwarnings('ignore')

card_df=pd.read_csv(
    './creditcard.csv'
    ,encoding='utf-8'
)

card_df.head()
```
![image](https://github.com/user-attachments/assets/8bb53d08-b998-4bc2-b97d-e3d8ae03f23a)

```
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline

import warnings
warnings.filterwarnings('ignore')

card_df=pd.read_csv(
    './creditcard.csv'
    ,encoding='utf-8'
)

from sklearn.model_selection import train_test_split

def get_preprocessed_df(df=None):
    df_copy = df.copy()
    df_copy.drop('Time',axis=1,inplace=True)
    return df_copy

def get_train_test_dataset(df=None):
    if df is None: 
        return '데이터베이스가 존재하지 않습니다'
    else:
        df_copy = get_preprocessed_df(df)
        X_features = df_copy.iloc[:, :-1]
        y_target = df_copy.iloc[:, -1]
        X_train, X_test, y_train, y_test = train_test_split(
            X_features,
            y_target,
            test_size=0.3,
            random_state=0,
            stratify=y_target
        )
        return X_train, X_test, y_train, y_test


X_train,X_test,y_train,y_test=get_train_test_dataset(card_df)

print('='*50)
print(f'학습 데이터 레이블 값 비율\n{y_train.value_counts()/y_train.shape[0]*100}')
print('='*50)
print(f'테스트 데이터 레이블 값 비율\n{y_test.value_counts()/y_test.shape[0]*100}')
```
![image](https://github.com/user-attachments/assets/328661f9-668d-4d1e-87de-b61dc8347596)

---
## 원본 데이터 가공 없이 모델 학습, 일반화 성능 확인

```
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline

import warnings
warnings.filterwarnings('ignore')

card_df=pd.read_csv(
    './creditcard.csv'
    ,encoding='utf-8'
)

from sklearn.model_selection import train_test_split

def get_preprocessed_df(df=None):
    df_copy = df.copy()
    df_copy.drop('Time',axis=1,inplace=True)
    return df_copy

def get_train_test_dataset(df=None):
    if df is None:  
        return '데이터베이스가 존재하지 않습니다'
    else:
        df_copy = get_preprocessed_df(df)
        X_features = df_copy.iloc[:, :-1]
        y_target = df_copy.iloc[:, -1]
        X_train, X_test, y_train, y_test = train_test_split(
            X_features,
            y_target,
            test_size=0.3,
            random_state=0,
            stratify=y_target
        )
        return X_train, X_test, y_train, y_test

X_train,X_test,y_train,y_test=get_train_test_dataset(card_df)

# print('='*50)
# print(f'학습 데이터 레이블 값 비율\n{y_train.value_counts()/y_train.shape[0]*100}')
# print('='*50)
# print(f'테스트 데이터 레이블 값 비율\n{y_test.value_counts()/y_test.shape[0]*100}')


# 모델 생성 성능 평가 : get_clf_eval(원래답, 예측값, 예측확률)
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.metrics import precision_score, recall_score
from sklearn.metrics import f1_score, roc_auc_score


def get_clf_eval(y_test, pred=None, pred_proba=None):
    confusion = confusion_matrix( y_test, pred)
    accuracy = accuracy_score(y_test , pred) # 정확도 점수
    precision = precision_score(y_test , pred) # 정밀도 점수
    recall = recall_score(y_test , pred) # 재현율 점수
    f1 = f1_score(y_test,pred) # 정밀도, 재현율 조화평균 값
    # ROC-AUC 추가
    roc_auc = roc_auc_score(y_test, pred_proba) # AUC 점수 : 불균형 데이터 셋에서 필요
    print('오차 행렬')
    print(confusion)
    # ROC-AUC print 추가
    print('정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f},\
    F1: {3:.4f}, AUC:{4:.4f}'.format(accuracy, precision, recall, f1, roc_auc))

from sklearn.linear_model import LogisticRegression

lr_clf =LogisticRegression(max_iter=1000)
lr_clf.fit(X_train,y_train)
lr_pred=lr_clf.predict(X_test)
lr_pred_proba=lr_clf.predict_proba(X_test)[:,1]

get_clf_eval(y_test,lr_pred,lr_pred_proba)

```
![image](https://github.com/user-attachments/assets/8977c25e-5025-4751-8a22-1c4ce2a3ef3d)

```
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline

import warnings
warnings.filterwarnings('ignore')

card_df=pd.read_csv(
    './creditcard.csv'
    ,encoding='utf-8'
)

from sklearn.model_selection import train_test_split

def get_preprocessed_df(df=None):
    df_copy = df.copy()
    df_copy.drop('Time',axis=1,inplace=True)
    return df_copy

def get_train_test_dataset(df=None):
    if df is None:  
        return '데이터베이스가 존재하지 않습니다'
    else:
        df_copy = get_preprocessed_df(df)
        X_features = df_copy.iloc[:, :-1]
        y_target = df_copy.iloc[:, -1]
        X_train, X_test, y_train, y_test = train_test_split(
            X_features,
            y_target,
            test_size=0.3,
            random_state=0,
            stratify=y_target
        )
        return X_train, X_test, y_train, y_test

X_train,X_test,y_train,y_test=get_train_test_dataset(card_df)

# print('='*50)
# print(f'학습 데이터 레이블 값 비율\n{y_train.value_counts()/y_train.shape[0]*100}')
# print('='*50)
# print(f'테스트 데이터 레이블 값 비율\n{y_test.value_counts()/y_test.shape[0]*100}')

from sklearn.linear_model import LogisticRegression

lr_clf =LogisticRegression(max_iter=1000)
lr_clf.fit(X_train,y_train)
lr_pred=lr_clf.predict(X_test)
lr_pred_proba=lr_clf.predict_proba(X_test)[:,1]


# 모델 생성 성능 평가 : get_clf_eval(원래답, 예측값, 예측확률)
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.metrics import precision_score, recall_score
from sklearn.metrics import f1_score, roc_auc_score


def get_clf_eval(y_test, pred=None, pred_proba=None):
    confusion = confusion_matrix( y_test, pred)
    accuracy = accuracy_score(y_test , pred) # 정확도 점수
    precision = precision_score(y_test , pred) # 정밀도 점수
    recall = recall_score(y_test , pred) # 재현율 점수
    f1 = f1_score(y_test,pred) # 정밀도, 재현율 조화평균 값
    # ROC-AUC 추가
    roc_auc = roc_auc_score(y_test, pred_proba) # AUC 점수 : 불균형 데이터 셋에서 필요
    print('오차 행렬')
    print(confusion)
    # ROC-AUC print 추가
    print('정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f}, F1: {3:.4f}, AUC:{4:.4f}'.format(accuracy, precision, recall, f1, roc_auc))

def get_model_train_eval(
    model                   # 분류 알고리즘
    ,ftr_train=None         # 학습 데이터
    ,ftr_test=None          # 테스트 데이터
    ,tgt_train=None         # 학습 데이터 레이블
    ,tgt_test=None          # 테스트 데이터 레이블
    ):
    model.fit(ftr_train,tgt_train)
    pred = model.predict(ftr_test)
    pred_proba=model.predict_proba(ftr_test)[:,1] # 예측 확률
    get_clf_eval(tgt_test,pred,pred_proba)

from lightgbm import LGBMClassifier

lgbm_clf = LGBMClassifier(
    n_estimators=1000
    ,num_leaves=64
    #, n_jobs=-1 # cpu 일때만 사용
    ,device='gpu' # gpu 일때만 사용
    ,boosting_from_average=False # 극도로 불균형한 레이블일 경우 False를 줘야 함
)

get_model_train_eval(lgbm_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)
```

![image](https://github.com/user-attachments/assets/bb4b5e60-f3bb-4bd1-9f6c-72c5f373acd5)

```
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline

import warnings
warnings.filterwarnings('ignore')

card_df=pd.read_csv(
    './creditcard.csv'
    ,encoding='utf-8'
)

from sklearn.model_selection import train_test_split

def get_preprocessed_df(df=None):
    df_copy = df.copy()
    df_copy.drop('Time',axis=1,inplace=True)
    return df_copy

def get_train_test_dataset(df=None):
    if df is None:  
        return '데이터베이스가 존재하지 않습니다'
    else:
        df_copy = get_preprocessed_df(df)
        X_features = df_copy.iloc[:, :-1]
        y_target = df_copy.iloc[:, -1]
        X_train, X_test, y_train, y_test = train_test_split(
            X_features,
            y_target,
            test_size=0.3,
            random_state=0,
            stratify=y_target
        )
        return X_train, X_test, y_train, y_test

X_train,X_test,y_train,y_test=get_train_test_dataset(card_df)

# print('='*50)
# print(f'학습 데이터 레이블 값 비율\n{y_train.value_counts()/y_train.shape[0]*100}')
# print('='*50)
# print(f'테스트 데이터 레이블 값 비율\n{y_test.value_counts()/y_test.shape[0]*100}')

from sklearn.linear_model import LogisticRegression

lr_clf =LogisticRegression(max_iter=1000)
lr_clf.fit(X_train,y_train)
lr_pred=lr_clf.predict(X_test)
lr_pred_proba=lr_clf.predict_proba(X_test)[:,1]


# 모델 생성 성능 평가 : get_clf_eval(원래답, 예측값, 예측확률)
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.metrics import precision_score, recall_score
from sklearn.metrics import f1_score, roc_auc_score


def get_clf_eval(y_test, pred=None, pred_proba=None):
    confusion = confusion_matrix( y_test, pred)
    accuracy = accuracy_score(y_test , pred) # 정확도 점수
    precision = precision_score(y_test , pred) # 정밀도 점수
    recall = recall_score(y_test , pred) # 재현율 점수
    f1 = f1_score(y_test,pred) # 정밀도, 재현율 조화평균 값
    # ROC-AUC 추가
    roc_auc = roc_auc_score(y_test, pred_proba) # AUC 점수 : 불균형 데이터 셋에서 필요
    print('오차 행렬')
    print(confusion)
    # ROC-AUC print 추가
    print('정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f}, F1: {3:.4f}, AUC:{4:.4f}'.format(accuracy, precision, recall, f1, roc_auc))

def get_model_train_eval(
    model                   # 분류 알고리즘
    ,ftr_train=None         # 학습 데이터
    ,ftr_test=None          # 테스트 데이터
    ,tgt_train=None         # 학습 데이터 레이블
    ,tgt_test=None          # 테스트 데이터 레이블
    ):
    model.fit(ftr_train,tgt_train)
    pred = model.predict(ftr_test)
    pred_proba=model.predict_proba(ftr_test)[:,1] # 예측 확률
    get_clf_eval(tgt_test,pred,pred_proba)


from lightgbm import LGBMClassifier

lgbm_clf = LGBMClassifier(
    n_estimators=1000
    ,num_leaves=64
    #, n_jobs=-1 # cpu 일때만 사용
    ,device='gpu' # gpu 일때만 사용
    ,boosting_from_average=False # 극도로 불균형한 레이블일 경우 False를 줘야 함
)

# lightGBM
get_model_train_eval(lgbm_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)

# LogisticRegression
get_model_train_eval(lr_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)
```
![image](https://github.com/user-attachments/assets/ac09dbe7-edea-40e5-8c34-0d8363bd1d5c)

---
## 데이터 분포 확인, 변환후 모델 학습/예측/평가

```
# 특정 피쳐(금액)의 데이터 분포 확인
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline

import warnings
warnings.filterwarnings('ignore')

import seaborn as sns

card_df=pd.read_csv(
    './creditcard.csv'
    ,encoding='utf-8'
)

plt.figure(figsize=(8,4))
plt.xticks(
    range(0,30000,1000)
    ,rotation=60
)
sns.histplot(card_df['Amount'],bins=100,kde=True)
plt.show()
```
![image](https://github.com/user-attachments/assets/b32af0f7-1e8b-4743-bd6b-f3629af769b7)

---

```
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline  

import warnings
warnings.filterwarnings('ignore')  

import seaborn as sns

# CSV 파일을 읽어와 DataFrame으로 저장
card_df = pd.read_csv(
    './creditcard.csv',  # 신용카드 거래 데이터 파일 경로
    encoding='utf-8'  # UTF-8 인코딩으로 파일을 읽음
)

# 외도(Skewness) 값 확인: 데이터의 비대칭성을 확인하기 위함
# 비대칭 데이터의 경우 로그 변환 등으로 정규성을 확보할 필요가 있음

# 첨도(Kurtosis) 값 확인: 극단값(outlier)의 존재 여부를 판단하기 위함
# 양의 첨도: 분포의 꼬리가 두껍고, 중앙이 뾰족 -> 극단값이 자주 발생
#           -> 극단값 처리가 필요하며, 적절한 모델 선택이 중요
# 음의 첨도: 분포의 꼬리가 얇고, 중앙이 평평 -> 극단값이 적게 발생
# 중간 첨도: 정규분포에 가까움 (외도와 첨도가 0에 가까움)

from scipy.stats import skew  # 외도 계산 함수
from scipy.stats import kurtosis  # 첨도 계산 함수

# 'Amount' 열의 외도(Skewness) 값 출력
print(f"외도 : {skew(card_df['Amount'])}")
print(f"첨도 : {kurtosis(card_df['Amount'])}")
```
![image](https://github.com/user-attachments/assets/9fc70ec9-5e36-4415-800c-3abcccc2cccc)

---
## 📊 모수적 vs 비모수적 검정 방법 정리

### 1. t-test (독립표본 t-검정, 대응표본 t-검정)
#### 🔹 특징
- 두 그룹의 **평균 차이**를 비교하는 **모수적 검정**  
- 데이터가 **정규성을 만족해야** 함  
- **대응표본 t-검정**: 같은 집단의 사전-사후 데이터를 비교  
- **독립표본 t-검정**: 서로 다른 두 집단을 비교  

#### ✅ 장점
- 해석이 직관적이며 널리 사용됨  
- 검정력이 강하여 비교적 적은 표본에서도 신뢰성 확보 가능  

#### ❌ 단점
- 정규성을 만족하지 않으면 사용하기 어려움  
- 등분산성을 가정해야 함 (등분산이 다르면 Welch’s t-test 사용)  

---

### 2. Mann-Whitney U test (윌콕슨 순위합 검정)
#### 🔹 특징
- 두 그룹의 **중위수 차이**를 비교하는 **비모수 검정**  
- 데이터의 크기 순서(순위)를 이용하여 비교  
- 정규성을 만족하지 않아도 사용 가능  

#### ✅ 장점
- 정규성 가정이 필요 없음 → 소규모 샘플에서도 활용 가능  
- 이상치(outlier)에 덜 민감함  

#### ❌ 단점
- 평균이 아닌 **중위수 비교**이므로 평균 차이를 확인하는 데 한계가 있음  
- 표본 크기가 클 경우 검정력이 낮아질 수 있음  

---

### 3. ANOVA (분산 분석, Analysis of Variance)
#### 🔹 특징
- 세 그룹 이상의 **평균 차이**를 비교하는 **모수적 검정**  
- 정규성과 등분산성을 만족해야 함  
- 집단 간 분산과 집단 내 분산을 이용하여 차이를 분석  

#### ✅ 장점
- 세 그룹 이상을 동시에 비교할 수 있어 **다중 비교 문제 방지**  
- 그룹 간 차이를 분석하는 데 효과적  

#### ❌ 단점
- 정규성과 등분산성을 만족하지 않으면 부적절함  
- 차이가 있다고만 알려주며, **어느 그룹 간 차이가 있는지 추가 분석(Tukey HSD 등)이 필요**  

---

### 4. Kruskal-Wallis test (크루스칼-왈리스 검정)
#### 🔹 특징
- 세 그룹 이상의 **중위수 차이**를 비교하는 **비모수 검정**  
- 정규성을 만족하지 않아도 사용 가능  
- 데이터의 순위를 이용하여 비교  

#### ✅ 장점
- 정규성을 가정할 필요 없음  
- 이상치에 덜 민감하여 다양한 데이터에 적용 가능  

#### ❌ 단점
- 평균이 아닌 **중위수를 비교**하므로 모수적 방법보다 덜 강력함  
- 그룹 간 어느 부분에서 차이가 나는지 추가 분석 필요  

---

### 📌 요약 정리

| 비교 대상 | 모수적 방법 | 비모수적 방법 |
|-----------|------------|--------------|
| **두 그룹 비교 (평균)** | t-test | Mann-Whitney U test (Wilcoxon rank-sum test) |
| **세 그룹 이상 비교 (평균)** | ANOVA | Kruskal-Wallis test |


| 검정 방법 | 특징 | 장점 | 단점 |
|----------|------|------|------|
| **t-test** | 두 그룹의 평균 비교 (모수) | 검정력이 강함, 직관적 해석 가능 | 정규성 필요, 등분산 가정 필요 |
| **Mann-Whitney U test** | 두 그룹의 중위수 비교 (비모수) | 정규성 필요 없음, 이상치에 강함 | 평균 비교 불가, 큰 표본에서 검정력 낮음 |
| **ANOVA** | 세 그룹 이상의 평균 비교 (모수) | 다중 비교 가능, 효과적인 그룹 차이 분석 | 정규성·등분산 필요, 추가 분석 필요 |
| **Kruskal-Wallis test** | 세 그룹 이상의 중위수 비교 (비모수) | 정규성 필요 없음, 이상치에 강함 | 평균 비교 불가, 추가 분석 필요 |

---
## 📊 정규성 판단 방법 정리

### **1. 통계적 검정 방법**
정규성을 확인하기 위한 다양한 **유의확률 기반 검정 방법**입니다.

#### 📌 **1) Shapiro-Wilk Test**
- **특징**: 가장 널리 사용되는 정규성 검정 방법  
- **사용 조건**: 샘플 크기가 작을 때(50 이하) 적합  

✅ **장점**  
- 소규모 샘플(≤50개)에 대해 높은 검정력 제공  
- 비교적 정확한 정규성 검정 가능  

❌ **단점**  
- 샘플 크기가 커지면 검정력이 너무 강해져 정규성을 쉽게 기각할 수 있음  
- 이상치(outlier)에 민감  

---

#### 📌 **2) Kolmogorov-Smirnov Test (K-S Test)**
- **특징**: 정규분포와 데이터 분포를 비교하여 정규성 검정  
- **사용 조건**: 대규모 샘플에서 사용 가능하지만, 정확성이 낮음  

✅ **장점**  
- 샘플 크기에 상관없이 적용 가능  
- 분포의 차이를 직접 비교하는 방식  

❌ **단점**  
- 검정력이 낮아 정규성을 정확히 판별하지 못할 수 있음  
- 샘플 크기가 크면 정규성을 기각할 가능성이 커짐  

---

#### 📌 **3) Anderson-Darling Test**
- **특징**: Kolmogorov-Smirnov Test보다 정밀한 정규성 검정 방법  
- **사용 조건**: 샘플 크기에 상관없이 활용 가능  

✅ **장점**  
- 샘플 크기에 크게 영향을 받지 않음  
- 분포의 꼬리 부분까지 고려하여 정규성을 판단  

❌ **단점**  
- Shapiro-Wilk Test보다 덜 직관적  
- 특정한 상황에서 p-value 해석이 어려울 수 있음  

---

#### 📌 **4) Jarque-Bera Test**
- **특징**: 데이터의 **왜도(skewness)와 첨도(kurtosis)** 를 이용한 정규성 검정  
- **사용 조건**: 대규모 샘플에서 사용  

✅ **장점**  
- 큰 샘플에서도 신뢰할 수 있는 결과 제공  
- 왜도와 첨도를 함께 고려하여 정규성 검정 가능  

❌ **단점**  
- 소규모 샘플에서는 검정력이 낮아 비효율적  
- 왜도와 첨도가 극단적인 경우 정규성을 정확히 판단하기 어려움  

---

### **2. 시각적 분석 방법**
정규성을 **직관적으로 확인**하는 방법입니다.

#### 📌 **1) 히스토그램 (Histogram)**
- **특징**: 데이터의 분포가 종모양(정규분포)인지 확인  

✅ **장점**  
- 정규성을 빠르게 시각적으로 확인 가능  
- 직관적인 해석 가능  

❌ **단점**  
- 샘플 크기가 작으면 부정확할 수 있음  
- 육안 판단이므로 주관적일 수 있음  

---

#### 📌 **2) Q-Q Plot (Quantile-Quantile Plot)**
- **특징**: 데이터의 분위수를 정규분포의 분위수와 비교  

✅ **장점**  
- 정규성을 시각적으로 쉽게 판단 가능  
- 정규성 여부뿐만 아니라 데이터 분포의 특성(왜도, 이상치)도 확인 가능  

❌ **단점**  
- 해석이 다소 어려울 수 있음  
- 육안 판단이므로 정확한 검정 방법은 아님  

---

#### 📌 **3) P-P Plot (Probability-Probability Plot)**
- **특징**: 표본 데이터의 누적분포와 정규분포의 누적분포를 비교  

✅ **장점**  
- Q-Q Plot과 유사하지만 누적분포를 기반으로 하기 때문에 정규성 검정에 더 유리  

❌ **단점**  
- 데이터 수가 많아질수록 해석이 어려워질 수 있음  

---

#### 📌 **4) Box Plot (상자 그림)**
- **특징**: 데이터의 대칭성과 이상치(outlier)를 확인  

✅ **장점**  
- 데이터의 분포를 직관적으로 볼 수 있음  
- 이상치가 있는지 쉽게 확인 가능  

❌ **단점**  
- 정규성을 직접적으로 검정할 수 없음  
- 샘플 크기가 작으면 왜곡될 가능성이 있음  

---

### **📌 정규성 판단 방법 비교표**

| 방법 | 특징 | 장점 | 단점 |
|------|------|------|------|
| **Shapiro-Wilk Test** | 가장 널리 사용되는 정규성 검정 | 소규모 샘플(≤50)에서 높은 검정력 | 대규모 샘플에서 과도하게 정규성을 기각할 가능성 있음 |
| **Kolmogorov-Smirnov Test** | 정규분포와 데이터 분포를 비교 | 샘플 크기에 관계없이 적용 가능 | 검정력이 낮아 정규성을 정확히 판별하기 어려움 |
| **Anderson-Darling Test** | K-S Test보다 정밀한 정규성 검정 | 샘플 크기 영향을 덜 받음 | 해석이 다소 어려울 수 있음 |
| **Jarque-Bera Test** | 왜도와 첨도를 이용한 정규성 검정 | 대규모 샘플에서 신뢰성 높음 | 소규모 샘플에서는 검정력이 낮음 |
| **히스토그램** | 데이터 분포가 정규성을 따르는지 확인 | 직관적인 해석 가능 | 샘플 크기가 작으면 부정확할 수 있음 |
| **Q-Q Plot** | 데이터 분위수와 정규분포 분위수를 비교 | 왜도, 이상치 등을 한눈에 파악 가능 | 해석이 다소 어려울 수 있음 |
| **P-P Plot** | 누적분포를 비교하여 정규성 검정 | Q-Q Plot보다 정규성 검정에 유리 | 데이터 수가 많아지면 해석 어려움 |
| **Box Plot** | 데이터의 대칭성과 이상치 확인 | 이상치를 쉽게 발견 가능 | 정규성을 직접 검정할 수 없음 |

---

### **✅ 정규성 판단 흐름**
1️⃣ **시각적 방법(Q-Q Plot, 히스토그램 등)으로 대략적인 정규성 확인**  
2️⃣ **통계적 검정(Shapiro-Wilk Test, K-S Test 등) 수행**  
3️⃣ **p-value ≥ 0.05** → 정규성 만족 (모수 검정 가능)  
4️⃣ **p-value < 0.05** → 정규성 불만족 (비모수 검정 필요)  

📌 **정규성 검정은 데이터 특성에 따라 적절한 방법을 선택하는 것이 중요합니다!** 🚀

---
## 📌 정규성 판단 방법

### 1️⃣ 통계적 검정 방법 (유의확률 기반)

| 방법 | 설명 | 사용 조건 |
|------|------|----------|
| **Shapiro-Wilk Test** | 가장 널리 사용되는 정규성 검정 방법 | 샘플 크기가 50 이하일 때 적합 |
| **Kolmogorov-Smirnov Test** | 정규분포와 데이터 분포를 비교 | 샘플 크기가 크면 검정력이 떨어짐 |
| **Anderson-Darling Test** | Kolmogorov-Smirnov Test보다 정밀한 검정 | 샘플 크기에 상관없이 활용 가능 |
| **Jarque-Bera Test** | 왜도(skewness)와 첨도(kurtosis)를 이용한 정규성 검정 | 대규모 샘플에서 많이 사용됨 |

💡 **유의확률(p-value)이 0.05보다 작으면 → 정규성을 만족하지 않음(귀무가설 기각)**

---

### 2️⃣ 시각적 분석 방법

| 방법 | 설명 |
|------|------|
| **히스토그램 (Histogram)** | 데이터가 종모양(정규분포)인지 확인 |
| **Q-Q Plot (Quantile-Quantile Plot)** | 데이터가 정규분포의 분위수와 일치하는지 확인 |
| **P-P Plot (Probability-Probability Plot)** | 표본의 누적분포와 정규분포의 누적분포를 비교 |
| **Box Plot (상자 그림)** | 대칭성이 있는지, 이상치(outlier)가 많은지 확인 |

💡 **데이터가 종모양을 띠고, Q-Q Plot이 직선에 가까우면 정규성을 만족**

---

```
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline  

import warnings
warnings.filterwarnings('ignore')  

import seaborn as sns

# CSV 파일을 읽어와 DataFrame으로 저장  
card_df = pd.read_csv(
    './creditcard.csv',  # 신용카드 거래 데이터 파일 경로
    encoding='utf-8'  # UTF-8 인코딩으로 파일을 읽음
)

from scipy.stats import shapiro

stat,p=shapiro(card_df['Amount'].values)
print(f'통계량 : {stat}')
print(f'p-value : {p}')
```
![image](https://github.com/user-attachments/assets/d2ed69b7-01f6-429c-b367-dd3533a28da8)

```
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline  

import warnings
warnings.filterwarnings('ignore')  

import seaborn as sns

# CSV 파일을 읽어와 DataFrame으로 저장  
card_df = pd.read_csv(
    './creditcard.csv',  # 신용카드 거래 데이터 파일 경로
    encoding='utf-8'  # UTF-8 인코딩으로 파일을 읽음
)

from scipy.stats import shapiro

stat,p=shapiro(card_df['Amount'].values)

print(f'통계량 : {stat}')
print(f'p-value : {p}')

if p>0.05:
    print('정규성을 만족')
else:
    print('정규성 없음')
```
![image](https://github.com/user-attachments/assets/f0ca2035-1dd7-44b2-9b53-8aa7041bb485)

```
# 정규성 있는 데이터 시각화
import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as stats

data=np.random.normal(loc=0,  scale=1,size=1000)

# QQ plot 생성
stats.probplot(data,dist='norm',plot=plt)
plt.grid()
plt.show()
```
![image](https://github.com/user-attachments/assets/91ddba78-a759-4bf2-824a-21bbf4882c49)

```
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline  

import warnings
warnings.filterwarnings('ignore')  

import seaborn as sns

# CSV 파일을 읽어와 DataFrame으로 저장  
card_df = pd.read_csv(
    './creditcard.csv',  # 신용카드 거래 데이터 파일 경로
    encoding='utf-8'  # UTF-8 인코딩으로 파일을 읽음
)

import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as stats

data=np.random.normal(loc=0,  scale=1,size=1000)

# QQ plot 생성
stats.probplot(card_df['Amount'].values,dist='norm',plot=plt)
plt.grid()
plt.show()
```
![image](https://github.com/user-attachments/assets/21ffed58-bd3a-404a-a2b7-a9f6c999101b)

```
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline  

import warnings
warnings.filterwarnings('ignore')  

import seaborn as sns

# CSV 파일을 읽어와 DataFrame으로 저장  
card_df = pd.read_csv(
    './creditcard.csv',  # 신용카드 거래 데이터 파일 경로
    encoding='utf-8'  # UTF-8 인코딩으로 파일을 읽음
)

# standarization : 데이터 정규분포 형태로 변환

from sklearn.preprocessing import StandardScaler

# 기존의 Amount feature의 평균,표준편차, 외도, 첨도
avg_amount=card_df['Amount'].mean()
std_amount=card_df['Amount'].std()

from scipy.stats import skew  # 외도 계산 함수
from scipy.stats import kurtosis  # 첨도 계산 함수
skew_amount=skew(card_df['Amount'])
kurtosis_amount=kurtosis(card_df['Amount'])

# StandardScaler 적용한 Amount feature의 평균,표준편차, 외도, 첨도
scaler=StandardScaler()
ss_amount=scaler.fit_transform(card_df['Amount'].values.reshape(-1,1))

print('기존 데이터 Amount 컬럼의 평균 ,표준편차 ,외도 ,첨도')
print(f'{avg_amount} / {std_amount} / {skew_amount} / {kurtosis_amount}')
print('='*100)
print('표준화한 데이터 Amount 컬럼의 평균 ,표준편차 ,외도 ,첨도')
print(f'{ss_amount.mean()} / {ss_amount.std()} / {skew(ss_amount)} / {kurtosis(ss_amount)}')
```
![image](https://github.com/user-attachments/assets/0c268337-ed99-4ad8-920b-8a62d9003aa8)

```
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline

import warnings
warnings.filterwarnings('ignore')

card_df=pd.read_csv(
    './creditcard.csv'
    ,encoding='utf-8'
)

from sklearn.model_selection import train_test_split

def get_preprocessed_df(df=None):
    df_copy = df.copy()
    df_copy.drop('Time',axis=1,inplace=True)
    return df_copy

def get_train_test_dataset(df=None):
    if df is None:  
        return '데이터베이스가 존재하지 않습니다'
    else:
        df_copy = get_preprocessed_df(df)
        X_features = df_copy.iloc[:, :-1]
        y_target = df_copy.iloc[:, -1]
        X_train, X_test, y_train, y_test = train_test_split(
            X_features,
            y_target,
            test_size=0.3,
            random_state=0,
            stratify=y_target
        )
        return X_train, X_test, y_train, y_test

X_train,X_test,y_train,y_test=get_train_test_dataset(card_df)

# print('='*50)
# print(f'학습 데이터 레이블 값 비율\n{y_train.value_counts()/y_train.shape[0]*100}')
# print('='*50)
# print(f'테스트 데이터 레이블 값 비율\n{y_test.value_counts()/y_test.shape[0]*100}')

from sklearn.linear_model import LogisticRegression

lr_clf =LogisticRegression(max_iter=1000)
lr_clf.fit(X_train,y_train)
lr_pred=lr_clf.predict(X_test)
lr_pred_proba=lr_clf.predict_proba(X_test)[:,1]


# 모델 생성 성능 평가 : get_clf_eval(원래답, 예측값, 예측확률)
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.metrics import precision_score, recall_score
from sklearn.metrics import f1_score, roc_auc_score


def get_clf_eval(y_test, pred=None, pred_proba=None):
    confusion = confusion_matrix( y_test, pred)
    accuracy = accuracy_score(y_test , pred) # 정확도 점수
    precision = precision_score(y_test , pred) # 정밀도 점수
    recall = recall_score(y_test , pred) # 재현율 점수
    f1 = f1_score(y_test,pred) # 정밀도, 재현율 조화평균 값
    # ROC-AUC 추가
    roc_auc = roc_auc_score(y_test, pred_proba) # AUC 점수 : 불균형 데이터 셋에서 필요
    print('오차 행렬')
    print(confusion)
    # ROC-AUC print 추가
    print('정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f}, F1: {3:.4f}, AUC:{4:.4f}'.format(accuracy, precision, recall, f1, roc_auc))

def get_model_train_eval(
    model                   # 분류 알고리즘
    ,ftr_train=None         # 학습 데이터
    ,ftr_test=None          # 테스트 데이터
    ,tgt_train=None         # 학습 데이터 레이블
    ,tgt_test=None          # 테스트 데이터 레이블
    ):
    model.fit(ftr_train,tgt_train)
    pred = model.predict(ftr_test)
    pred_proba=model.predict_proba(ftr_test)[:,1] # 예측 확률
    get_clf_eval(tgt_test,pred,pred_proba)


from lightgbm import LGBMClassifier

lgbm_clf = LGBMClassifier(
    n_estimators=1000
    ,num_leaves=64
    #, n_jobs=-1 # cpu 일때만 사용
    ,device='gpu' # gpu 일때만 사용
    ,boosting_from_average=False # 극도로 불균형한 레이블일 경우 False를 줘야 함
)

print('StandarScaler 처리 전')
print('lightGBM')
# lightGBM
get_model_train_eval(lgbm_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)
print('='*100)
print('LogisticRegression')
# LogisticRegression
get_model_train_eval(lr_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)
print('='*100)
print('StandarScaler 처리 후')
# 데이터를 StandarScaler 처리 후 성능 평가
from sklearn.preprocessing import StandardScaler

def get_preprocessed_df(df=None):
    df_copy = df.copy()
    scaler=StandardScaler()
    ss_amount=scaler.fit_transform(df_copy['Amount'].values.reshape(-1,1))
    df_copy.insert(0,'Amount_Scaled',ss_amount)
    df_copy.drop(['Time','Amount'],axis=1,inplace=True)
    return df_copy # 전처리 된 데이터프레임 반환

# 원본 데이터프레임 : card_df
X_train,X_test,y_train,y_test=get_train_test_dataset(card_df)

print('로지스틱 회귀 예측 성능')
get_model_train_eval(lr_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)
print('='*100)
print('lightGBM 예측 성능')
get_model_train_eval(lgbm_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)
```
```
StandarScaler 처리 전
lightGBM
오차 행렬
[[85243    52]
 [   69    79]]
정확도: 0.9986, 정밀도: 0.6031, 재현율: 0.5338, F1: 0.5663, AUC:0.7665
====================================================================================================
LogisticRegression
오차 행렬
[[85280    15]
 [   54    94]]
정확도: 0.9992, 정밀도: 0.8624, 재현율: 0.6351, F1: 0.7315, AUC:0.9727
====================================================================================================
StandarScaler 처리 후
로지스틱 회귀 예측 성능
오차 행렬
[[85281    14]
 [   55    93]]
정확도: 0.9992, 정밀도: 0.8692, 재현율: 0.6284, F1: 0.7294, AUC:0.9706
====================================================================================================
lightGBM 예측 성능
오차 행렬
[[85258    37]
 [   62    86]]
정확도: 0.9988, 정밀도: 0.6992, 재현율: 0.5811, F1: 0.6347, AUC:0.7903
```

```
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline  

import warnings
warnings.filterwarnings('ignore')  

import seaborn as sns

# CSV 파일을 읽어와 DataFrame으로 저장  
card_df = pd.read_csv(
    './creditcard.csv',  # 신용카드 거래 데이터 파일 경로
    encoding='utf-8'  # UTF-8 인코딩으로 파일을 읽음
)

# 기존의 Amount feature의 평균, 표준편차, 외도, 첨도
avg_amount = card_df['Amount'].mean()
std_amount = card_df['Amount'].std()

from scipy.stats import skew, kurtosis  # 외도, 첨도 계산 함수

skew_amount = skew(card_df['Amount'])
kurtosis_amount = kurtosis(card_df['Amount'])

# StandardScaler 적용한 Amount feature의 평균,표준편차, 외도, 첨도
from sklearn.preprocessing import StandardScaler
scaler=StandardScaler()
ss_amount=scaler.fit_transform(card_df['Amount'].values.reshape(-1,1))


# 로그 변환 적용
log_amount = np.log1p(card_df['Amount'])


# 로그 변환 후 평균, 표준편차, 외도, 첨도
log_avg_amount = log_amount.mean()
log_std_amount = log_amount.std()
log_skew_amount = skew(log_amount)
log_kurtosis_amount = kurtosis(log_amount)

print('='*100)
print('기존 데이터 Amount 컬럼의 평균 ,표준편차 ,외도 ,첨도')
print(f'{avg_amount} / {std_amount} / {skew_amount} / {kurtosis_amount}')

print('='*100)
print('log화한 데이터 Amount 컬럼의 평균 ,표준편차 ,외도 ,첨도')
print(f'{ss_amount.mean()} / {ss_amount.std()} / {skew(ss_amount)} / {kurtosis(ss_amount)}')
print('='*100)
print('로그 변환한 데이터 Amount 컬럼의 평균 ,표준편차 ,외도 ,첨도')
print(f'{log_avg_amount} / {log_std_amount} / {log_skew_amount} / {log_kurtosis_amount}')
print('='*100)
```
![image](https://github.com/user-attachments/assets/e974af5e-6c00-4131-aa3c-4a9e7a398ae2)

```
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline

import warnings
warnings.filterwarnings('ignore')

card_df=pd.read_csv(
    './creditcard.csv'
    ,encoding='utf-8'
)

from sklearn.model_selection import train_test_split

def get_preprocessed_df(df=None):
    df_copy = df.copy()
    df_copy.drop('Time',axis=1,inplace=True)
    return df_copy

def get_train_test_dataset(df=None):
    if df is None:  
        return '데이터베이스가 존재하지 않습니다'
    else:
        df_copy = get_preprocessed_df(df)
        X_features = df_copy.iloc[:, :-1]
        y_target = df_copy.iloc[:, -1]
        X_train, X_test, y_train, y_test = train_test_split(
            X_features,
            y_target,
            test_size=0.3,
            random_state=0,
            stratify=y_target
        )
        return X_train, X_test, y_train, y_test

X_train,X_test,y_train,y_test=get_train_test_dataset(card_df)

# print('='*50)
# print(f'학습 데이터 레이블 값 비율\n{y_train.value_counts()/y_train.shape[0]*100}')
# print('='*50)
# print(f'테스트 데이터 레이블 값 비율\n{y_test.value_counts()/y_test.shape[0]*100}')

from sklearn.linear_model import LogisticRegression

lr_clf =LogisticRegression(max_iter=1000)
lr_clf.fit(X_train,y_train)
lr_pred=lr_clf.predict(X_test)
lr_pred_proba=lr_clf.predict_proba(X_test)[:,1]


# 모델 생성 성능 평가 : get_clf_eval(원래답, 예측값, 예측확률)
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.metrics import precision_score, recall_score
from sklearn.metrics import f1_score, roc_auc_score


def get_clf_eval(y_test, pred=None, pred_proba=None):
    confusion = confusion_matrix( y_test, pred)
    accuracy = accuracy_score(y_test , pred) # 정확도 점수
    precision = precision_score(y_test , pred) # 정밀도 점수
    recall = recall_score(y_test , pred) # 재현율 점수
    f1 = f1_score(y_test,pred) # 정밀도, 재현율 조화평균 값
    # ROC-AUC 추가
    roc_auc = roc_auc_score(y_test, pred_proba) # AUC 점수 : 불균형 데이터 셋에서 필요
    print('오차 행렬')
    print(confusion)
    # ROC-AUC print 추가
    print('정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f}, F1: {3:.4f}, AUC:{4:.4f}'.format(accuracy, precision, recall, f1, roc_auc))

def get_model_train_eval(
    model                   # 분류 알고리즘
    ,ftr_train=None         # 학습 데이터
    ,ftr_test=None          # 테스트 데이터
    ,tgt_train=None         # 학습 데이터 레이블
    ,tgt_test=None          # 테스트 데이터 레이블
    ):
    model.fit(ftr_train,tgt_train)
    pred = model.predict(ftr_test)
    pred_proba=model.predict_proba(ftr_test)[:,1] # 예측 확률
    get_clf_eval(tgt_test,pred,pred_proba)


from lightgbm import LGBMClassifier

lgbm_clf = LGBMClassifier(
    n_estimators=1000
    ,num_leaves=64
    #, n_jobs=-1 # cpu 일때만 사용
    ,device='gpu' # gpu 일때만 사용
    ,boosting_from_average=False # 극도로 불균형한 레이블일 경우 False를 줘야 함
)

print('StandarScaler 처리 전')
print('lightGBM')
# lightGBM
get_model_train_eval(lgbm_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)
print('='*100)
print('LogisticRegression')
# LogisticRegression
get_model_train_eval(lr_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)
print('='*100)
print('StandarScaler 처리 후')

# 데이터를 StandarScaler 처리 후 성능 평가
from sklearn.preprocessing import StandardScaler

def get_preprocessed_df(df=None):
    df_copy = df.copy()
    scaler=StandardScaler()
    ss_amount=scaler.fit_transform(df_copy['Amount'].values.reshape(-1,1))
    df_copy.insert(0,'Amount_Scaled',ss_amount)
    df_copy.drop(['Time','Amount'],axis=1,inplace=True)
    return df_copy # 전처리 된 데이터프레임 반환

# 원본 데이터프레임 : card_df
X_train,X_test,y_train,y_test=get_train_test_dataset(card_df)


print('로지스틱 회귀 예측 성능')
get_model_train_eval(lr_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)
print('='*100)
print('lightGBM 예측 성능')
get_model_train_eval(lgbm_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)

print('='*100)
print('log 처리 후')
print('='*100)

# 데이터를 log 처리 후 성능 평가

def get_preprocessed_df(df=None):
    df_copy=df.copy()
    amount_n = np.log1p(df_copy['Amount'])
    df_copy.insert(0,'Amount_Scaled', ss_amount)
    df_copy.drop(['Time','Amount'], axis=1, inplace=True)
    
    return df_copy # 전처리된 데이터 프레임 반환 

X_train,X_test,y_train,y_test=get_train_test_dataset(card_df)


print('로지스틱 회귀 예측 성능')
get_model_train_eval(lr_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)
print('='*100)
print('lightGBM 예측 성능')
get_model_train_eval(lgbm_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)
```
```
StandarScaler 처리 전
lightGBM
오차 행렬
[[85226    69]
 [  124    24]]
정확도: 0.9977, 정밀도: 0.2581, 재현율: 0.1622, F1: 0.1992, AUC:0.5806
====================================================================================================
LogisticRegression
오차 행렬
[[85280    15]
 [   54    94]]
정확도: 0.9992, 정밀도: 0.8624, 재현율: 0.6351, F1: 0.7315, AUC:0.9727
====================================================================================================
StandarScaler 처리 후
로지스틱 회귀 예측 성능
오차 행렬
[[85281    14]
 [   55    93]]
정확도: 0.9992, 정밀도: 0.8692, 재현율: 0.6284, F1: 0.7294, AUC:0.9706
====================================================================================================
lightGBM 예측 성능
오차 행렬
[[85224    71]
 [  127    21]]
정확도: 0.9977, 정밀도: 0.2283, 재현율: 0.1419, F1: 0.1750, AUC:0.5705
====================================================================================================
log 처리 후
====================================================================================================
로지스틱 회귀 예측 성능
오차 행렬
[[85281    14]
 [   55    93]]
정확도: 0.9992, 정밀도: 0.8692, 재현율: 0.6284, F1: 0.7294, AUC:0.9706
====================================================================================================
lightGBM 예측 성능
오차 행렬
[[85224    71]
 [  148     0]]
정확도: 0.9974, 정밀도: 0.0000, 재현율: 0.0000, F1: 0.0000, AUC:0.4995
```
---
## 이상치 제거
```
# 이상치 검출/제거
# 상관계수 확인
import seaborn as sns

plt.Figure(
    figsize=(9,9)
)
corr=card_df.corr()

sns.heatmap(
    corr
    ,cmap='RdBu'
)
plt.show()
```
![image](https://github.com/user-attachments/assets/ae083d39-50f1-41db-b49b-0de1a43ed207)

```
fraud=card_df[card_df['Class']==1]['V14']
fraud
```
![image](https://github.com/user-attachments/assets/232fc48d-c8ef-4be7-839b-1455b6b8439e)

```
fraud=card_df[card_df['Class']==1]['V14']
fraud.values
```
![image](https://github.com/user-attachments/assets/5d51d979-54ae-46f2-8c41-b6c6b27dde13)

```
q1= np.percentile(card_df[card_df['Class']==1]['V14'].values,25)
q3= np.percentile(card_df[card_df['Class']==1]['V14'].values,75)

print(q1)
print(q3)
iqr=q3-q1
print(f'IQR value : {iqr}')
```
![image](https://github.com/user-attachments/assets/32d7492a-4a2b-44ba-9931-ee42dc97a87a)

```
# 이상치 데이터에 대한 인덱스를 반환하는 함수

# df=card_df 대입 예정
# column='V14
def get_outlier(df=None,column=None,weight=1.5):
    # 사기(class==1)에 해당하는 컬럼만 추출, 사분위(Q1,Q3) 지점을 구함
    fraud=df[df['Class']==1][column]
    q_25=np.percentile(fraud.values,25) # Q1 위치 값
    q_75=np.percentile(fraud.values,75) # Q3 위치 값
    # IQR 구하고, 1.5 곱하기
    iqr = q_75 - q_25
    iqr_weight=iqr*weight
    low_value=q_25-iqr_weight # min
    high_value=q_75+iqr_weight # max
    
    # 최대값 보다 크고, 최소값 보다 작은 인덱스 반환
    outlier_index=fraud[(fraud<low_value) | (fraud>high_value)].index
    return outlier_index

outlier_index=get_outlier(df=card_df,column='V14')
print(f'이상치 데이터의 인덱스 : {outlier_index}')
```
![image](https://github.com/user-attachments/assets/9ddf8564-eebe-4686-bbad-66d1d7f1519b)

```
# get_preprocessed_df() 에 이상치 제거 하는 기능 추가
# 로그변환, 이상치 제거 추가
def get_preprocessed_df(df=None):
    df_copy=df.copy()
    # Amount feature만 로그 변환
    amount_n=np.log1p(df_copy['Amount'])
    # 로그변환한 피처를 데이터 프레임에 삽입(insert)
    df_copy.drop(['Time','Amount'],axis=1,inplace=True)
    df_copy.insert(0, 'Amount_scaled', amount_n)
    # 이상치 데이터 제거
    outlier_index=get_outlier(df=df_copy, column='V14', weight=1.5)
    df_copy.drop(outlier_index, axis=0, inplace=True)
    return df_copy

X_train,X_test,y_train,y_test=get_train_test_dataset(df=card_df)
print('###로지스틱 회귀###')
get_model_train_eval(
    lr_clf
    ,ftr_train=X_train
    ,ftr_test=X_test
    ,tgt_train=y_train
    ,tgt_test=y_test
)

print('###lightgbm 예측 성능###')
get_model_train_eval(
    lgbm_clf
    ,ftr_train=X_train
    ,ftr_test=X_test
    ,tgt_train=y_train
    ,tgt_test=y_test
)
```
![image](https://github.com/user-attachments/assets/43c1db71-a663-499f-a2c0-b7e8122ebb50)

---
# 다변량 이상치 탐지 방법

다변량 이상치 탐지에는 다양한 방법이 있으며, 각 방법은 데이터의 특성에 따라 효과적일 수 있습니다. 아래는 여러 가지 이상치 탐지 방법을 설명한 내용입니다.

---

## 1. Mahalanobis Distance (마할라노비스 거리)
**Mahalanobis Distance**는 각 데이터 포인트가 평균값에서 얼마나 벗어나 있는지를 측정하는 거리입니다. 이 방식은 **공분산 행렬**을 고려하여 다변량 데이터를 분석하기 때문에, 변수들 간의 상관관계를 반영한 이상치 탐지에 효과적입니다.

- **공식**:
  $$
  \[
  D_M(x) = \sqrt{(x - \mu)^T S^{-1} (x - \mu)}
  \]
  $$
  
  여기서, \(x\)는 관측값, \(\mu\)는 평균, \(S^{-1}\)는 공분산 행렬의 역행렬입니다.
  
- **특징**:
  - **다변량** 데이터에서 이상치를 찾는 데 유용.
  - 고차원 데이터를 다룰 때, 변수 간의 상관관계를 고려하여 이상치를 탐지할 수 있음.
  
- **적용**:
  - 데이터의 **공분산**을 이용해 각 점이 평균에서 얼마나 벗어났는지 계산.
  - 마할라노비스 거리가 큰 값이 이상치로 간주됩니다.
  
- **단점**:
  - 데이터가 **정규 분포**를 따른다고 가정합니다.
  - 공분산 행렬 계산 시, **고차원** 데이터에서 계산이 어려울 수 있습니다.

---

## 2. Isolation Forest (이상치 탐지를 위한 랜덤 포레스트)
**Isolation Forest**는 **앙상블 기반의 이상치 탐지 방법**으로, 트리 구조를 이용하여 데이터의 이상치를 구별하는 방법입니다. 다른 방법들보다 **고차원 데이터에서 효과적**입니다.

- **작동 원리**:
  - 데이터를 무작위로 분할하여 트리 구조를 만듭니다. 각 데이터 포인트가 트리에서 분리되는 **속도**를 기반으로 이상치를 탐지합니다.
  - 정상 데이터는 여러 번 분할이 필요하지만, 이상치는 몇 번만 분할되면 쉽게 분리됩니다.
  
- **특징**:
  - 데이터가 **다양한 차원**을 가질 때 효과적.
  - 빠른 계산 속도를 자랑합니다.
  - 비지도 학습 방식으로, **레이블이 없는 데이터**에도 적용 가능합니다.

- **적용**:
  - 랜덤 트리 방식으로 데이터를 분리하며, 이상치는 **분리되기 쉽고, 분할 횟수가 적습니다**.
  - 트리의 분할이 적은 데이터 포인트를 이상치로 간주합니다.

- **단점**:
  - 일부 데이터에서는 **과적합**을 유발할 수 있습니다.
  - 트리의 깊이가 너무 깊어지면, 계산 비용이 커질 수 있습니다.

---

## 3. Local Outlier Factor (LOF, 로컬 이상치 팩터)
**Local Outlier Factor**는 데이터의 **밀도**를 기반으로 이상치를 탐지하는 방법입니다. 밀도가 낮은 지역에 위치한 데이터 포인트를 이상치로 간주합니다. 특히 **지역적 이상치(local outlier)**를 잘 탐지하는 데 유용합니다.

- **작동 원리**:
  - 각 데이터 포인트에 대해 **근접 이웃**을 찾고, 그 이웃들과의 **밀도 차이**를 계산합니다.
  - 밀도가 낮은 점을 이상치로 판단합니다.
  
- **특징**:
  - **밀도가 낮은 지역**에 위치한 데이터를 이상치로 간주합니다.
  - 이상치가 **다양한 밀도를 가진 데이터에 유효**합니다.
  
- **적용**:
  - 근처 이웃들과 비교하여, 특정 데이터 포인트가 얼마나 밀도가 낮은지 계산합니다.
  - **근접 이웃이 밀도가 낮은 경우** 이상치로 간주됩니다.

- **단점**:
  - **고차원 데이터**에서는 효과가 떨어질 수 있습니다.
  - **이웃 개수**에 민감할 수 있습니다.

---

## 4. DBSCAN (Density-Based Spatial Clustering of Applications with Noise)
**DBSCAN**은 **밀도 기반의 클러스터링** 알고리즘으로, 이상치 탐지에서도 활용됩니다. DBSCAN은 데이터가 밀집된 지역을 **클러스터**로 분류하고, 밀도가 낮은 지역을 **이상치**로 간주합니다.

- **작동 원리**:
  - **핵심 객체(core points)**와 **경계 객체(border points)**, **이상치(noise points)**로 데이터를 분류합니다.
  - 밀도가 낮은 데이터 포인트는 **이상치**로 간주됩니다.
  
- **특징**:
  - **비선형적**인 클러스터링을 지원하며, 이상치를 효과적으로 구별합니다.
  - **클러스터의 밀도**가 중요한 기준입니다.
  
- **적용**:
  - 밀도가 낮은 영역에 있는 데이터 포인트를 **이상치**로 판단합니다.
  - 다양한 밀도 환경에서도 잘 작동합니다.
  
- **단점**:
  - **고차원** 데이터에서는 성능이 떨어질 수 있습니다.
  - **핵심 객체와 이웃**의 기준인 **Epsilon** 값을 설정하는 데 민감합니다.

---

## 5. 시계열 데이터일 경우, Moving Average (MA) + Threshold
시계열 데이터에서 이상치를 탐지하기 위한 간단한 방법은 **이동 평균 (Moving Average, MA)**과 **임계값 (Threshold)**을 사용하는 것입니다. 이 방법은 시간에 따른 변화 패턴을 고려하여 이상치를 탐지합니다.

- **작동 원리**:
  - **이동 평균**을 계산하여, 데이터의 정상 범위(평균 범위)를 추정합니다.
  - **기준 임계값(threshold)**을 설정하여, 평균으로부터 벗어나는 점을 이상치로 간주합니다.

- **특징**:
  - 시계열 데이터의 **평균 흐름**을 추적하고, 급격한 변화를 **이상치**로 식별합니다.
  - **단기 변화**에 민감하게 반응할 수 있습니다.
  
- **적용**:
  - 시간의 흐름에 따라 변하는 **이동 평균**과 이를 기준으로 설정한 임계값을 비교하여 이상치를 탐지합니다.
  - 예를 들어, 현재 값이 **이동 평균 + threshold**를 벗어나는 경우 이상치로 간주됩니다.
  
- **단점**:
  - **동적 환경**에서는 기준을 설정하기 어려울 수 있습니다.
  - **긴 시간 범위**에서는 과거 데이터의 변화가 영향을 미칠 수 있습니다.

---

# 이상치 탐지 방법 요약

| **방법**                      | **주요 특징**                                                | **적합한 상황**                                               |
|-------------------------------|------------------------------------------------------------|---------------------------------------------------------------|
| **Mahalanobis Distance**       | 공분산을 고려한 다변량 이상치 탐지                         | 변수 간 상관 관계가 중요한 경우                                 |
| **Isolation Forest**           | 랜덤 트리 기반의 이상치 탐지 (앙상블)                       | 고차원 데이터에서 효과적, 빠른 계산                              |
| **Local Outlier Factor (LOF)** | 밀도 기반 이상치 탐지, 지역적 이상치 탐지                   | 밀도가 다른 데이터에서 이상치 탐지에 유용                      |
| **DBSCAN**                     | 밀도 기반 클러스터링, 이상치 탐지                          | 비선형 분포나 클러스터링을 기반으로 이상치 탐지                |
| **Moving Average + Threshold** | 시계열 데이터에서 평균과 임계값을 기반으로 이상치 탐지    | 시계열 데이터에서 급격한 변화 감지에 유용                      |

각 방법은 특정 상황에서 더 효과적일 수 있으므로, 데이터의 특성과 분석하려는 문제에 따라 적합한 방법을 선택하는 것이 중요합니다.

---
## 시계열 데이터 이상치 탐지: Moving Average + Threshold

### 1. 개요
시계열 데이터에서 **이상 데이터 탐지(Anomaly Detection)**는 데이터에서 예상치 못한 변동이나 패턴을 식별하는 중요한 작업입니다. 그 중 하나인 **Moving Average + Threshold** 방법은 이동 평균을 사용하여 시계열의 전반적인 추세를 파악하고, 이를 기준으로 이상치를 탐지하는 기법입니다.

### 2. Moving Average (이동 평균)
이동 평균은 시계열 데이터의 **최근 값들을 평균내어** 데이터의 변동성을 줄이고, 추세를 파악하는 데 유용한 방법입니다. 이동 평균에는 여러 종류가 있지만, 가장 많이 사용되는 두 가지는:

- **단순 이동 평균(SMA, Simple Moving Average)**: 주어진 기간 동안의 데이터 값들의 **단순 평균**을 계산합니다.
- **지수 가중 이동 평균(EMA, Exponential Moving Average)**: 최신 데이터에 더 많은 가중치를 두어 평균을 계산합니다.

#### 이동 평균 계산 공식 (SMA):

$$
SMA(t) = \frac{1}{n} \sum_{i=t-n+1}^{t} x_i
$$

여기서,
- `x_i`는 데이터 포인트,
- `n`은 이동 평균의 윈도우 크기,
- `t`는 현재 시간 인덱스입니다.

### 3. Threshold (임계값 설정)
이동 평균을 계산한 후, 데이터를 기준값(이동 평균)과 비교하여 이상치를 탐지합니다. 이상치를 탐지하는 방법은 일반적으로 **표준편차**를 이용해 **임계값(threshold)**을 설정하는 방식입니다.

#### 예시:
- **상한선** = 이동 평균 + 2 * 표준편차
- **하한선** = 이동 평균 - 2 * 표준편차

이 범위를 벗어나는 값은 이상치로 간주될 수 있습니다.

### 4. Moving Average + Threshold을 활용한 이상치 탐지 방법

#### 1) 이동 평균 계산:
- 먼저, 시계열 데이터에서 일정 기간(예: 10일)의 이동 평균을 계산합니다.

#### 2) 표준편차 계산:
- 이동 평균을 기준으로, 해당 기간의 **표준편차**를 구합니다. 표준편차는 데이터의 분포를 나타내며, 값이 크게 벗어날 경우 이상치로 간주할 수 있습니다.

#### 3) 임계값 설정:
- 이동 평균과 표준편차를 기반으로 **상한선**과 **하한선**을 설정합니다. 일반적으로 "이동 평균 ± 2배 표준편차"를 기준으로 설정합니다.

#### 4) 이상치 탐지:
- 각 데이터 포인트가 이동 평균에서 벗어난 정도를 계산하고, 설정된 임계값을 초과하면 이상치로 판단합니다.

### 5. 예시

이 기법을 시각적으로 설명하면 다음과 같습니다:

- **시계열 데이터**: 주어진 시간에 따라 변화하는 데이터입니다.
- **이동 평균**: 일정 기간 동안의 평균을 계산하여 데이터의 변동성을 줄입니다.
- **상한선/하한선**: 이동 평균 ± 2배 표준편차로 설정된 상한선과 하한선으로 이상치 범위를 정의합니다.
- **이상치**: 이 범위를 벗어난 값들이 이상치로 탐지됩니다.

#### 시각화:
1. 시계열 데이터와 이동 평균을 플로팅합니다.
2. 상한선과 하한선을 플로팅합니다.
3. 이동 평균을 벗어난 값을 빨간 점으로 표시하여 이상치를 강조합니다.

### 6. 장점 및 활용
- **장점**: 이동 평균과 표준편차를 사용하면 시계열 데이터의 **전반적인 추세**를 파악하고, **단기적인 변동**을 무시할 수 있어 이상치를 효과적으로 탐지할 수 있습니다.
- **활용 분야**:
  - **금융**: 주식 가격 변동 분석, 거래 신호 탐지
  - **IoT**: 센서 데이터의 이상 탐지
  - **웹 분석**: 트래픽 패턴에서 비정상적인 변화 탐지

### 7. 결론
**Moving Average + Threshold** 기법은 시계열 데이터에서 이상치를 효과적으로 탐지할 수 있는 방법입니다. 이 기법을 활용하여 데이터의 추세를 파악하고, 중요한 이상 변화를 조기에 탐지할 수 있습니다.

---
