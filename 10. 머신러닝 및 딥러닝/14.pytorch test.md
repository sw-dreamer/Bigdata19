pytorch 설정 
nvidia 버전이 1660이어서 cuda 11.8으로 다운을 하여 설정을 완료 해줍니다. 그 후 아래 명령어로 설치를 합니다.

```
pip install --no-cache-dir torch==2.1.1+cu118 torchvision==0.16.1+cu118 -f https://download.pytorch.org/whl/torch_stable.html
```
```
pip install numpy==1.23.5
```
아래 명령어로 확인을 합니다.
```
import torch


# CUDA(GPU) 사용 가능한지 확인 (True: GPU 사용 가능, False: 사용 불가)
gpu_available = torch.cuda.is_available()
print(f'GPU 사용 가능 여부: {gpu_available}')


if gpu_available:
    # 사용 가능한 GPU 개수 확인
    gpu_count = torch.cuda.device_count()
    print(f'사용 가능한 GPU 개수: {gpu_count}')


    # GPU 이름과 사양 확인
    for gpu_idx in range(gpu_count):
        gpu_name = torch.cuda.get_device_name(gpu_idx)
        print(f'GPU {gpu_idx}: {gpu_name}')


    # 현재 사용 중인 GPU 번호 확인
    current_gpu = torch.cuda.current_device()
    print(f'현재 사용 중인 GPU 번호: {current_gpu}')


else:
    print("GPU가 감지되지 않았습니다. CPU를 사용 중입니다.")
```
![image](https://github.com/user-attachments/assets/0b3f9352-b37d-4156-a1f7-ca100c93b7e7)

---
## 파이토치 탠서 생성
```
import torch
import numpy as np

# 넘파이 환경 설정 : 소수점 3자리까지 표시
np.set_printoptions(precision=3)

# 리스트 생성
a=[1,2,3]

# array 생성
b=np.array(
    [4,5,6]
    ,dtype=np.int32
)

# 리스트로 탠서 생성
t_a=torch.tensor(a)
print(t_a)

# array -> tensor
t_b=torch.from_numpy(b)
print(t_b)
```
![image](https://github.com/user-attachments/assets/0f2a378d-78a4-4d57-a771-b4f915760828)

```
# tensor create : one

import torch

t_ones=torch.ones(2,3) # 2,3은 2 rox x 3 col 로 하겠다는 의미

t_ones
```
![image](https://github.com/user-attachments/assets/6cee664a-285c-4c6a-9252-fea53041a70e)

```
t_ones=torch.zeros(2,3) # 2,3은 2 rox x 3 col 로 하겠다는 의미

t_ones
```
![image](https://github.com/user-attachments/assets/1e4c7a8a-5ecc-4e85-8254-f10f5204e7f6)

---
## 탠서 데이터 타입과 크기 조작

```
import torch

# 텐서는 기본적으로 다차원 배열을 의미하는데, 벡터나 행렬을 포함하는 더 일반적인 개념이라고 볼 수 있습니다. 
# torch.to() : 탠서를 다른 장치나 데이터 타입으로 이동할때 사용
tensor_ = torch.tensor([1, 2, 3])

print(f'tensor_ : {tensor_}')  # 저장되 있는 위치가 dram

# tensor 객체에서 'to' 메서드를 호출하여 GPU로 이동
tensor_gpu = tensor_.to('cuda')

print(f'tensor_gpu : {tensor_gpu}')
```
![image](https://github.com/user-attachments/assets/97e4dad2-1ac0-4d9e-bf43-7eea4388c13a)

```
import torch


# 텐서 전치하기
t=torch.rand(3,5)
t_tr=torch.transpose(t,0,1)
print(f't shape : {t.shape}, t_tr shape : {t_tr.shape}')
print(f't : {t}')
print(f't_tr : {t_tr}')
```
![image](https://github.com/user-attachments/assets/d732f154-53a0-4aac-898b-a7ec8a9a46e0)

```
# 텐서 크기 변경 : reshape
t=torch.zeros(30)
print(t)
t_reshape=t.reshape(5,6)
print(t_reshape)
```
![image](https://github.com/user-attachments/assets/4ead4551-81f4-4950-b92e-a3a6a8ba73fe)

```
# 불필요한 차원 삭제 : squeeze
t=torch.zeros(1,2,1,4,1) # 5차원이다
print(t)

print(f't shape : {t.shape}')
t_squeeze=t.squeeze(
    2
)
print(t_squeeze)
print(f't_squeeze shape : {t_squeeze.shape}')
```
![image](https://github.com/user-attachments/assets/97ff492d-edd1-4a45-9d2a-6fa23a12f571)

```
import numpy as np
import torch
# 차원 추가 : unsqueeze
t=torch.tensor(
    [1,2,3]
)
print(f't shape : {t.shape}')
print(t)

t_n=np.array(
    [1,2,3]
)
print(f't_n shape : {t_n.shape}')

# 텐서 차원 추가
u_t=t.unsqueeze(0) # 0 번째 차원 추가

print(f'u_t shape : {u_t.shape}')
print(u_t)
```
![image](https://github.com/user-attachments/assets/fdb43aa6-e925-4f73-aa8a-cd1e2c5dacd4)

```
# element-wise : 원소별 연산
# 원소별 곱셈
torch.manual_seed(1)
t1=2*torch.rand(5,2)-1
print(t1)
```
![image](https://github.com/user-attachments/assets/8142a826-0183-4378-8081-0133a4486200)

```
# element-wise : 원소별 연산
# 원소별 곱셈
torch.manual_seed(1)
t1=2*torch.rand(5,2)-1
print(t1)
t2=torch.normal(mean=0,std=1,size=(5,2))
print(t2)
t3=torch.multiply(t1,t2)
print(t3)
```
![image](https://github.com/user-attachments/assets/84d2c77b-ef38-46a7-895c-fc12a42f76c8)

```
# 행렬 곱 : dot product (내적)
t1=2*torch.rand(5,2)-1
print(t1.shape)
t2=torch.normal(mean=0,std=1,size=(5,2))
print(t2.shape)
print(t2.T.shape)
t5= torch.matmul(t1,t2.T)
print(t5)
```
![image](https://github.com/user-attachments/assets/a2d31c71-9f16-4ab6-90b1-cc81f9e80323)

