pytorch 설정 
nvidia 버전이 1660이어서 cuda 11.8으로 다운을 하여 설정을 완료 해줍니다. 그 후 아래 명령어로 설치를 합니다.

```
pip install --no-cache-dir torch==2.1.1+cu118 torchvision==0.16.1+cu118 -f https://download.pytorch.org/whl/torch_stable.html
```
```
pip install numpy==1.23.5
```
아래 명령어로 확인을 합니다.
```
import torch


# CUDA(GPU) 사용 가능한지 확인 (True: GPU 사용 가능, False: 사용 불가)
gpu_available = torch.cuda.is_available()
print(f'GPU 사용 가능 여부: {gpu_available}')


if gpu_available:
    # 사용 가능한 GPU 개수 확인
    gpu_count = torch.cuda.device_count()
    print(f'사용 가능한 GPU 개수: {gpu_count}')


    # GPU 이름과 사양 확인
    for gpu_idx in range(gpu_count):
        gpu_name = torch.cuda.get_device_name(gpu_idx)
        print(f'GPU {gpu_idx}: {gpu_name}')


    # 현재 사용 중인 GPU 번호 확인
    current_gpu = torch.cuda.current_device()
    print(f'현재 사용 중인 GPU 번호: {current_gpu}')


else:
    print("GPU가 감지되지 않았습니다. CPU를 사용 중입니다.")
```
![image](https://github.com/user-attachments/assets/0b3f9352-b37d-4156-a1f7-ca100c93b7e7)

---
## 파이토치 탠서 생성
```
import torch
import numpy as np

# 넘파이 환경 설정 : 소수점 3자리까지 표시
np.set_printoptions(precision=3)

# 리스트 생성
a=[1,2,3]

# array 생성
b=np.array(
    [4,5,6]
    ,dtype=np.int32
)

# 리스트로 탠서 생성
t_a=torch.tensor(a)
print(t_a)

# array -> tensor
t_b=torch.from_numpy(b)
print(t_b)
```
![image](https://github.com/user-attachments/assets/0f2a378d-78a4-4d57-a771-b4f915760828)

```
# tensor create : one

import torch

t_ones=torch.ones(2,3) # 2,3은 2 rox x 3 col 로 하겠다는 의미

t_ones
```
![image](https://github.com/user-attachments/assets/6cee664a-285c-4c6a-9252-fea53041a70e)

```
t_ones=torch.zeros(2,3) # 2,3은 2 rox x 3 col 로 하겠다는 의미

t_ones
```
![image](https://github.com/user-attachments/assets/1e4c7a8a-5ecc-4e85-8254-f10f5204e7f6)

---
## 탠서 데이터 타입과 크기 조작

```
import torch

# 텐서는 기본적으로 다차원 배열을 의미하는데, 벡터나 행렬을 포함하는 더 일반적인 개념이라고 볼 수 있습니다. 
# torch.to() : 탠서를 다른 장치나 데이터 타입으로 이동할때 사용
tensor_ = torch.tensor([1, 2, 3])

print(f'tensor_ : {tensor_}')  # 저장되 있는 위치가 dram

# tensor 객체에서 'to' 메서드를 호출하여 GPU로 이동
tensor_gpu = tensor_.to('cuda')

print(f'tensor_gpu : {tensor_gpu}')
```
![image](https://github.com/user-attachments/assets/97e4dad2-1ac0-4d9e-bf43-7eea4388c13a)

```
import torch


# 텐서 전치하기
t=torch.rand(3,5)
t_tr=torch.transpose(t,0,1)
print(f't shape : {t.shape}, t_tr shape : {t_tr.shape}')
print(f't : {t}')
print(f't_tr : {t_tr}')
```
![image](https://github.com/user-attachments/assets/d732f154-53a0-4aac-898b-a7ec8a9a46e0)

```
# 텐서 크기 변경 : reshape
t=torch.zeros(30)
print(t)
t_reshape=t.reshape(5,6)
print(t_reshape)
```
![image](https://github.com/user-attachments/assets/4ead4551-81f4-4950-b92e-a3a6a8ba73fe)

```
# 불필요한 차원 삭제 : squeeze
t=torch.zeros(1,2,1,4,1) # 5차원이다
print(t)

print(f't shape : {t.shape}')
t_squeeze=t.squeeze(
    2
)
print(t_squeeze)
print(f't_squeeze shape : {t_squeeze.shape}')
```
![image](https://github.com/user-attachments/assets/97ff492d-edd1-4a45-9d2a-6fa23a12f571)

```
import numpy as np
import torch
# 차원 추가 : unsqueeze
t=torch.tensor(
    [1,2,3]
)
print(f't shape : {t.shape}')
print(t)

t_n=np.array(
    [1,2,3]
)
print(f't_n shape : {t_n.shape}')

# 텐서 차원 추가
u_t=t.unsqueeze(0) # 0 번째 차원 추가

print(f'u_t shape : {u_t.shape}')
print(u_t)
```
![image](https://github.com/user-attachments/assets/fdb43aa6-e925-4f73-aa8a-cd1e2c5dacd4)

```
# element-wise : 원소별 연산
# 원소별 곱셈
torch.manual_seed(1)
t1=2*torch.rand(5,2)-1
print(t1)
```
![image](https://github.com/user-attachments/assets/8142a826-0183-4378-8081-0133a4486200)

```
# element-wise : 원소별 연산
# 원소별 곱셈
torch.manual_seed(1)
t1=2*torch.rand(5,2)-1
print(t1)
t2=torch.normal(mean=0,std=1,size=(5,2))
print(t2)
t3=torch.multiply(t1,t2)
print(t3)
```
![image](https://github.com/user-attachments/assets/84d2c77b-ef38-46a7-895c-fc12a42f76c8)

```
# 행렬 곱 : dot product (내적)
t1=2*torch.rand(5,2)-1
print(t1.shape)
t2=torch.normal(mean=0,std=1,size=(5,2))
print(t2.shape)
print(torch.transpose(t2,0,1))
t5= torch.matmul(t1,torch.transpose(t2,0,1))
print(t5)
print(t5.shape)
```
![image](https://github.com/user-attachments/assets/85629cf0-f892-4e75-8093-7e93b68d26e8)

---
## 파이토치 입력 파이프라인 구축

```
import torch
from torch.utils.data import DataLoader
t=torch.arange(6,dtype=torch.float32)
data_loader=DataLoader(t)

for item in data_loader:
    print(item)     
print('='*100)
data_loader=DataLoader(t,batch_size=3,drop_last=False)
for batch in data_loader:
    print(f'batch : {batch}')  
print('='*100)
data_loader=DataLoader(t,batch_size=3,drop_last=True)
for batch in data_loader:
    print(f'batch : {batch}')
```
![image](https://github.com/user-attachments/assets/6b443984-40d4-426f-aa8d-b48c7ae63f22)

```
import torch
from torch.utils.data import DataLoader
t=torch.arange(7,dtype=torch.float32)
data_loader=DataLoader(t)

for item in data_loader:
    print(item)     
print('='*100)
data_loader=DataLoader(t,batch_size=3,drop_last=False)
for batch in data_loader:
    print(f'batch : {batch}')  
print('='*100)
data_loader=DataLoader(t,batch_size=3,drop_last=True)
for batch in data_loader:
    print(f'batch : {batch}')
```
![image](https://github.com/user-attachments/assets/42300249-a7f5-4170-8865-f6770f4b0489)

```
from torch.utils.data import DataLoader,TensorDataset

# 텐서 데이터 준비
f=torch.randn(100,3)
print(f'텐서 구조 : {f.shape}')
for idx in range(len(f)):
    print(f[idx])
print('='*100)
labels=torch.randint(0,2,(100,))
print(f'labels shape: {labels.shape}')
print(f'lables :\n{labels}')
print('='*100)
# 텐서를 이용한 데이터셋을 생성
dataset=TensorDataset(f,labels)
print(dataset[0])
for idx in range(len(dataset)):
    print(f'{idx}번째 datset : {dataset[idx]}')
```
```
텐서 구조 : torch.Size([100, 3])
tensor([ 0.5965, -0.2248, -1.3013])
tensor([-0.1356,  1.3967, -0.2600])
tensor([-0.7876, -0.4009, -0.0892])
tensor([-0.1714,  1.3057,  0.0668])
tensor([ 0.6350, -2.1654,  1.3780])
tensor([-1.5427,  0.8177,  0.7472])
tensor([ 0.3395, -1.5277, -0.9528])
tensor([-0.1002, -0.6491,  0.0774])
tensor([ 0.9316,  1.1905, -0.0228])
tensor([ 0.9538, -1.4761, -1.5200])
tensor([-1.3992,  0.2526, -1.0721])
tensor([ 0.5617, -0.1813,  0.6479])
tensor([-1.3754, -1.0257, -1.9801])
tensor([ 0.2045, -0.7652,  2.1713])
tensor([-0.0788, -0.1179, -0.7340])
tensor([-0.3688, -0.2374, -2.5091])
tensor([1.2371, 0.2285, 0.0802])
tensor([-1.4025, -0.3743,  1.2562])
tensor([-0.4114, -1.1657, -0.3619])
tensor([ 0.0813, -0.2030,  1.7867])
tensor([-1.8098, -0.4974, -0.3413])
tensor([ 0.0882, -1.4530,  1.7697])
tensor([ 0.8969,  0.0173, -1.7575])
tensor([ 0.4261, -0.5796,  0.0505])
tensor([-0.1233, -0.4801, -0.1846])
tensor([-0.7148, -0.5029,  0.1585])
tensor([ 0.0535, -0.1655,  0.1797])
tensor([ 0.0186, -0.7067, -0.0158])
tensor([ 1.4318,  2.8418, -0.9929])
tensor([ 1.4532, -0.7069, -0.5511])
tensor([-0.3846,  2.2370,  1.1127])
tensor([-0.1012, -0.1209,  1.4870])
tensor([ 1.5411, -1.8538,  0.3008])
tensor([ 1.6506,  0.0892, -1.1751])
tensor([-1.2228, -0.7439,  1.8599])
tensor([1.1933, 1.0049, 2.3180])
tensor([-0.3098, -0.4866, -1.2543])
tensor([0.0805, 0.2112, 1.6861])
tensor([-0.9734,  0.7408, -0.7772])
tensor([-1.6399, -1.0774, -0.6151])
tensor([-0.3536,  1.4637, -1.0469])
tensor([-1.4068,  0.5273,  1.1568])
tensor([-0.8616,  0.5580,  0.6418])
tensor([-0.9873, -1.2825, -0.7874])
tensor([ 2.5974,  1.0973, -0.1721])
tensor([-0.7066, -0.5636,  0.7665])
tensor([ 2.6821,  0.1160, -0.8742])
tensor([ 0.2659, -1.9265, -0.3953])
tensor([-0.6433, -1.0689,  0.1209])
tensor([1.2312, 0.5676, 0.4610])
tensor([-1.1579, -1.8862, -0.2570])
tensor([0.7474, 1.0907, 0.0654])
tensor([-3.0565e-01,  1.0469e-03,  1.4465e+00])
tensor([0.0050, 1.6177, 0.4709])
tensor([ 1.2555, -0.3513,  0.0651])
tensor([1.2227, 0.5333, 0.5360])
tensor([ 0.8766,  0.6157, -1.9573])
tensor([-0.1894, -0.1300,  1.2484])
tensor([ 1.0727, -0.6915,  0.2683])
tensor([ 0.0778,  0.1452, -0.7557])
tensor([-2.1238,  0.4668,  0.0973])
tensor([0.9798, 0.3907, 0.4044])
tensor([-0.3112,  0.0349, -0.4879])
tensor([-0.3432,  0.0858, -0.2718])
tensor([ 1.5555, -1.0168,  1.3520])
tensor([1.2340, 1.2053, 1.0867])
tensor([ 1.2288, -0.3632, -2.0115])
tensor([ 0.5870,  1.5174, -1.0761])
tensor([-0.1530,  0.4114, -0.1329])
tensor([-1.3254,  0.7425,  1.6147])
tensor([0.6477, 0.5770, 0.6615])
tensor([-0.0672,  0.0892,  0.6578])
tensor([1.8993, 1.1570, 1.8926])
tensor([-0.6104, -0.9517,  0.4295])
tensor([ 0.3012, -0.6417,  0.4153])
tensor([ 1.0272, -0.0930,  0.7601])
tensor([ 0.7303,  0.3738, -1.0710])
tensor([-0.4233, -0.3168, -0.7196])
tensor([1.4817, 0.7792, 0.8348])
tensor([ 0.3894, -2.4872, -1.3875])
tensor([-0.1046, -0.3272, -0.3847])
tensor([-0.1513, -0.7124, -0.2907])
tensor([ 1.2453, -0.4189, -0.4971])
tensor([1.6850, 1.0114, 1.5273])
tensor([-0.7367,  0.2655, -0.0789])
tensor([-0.2676,  0.6184, -0.9178])
tensor([ 0.6649, -1.1846,  0.4253])
tensor([ 0.2321, -0.6932,  1.5286])
tensor([-1.1558,  1.0350,  0.6300])
tensor([-1.1531, -0.4977, -0.1588])
tensor([ 1.1468, -0.1172, -1.4771])
tensor([0.1248, 1.0104, 0.8202])
tensor([ 1.8008, -0.0524,  0.1364])
tensor([-1.2214, -0.5140, -0.4434])
tensor([-1.5338,  0.5087,  0.4622])
tensor([-1.2767, -2.5545, -0.3935])
tensor([0.2856, 1.1013, 0.0768])
tensor([-1.6010, -0.5405, -0.4149])
tensor([-0.3963, -0.4048,  0.5114])
tensor([-0.1092,  1.1605,  0.6342])
====================================================================================================
labels shape: torch.Size([100])
lables :
tensor([0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0])
====================================================================================================
(tensor([ 0.5965, -0.2248, -1.3013]), tensor(0))
0번째 datset : (tensor([ 0.5965, -0.2248, -1.3013]), tensor(0))
1번째 datset : (tensor([-0.1356,  1.3967, -0.2600]), tensor(0))
2번째 datset : (tensor([-0.7876, -0.4009, -0.0892]), tensor(0))
3번째 datset : (tensor([-0.1714,  1.3057,  0.0668]), tensor(0))
4번째 datset : (tensor([ 0.6350, -2.1654,  1.3780]), tensor(1))
5번째 datset : (tensor([-1.5427,  0.8177,  0.7472]), tensor(1))
6번째 datset : (tensor([ 0.3395, -1.5277, -0.9528]), tensor(1))
7번째 datset : (tensor([-0.1002, -0.6491,  0.0774]), tensor(1))
8번째 datset : (tensor([ 0.9316,  1.1905, -0.0228]), tensor(0))
9번째 datset : (tensor([ 0.9538, -1.4761, -1.5200]), tensor(0))
10번째 datset : (tensor([-1.3992,  0.2526, -1.0721]), tensor(0))
11번째 datset : (tensor([ 0.5617, -0.1813,  0.6479]), tensor(1))
12번째 datset : (tensor([-1.3754, -1.0257, -1.9801]), tensor(1))
13번째 datset : (tensor([ 0.2045, -0.7652,  2.1713]), tensor(0))
14번째 datset : (tensor([-0.0788, -0.1179, -0.7340]), tensor(0))
15번째 datset : (tensor([-0.3688, -0.2374, -2.5091]), tensor(0))
16번째 datset : (tensor([1.2371, 0.2285, 0.0802]), tensor(1))
17번째 datset : (tensor([-1.4025, -0.3743,  1.2562]), tensor(1))
18번째 datset : (tensor([-0.4114, -1.1657, -0.3619]), tensor(1))
19번째 datset : (tensor([ 0.0813, -0.2030,  1.7867]), tensor(1))
20번째 datset : (tensor([-1.8098, -0.4974, -0.3413]), tensor(0))
21번째 datset : (tensor([ 0.0882, -1.4530,  1.7697]), tensor(0))
22번째 datset : (tensor([ 0.8969,  0.0173, -1.7575]), tensor(1))
23번째 datset : (tensor([ 0.4261, -0.5796,  0.0505]), tensor(1))
24번째 datset : (tensor([-0.1233, -0.4801, -0.1846]), tensor(1))
25번째 datset : (tensor([-0.7148, -0.5029,  0.1585]), tensor(0))
26번째 datset : (tensor([ 0.0535, -0.1655,  0.1797]), tensor(1))
27번째 datset : (tensor([ 0.0186, -0.7067, -0.0158]), tensor(0))
28번째 datset : (tensor([ 1.4318,  2.8418, -0.9929]), tensor(1))
29번째 datset : (tensor([ 1.4532, -0.7069, -0.5511]), tensor(0))
30번째 datset : (tensor([-0.3846,  2.2370,  1.1127]), tensor(0))
31번째 datset : (tensor([-0.1012, -0.1209,  1.4870]), tensor(1))
32번째 datset : (tensor([ 1.5411, -1.8538,  0.3008]), tensor(1))
33번째 datset : (tensor([ 1.6506,  0.0892, -1.1751]), tensor(1))
34번째 datset : (tensor([-1.2228, -0.7439,  1.8599]), tensor(1))
35번째 datset : (tensor([1.1933, 1.0049, 2.3180]), tensor(1))
36번째 datset : (tensor([-0.3098, -0.4866, -1.2543]), tensor(1))
37번째 datset : (tensor([0.0805, 0.2112, 1.6861]), tensor(0))
38번째 datset : (tensor([-0.9734,  0.7408, -0.7772]), tensor(1))
39번째 datset : (tensor([-1.6399, -1.0774, -0.6151]), tensor(0))
40번째 datset : (tensor([-0.3536,  1.4637, -1.0469]), tensor(1))
41번째 datset : (tensor([-1.4068,  0.5273,  1.1568]), tensor(1))
42번째 datset : (tensor([-0.8616,  0.5580,  0.6418]), tensor(0))
43번째 datset : (tensor([-0.9873, -1.2825, -0.7874]), tensor(0))
44번째 datset : (tensor([ 2.5974,  1.0973, -0.1721]), tensor(1))
45번째 datset : (tensor([-0.7066, -0.5636,  0.7665]), tensor(0))
46번째 datset : (tensor([ 2.6821,  0.1160, -0.8742]), tensor(0))
47번째 datset : (tensor([ 0.2659, -1.9265, -0.3953]), tensor(1))
48번째 datset : (tensor([-0.6433, -1.0689,  0.1209]), tensor(1))
49번째 datset : (tensor([1.2312, 0.5676, 0.4610]), tensor(1))
50번째 datset : (tensor([-1.1579, -1.8862, -0.2570]), tensor(1))
51번째 datset : (tensor([0.7474, 1.0907, 0.0654]), tensor(1))
52번째 datset : (tensor([-3.0565e-01,  1.0469e-03,  1.4465e+00]), tensor(0))
53번째 datset : (tensor([0.0050, 1.6177, 0.4709]), tensor(1))
54번째 datset : (tensor([ 1.2555, -0.3513,  0.0651]), tensor(1))
55번째 datset : (tensor([1.2227, 0.5333, 0.5360]), tensor(0))
56번째 datset : (tensor([ 0.8766,  0.6157, -1.9573]), tensor(1))
57번째 datset : (tensor([-0.1894, -0.1300,  1.2484]), tensor(1))
58번째 datset : (tensor([ 1.0727, -0.6915,  0.2683]), tensor(0))
59번째 datset : (tensor([ 0.0778,  0.1452, -0.7557]), tensor(1))
60번째 datset : (tensor([-2.1238,  0.4668,  0.0973]), tensor(0))
61번째 datset : (tensor([0.9798, 0.3907, 0.4044]), tensor(0))
62번째 datset : (tensor([-0.3112,  0.0349, -0.4879]), tensor(0))
63번째 datset : (tensor([-0.3432,  0.0858, -0.2718]), tensor(0))
64번째 datset : (tensor([ 1.5555, -1.0168,  1.3520]), tensor(1))
65번째 datset : (tensor([1.2340, 1.2053, 1.0867]), tensor(0))
66번째 datset : (tensor([ 1.2288, -0.3632, -2.0115]), tensor(1))
67번째 datset : (tensor([ 0.5870,  1.5174, -1.0761]), tensor(1))
68번째 datset : (tensor([-0.1530,  0.4114, -0.1329]), tensor(0))
69번째 datset : (tensor([-1.3254,  0.7425,  1.6147]), tensor(0))
70번째 datset : (tensor([0.6477, 0.5770, 0.6615]), tensor(1))
71번째 datset : (tensor([-0.0672,  0.0892,  0.6578]), tensor(1))
72번째 datset : (tensor([1.8993, 1.1570, 1.8926]), tensor(1))
73번째 datset : (tensor([-0.6104, -0.9517,  0.4295]), tensor(0))
74번째 datset : (tensor([ 0.3012, -0.6417,  0.4153]), tensor(0))
75번째 datset : (tensor([ 1.0272, -0.0930,  0.7601]), tensor(0))
76번째 datset : (tensor([ 0.7303,  0.3738, -1.0710]), tensor(1))
77번째 datset : (tensor([-0.4233, -0.3168, -0.7196]), tensor(1))
78번째 datset : (tensor([1.4817, 0.7792, 0.8348]), tensor(1))
79번째 datset : (tensor([ 0.3894, -2.4872, -1.3875]), tensor(0))
80번째 datset : (tensor([-0.1046, -0.3272, -0.3847]), tensor(1))
81번째 datset : (tensor([-0.1513, -0.7124, -0.2907]), tensor(1))
82번째 datset : (tensor([ 1.2453, -0.4189, -0.4971]), tensor(0))
83번째 datset : (tensor([1.6850, 1.0114, 1.5273]), tensor(0))
84번째 datset : (tensor([-0.7367,  0.2655, -0.0789]), tensor(0))
85번째 datset : (tensor([-0.2676,  0.6184, -0.9178]), tensor(0))
86번째 datset : (tensor([ 0.6649, -1.1846,  0.4253]), tensor(1))
87번째 datset : (tensor([ 0.2321, -0.6932,  1.5286]), tensor(1))
88번째 datset : (tensor([-1.1558,  1.0350,  0.6300]), tensor(0))
89번째 datset : (tensor([-1.1531, -0.4977, -0.1588]), tensor(0))
90번째 datset : (tensor([ 1.1468, -0.1172, -1.4771]), tensor(0))
91번째 datset : (tensor([0.1248, 1.0104, 0.8202]), tensor(1))
92번째 datset : (tensor([ 1.8008, -0.0524,  0.1364]), tensor(1))
93번째 datset : (tensor([-1.2214, -0.5140, -0.4434]), tensor(0))
94번째 datset : (tensor([-1.5338,  0.5087,  0.4622]), tensor(1))
95번째 datset : (tensor([-1.2767, -2.5545, -0.3935]), tensor(1))
96번째 datset : (tensor([0.2856, 1.1013, 0.0768]), tensor(0))
97번째 datset : (tensor([-1.6010, -0.5405, -0.4149]), tensor(1))
98번째 datset : (tensor([-0.3963, -0.4048,  0.5114]), tensor(1))
99번째 datset : (tensor([-0.1092,  1.1605,  0.6342]), tensor(0))
```
```
from torch.utils.data import DataLoader,TensorDataset

# 텐서 데이터 준비
f=torch.randn(100,3)
# print(f'텐서 구조 : {f.shape}')
# for idx in range(len(f)):
    # print(f[idx])
# print('='*100)
labels=torch.randint(0,2,(100,))
# print(f'labels shape: {labels.shape}')
# print(f'lables :\n{labels}')
# print('='*100)
# 텐서를 이용한 데이터셋을 생성
dataset=TensorDataset(f,labels)
# print(dataset[0])
# for idx in range(len(dataset)):
    # print(f'{idx}번째 datset : {dataset[idx]}')
# 데이터로더 생성
data_loader=DataLoader(dataset=dataset,batch_size=16,shuffle=True)
for batch_features, batch_labels in data_loader:
    print(f'배치 사이즈 : {batch_features.size()}, 배치 라벨 사이즈 : {batch_labels.size()}')
```
![image](https://github.com/user-attachments/assets/95ce3db0-2cec-4a4d-9b9f-4e740cf3a361)
