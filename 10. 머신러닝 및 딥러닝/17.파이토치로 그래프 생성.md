# 🔢 PyTorch 계산 그래프 (Computational Graph)

PyTorch는 **동적 계산 그래프 (Dynamic Computational Graph)** 방식을 사용하여 유연하고 직관적인 모델 개발을 가능하게 합니다. 계산이 수행되는 즉시 그래프가 생성되며, 자동으로 미분을 수행할 수 있습니다.

## 📘 개념 요약

- **계산 그래프**: 연산(operations)과 변수(tensors)를 노드로 구성한 구조.
- **동적 그래프**: 연산이 실행될 때 실시간으로 그래프를 생성.
- **자동 미분**: `requires_grad=True`인 텐서는 연산 기록을 저장하여 `.backward()` 호출 시 미분 계산 가능.
- **연산 추적**: `.grad_fn` 속성을 통해 각 텐서가 어떤 연산을 통해 만들어졌는지 확인할 수 있음.

## ⚖️ 동적 그래프 vs 정적 그래프

| 항목             | 동적 계산 그래프 (PyTorch) | 정적 계산 그래프 (TensorFlow 1.x 등) |
|------------------|-----------------------------|----------------------------------------|
| 그래프 생성 시점 | 실행 중 (runtime)           | 실행 전 (compile time)                 |
| 유연성           | 매우 높음                   | 낮음                                   |
| 디버깅           | 직관적                      | 복잡함                                 |
| 대표 프레임워크  | PyTorch, Chainer            | TensorFlow 1.x, MXNet                  |

---
```
# 간단한 그래프 생성
import torch

def compute_z(a, b, c):
    r1 = torch.sub(a, b)
    r2 = torch.mul(r1, 2)
    z = torch.add(r2, c)
    return z

print(f'스칼라(단일값) 입력 : {compute_z(torch.tensor(1), torch.tensor(2), torch.tensor(3))}')
print(f'랭크 1 입력 : {compute_z(torch.tensor([1]), torch.tensor([2]), torch.tensor([3]))}')
print(f'랭크 2 입력 : {compute_z(torch.tensor([[1]]), torch.tensor([[2]]), torch.tensor([[3]]))}')
```
![image](https://github.com/user-attachments/assets/bc623808-8c3b-4fba-8df3-4b8cdc86c5ef)

```
from IPython.display import Image
Image(url='https://raw.githubusercontent.com/rickiepark/ml-with-pytorch/main/ch13/figures/13_01.png', width=400)
```
![image](https://github.com/user-attachments/assets/a6947987-4f87-4eed-87fa-b8c544daaca8)

---
## 모델 파라미터를 저장하고 업데이트하기 위한 탠서 객체

| **Layer**         | **Activation Function** | **Initialization**   |
|-------------------|-------------------------|----------------------|
| Linear            | Sigmoid                 | Xavier               |
| Linear            | ReLU                    | He                   |
| CNN               | ReLU                    | He                   |
| LSTM              | Sigmoid                 | Xavier               |

```
import torch
import torch.nn as nn

model=nn.Sequential(
    nn.Linear(4,16) # 4 : input_size, 16 : hidden_size
    , nn.ReLU()
    ,nn.Linear(16,32) # hidden_size :16, hidden_size=32
    ,nn.ReLU()
)
model
```
![image](https://github.com/user-attachments/assets/9340ffb8-9647-4454-b7d3-84485fe86b36)

```
import torch
import torch.nn as nn

class Model(nn.Module):
    def __init__(self, input_size,hidden_size):
        super().__init__()
        self.layer1=nn.Linear(input_size,hidden_size)
        self.layer2=nn.Linear(hidden_size,hidden_size)
    def forward(self,x):
        x=self.layer1(x)
        x=nn.ReLU(x)
        x=self.layer2(x)
        x=nn.ReLU()
        return x
        
        
model=nn.Sequential(
    nn.Linear(4,16) # 4 : input_size, 16 : hidden_size
    , nn.ReLU()
    ,nn.Linear(16,32) # hidden_size :16, hidden_size=32
    ,nn.ReLU()
)
model
```
![image](https://github.com/user-attachments/assets/55ed987b-07b4-4623-a43e-194c8e279fb2)
