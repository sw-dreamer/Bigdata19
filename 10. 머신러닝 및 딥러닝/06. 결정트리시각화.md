## 결정트리시각화

```
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

# DecisionTree Classifier 생성
dt_clf = DecisionTreeClassifier(random_state=156, max_depth=3)

# 붓꽃 데이터를 로딩하고, 학습과 테스트 데이터 셋으로 분리
iris_data = load_iris()
X_train , X_test , y_train , y_test = train_test_split(
    iris_data.data
    , iris_data.target
    , test_size=0.2
    , random_state=11)

# DecisionTreeClassifer 학습. 
dt_clf.fit(X_train , y_train)
print(f'DecisionTreeClassifer 학습\n {dt_clf.fit(X_train , y_train)}')
```
![image](https://github.com/user-attachments/assets/9fe0d92b-e50c-4be0-aa38-98a5fc7b565f)

```
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

# DecisionTree Classifier 생성
dt_clf = DecisionTreeClassifier(random_state=156, max_depth=3)

# 붓꽃 데이터를 로딩하고, 학습과 테스트 데이터 셋으로 분리
iris_data = load_iris()
X_train , X_test , y_train , y_test = train_test_split(
    iris_data.data
    , iris_data.target
    , test_size=0.2
    , random_state=11)

# DecisionTreeClassifer 학습. 
dt_clf.fit(X_train , y_train)
# print(f'DecisionTreeClassifer 학습\n{dt_clf.fit(X_train , y_train)}')

from sklearn.tree import export_graphviz

# export_graphviz()의 호출 결과로 out_file로 지정된 tree.dot 파일을 생성함.
export_graphviz(
    dt_clf, out_file="./tree.dot"
    , class_names=iris_data.target_names
    ,feature_names = iris_data.feature_names
    , impurity=True
    , filled=True
)
```
![image](https://github.com/user-attachments/assets/134a596c-bbcb-4531-bb6c-8fdbda03608f)

```
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

# DecisionTree Classifier 생성
dt_clf = DecisionTreeClassifier(random_state=156, max_depth=3)

# 붓꽃 데이터를 로딩하고, 학습과 테스트 데이터 셋으로 분리
iris_data = load_iris()
X_train , X_test , y_train , y_test = train_test_split(
    iris_data.data
    , iris_data.target
    , test_size=0.2
    , random_state=11)

# DecisionTreeClassifer 학습. 
dt_clf.fit(X_train , y_train)
# print(f'DecisionTreeClassifer 학습\n{dt_clf.fit(X_train , y_train)}')

from sklearn.tree import export_graphviz

# export_graphviz()의 호출 결과로 out_file로 지정된 tree.dot 파일을 생성함.
export_graphviz(
    dt_clf, out_file="./tree.dot"
    , class_names=iris_data.target_names
    ,feature_names = iris_data.feature_names
    , impurity=True
    , filled=True
)

# 피처 중요도 시각화
# 피처 중요도 파악
import seaborn as sns
import numpy as np
# %matplotlib inline


# feature importance 추출
print("Feature importances:\n{0}".format(np.round(dt_clf.feature_importances_, 3)))


# feature별 importance 매핑
for name, value in zip(iris_data.feature_names , dt_clf.feature_importances_):
    print('{0} : {1:.3f}'.format(name, value))


# feature importance를 column 별로 시각화 하기
sns.barplot(x=dt_clf.feature_importances_ , y=iris_data.feature_names)
```
![image](https://github.com/user-attachments/assets/f030ae6a-c5f1-4233-a0cf-6c1f61306135)

![image](https://github.com/user-attachments/assets/e7e43d0a-5cf5-4323-8002-6104a5ac4f66)

```
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

# DecisionTree Classifier 생성
dt_clf = DecisionTreeClassifier(random_state=156, max_depth=3)

# 붓꽃 데이터를 로딩하고, 학습과 테스트 데이터 셋으로 분리
iris_data = load_iris()
X_train , X_test , y_train , y_test = train_test_split(
    iris_data.data
    , iris_data.target
    , test_size=0.2
    , random_state=11)

# DecisionTreeClassifer 학습. 
dt_clf.fit(X_train , y_train)
# print(f'DecisionTreeClassifer 학습\n{dt_clf.fit(X_train , y_train)}')

from sklearn.tree import export_graphviz

# export_graphviz()의 호출 결과로 out_file로 지정된 tree.dot 파일을 생성함.
export_graphviz(
    dt_clf, out_file="./tree.dot"
    , class_names=iris_data.target_names
    ,feature_names = iris_data.feature_names
    , impurity=True
    , filled=True
)

# 피처 중요도 시각화
# 피처 중요도 파악
import seaborn as sns
import numpy as np
# %matplotlib inline


# feature importance 추출
print("Feature importances:\n{0}".format(np.round(dt_clf.feature_importances_, 3)))


# feature별 importance 매핑
for name, value in zip(iris_data.feature_names , dt_clf.feature_importances_):
    print('{0} : {1:.3f}'.format(name, value))


# feature importance를 column 별로 시각화 하기
sns.barplot(x=dt_clf.feature_importances_ , y=iris_data.feature_names)

# 과적합 처리
# 데이터 생성
from sklearn.datasets import make_classification
import matplotlib.pyplot as plt
%matplotlib inline


plt.title("3 Class values with 2 Features Sample data creation")


# 2차원 시각화를 위해서 feature는 2개, 결정값 클래스는 3가지 유형의 classification 샘플 데이터 생성.
X_features, y_labels = make_classification(n_features=2, n_redundant=0, n_informative=2,
                             n_classes=3, n_clusters_per_class=1,random_state=0)


# plot 형태로 2개의 feature로 2차원 좌표 시각화, 각 클래스값은 다른 색깔로 표시됨.
plt.scatter(X_features[:, 0], X_features[:, 1], marker='o', c=y_labels, s=25, cmap='rainbow', edgecolor='k')
plt.show()
```
![image](https://github.com/user-attachments/assets/35f72173-6a25-4ed6-bdd7-eeb5f8b05677)

![image](https://github.com/user-attachments/assets/975b5d7c-b2eb-4c02-a064-4a519fbd9bff)
