# CelebA 데이터셋

```
import torch
print(f"CUDA available: {torch.cuda.is_available()}")
print(f"PyTorch version: {torch.__version__}")
print(f"CUDA version: {torch.version.cuda if torch.cuda.is_available() else 'Not available'}")
```
```
CUDA available: True
PyTorch version: 2.7.0+cu118
CUDA version: 11.8
```
```
import torch
import torchvision
import numpy as np
import os

image_path = './celeba'

celeba_train_dataset = torchvision.datasets.CelebA(
    image_path,
    split='train',
    target_type='attr',
    download=True
)

celeba_valid_dataset = torchvision.datasets.CelebA(
    image_path,
    split='valid',
    target_type='attr',
    download=True
)

celeba_test_dataset = torchvision.datasets.CelebA(
    image_path,
    split='test',
    target_type='attr',
    download=True
)

print(f'훈련세트 : {len(celeba_train_dataset)}')
print(f'검증세트 : {len(celeba_valid_dataset)}')
print(f'테스트세트 : {len(celeba_test_dataset)}')
```
![image](https://github.com/user-attachments/assets/4ca601af-7d22-4397-9fbe-64dc1a3b39d3)

```
from torchvision import transforms
import matplotlib.pyplot as plt


## 다섯 개 샘플


fig = plt.figure(figsize=(16, 8.5))


## 1열: 바운딩 박스로 자르기
ax = fig.add_subplot(2, 5, 1)
img, attr = celeba_train_dataset[0]
ax.set_title('Crop to a \nbounding-box', size=15)
ax.imshow(img)
ax = fig.add_subplot(2, 5, 6)
img_cropped = transforms.functional.crop(img, 50, 20, 128, 128)
ax.imshow(img_cropped)


plt.show()
```
![image](https://github.com/user-attachments/assets/3db31936-8641-47fa-bacc-e9ef8d6c8bad)

```
import matplotlib.pyplot as plt
import torchvision.transforms as transforms

# 데이터셋에서 이미지를 불러옵니다.
img, attr = celeba_train_dataset[1]

# 새로운 figure 객체를 생성합니다.
fig = plt.figure(figsize=(10, 6))

# 첫 번째 서브플롯: 원본 이미지
ax = fig.add_subplot(2, 5, 2)
ax.set_title('Original Image', size=15)
ax.imshow(img)

# 두 번째 서브플롯: 수평 뒤집기한 이미지
ax = fig.add_subplot(2, 5, 7)
img_flipped = transforms.functional.hflip(img)
ax.set_title('Flip (horizontal)', size=15)
ax.imshow(img_flipped)

plt.show()
```
![image](https://github.com/user-attachments/assets/782d5f6e-f6bd-4bbe-b660-afbd14ef1ce2)

```
import matplotlib.pyplot as plt
import torchvision.transforms as transforms

# fig 객체 생성 (그래프를 2x5의 그리드로 설정)
fig = plt.figure(figsize=(15, 6))

## 3열: 대비 조정
ax = fig.add_subplot(2, 5, 3)
img, attr = celeba_train_dataset[2]
ax.set_title('Adjust constrast', size=15)
ax.imshow(img)
ax = fig.add_subplot(2, 5, 8)
img_adj_contrast = transforms.functional.adjust_contrast(img, contrast_factor=2)
ax.imshow(img_adj_contrast)


## 4열: 명도 조정
ax = fig.add_subplot(2, 5, 4)
img, attr = celeba_train_dataset[3]
ax.set_title('Adjust brightness', size=15)
ax.imshow(img)
ax = fig.add_subplot(2, 5, 9)
img_adj_brightness = transforms.functional.adjust_brightness(img, brightness_factor=1.3)
ax.imshow(img_adj_brightness)


## 5열: 이미지 중앙 자르기
ax = fig.add_subplot(2, 5, 5)
img, attr = celeba_train_dataset[4]
ax.set_title('Center crop\nand resize', size=15)
ax.imshow(img)
ax = fig.add_subplot(2, 5, 10)
img_center_crop = transforms.functional.center_crop(img, [0.7*218, 0.7*178])
img_resized = transforms.functional.resize(img_center_crop, size=(218, 178))
ax.imshow(img_resized)

plt.show()
```
![image](https://github.com/user-attachments/assets/341aa3da-9ac3-4253-a60e-e42b752caf27)

```
import torch
torch.manual_seed(1)

fig = plt.figure(figsize=(14, 12))

for i, (img, attr) in enumerate(celeba_train_dataset):
    ax = fig.add_subplot(3, 4, i*4+1)
    ax.imshow(img)
    if i == 0:
        ax.set_title('Orig.', size=15)


    ax = fig.add_subplot(3, 4, i*4+2)
    img_transform = transforms.Compose([transforms.RandomCrop([178, 178])])
    img_cropped = img_transform(img)
    ax.imshow(img_cropped)
    if i == 0:
        ax.set_title('Step 1: Random crop', size=15)


    ax = fig.add_subplot(3, 4, i*4+3)
    img_transform = transforms.Compose([transforms.RandomHorizontalFlip()])
    img_flip = img_transform(img_cropped)
    ax.imshow(img_flip)
    if i == 0:
        ax.set_title('Step 2: Random flip', size=15)


    ax = fig.add_subplot(3, 4, i*4+4)
    img_resized = transforms.functional.resize(img_flip, size=(128, 128))
    ax.imshow(img_resized)
    if i == 0:
        ax.set_title('Step 3: Resize', size=15)


    if i == 2:
        break


# plt.savefig('figures/14_15.png', dpi=300)
plt.show()
```
![image](https://github.com/user-attachments/assets/008fbb69-8502-4edc-ac2e-d789c78239bf)

```
get_smile = lambda attr: attr[31]


transform_train = transforms.Compose([
    transforms.RandomCrop([178, 178]),
    transforms.RandomHorizontalFlip(),
    transforms.Resize([64, 64]),
    transforms.ToTensor(),
])


transform = transforms.Compose([
    transforms.CenterCrop([178, 178]),
    transforms.Resize([64, 64]),
    transforms.ToTensor()
,])

from torch.utils.data import DataLoader


celeba_train_dataset = torchvision.datasets.CelebA(image_path,
                                                   split='train',
                                                   target_type='attr',
                                                   download=False,
                                                   transform=transform_train,
                                                   target_transform=get_smile)


torch.manual_seed(1)
data_loader = DataLoader(celeba_train_dataset, batch_size=2)


fig = plt.figure(figsize=(15, 6))


num_epochs = 5
for j in range(num_epochs):
    img_batch, label_batch = next(iter(data_loader))
    img = img_batch[0]
    ax = fig.add_subplot(2, 5, j + 1)
    ax.set_xticks([])
    ax.set_yticks([])
    ax.set_title(f'Epoch {j}:', size=15)
    ax.imshow(img.permute(1, 2, 0))


    img = img_batch[1]
    ax = fig.add_subplot(2, 5, j + 6)
    ax.set_xticks([])
    ax.set_yticks([])
    ax.imshow(img.permute(1, 2, 0))




#plt.savefig('figures/14_16.png', dpi=300)
plt.show()
```
![image](https://github.com/user-attachments/assets/05db7920-8ad8-4e9f-9850-12476ae5c6ee)

```
# valid, test DataLoader
celeba_valid_dataset = torchvision.datasets.CelebA(image_path,
                                                   split='valid',
                                                   target_type='attr',
                                                   download=False,
                                                   transform=transform,
                                                   target_transform=get_smile)


celeba_test_dataset = torchvision.datasets.CelebA(image_path,
                                                   split='test',
                                                   target_type='attr',
                                                   download=False,
                                                   transform=transform,
                                                   target_transform=get_smile)


from torch.utils.data import Subset
celeba_train_dataset = Subset(celeba_train_dataset, torch.arange(16000))
celeba_valid_dataset = Subset(celeba_valid_dataset, torch.arange(1000))


print('훈련 세트:', len(celeba_train_dataset))
print('검증 세트:', len(celeba_valid_dataset))
```
![image](https://github.com/user-attachments/assets/7bcf9e80-602b-420c-82f9-4e658fc70ce7)

```
batch_size = 32

torch.manual_seed(1)
train_dl = DataLoader(celeba_train_dataset, batch_size, shuffle=True)
valid_dl = DataLoader(celeba_valid_dataset, batch_size, shuffle=False)
test_dl = DataLoader(celeba_test_dataset, batch_size, shuffle=False)
```
