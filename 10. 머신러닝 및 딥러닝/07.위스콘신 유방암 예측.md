## XGboost

```
import xgboost as xgb
from xgboost import XGBClassifier

print(xgb.__version__)
```
![image](https://github.com/user-attachments/assets/64e15a03-ad4d-4735-8d71-f1a33fb5ca1c)

```
import xgboost as xgb
from xgboost import XGBClassifier
from xgboost import plot_importance
import pandas as pd
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

dataset=load_breast_cancer()
X_features=dataset.data
y_label=dataset.target
cancer_dr=pd.DataFrame(
    data=X_features
    ,columns=dataset.feature_names
)
cancer_dr['target']=y_label
cancer_dr.head(3)
```
![image](https://github.com/user-attachments/assets/f21e1815-807a-4655-a58e-dfd7af66750b)

```
import xgboost as xgb
from xgboost import XGBClassifier
from xgboost import plot_importance
import pandas as pd
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

dataset=load_breast_cancer()
X_features=dataset.data
y_label=dataset.target
cancer_dr=pd.DataFrame(
    data=X_features
    ,columns=dataset.feature_names
)
cancer_dr['target']=y_label
print(dataset.target_names) # malignant(악성) :0,benign(양성) :1
```
![image](https://github.com/user-attachments/assets/0284842c-cecb-4b25-8267-9722fe044ef6)

```
import xgboost as xgb
from xgboost import XGBClassifier
from xgboost import plot_importance
import pandas as pd
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

dataset=load_breast_cancer()
X_features=dataset.data
y_label=dataset.target
cancer_dr=pd.DataFrame(
    data=X_features
    ,columns=dataset.feature_names
)
cancer_dr['target']=y_label
print(f"target의 분포 :\n{cancer_dr['target'].value_counts()}")
```
![image](https://github.com/user-attachments/assets/27b1320a-79d8-4203-85ff-00e31d32b978)

```
import xgboost as xgb
from xgboost import XGBClassifier
from xgboost import plot_importance
import pandas as pd
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

dataset=load_breast_cancer()
X_features=dataset.data
y_label=dataset.target
cancer_dr=pd.DataFrame(
    data=X_features
    ,columns=dataset.feature_names
)
cancer_dr['target']=y_label

# cancer_df에서 feature용 DataFrame과 label용 Series 객체 추출 : 위 데이터프레임을 데이터와 레이블(target) 분리
# 맨 마지막 컬럼이 Label임
# Feature용 DataFrame은 cancer_df의 첫번째 칼럼에서 맨 마지막 두번째 칼럼까지를 :-1로 슬라이싱으로 추출
X_features=cancer_dr.iloc[:,:-1] # 데이터 추출
y_label=cancer_dr.iloc[:,-1] # 레이블 추출

# 전체 데이터 중 80% 학습용 데이터, 20%는 테스트용 데이터 추출
X_train,X_test,y_train,y_test=train_test_split(
    X_features
    ,y_label
    ,train_size=0.8
    ,random_state=156
)
# xgboost : 검증을 한다 => 조기 종료
# 검증하려면 데이터 필요
# 위에서 만든 X_train, y_train을 다시 쪼개서 90%는 학습과 10%는 검즘용 데이터로 분리
X_tr,X_val,y_tr,y_val=train_test_split(
    X_train
    ,y_train
    ,test_size=0.1
    ,random_state=156
)
print(f'X train shape : {X_train.shape}')
print(f'X test shape : {X_test.shape}')

print(f'X tr shape : { X_tr.shape}')
print(f'X val shape : { X_val.shape}')


```
![image](https://github.com/user-attachments/assets/5b8ce190-71e5-4d37-b7a8-39ce5de36da6)

```
import xgboost as xgb
from xgboost import XGBClassifier
from xgboost import plot_importance
import pandas as pd
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

dataset=load_breast_cancer()
X_features=dataset.data
y_label=dataset.target
cancer_dr=pd.DataFrame(
    data=X_features
    ,columns=dataset.feature_names
)
cancer_dr['target']=y_label

# cancer_df에서 feature용 DataFrame과 label용 Series 객체 추출 : 위 데이터프레임을 데이터와 레이블(target) 분리
# 맨 마지막 컬럼이 Label임
# Feature용 DataFrame은 cancer_df의 첫번째 칼럼에서 맨 마지막 두번째 칼럼까지를 :-1로 슬라이싱으로 추출
X_features=cancer_dr.iloc[:,:-1] # 데이터 추출
y_label=cancer_dr.iloc[:,-1] # 레이블 추출

# 전체 데이터 중 80% 학습용 데이터, 20%는 테스트용 데이터 추출
X_train,X_test,y_train,y_test=train_test_split(
    X_features
    ,y_label
    ,train_size=0.8
    ,random_state=156
)
# xgboost : 검증을 한다 => 조기 종료
# 검증하려면 데이터 필요
# 위에서 만든 X_train, y_train을 다시 쪼개서 90%는 학습과 10%는 검즘용 데이터로 분리
X_tr,X_val,y_tr,y_val=train_test_split(
    X_train
    ,y_train
    ,test_size=0.1
    ,random_state=156
)

# 파이썬 기반 XGBoost 사용 : array 바로 사용 안됨 : 전용 데이터 타입을 사용
# array를 전용 데이터 타입으로 변환 함수 : Dmatrix()
# 학습, 검증, 테스트용 DMatrix를 생성
# 1. 학습데이터 변환
dtr=xgb.DMatrix(data=X_tr,label=y_tr)
# 2. 검증데이터 변환
dval=xgb.DMatrix(data=X_val,label=y_val)
dtest=xgb.DMatrix(data=X_test,label=y_test)
print(f'dtr : {dtr}')
print(f'dval : {dval}')
print(f'dttest : {dtest}')
```
![image](https://github.com/user-attachments/assets/09838836-f4ae-4bd5-b015-ef395b080407)

```
import xgboost as xgb
from xgboost import XGBClassifier
from xgboost import plot_importance
import pandas as pd
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

dataset=load_breast_cancer()
X_features=dataset.data
y_label=dataset.target
cancer_dr=pd.DataFrame(
    data=X_features
    ,columns=dataset.feature_names
)
cancer_dr['target']=y_label

# cancer_df에서 feature용 DataFrame과 label용 Series 객체 추출 : 위 데이터프레임을 데이터와 레이블(target) 분리
# 맨 마지막 컬럼이 Label임
# Feature용 DataFrame은 cancer_df의 첫번째 칼럼에서 맨 마지막 두번째 칼럼까지를 :-1로 슬라이싱으로 추출
X_features=cancer_dr.iloc[:,:-1] # 데이터 추출
y_label=cancer_dr.iloc[:,-1] # 레이블 추출

# 전체 데이터 중 80% 학습용 데이터, 20%는 테스트용 데이터 추출
X_train,X_test,y_train,y_test=train_test_split(
    X_features
    ,y_label
    ,train_size=0.8
    ,random_state=156
)
# xgboost : 검증을 한다 => 조기 종료
# 검증하려면 데이터 필요
# 위에서 만든 X_train, y_train을 다시 쪼개서 90%는 학습과 10%는 검즘용 데이터로 분리
X_tr,X_val,y_tr,y_val=train_test_split(
    X_train
    ,y_train
    ,test_size=0.1
    ,random_state=156
)

# 파이썬 기반 XGBoost 사용 : array 바로 사용 안됨 : 전용 데이터 타입을 사용
# array를 전용 데이터 타입으로 변환 함수 : DMatrix()
# 학습, 검증, 테스트용 DMatrix를 생성
# 1. 학습데이터 변환
dtr=xgb.DMatrix(data=X_tr,label=y_tr)
# 2. 검증데이터 변환
dval=xgb.DMatrix(data=X_val,label=y_val)
# 3. 테스트데이터 변환
dtest=xgb.DMatrix(data=X_test,label=y_test)

print(f'dtr : {dtr}')
print(f'dval : {dval}')
print(f'dttest : {dtest}')
```
![image](https://github.com/user-attachments/assets/108b4861-db9b-4190-9c1b-d6ae2ae6f0e6)

```
import xgboost as xgb
from xgboost import XGBClassifier
from xgboost import plot_importance
import pandas as pd
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

dataset=load_breast_cancer()
X_features=dataset.data
y_label=dataset.target
cancer_dr=pd.DataFrame(
    data=X_features
    ,columns=dataset.feature_names
)
cancer_dr['target']=y_label

# cancer_df에서 feature용 DataFrame과 label용 Series 객체 추출 : 위 데이터프레임을 데이터와 레이블(target) 분리
# 맨 마지막 컬럼이 Label임
# Feature용 DataFrame은 cancer_df의 첫번째 칼럼에서 맨 마지막 두번째 칼럼까지를 :-1로 슬라이싱으로 추출
X_features=cancer_dr.iloc[:,:-1] # 데이터 추출
y_label=cancer_dr.iloc[:,-1] # 레이블 추출

# 전체 데이터 중 80% 학습용 데이터, 20%는 테스트용 데이터 추출
X_train,X_test,y_train,y_test=train_test_split(
    X_features
    ,y_label
    ,train_size=0.8
    ,random_state=156
)
# xgboost : 검증을 한다 => 조기 종료
# 검증하려면 데이터 필요
# 위에서 만든 X_train, y_train을 다시 쪼개서 90%는 학습과 10%는 검즘용 데이터로 분리
X_tr,X_val,y_tr,y_val=train_test_split(
    X_train
    ,y_train
    ,test_size=0.1
    ,random_state=156
)

# 파이썬 기반 XGBoost 사용 : array 바로 사용 안됨 : 전용 데이터 타입을 사용
# array를 전용 데이터 타입으로 변환 함수 : Dmatrix()
# 학습, 검증, 테스트용 DMatrix를 생성
# 1. 학습데이터 변환
dtr=xgb.DMatrix(data=X_tr,label=y_tr)
# 2. 검증데이터 변환
dval=xgb.DMatrix(data=X_val,label=y_val)
# 3. 테스트데이터 변환
dtest=xgb.DMatrix(data=X_test,label=y_test)

params = {
    'max_depth': 3,  # 트리의 최대 깊이 (값이 클수록 모델이 복잡해지며 과적합 위험 증가)
    'eta': 0.05,  # 학습률 (값이 작을수록 학습 속도가 느려지지만 일반화 성능이 향상될 가능성이 있음)
    'objective': 'binary:logistic',  # 이진 분류 문제를 위한 로지스틱 회귀 설정
    'eval_metric': 'logloss'  # 평가 지표로 로그 손실(log loss) 사용 (값이 작을수록 성능이 좋음)
}

num_rounds = 400  # 부스팅 반복 횟수 (트리를 몇 개 생성할 것인지 결정)

# 학습 데이터 셋(dtr)은 'train', 평가 데이터 셋(dval)은 'eval'로 명기하여 학습 진행 상황을 모니터링
eval_list = [(dtr, 'train'), (dval, 'eval')]

# XGBoost 모델 학습
xgb_model = xgb.train(
    params=params,  # XGBoost 하이퍼파라미터 설정
    dtrain=dtr,  # 학습 데이터 셋
    num_boost_round=num_rounds,  # 부스팅 반복 횟수
    early_stopping_rounds=50,  # 50회 연속 개선이 없으면 학습 조기 종료 (과적합 방지)
    evals=eval_list  # 학습 과정에서 평가할 데이터 셋 목록 (학습 데이터와 검증 데이터)
)

xgb_model
```
![image](https://github.com/user-attachments/assets/237ffb42-8c45-4189-ad27-93515b74f6f2)

```
import xgboost as xgb
from xgboost import XGBClassifier
from xgboost import plot_importance
import pandas as pd
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

dataset=load_breast_cancer()
X_features=dataset.data
y_label=dataset.target
cancer_dr=pd.DataFrame(
    data=X_features
    ,columns=dataset.feature_names
)
cancer_dr['target']=y_label

# cancer_df에서 feature용 DataFrame과 label용 Series 객체 추출 : 위 데이터프레임을 데이터와 레이블(target) 분리
# 맨 마지막 컬럼이 Label임
# Feature용 DataFrame은 cancer_df의 첫번째 칼럼에서 맨 마지막 두번째 칼럼까지를 :-1로 슬라이싱으로 추출
X_features=cancer_dr.iloc[:,:-1] # 데이터 추출
y_label=cancer_dr.iloc[:,-1] # 레이블 추출

# 전체 데이터 중 80% 학습용 데이터, 20%는 테스트용 데이터 추출
X_train,X_test,y_train,y_test=train_test_split(
    X_features
    ,y_label
    ,train_size=0.8
    ,random_state=156
)
# xgboost : 검증을 한다 => 조기 종료
# 검증하려면 데이터 필요
# 위에서 만든 X_train, y_train을 다시 쪼개서 90%는 학습과 10%는 검즘용 데이터로 분리
X_tr,X_val,y_tr,y_val=train_test_split(
    X_train
    ,y_train
    ,test_size=0.1
    ,random_state=156
)

# 파이썬 기반 XGBoost 사용 : array 바로 사용 안됨 : 전용 데이터 타입을 사용
# array를 전용 데이터 타입으로 변환 함수 : Dmatrix()
# 학습, 검증, 테스트용 DMatrix를 생성
# 1. 학습데이터 변환
dtr=xgb.DMatrix(data=X_tr,label=y_tr)
# 2. 검증데이터 변환
dval=xgb.DMatrix(data=X_val,label=y_val)
# 3. 테스트데이터 변환
dtest=xgb.DMatrix(data=X_test,label=y_test)

params = {
    'max_depth': 3,  # 트리의 최대 깊이 (값이 클수록 모델이 복잡해지며 과적합 위험 증가)
    'eta': 0.05,  # 학습률 (값이 작을수록 학습 속도가 느려지지만 일반화 성능이 향상될 가능성이 있음)
    'objective': 'binary:logistic',  # 이진 분류 문제를 위한 로지스틱 회귀 설정
    'eval_metric': 'logloss'  # 평가 지표로 로그 손실(log loss) 사용 (값이 작을수록 성능이 좋음)
}

num_rounds = 400  # 부스팅 반복 횟수 (트리를 몇 개 생성할 것인지 결정)

# 학습 데이터 셋(dtr)은 'train', 평가 데이터 셋(dval)은 'eval'로 명기하여 학습 진행 상황을 모니터링
eval_list = [(dtr, 'train'), (dval, 'eval')]

# XGBoost 모델 학습
xgb_model = xgb.train(
    params=params,  # XGBoost 하이퍼파라미터 설정
    dtrain=dtr,  # 학습 데이터 셋
    num_boost_round=num_rounds,  # 부스팅 반복 횟수
    early_stopping_rounds=50,  # 50회 연속 개선이 없으면 학습 조기 종료 (과적합 방지)
    evals=eval_list  # 학습 과정에서 평가할 데이터 셋 목록 (학습 데이터와 검증 데이터)
)

pred_probs=xgb_model.predict(dtest)

print('predict() 수행 결과값을 10개만 표시, 예측 확률 값으로 표시됨')
print(np.round(pred_probs[:10],3))
# 예측 확률이 0.5보다 크면 1, 그렇지 않으면 0으로 예측값 결정하여 List 객체인 preds에 저장
preds=[1 if x >0.5 else 0 for x in pred_probs]
print(f'예측값 10개만 표시 : {preds[:10]}')

```
![image](https://github.com/user-attachments/assets/6279d4db-60c1-42c2-b5f7-d5ec89d7df90)

```
import xgboost as xgb
from xgboost import XGBClassifier
from xgboost import plot_importance
import pandas as pd
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

dataset=load_breast_cancer()
X_features=dataset.data
y_label=dataset.target
cancer_dr=pd.DataFrame(
    data=X_features
    ,columns=dataset.feature_names
)
cancer_dr['target']=y_label

# cancer_df에서 feature용 DataFrame과 label용 Series 객체 추출 : 위 데이터프레임을 데이터와 레이블(target) 분리
# 맨 마지막 컬럼이 Label임
# Feature용 DataFrame은 cancer_df의 첫번째 칼럼에서 맨 마지막 두번째 칼럼까지를 :-1로 슬라이싱으로 추출
X_features=cancer_dr.iloc[:,:-1] # 데이터 추출
y_label=cancer_dr.iloc[:,-1] # 레이블 추출

# 전체 데이터 중 80% 학습용 데이터, 20%는 테스트용 데이터 추출
X_train,X_test,y_train,y_test=train_test_split(
    X_features
    ,y_label
    ,train_size=0.8
    ,random_state=156
)
# xgboost : 검증을 한다 => 조기 종료
# 검증하려면 데이터 필요
# 위에서 만든 X_train, y_train을 다시 쪼개서 90%는 학습과 10%는 검즘용 데이터로 분리
X_tr,X_val,y_tr,y_val=train_test_split(
    X_train
    ,y_train
    ,test_size=0.1
    ,random_state=156
)

# 파이썬 기반 XGBoost 사용 : array 바로 사용 안됨 : 전용 데이터 타입을 사용
# array를 전용 데이터 타입으로 변환 함수 : Dmatrix()
# 학습, 검증, 테스트용 DMatrix를 생성
# 1. 학습데이터 변환
dtr=xgb.DMatrix(data=X_tr,label=y_tr)
# 2. 검증데이터 변환
dval=xgb.DMatrix(data=X_val,label=y_val)
# 3. 테스트데이터 변환
dtest=xgb.DMatrix(data=X_test,label=y_test)

# 파라미터 세팅
# 유방암 이진 분류
params = {
    'max_depth': 3,  # 트리의 최대 깊이 (값이 클수록 모델이 복잡해지며 과적합 위험 증가)
    'eta': 0.1,  # 학습률 (값이 작을수록 학습 속도가 느려지지만 일반화 성능이 향상될 가능성이 있음)
    'objective': 'binary:logistic',  # 이진 분류 문제를 위한 로지스틱 회귀 설정
    'eval_metric': 'logloss'  # 평가 지표로 로그 손실(log loss) 사용 (값이 작을수록 성능이 좋음)
}

num_rounds = 1000  # 부스팅 반복 횟수 (트리를 몇 개 생성할 것인지 결정)

# 학습 데이터와 검증 데이터 설정
# 학습 데이터 셋(dtr)은 'train', 평가 데이터 셋(dval)은 'eval'로 명기하여 학습 진행 상황을 모니터링
eval_list = [(dtr, 'train'), (dval, 'eval')]

# XGBoost 모델 학습
xgb_model = xgb.train(
    params=params,  # XGBoost 하이퍼파라미터 설정
    dtrain=dtr,  # 학습 데이터 셋
    num_boost_round=num_rounds,  # 부스팅 반복 횟수
    early_stopping_rounds=50,  # 50회 연속 개선이 없으면 학습 조기 종료 (과적합 방지)
    evals=eval_list  # 학습 과정에서 평가할 데이터 셋 목록 (학습 데이터와 검증 데이터)
)

pred_probs=xgb_model.predict(dtest)

print('predict() 수행 결과값을 10개만 표시, 예측 확률 값으로 표시됨')
print(np.round(pred_probs[:10],3))
# 예측 확률이 0.5보다 크면 1, 그렇지 않으면 0으로 예측값 결정하여 List 객체인 preds에 저장
preds=[1 if x >0.5 else 0 for x in pred_probs]
print(f'예측값 10개만 표시 : {preds[:10]}')
```
![image](https://github.com/user-attachments/assets/815db2d5-2c22-4fba-81f3-ac7f334cf33b)

```
import xgboost as xgb
from xgboost import XGBClassifier
from xgboost import plot_importance
import pandas as pd
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

dataset=load_breast_cancer()
X_features=dataset.data
y_label=dataset.target
cancer_dr=pd.DataFrame(
    data=X_features
    ,columns=dataset.feature_names
)
cancer_dr['target']=y_label

# cancer_df에서 feature용 DataFrame과 label용 Series 객체 추출 : 위 데이터프레임을 데이터와 레이블(target) 분리
# 맨 마지막 컬럼이 Label임
# Feature용 DataFrame은 cancer_df의 첫번째 칼럼에서 맨 마지막 두번째 칼럼까지를 :-1로 슬라이싱으로 추출
X_features=cancer_dr.iloc[:,:-1] # 데이터 추출
y_label=cancer_dr.iloc[:,-1] # 레이블 추출

# 전체 데이터 중 80% 학습용 데이터, 20%는 테스트용 데이터 추출
X_train,X_test,y_train,y_test=train_test_split(
    X_features
    ,y_label
    ,train_size=0.8
    ,random_state=156
)
# xgboost : 검증을 한다 => 조기 종료
# 검증하려면 데이터 필요
# 위에서 만든 X_train, y_train을 다시 쪼개서 90%는 학습과 10%는 검즘용 데이터로 분리
X_tr,X_val,y_tr,y_val=train_test_split(
    X_train
    ,y_train
    ,test_size=0.1
    ,random_state=156
)

# 파이썬 기반 XGBoost 사용 : array 바로 사용 안됨 : 전용 데이터 타입을 사용
# array를 전용 데이터 타입으로 변환 함수 : Dmatrix()
# 학습, 검증, 테스트용 DMatrix를 생성
# 1. 학습데이터 변환
dtr=xgb.DMatrix(data=X_tr,label=y_tr)
# 2. 검증데이터 변환
dval=xgb.DMatrix(data=X_val,label=y_val)
# 3. 테스트데이터 변환
dtest=xgb.DMatrix(data=X_test,label=y_test)

# 파라미터 세팅
# 유방암 이진 분류
params = {
    'max_depth': 3,  # 트리의 최대 깊이 (값이 클수록 모델이 복잡해지며 과적합 위험 증가)
    'eta': 0.1,  # 학습률 (값이 작을수록 학습 속도가 느려지지만 일반화 성능이 향상될 가능성이 있음)
    'objective': 'binary:logistic',  # 이진 분류 문제를 위한 로지스틱 회귀 설정
    'eval_metric': 'logloss'  # 평가 지표로 로그 손실(log loss) 사용 (값이 작을수록 성능이 좋음)
}

num_rounds = 1000  # 부스팅 반복 횟수 (트리를 몇 개 생성할 것인지 결정)

# 학습 데이터와 검증 데이터 설정
# 학습 데이터 셋(dtr)은 'train', 평가 데이터 셋(dval)은 'eval'로 명기하여 학습 진행 상황을 모니터링
eval_list = [(dtr, 'train'), (dval, 'eval')]

# XGBoost 모델 학습
xgb_model = xgb.train(
    params=params,  # XGBoost 하이퍼파라미터 설정
    dtrain=dtr,  # 학습 데이터 셋
    num_boost_round=num_rounds,  # 부스팅 반복 횟수
    # early_stopping_rounds=50,  # 50회 연속 개선이 없으면 학습 조기 종료 (과적합 방지)
    evals=eval_list  # 학습 과정에서 평가할 데이터 셋 목록 (학습 데이터와 검증 데이터)
)

pred_probs=xgb_model.predict(dtest)

print('predict() 수행 결과값을 10개만 표시, 예측 확률 값으로 표시됨')
print(np.round(pred_probs[:10],3))
# 예측 확률이 0.5보다 크면 1, 그렇지 않으면 0으로 예측값 결정하여 List 객체인 preds에 저장
preds=[1 if x >0.5 else 0 for x in pred_probs]
print(f'예측값 10개만 표시 : {preds[:10]}')
```
![image](https://github.com/user-attachments/assets/1196f95c-941b-4ec0-9f8a-919d1d7db935)

```
import xgboost as xgb
from xgboost import XGBClassifier
from xgboost import plot_importance
import pandas as pd
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

dataset=load_breast_cancer()
X_features=dataset.data
y_label=dataset.target
cancer_dr=pd.DataFrame(
    data=X_features
    ,columns=dataset.feature_names
)
cancer_dr['target']=y_label

# cancer_df에서 feature용 DataFrame과 label용 Series 객체 추출 : 위 데이터프레임을 데이터와 레이블(target) 분리
# 맨 마지막 컬럼이 Label임
# Feature용 DataFrame은 cancer_df의 첫번째 칼럼에서 맨 마지막 두번째 칼럼까지를 :-1로 슬라이싱으로 추출
X_features=cancer_dr.iloc[:,:-1] # 데이터 추출
y_label=cancer_dr.iloc[:,-1] # 레이블 추출

# 전체 데이터 중 80% 학습용 데이터, 20%는 테스트용 데이터 추출
X_train,X_test,y_train,y_test=train_test_split(
    X_features
    ,y_label
    ,train_size=0.8
    ,random_state=156
)
# xgboost : 검증을 한다 => 조기 종료
# 검증하려면 데이터 필요
# 위에서 만든 X_train, y_train을 다시 쪼개서 90%는 학습과 10%는 검즘용 데이터로 분리
X_tr,X_val,y_tr,y_val=train_test_split(
    X_train
    ,y_train
    ,test_size=0.1
    ,random_state=156
)

# 파이썬 기반 XGBoost 사용 : array 바로 사용 안됨 : 전용 데이터 타입을 사용
# array를 전용 데이터 타입으로 변환 함수 : Dmatrix()
# 학습, 검증, 테스트용 DMatrix를 생성
# 1. 학습데이터 변환
dtr=xgb.DMatrix(data=X_tr,label=y_tr)
# 2. 검증데이터 변환
dval=xgb.DMatrix(data=X_val,label=y_val)
# 3. 테스트데이터 변환
dtest=xgb.DMatrix(data=X_test,label=y_test)

# 파라미터 세팅
# 유방암 이진 분류
params = {
    'max_depth': 3,  # 트리의 최대 깊이 (값이 클수록 모델이 복잡해지며 과적합 위험 증가)
    'eta': 0.1,  # 학습률 (값이 작을수록 학습 속도가 느려지지만 일반화 성능이 향상될 가능성이 있음)
    'objective': 'binary:logistic',  # 이진 분류 문제를 위한 로지스틱 회귀 설정
    'eval_metric': 'logloss'  # 평가 지표로 로그 손실(log loss) 사용 (값이 작을수록 성능이 좋음)
}

num_rounds = 10000  # 부스팅 반복 횟수 (트리를 몇 개 생성할 것인지 결정)

# 학습 데이터와 검증 데이터 설정
# 학습 데이터 셋(dtr)은 'train', 평가 데이터 셋(dval)은 'eval'로 명기하여 학습 진행 상황을 모니터링
eval_list = [(dtr, 'train'), (dval, 'eval')]

# XGBoost 모델 학습
xgb_model = xgb.train(
    params=params,  # XGBoost 하이퍼파라미터 설정
    dtrain=dtr,  # 학습 데이터 셋
    num_boost_round=num_rounds,  # 부스팅 반복 횟수
    early_stopping_rounds=500,  # 50회 연속 개선이 없으면 학습 조기 종료 (과적합 방지)
    evals=eval_list  # 학습 과정에서 평가할 데이터 셋 목록 (학습 데이터와 검증 데이터)
)

pred_probs=xgb_model.predict(dtest)

# print('predict() 수행 결과값을 10개만 표시, 예측 확률 값으로 표시됨')
# print(np.round(pred_probs[:10],3))
# 예측 확률이 0.5보다 크면 1, 그렇지 않으면 0으로 예측값 결정하여 List 객체인 preds에 저장
preds=[1 if x >0.5 else 0 for x in pred_probs]
# print(f'예측값 10개만 표시 : {preds[:10]}')
print(y_test[:10],end='')
```
![image](https://github.com/user-attachments/assets/d4912463-3c86-4b04-a41b-32ba4f1b959f)
