# XGboost

---
## python 기반 XGboost

```
import xgboost as xgb
from xgboost import XGBClassifier

print(xgb.__version__)
```
![image](https://github.com/user-attachments/assets/64e15a03-ad4d-4735-8d71-f1a33fb5ca1c)

```
import xgboost as xgb
from xgboost import XGBClassifier
from xgboost import plot_importance
import pandas as pd
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

dataset=load_breast_cancer()
X_features=dataset.data
y_label=dataset.target
cancer_dr=pd.DataFrame(
    data=X_features
    ,columns=dataset.feature_names
)
cancer_dr['target']=y_label
cancer_dr.head(3)
```
![image](https://github.com/user-attachments/assets/f21e1815-807a-4655-a58e-dfd7af66750b)

```
import xgboost as xgb
from xgboost import XGBClassifier
from xgboost import plot_importance
import pandas as pd
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

dataset=load_breast_cancer()
X_features=dataset.data
y_label=dataset.target
cancer_dr=pd.DataFrame(
    data=X_features
    ,columns=dataset.feature_names
)
cancer_dr['target']=y_label
print(dataset.target_names) # malignant(악성) :0,benign(양성) :1
```
![image](https://github.com/user-attachments/assets/0284842c-cecb-4b25-8267-9722fe044ef6)

```
import xgboost as xgb
from xgboost import XGBClassifier
from xgboost import plot_importance
import pandas as pd
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

dataset=load_breast_cancer()
X_features=dataset.data
y_label=dataset.target
cancer_dr=pd.DataFrame(
    data=X_features
    ,columns=dataset.feature_names
)
cancer_dr['target']=y_label
print(f"target의 분포 :\n{cancer_dr['target'].value_counts()}")
```
![image](https://github.com/user-attachments/assets/27b1320a-79d8-4203-85ff-00e31d32b978)

```
import xgboost as xgb
from xgboost import XGBClassifier
from xgboost import plot_importance
import pandas as pd
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

dataset=load_breast_cancer()
X_features=dataset.data
y_label=dataset.target
cancer_dr=pd.DataFrame(
    data=X_features
    ,columns=dataset.feature_names
)
cancer_dr['target']=y_label

# cancer_df에서 feature용 DataFrame과 label용 Series 객체 추출 : 위 데이터프레임을 데이터와 레이블(target) 분리
# 맨 마지막 컬럼이 Label임
# Feature용 DataFrame은 cancer_df의 첫번째 칼럼에서 맨 마지막 두번째 칼럼까지를 :-1로 슬라이싱으로 추출
X_features=cancer_dr.iloc[:,:-1] # 데이터 추출
y_label=cancer_dr.iloc[:,-1] # 레이블 추출

# 전체 데이터 중 80% 학습용 데이터, 20%는 테스트용 데이터 추출
X_train,X_test,y_train,y_test=train_test_split(
    X_features
    ,y_label
    ,train_size=0.8
    ,random_state=156
)
# xgboost : 검증을 한다 => 조기 종료
# 검증하려면 데이터 필요
# 위에서 만든 X_train, y_train을 다시 쪼개서 90%는 학습과 10%는 검즘용 데이터로 분리
X_tr,X_val,y_tr,y_val=train_test_split(
    X_train
    ,y_train
    ,test_size=0.1
    ,random_state=156
)
print(f'X train shape : {X_train.shape}')
print(f'X test shape : {X_test.shape}')

print(f'X tr shape : { X_tr.shape}')
print(f'X val shape : { X_val.shape}')


```
![image](https://github.com/user-attachments/assets/5b8ce190-71e5-4d37-b7a8-39ce5de36da6)

```
import xgboost as xgb
from xgboost import XGBClassifier
from xgboost import plot_importance
import pandas as pd
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

dataset=load_breast_cancer()
X_features=dataset.data
y_label=dataset.target
cancer_dr=pd.DataFrame(
    data=X_features
    ,columns=dataset.feature_names
)
cancer_dr['target']=y_label

# cancer_df에서 feature용 DataFrame과 label용 Series 객체 추출 : 위 데이터프레임을 데이터와 레이블(target) 분리
# 맨 마지막 컬럼이 Label임
# Feature용 DataFrame은 cancer_df의 첫번째 칼럼에서 맨 마지막 두번째 칼럼까지를 :-1로 슬라이싱으로 추출
X_features=cancer_dr.iloc[:,:-1] # 데이터 추출
y_label=cancer_dr.iloc[:,-1] # 레이블 추출

# 전체 데이터 중 80% 학습용 데이터, 20%는 테스트용 데이터 추출
X_train,X_test,y_train,y_test=train_test_split(
    X_features
    ,y_label
    ,train_size=0.8
    ,random_state=156
)
# xgboost : 검증을 한다 => 조기 종료
# 검증하려면 데이터 필요
# 위에서 만든 X_train, y_train을 다시 쪼개서 90%는 학습과 10%는 검즘용 데이터로 분리
X_tr,X_val,y_tr,y_val=train_test_split(
    X_train
    ,y_train
    ,test_size=0.1
    ,random_state=156
)

# 파이썬 기반 XGBoost 사용 : array 바로 사용 안됨 : 전용 데이터 타입을 사용
# array를 전용 데이터 타입으로 변환 함수 : Dmatrix()
# 학습, 검증, 테스트용 DMatrix를 생성
# 1. 학습데이터 변환
dtr=xgb.DMatrix(data=X_tr,label=y_tr)
# 2. 검증데이터 변환
dval=xgb.DMatrix(data=X_val,label=y_val)
dtest=xgb.DMatrix(data=X_test,label=y_test)
print(f'dtr : {dtr}')
print(f'dval : {dval}')
print(f'dttest : {dtest}')
```
![image](https://github.com/user-attachments/assets/09838836-f4ae-4bd5-b015-ef395b080407)

```
import xgboost as xgb
from xgboost import XGBClassifier
from xgboost import plot_importance
import pandas as pd
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

dataset=load_breast_cancer()
X_features=dataset.data
y_label=dataset.target
cancer_dr=pd.DataFrame(
    data=X_features
    ,columns=dataset.feature_names
)
cancer_dr['target']=y_label

# cancer_df에서 feature용 DataFrame과 label용 Series 객체 추출 : 위 데이터프레임을 데이터와 레이블(target) 분리
# 맨 마지막 컬럼이 Label임
# Feature용 DataFrame은 cancer_df의 첫번째 칼럼에서 맨 마지막 두번째 칼럼까지를 :-1로 슬라이싱으로 추출
X_features=cancer_dr.iloc[:,:-1] # 데이터 추출
y_label=cancer_dr.iloc[:,-1] # 레이블 추출

# 전체 데이터 중 80% 학습용 데이터, 20%는 테스트용 데이터 추출
X_train,X_test,y_train,y_test=train_test_split(
    X_features
    ,y_label
    ,train_size=0.8
    ,random_state=156
)
# xgboost : 검증을 한다 => 조기 종료
# 검증하려면 데이터 필요
# 위에서 만든 X_train, y_train을 다시 쪼개서 90%는 학습과 10%는 검즘용 데이터로 분리
X_tr,X_val,y_tr,y_val=train_test_split(
    X_train
    ,y_train
    ,test_size=0.1
    ,random_state=156
)

# 파이썬 기반 XGBoost 사용 : array 바로 사용 안됨 : 전용 데이터 타입을 사용
# array를 전용 데이터 타입으로 변환 함수 : DMatrix()
# 학습, 검증, 테스트용 DMatrix를 생성
# 1. 학습데이터 변환
dtr=xgb.DMatrix(data=X_tr,label=y_tr)
# 2. 검증데이터 변환
dval=xgb.DMatrix(data=X_val,label=y_val)
# 3. 테스트데이터 변환
dtest=xgb.DMatrix(data=X_test,label=y_test)

print(f'dtr : {dtr}')
print(f'dval : {dval}')
print(f'dttest : {dtest}')
```
![image](https://github.com/user-attachments/assets/108b4861-db9b-4190-9c1b-d6ae2ae6f0e6)

```
import xgboost as xgb
from xgboost import XGBClassifier
from xgboost import plot_importance
import pandas as pd
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

dataset=load_breast_cancer()
X_features=dataset.data
y_label=dataset.target
cancer_dr=pd.DataFrame(
    data=X_features
    ,columns=dataset.feature_names
)
cancer_dr['target']=y_label

# cancer_df에서 feature용 DataFrame과 label용 Series 객체 추출 : 위 데이터프레임을 데이터와 레이블(target) 분리
# 맨 마지막 컬럼이 Label임
# Feature용 DataFrame은 cancer_df의 첫번째 칼럼에서 맨 마지막 두번째 칼럼까지를 :-1로 슬라이싱으로 추출
X_features=cancer_dr.iloc[:,:-1] # 데이터 추출
y_label=cancer_dr.iloc[:,-1] # 레이블 추출

# 전체 데이터 중 80% 학습용 데이터, 20%는 테스트용 데이터 추출
X_train,X_test,y_train,y_test=train_test_split(
    X_features
    ,y_label
    ,train_size=0.8
    ,random_state=156
)
# xgboost : 검증을 한다 => 조기 종료
# 검증하려면 데이터 필요
# 위에서 만든 X_train, y_train을 다시 쪼개서 90%는 학습과 10%는 검즘용 데이터로 분리
X_tr,X_val,y_tr,y_val=train_test_split(
    X_train
    ,y_train
    ,test_size=0.1
    ,random_state=156
)

# 파이썬 기반 XGBoost 사용 : array 바로 사용 안됨 : 전용 데이터 타입을 사용
# array를 전용 데이터 타입으로 변환 함수 : Dmatrix()
# 학습, 검증, 테스트용 DMatrix를 생성
# 1. 학습데이터 변환
dtr=xgb.DMatrix(data=X_tr,label=y_tr)
# 2. 검증데이터 변환
dval=xgb.DMatrix(data=X_val,label=y_val)
# 3. 테스트데이터 변환
dtest=xgb.DMatrix(data=X_test,label=y_test)

params = {
    'max_depth': 3,  # 트리의 최대 깊이 (값이 클수록 모델이 복잡해지며 과적합 위험 증가)
    'eta': 0.05,  # 학습률 (값이 작을수록 학습 속도가 느려지지만 일반화 성능이 향상될 가능성이 있음)
    'objective': 'binary:logistic',  # 이진 분류 문제를 위한 로지스틱 회귀 설정
    'eval_metric': 'logloss'  # 평가 지표로 로그 손실(log loss) 사용 (값이 작을수록 성능이 좋음)
}

num_rounds = 400  # 부스팅 반복 횟수 (트리를 몇 개 생성할 것인지 결정)

# 학습 데이터 셋(dtr)은 'train', 평가 데이터 셋(dval)은 'eval'로 명기하여 학습 진행 상황을 모니터링
eval_list = [(dtr, 'train'), (dval, 'eval')]

# XGBoost 모델 학습
xgb_model = xgb.train(
    params=params,  # XGBoost 하이퍼파라미터 설정
    dtrain=dtr,  # 학습 데이터 셋
    num_boost_round=num_rounds,  # 부스팅 반복 횟수
    early_stopping_rounds=50,  # 50회 연속 개선이 없으면 학습 조기 종료 (과적합 방지)
    evals=eval_list  # 학습 과정에서 평가할 데이터 셋 목록 (학습 데이터와 검증 데이터)
)

xgb_model
```
![image](https://github.com/user-attachments/assets/237ffb42-8c45-4189-ad27-93515b74f6f2)

```
import xgboost as xgb
from xgboost import XGBClassifier
from xgboost import plot_importance
import pandas as pd
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

dataset=load_breast_cancer()
X_features=dataset.data
y_label=dataset.target
cancer_dr=pd.DataFrame(
    data=X_features
    ,columns=dataset.feature_names
)
cancer_dr['target']=y_label

# cancer_df에서 feature용 DataFrame과 label용 Series 객체 추출 : 위 데이터프레임을 데이터와 레이블(target) 분리
# 맨 마지막 컬럼이 Label임
# Feature용 DataFrame은 cancer_df의 첫번째 칼럼에서 맨 마지막 두번째 칼럼까지를 :-1로 슬라이싱으로 추출
X_features=cancer_dr.iloc[:,:-1] # 데이터 추출
y_label=cancer_dr.iloc[:,-1] # 레이블 추출

# 전체 데이터 중 80% 학습용 데이터, 20%는 테스트용 데이터 추출
X_train,X_test,y_train,y_test=train_test_split(
    X_features
    ,y_label
    ,train_size=0.8
    ,random_state=156
)
# xgboost : 검증을 한다 => 조기 종료
# 검증하려면 데이터 필요
# 위에서 만든 X_train, y_train을 다시 쪼개서 90%는 학습과 10%는 검즘용 데이터로 분리
X_tr,X_val,y_tr,y_val=train_test_split(
    X_train
    ,y_train
    ,test_size=0.1
    ,random_state=156
)

# 파이썬 기반 XGBoost 사용 : array 바로 사용 안됨 : 전용 데이터 타입을 사용
# array를 전용 데이터 타입으로 변환 함수 : Dmatrix()
# 학습, 검증, 테스트용 DMatrix를 생성
# 1. 학습데이터 변환
dtr=xgb.DMatrix(data=X_tr,label=y_tr)
# 2. 검증데이터 변환
dval=xgb.DMatrix(data=X_val,label=y_val)
# 3. 테스트데이터 변환
dtest=xgb.DMatrix(data=X_test,label=y_test)

params = {
    'max_depth': 3,  # 트리의 최대 깊이 (값이 클수록 모델이 복잡해지며 과적합 위험 증가)
    'eta': 0.05,  # 학습률 (값이 작을수록 학습 속도가 느려지지만 일반화 성능이 향상될 가능성이 있음)
    'objective': 'binary:logistic',  # 이진 분류 문제를 위한 로지스틱 회귀 설정
    'eval_metric': 'logloss'  # 평가 지표로 로그 손실(log loss) 사용 (값이 작을수록 성능이 좋음)
}

num_rounds = 400  # 부스팅 반복 횟수 (트리를 몇 개 생성할 것인지 결정)

# 학습 데이터 셋(dtr)은 'train', 평가 데이터 셋(dval)은 'eval'로 명기하여 학습 진행 상황을 모니터링
eval_list = [(dtr, 'train'), (dval, 'eval')]

# XGBoost 모델 학습
xgb_model = xgb.train(
    params=params,  # XGBoost 하이퍼파라미터 설정
    dtrain=dtr,  # 학습 데이터 셋
    num_boost_round=num_rounds,  # 부스팅 반복 횟수
    early_stopping_rounds=50,  # 50회 연속 개선이 없으면 학습 조기 종료 (과적합 방지)
    evals=eval_list  # 학습 과정에서 평가할 데이터 셋 목록 (학습 데이터와 검증 데이터)
)

pred_probs=xgb_model.predict(dtest)

print('predict() 수행 결과값을 10개만 표시, 예측 확률 값으로 표시됨')
print(np.round(pred_probs[:10],3))
# 예측 확률이 0.5보다 크면 1, 그렇지 않으면 0으로 예측값 결정하여 List 객체인 preds에 저장
preds=[1 if x >0.5 else 0 for x in pred_probs]
print(f'예측값 10개만 표시 : {preds[:10]}')

```
![image](https://github.com/user-attachments/assets/6279d4db-60c1-42c2-b5f7-d5ec89d7df90)

```
import xgboost as xgb
from xgboost import XGBClassifier
from xgboost import plot_importance
import pandas as pd
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

dataset=load_breast_cancer()
X_features=dataset.data
y_label=dataset.target
cancer_dr=pd.DataFrame(
    data=X_features
    ,columns=dataset.feature_names
)
cancer_dr['target']=y_label

# cancer_df에서 feature용 DataFrame과 label용 Series 객체 추출 : 위 데이터프레임을 데이터와 레이블(target) 분리
# 맨 마지막 컬럼이 Label임
# Feature용 DataFrame은 cancer_df의 첫번째 칼럼에서 맨 마지막 두번째 칼럼까지를 :-1로 슬라이싱으로 추출
X_features=cancer_dr.iloc[:,:-1] # 데이터 추출
y_label=cancer_dr.iloc[:,-1] # 레이블 추출

# 전체 데이터 중 80% 학습용 데이터, 20%는 테스트용 데이터 추출
X_train,X_test,y_train,y_test=train_test_split(
    X_features
    ,y_label
    ,train_size=0.8
    ,random_state=156
)
# xgboost : 검증을 한다 => 조기 종료
# 검증하려면 데이터 필요
# 위에서 만든 X_train, y_train을 다시 쪼개서 90%는 학습과 10%는 검즘용 데이터로 분리
X_tr,X_val,y_tr,y_val=train_test_split(
    X_train
    ,y_train
    ,test_size=0.1
    ,random_state=156
)

# 파이썬 기반 XGBoost 사용 : array 바로 사용 안됨 : 전용 데이터 타입을 사용
# array를 전용 데이터 타입으로 변환 함수 : Dmatrix()
# 학습, 검증, 테스트용 DMatrix를 생성
# 1. 학습데이터 변환
dtr=xgb.DMatrix(data=X_tr,label=y_tr)
# 2. 검증데이터 변환
dval=xgb.DMatrix(data=X_val,label=y_val)
# 3. 테스트데이터 변환
dtest=xgb.DMatrix(data=X_test,label=y_test)

# 파라미터 세팅
# 유방암 이진 분류
params = {
    'max_depth': 3,  # 트리의 최대 깊이 (값이 클수록 모델이 복잡해지며 과적합 위험 증가)
    'eta': 0.1,  # 학습률 (값이 작을수록 학습 속도가 느려지지만 일반화 성능이 향상될 가능성이 있음)
    'objective': 'binary:logistic',  # 이진 분류 문제를 위한 로지스틱 회귀 설정
    'eval_metric': 'logloss'  # 평가 지표로 로그 손실(log loss) 사용 (값이 작을수록 성능이 좋음)
}

num_rounds = 1000  # 부스팅 반복 횟수 (트리를 몇 개 생성할 것인지 결정)

# 학습 데이터와 검증 데이터 설정
# 학습 데이터 셋(dtr)은 'train', 평가 데이터 셋(dval)은 'eval'로 명기하여 학습 진행 상황을 모니터링
eval_list = [(dtr, 'train'), (dval, 'eval')]

# XGBoost 모델 학습
xgb_model = xgb.train(
    params=params,  # XGBoost 하이퍼파라미터 설정
    dtrain=dtr,  # 학습 데이터 셋
    num_boost_round=num_rounds,  # 부스팅 반복 횟수
    early_stopping_rounds=50,  # 50회 연속 개선이 없으면 학습 조기 종료 (과적합 방지)
    evals=eval_list  # 학습 과정에서 평가할 데이터 셋 목록 (학습 데이터와 검증 데이터)
)

pred_probs=xgb_model.predict(dtest)

print('predict() 수행 결과값을 10개만 표시, 예측 확률 값으로 표시됨')
print(np.round(pred_probs[:10],3))
# 예측 확률이 0.5보다 크면 1, 그렇지 않으면 0으로 예측값 결정하여 List 객체인 preds에 저장
preds=[1 if x >0.5 else 0 for x in pred_probs]
print(f'예측값 10개만 표시 : {preds[:10]}')
```
![image](https://github.com/user-attachments/assets/815db2d5-2c22-4fba-81f3-ac7f334cf33b)

```
import xgboost as xgb
from xgboost import XGBClassifier
from xgboost import plot_importance
import pandas as pd
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

dataset=load_breast_cancer()
X_features=dataset.data
y_label=dataset.target
cancer_dr=pd.DataFrame(
    data=X_features
    ,columns=dataset.feature_names
)
cancer_dr['target']=y_label

# cancer_df에서 feature용 DataFrame과 label용 Series 객체 추출 : 위 데이터프레임을 데이터와 레이블(target) 분리
# 맨 마지막 컬럼이 Label임
# Feature용 DataFrame은 cancer_df의 첫번째 칼럼에서 맨 마지막 두번째 칼럼까지를 :-1로 슬라이싱으로 추출
X_features=cancer_dr.iloc[:,:-1] # 데이터 추출
y_label=cancer_dr.iloc[:,-1] # 레이블 추출

# 전체 데이터 중 80% 학습용 데이터, 20%는 테스트용 데이터 추출
X_train,X_test,y_train,y_test=train_test_split(
    X_features
    ,y_label
    ,train_size=0.8
    ,random_state=156
)
# xgboost : 검증을 한다 => 조기 종료
# 검증하려면 데이터 필요
# 위에서 만든 X_train, y_train을 다시 쪼개서 90%는 학습과 10%는 검즘용 데이터로 분리
X_tr,X_val,y_tr,y_val=train_test_split(
    X_train
    ,y_train
    ,test_size=0.1
    ,random_state=156
)

# 파이썬 기반 XGBoost 사용 : array 바로 사용 안됨 : 전용 데이터 타입을 사용
# array를 전용 데이터 타입으로 변환 함수 : Dmatrix()
# 학습, 검증, 테스트용 DMatrix를 생성
# 1. 학습데이터 변환
dtr=xgb.DMatrix(data=X_tr,label=y_tr)
# 2. 검증데이터 변환
dval=xgb.DMatrix(data=X_val,label=y_val)
# 3. 테스트데이터 변환
dtest=xgb.DMatrix(data=X_test,label=y_test)

# 파라미터 세팅
# 유방암 이진 분류
params = {
    'max_depth': 3,  # 트리의 최대 깊이 (값이 클수록 모델이 복잡해지며 과적합 위험 증가)
    'eta': 0.1,  # 학습률 (값이 작을수록 학습 속도가 느려지지만 일반화 성능이 향상될 가능성이 있음)
    'objective': 'binary:logistic',  # 이진 분류 문제를 위한 로지스틱 회귀 설정
    'eval_metric': 'logloss'  # 평가 지표로 로그 손실(log loss) 사용 (값이 작을수록 성능이 좋음)
}

num_rounds = 1000  # 부스팅 반복 횟수 (트리를 몇 개 생성할 것인지 결정)

# 학습 데이터와 검증 데이터 설정
# 학습 데이터 셋(dtr)은 'train', 평가 데이터 셋(dval)은 'eval'로 명기하여 학습 진행 상황을 모니터링
eval_list = [(dtr, 'train'), (dval, 'eval')]

# XGBoost 모델 학습
xgb_model = xgb.train(
    params=params,  # XGBoost 하이퍼파라미터 설정
    dtrain=dtr,  # 학습 데이터 셋
    num_boost_round=num_rounds,  # 부스팅 반복 횟수
    # early_stopping_rounds=50,  # 50회 연속 개선이 없으면 학습 조기 종료 (과적합 방지)
    evals=eval_list  # 학습 과정에서 평가할 데이터 셋 목록 (학습 데이터와 검증 데이터)
)

pred_probs=xgb_model.predict(dtest)

print('predict() 수행 결과값을 10개만 표시, 예측 확률 값으로 표시됨')
print(np.round(pred_probs[:10],3))
# 예측 확률이 0.5보다 크면 1, 그렇지 않으면 0으로 예측값 결정하여 List 객체인 preds에 저장
preds=[1 if x >0.5 else 0 for x in pred_probs]
print(f'예측값 10개만 표시 : {preds[:10]}')
```
![image](https://github.com/user-attachments/assets/1196f95c-941b-4ec0-9f8a-919d1d7db935)

```
import xgboost as xgb
from xgboost import XGBClassifier
from xgboost import plot_importance
import pandas as pd
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

dataset=load_breast_cancer()
X_features=dataset.data
y_label=dataset.target
cancer_dr=pd.DataFrame(
    data=X_features
    ,columns=dataset.feature_names
)
cancer_dr['target']=y_label

# cancer_df에서 feature용 DataFrame과 label용 Series 객체 추출 : 위 데이터프레임을 데이터와 레이블(target) 분리
# 맨 마지막 컬럼이 Label임
# Feature용 DataFrame은 cancer_df의 첫번째 칼럼에서 맨 마지막 두번째 칼럼까지를 :-1로 슬라이싱으로 추출
X_features=cancer_dr.iloc[:,:-1] # 데이터 추출
y_label=cancer_dr.iloc[:,-1] # 레이블 추출

# 전체 데이터 중 80% 학습용 데이터, 20%는 테스트용 데이터 추출
X_train,X_test,y_train,y_test=train_test_split(
    X_features
    ,y_label
    ,train_size=0.8
    ,random_state=156
)
# xgboost : 검증을 한다 => 조기 종료
# 검증하려면 데이터 필요
# 위에서 만든 X_train, y_train을 다시 쪼개서 90%는 학습과 10%는 검즘용 데이터로 분리
X_tr,X_val,y_tr,y_val=train_test_split(
    X_train
    ,y_train
    ,test_size=0.1
    ,random_state=156
)

# 파이썬 기반 XGBoost 사용 : array 바로 사용 안됨 : 전용 데이터 타입을 사용
# array를 전용 데이터 타입으로 변환 함수 : Dmatrix()
# 학습, 검증, 테스트용 DMatrix를 생성
# 1. 학습데이터 변환
dtr=xgb.DMatrix(data=X_tr,label=y_tr)
# 2. 검증데이터 변환
dval=xgb.DMatrix(data=X_val,label=y_val)
# 3. 테스트데이터 변환
dtest=xgb.DMatrix(data=X_test,label=y_test)

# 파라미터 세팅
# 유방암 이진 분류
params = {
    'max_depth': 3,  # 트리의 최대 깊이 (값이 클수록 모델이 복잡해지며 과적합 위험 증가)
    'eta': 0.1,  # 학습률 (값이 작을수록 학습 속도가 느려지지만 일반화 성능이 향상될 가능성이 있음)
    'objective': 'binary:logistic',  # 이진 분류 문제를 위한 로지스틱 회귀 설정
    'eval_metric': 'logloss'  # 평가 지표로 로그 손실(log loss) 사용 (값이 작을수록 성능이 좋음)
}

num_rounds = 10000  # 부스팅 반복 횟수 (트리를 몇 개 생성할 것인지 결정)

# 학습 데이터와 검증 데이터 설정
# 학습 데이터 셋(dtr)은 'train', 평가 데이터 셋(dval)은 'eval'로 명기하여 학습 진행 상황을 모니터링
eval_list = [(dtr, 'train'), (dval, 'eval')]

# XGBoost 모델 학습
xgb_model = xgb.train(
    params=params,  # XGBoost 하이퍼파라미터 설정
    dtrain=dtr,  # 학습 데이터 셋
    num_boost_round=num_rounds,  # 부스팅 반복 횟수
    early_stopping_rounds=500,  # 50회 연속 개선이 없으면 학습 조기 종료 (과적합 방지)
    evals=eval_list  # 학습 과정에서 평가할 데이터 셋 목록 (학습 데이터와 검증 데이터)
)

pred_probs=xgb_model.predict(dtest)

# print('predict() 수행 결과값을 10개만 표시, 예측 확률 값으로 표시됨')
# print(np.round(pred_probs[:10],3))
# 예측 확률이 0.5보다 크면 1, 그렇지 않으면 0으로 예측값 결정하여 List 객체인 preds에 저장
preds=[1 if x >0.5 else 0 for x in pred_probs]
print(f'예측값 10개만 표시 : {preds[:10]}')
print(y_test[:10],end='')
```
![image](https://github.com/user-attachments/assets/be4ef632-c2b3-4aee-8b89-868b159ed233)

```
import xgboost as xgb
from xgboost import XGBClassifier
from xgboost import plot_importance
import pandas as pd
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

dataset=load_breast_cancer()
X_features=dataset.data
y_label=dataset.target
cancer_dr=pd.DataFrame(
    data=X_features
    ,columns=dataset.feature_names
)
cancer_dr['target']=y_label

# cancer_df에서 feature용 DataFrame과 label용 Series 객체 추출 : 위 데이터프레임을 데이터와 레이블(target) 분리
# 맨 마지막 컬럼이 Label임
# Feature용 DataFrame은 cancer_df의 첫번째 칼럼에서 맨 마지막 두번째 칼럼까지를 :-1로 슬라이싱으로 추출
X_features=cancer_dr.iloc[:,:-1] # 데이터 추출
y_label=cancer_dr.iloc[:,-1] # 레이블 추출

# 전체 데이터 중 80% 학습용 데이터, 20%는 테스트용 데이터 추출
X_train,X_test,y_train,y_test=train_test_split(
    X_features
    ,y_label
    ,train_size=0.8
    ,random_state=156
)
# xgboost : 검증을 한다 => 조기 종료
# 검증하려면 데이터 필요
# 위에서 만든 X_train, y_train을 다시 쪼개서 90%는 학습과 10%는 검즘용 데이터로 분리
X_tr,X_val,y_tr,y_val=train_test_split(
    X_train
    ,y_train
    ,test_size=0.1
    ,random_state=156
)

# 파이썬 기반 XGBoost 사용 : array 바로 사용 안됨 : 전용 데이터 타입을 사용
# array를 전용 데이터 타입으로 변환 함수 : Dmatrix()
# 학습, 검증, 테스트용 DMatrix를 생성
# 1. 학습데이터 변환
dtr=xgb.DMatrix(data=X_tr,label=y_tr)
# 2. 검증데이터 변환
dval=xgb.DMatrix(data=X_val,label=y_val)
# 3. 테스트데이터 변환
dtest=xgb.DMatrix(data=X_test,label=y_test)

# 파라미터 세팅
# 유방암 이진 분류
params = {
    'max_depth': 3,  # 트리의 최대 깊이 (값이 클수록 모델이 복잡해지며 과적합 위험 증가)
    'eta': 0.1,  # 학습률 (값이 작을수록 학습 속도가 느려지지만 일반화 성능이 향상될 가능성이 있음)
    'objective': 'binary:logistic',  # 이진 분류 문제를 위한 로지스틱 회귀 설정
    'eval_metric': 'logloss'  # 평가 지표로 로그 손실(log loss) 사용 (값이 작을수록 성능이 좋음)
}

num_rounds = 10000  # 부스팅 반복 횟수 (트리를 몇 개 생성할 것인지 결정)

# 학습 데이터와 검증 데이터 설정
# 학습 데이터 셋(dtr)은 'train', 평가 데이터 셋(dval)은 'eval'로 명기하여 학습 진행 상황을 모니터링
eval_list = [(dtr, 'train'), (dval, 'eval')]

# XGBoost 모델 학습
xgb_model = xgb.train(
    params=params,  # XGBoost 하이퍼파라미터 설정
    dtrain=dtr,  # 학습 데이터 셋
    num_boost_round=num_rounds,  # 부스팅 반복 횟수
    early_stopping_rounds=500,  # 50회 연속 개선이 없으면 학습 조기 종료 (과적합 방지)
    evals=eval_list  # 학습 과정에서 평가할 데이터 셋 목록 (학습 데이터와 검증 데이터)
)

pred_probs=xgb_model.predict(dtest)

# print('predict() 수행 결과값을 10개만 표시, 예측 확률 값으로 표시됨')
# print(np.round(pred_probs[:10],3))
# 예측 확률이 0.5보다 크면 1, 그렇지 않으면 0으로 예측값 결정하여 List 객체인 preds에 저장
preds=[1 if x >0.5 else 0 for x in pred_probs]
# print(f'예측값 10개만 표시 : {preds[:10]}')
# print(y_test[:10],end='')

# 유방암 여부 예측
# 오류에 민감한 모델 : 정확도만 보면 안된다
# 정밀도, 재현률, AUC
def get_clf_eval(y_test,preds=None,pred_probs=None):     # 원래답, 예측값(0,1), 예측 확률(0,1)
    from sklearn.metrics import confusion_matrix        # 혼돈행렬을 구하는 함수 (원래답, 예측값)
    from sklearn.metrics import accuracy_score          # 정확도 (원래답, 예측값)
    from sklearn.metrics import precision_score         # 정밀도 (원래답, 예측값)
    from sklearn.metrics import recall_score            # 재현률 (원래답, 예측값)
    from sklearn.metrics import f1_score                # 정밀도와 재현률의 조화 평균 (원래답, 예측값)
    from sklearn.metrics import roc_auc_score           # AUC 점수 : (원래답, 예측확률)
    confusion=confusion_matrix(
        y_test
        ,preds
    )
    acc=accuracy_score(
        y_test
        ,preds
    )
    precision=precision_score(
        y_test
        ,preds
    )
    recall_score_=recall_score(
        y_test
        ,preds
    )
    f1=f1_score(
        y_test
        ,preds
    )
    auc=roc_auc_score(
        y_test
        ,pred_probs
    )
    print(f'오차행렬\n{confusion}')
    
    # 평가지표 출력
    print(f'정확도 : {acc:.4f}, 정밀도 : {precision:.4f}, 재현률 : {recall_score_:.4f}, F1 : {f1:.4f}, AUC : {auc:.4f}')

get_clf_eval(y_test,preds,pred_probs)
```
![image](https://github.com/user-attachments/assets/9a354746-81eb-4897-8e92-45540f98a25a)


```
import xgboost as xgb
from xgboost import XGBClassifier
from xgboost import plot_importance
import pandas as pd
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

dataset=load_breast_cancer()
X_features=dataset.data
y_label=dataset.target
cancer_dr=pd.DataFrame(
    data=X_features
    ,columns=dataset.feature_names
)
cancer_dr['target']=y_label

# cancer_df에서 feature용 DataFrame과 label용 Series 객체 추출 : 위 데이터프레임을 데이터와 레이블(target) 분리
# 맨 마지막 컬럼이 Label임
# Feature용 DataFrame은 cancer_df의 첫번째 칼럼에서 맨 마지막 두번째 칼럼까지를 :-1로 슬라이싱으로 추출
X_features=cancer_dr.iloc[:,:-1] # 데이터 추출
y_label=cancer_dr.iloc[:,-1] # 레이블 추출

# 전체 데이터 중 80% 학습용 데이터, 20%는 테스트용 데이터 추출
X_train,X_test,y_train,y_test=train_test_split(
    X_features
    ,y_label
    ,train_size=0.8
    ,random_state=156
)
# xgboost : 검증을 한다 => 조기 종료
# 검증하려면 데이터 필요
# 위에서 만든 X_train, y_train을 다시 쪼개서 90%는 학습과 10%는 검즘용 데이터로 분리
X_tr,X_val,y_tr,y_val=train_test_split(
    X_train
    ,y_train
    ,test_size=0.1
    ,random_state=156
)

# 파이썬 기반 XGBoost 사용 : array 바로 사용 안됨 : 전용 데이터 타입을 사용
# array를 전용 데이터 타입으로 변환 함수 : Dmatrix()
# 학습, 검증, 테스트용 DMatrix를 생성
# 1. 학습데이터 변환
dtr=xgb.DMatrix(data=X_tr,label=y_tr)
# 2. 검증데이터 변환
dval=xgb.DMatrix(data=X_val,label=y_val)
# 3. 테스트데이터 변환
dtest=xgb.DMatrix(data=X_test,label=y_test)

# 파라미터 세팅
# 유방암 이진 분류
params = {
    'max_depth': 3,  # 트리의 최대 깊이 (값이 클수록 모델이 복잡해지며 과적합 위험 증가)
    'eta': 0.1,  # 학습률 (값이 작을수록 학습 속도가 느려지지만 일반화 성능이 향상될 가능성이 있음)
    'objective': 'binary:logistic',  # 이진 분류 문제를 위한 로지스틱 회귀 설정
    'eval_metric': 'logloss'  # 평가 지표로 로그 손실(log loss) 사용 (값이 작을수록 성능이 좋음)
}

num_rounds = 10000  # 부스팅 반복 횟수 (트리를 몇 개 생성할 것인지 결정)

# 학습 데이터와 검증 데이터 설정
# 학습 데이터 셋(dtr)은 'train', 평가 데이터 셋(dval)은 'eval'로 명기하여 학습 진행 상황을 모니터링
eval_list = [(dtr, 'train'), (dval, 'eval')]

# XGBoost 모델 학습
xgb_model = xgb.train(
    params=params,  # XGBoost 하이퍼파라미터 설정
    dtrain=dtr,  # 학습 데이터 셋
    num_boost_round=num_rounds,  # 부스팅 반복 횟수
    early_stopping_rounds=500,  # 50회 연속 개선이 없으면 학습 조기 종료 (과적합 방지)
    evals=eval_list  # 학습 과정에서 평가할 데이터 셋 목록 (학습 데이터와 검증 데이터)
)

pred_probs=xgb_model.predict(dtest)

# print('predict() 수행 결과값을 10개만 표시, 예측 확률 값으로 표시됨')
# print(np.round(pred_probs[:10],3))
# 예측 확률이 0.5보다 크면 1, 그렇지 않으면 0으로 예측값 결정하여 List 객체인 preds에 저장
preds=[1 if x >0.5 else 0 for x in pred_probs]
# print(f'예측값 10개만 표시 : {preds[:10]}')
# print(y_test[:10],end='')

# 유방암 여부 예측
# 오류에 민감한 모델 : 정확도만 보면 안된다
# 정밀도, 재현률, AUC
def get_clf_eval(y_test,preds=None,pred_probs=None):     # 원래답, 예측값(0,1), 예측 확률(0,1)
    from sklearn.metrics import confusion_matrix        # 혼돈행렬을 구하는 함수 (원래답, 예측값)
    from sklearn.metrics import accuracy_score          # 정확도 (원래답, 예측값)
    from sklearn.metrics import precision_score         # 정밀도 (원래답, 예측값)
    from sklearn.metrics import recall_score            # 재현률 (원래답, 예측값)
    from sklearn.metrics import f1_score                # 정밀도와 재현률의 조화 평균 (원래답, 예측값)
    from sklearn.metrics import roc_auc_score           # AUC 점수 : (원래답, 예측확률)
    confusion=confusion_matrix(
        y_test
        ,preds
    )
    acc=accuracy_score(
        y_test
        ,preds
    )
    precision=precision_score(
        y_test
        ,preds
    )
    recall_score_=recall_score(
        y_test
        ,preds
    )
    f1=f1_score(
        y_test
        ,preds
    )
    auc=roc_auc_score(
        y_test
        ,pred_probs
    )
    print(f'오차행렬\n{confusion}')
    
    # 평가지표 출력
    print(f'정확도 : {acc:.4f}, 정밀도 : {precision:.4f}, 재현률 : {recall_score_:.4f}, F1 : {f1:.4f}, AUC : {auc:.4f}')

# get_clf_eval(y_test,preds,pred_probs)

# 피쳐 중요도 : plot_importance()
import matplotlib.pyplot as plt

# importance_type : 중요도 추출하는 기준
#                   weight(기본값) : 특정 피쳐가 트리를 분기할 때 사용한 빈도수를 의미
#                                   얼마나 자주 트리 분할에 사용되었는지를 나타냄
#                                   다만, 많이 사용되었다고 해서 중요한 특성이 되는건 아님

#                   gain :  특성 트리를 분기할 때, 모델의 성능 개선에 얼마나 기여했는지를 평균
#                           gain 값이 클수록 많이 기여
#                           실제 특성의 중요도 평가 기능

#                   cover : 각 특성이 트리 분기에서 선택되었을 때, 분할에 의해 영향 받은 샘플들의 비율의 평균
#                           많이 사용하지 않는다

plot_importance(
    xgb_model
    ,importance_type='weight'
)
plot_importance(
    xgb_model
    ,importance_type='gain'
)
plot_importance(
    xgb_model
    ,importance_type='cover'
)
plt.show()
```
![image](https://github.com/user-attachments/assets/7218b38c-8828-44a0-ae17-3f7ae40a968d)

![image](https://github.com/user-attachments/assets/a8f0b7a1-9fde-4d10-8031-1c7aae47b761)

![image](https://github.com/user-attachments/assets/bafbf645-b2d0-46dd-8297-24038921591e)

### 보충 설명
1. weight (기본 값)

   설명: 각 특성이 모델을 학습하는 동안 분할에 얼마나 자주 사용되었는지를 기반으로 중요도를 평가합니다. 즉, 해당 특성이 트리 분할에서 몇 번 등장했는지를 나타냅니다.

   의미: weight는 그 특성이 모델에서 얼마나 자주 선택되었는지의 빈도를 보여줍니다. 하지만, 그 특성이 분할 성능에 얼마나 기여했는지에 대한 정보는 제공하지 않습니다.

   장점: 계산이 빠르고 간단합니다.

   단점: 분할에서 자주 등장한다고 해서 그 특성이 반드시 중요한 것은 아니므로, 성능에 대한 기여도를 완벽히 반영하지는 않습니다.

2. gain

   설명: 해당 특성이 트리 분할에서 평균적으로 얻은 정보 이득(Information Gain)을 기준으로 중요도를 평가합니다. 정보 이득은 해당 특성이 분할 시 모델 성능을 얼마나 향상시켰는지를 나타냅니다.

   의미: gain은 특성이 예측 성능에 얼마나 기여했는지를 측정합니다. 즉, 해당 특성이 분할에 얼마나 중요한 역할을 했는지 반영합니다.

   장점: 특성이 예측 성능에 얼마나 기여했는지 구체적으로 알 수 있기 때문에 중요한 특성을 식별하는 데 유용합니다.

   단점: 계산이 상대적으로 더 복잡하고, 다른 기준들과 함께 사용할 때 비교가 필요합니다.

3. cover

   설명: 해당 특성이 분할을 일으킨 샘플의 가중치의 평균을 기준으로 중요도를 평가합니다. 즉, 해당 특성이 분할을 통해 얼마나 많은 샘플을 커버했는지, 즉 얼마나 많은 데이터 포인트에 영향을 미쳤는지를 나타냅니다.

   의미: cover는 특성이 얼마나 넓은 범위의 데이터에 영향을 미쳤는지를 측정합니다. 많은 샘플을 영향을 미친 특성이 더 중요하다고 평가될 수 있습니다.

   장점 : 모델이 어떻게 데이터를 분할하는지에 대한 또 다른 관점을 제공합니다.

   단점: 성능 향상에 대한 정보를 제공하지 않기 때문에, 데이터의 분포나 샘플 수가 중요한 경우에만 유효할 수 있습니다.

---
## sklearn 기반 XGboost

```
import xgboost as xgb
from xgboost import XGBClassifier
from xgboost import plot_importance
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

dataset=load_breast_cancer()
X_features=dataset.data
y_label=dataset.target
cancer_dr=pd.DataFrame(
    data=X_features
    ,columns=dataset.feature_names
)
cancer_dr['target']=y_label

X_features=cancer_dr.iloc[:,:-1] 
y_label=cancer_dr.iloc[:,-1] 

X_train,X_test,y_train,y_test=train_test_split(
    X_features
    ,y_label
    ,train_size=0.8
    ,random_state=156
)

X_tr,X_val,y_tr,y_val=train_test_split(
    X_train
    ,y_train
    ,test_size=0.1
    ,random_state=156
)

dtr=xgb.DMatrix(data=X_tr,label=y_tr)
dval=xgb.DMatrix(data=X_val,label=y_val)
dtest=xgb.DMatrix(data=X_test,label=y_test)

params = {
    'max_depth': 3,  # 트리의 최대 깊이 (값이 클수록 모델이 복잡해지며 과적합 위험 증가)
    'eta': 0.1,  # 학습률 (값이 작을수록 학습 속도가 느려지지만 일반화 성능이 향상될 가능성이 있음)
    'objective': 'binary:logistic',  # 이진 분류 문제를 위한 로지스틱 회귀 설정
    'eval_metric': 'logloss'  # 평가 지표로 로그 손실(log loss) 사용 (값이 작을수록 성능이 좋음)
}

num_rounds = 10000  

eval_list = [(dtr, 'train'), (dval, 'eval')]

# xgb_model = xgb.train(
#     params=params,  # XGBoost 하이퍼파라미터 설정
#     dtrain=dtr,  # 학습 데이터 셋
#     num_boost_round=num_rounds,  # 부스팅 반복 횟수
#     early_stopping_rounds=500,  # 50회 연속 개선이 없으면 학습 조기 종료 (과적합 방지)
#     evals=eval_list  # 학습 과정에서 평가할 데이터 셋 목록 (학습 데이터와 검증 데이터)
# )

pred_probs=xgb_model.predict(dtest)

preds=[1 if x >0.5 else 0 for x in pred_probs]

def get_clf_eval(y_test,preds=None,pred_probs=None):     # 원래답, 예측값(0,1), 예측 확률(0,1)
    from sklearn.metrics import confusion_matrix        # 혼돈행렬을 구하는 함수 (원래답, 예측값)
    from sklearn.metrics import accuracy_score          # 정확도 (원래답, 예측값)
    from sklearn.metrics import precision_score         # 정밀도 (원래답, 예측값)
    from sklearn.metrics import recall_score            # 재현률 (원래답, 예측값)
    from sklearn.metrics import f1_score                # 정밀도와 재현률의 조화 평균 (원래답, 예측값)
    from sklearn.metrics import roc_auc_score           # AUC 점수 : (원래답, 예측확률)
    confusion=confusion_matrix(
        y_test
        ,preds
    )
    acc=accuracy_score(
        y_test
        ,preds
    )
    precision=precision_score(
        y_test
        ,preds
    )
    recall_score_=recall_score(
        y_test
        ,preds
    )
    f1=f1_score(
        y_test
        ,preds
    )
    auc=roc_auc_score(
        y_test
        ,pred_probs
    )
    print(f'오차행렬\n{confusion}')
    
    # 평가지표 출력
    print(f'정확도 : {acc:.4f}, 정밀도 : {precision:.4f}, 재현률 : {recall_score_:.4f}, F1 : {f1:.4f}, AUC : {auc:.4f}')


# 사이킷런 기반 XGboost 사용
from xgboost import XGBClassifier

xgb_wrapper=XGBClassifier(
    n_estimators=1000
    ,learning_rate=0.1
    ,max_depth=3
    ,eval_metric='logloss'
)
xgb_wrapper.fit(X_train,y_train,verbose=True)
w_preds=xgb_wrapper.predict(X_test) # 예측값
w_pred_proba=xgb_wrapper.predict_proba(X_test)[:,1] #이진분류 예측 확률
get_clf_eval(y_test,w_preds,w_pred_proba)

```
![image](https://github.com/user-attachments/assets/94c2e2d1-50b1-44fc-bba2-db6ac2480d09)


```
import xgboost as xgb
from xgboost import XGBClassifier
from xgboost import plot_importance
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

dataset=load_breast_cancer()
X_features=dataset.data
y_label=dataset.target
cancer_dr=pd.DataFrame(
    data=X_features
    ,columns=dataset.feature_names
)
cancer_dr['target']=y_label

X_features=cancer_dr.iloc[:,:-1] 
y_label=cancer_dr.iloc[:,-1] 

X_train,X_test,y_train,y_test=train_test_split(
    X_features
    ,y_label
    ,train_size=0.8
    ,random_state=156
)

X_tr,X_val,y_tr,y_val=train_test_split(
    X_train
    ,y_train
    ,test_size=0.1
    ,random_state=156
)

dtr=xgb.DMatrix(data=X_tr,label=y_tr)
dval=xgb.DMatrix(data=X_val,label=y_val)
dtest=xgb.DMatrix(data=X_test,label=y_test)

params = {
    'max_depth': 3,  # 트리의 최대 깊이 (값이 클수록 모델이 복잡해지며 과적합 위험 증가)
    'eta': 0.1,  # 학습률 (값이 작을수록 학습 속도가 느려지지만 일반화 성능이 향상될 가능성이 있음)
    'objective': 'binary:logistic',  # 이진 분류 문제를 위한 로지스틱 회귀 설정
    'eval_metric': 'logloss'  # 평가 지표로 로그 손실(log loss) 사용 (값이 작을수록 성능이 좋음)
}

num_rounds = 10000  

eval_list = [(dtr, 'train'), (dval, 'eval')]

# xgb_model = xgb.train(
#     params=params,  # XGBoost 하이퍼파라미터 설정
#     dtrain=dtr,  # 학습 데이터 셋
#     num_boost_round=num_rounds,  # 부스팅 반복 횟수
#     early_stopping_rounds=500,  # 50회 연속 개선이 없으면 학습 조기 종료 (과적합 방지)
#     evals=eval_list  # 학습 과정에서 평가할 데이터 셋 목록 (학습 데이터와 검증 데이터)
# )

# pred_probs=xgb_model.predict(dtest)

# preds=[1 if x >0.5 else 0 for x in pred_probs]

def get_clf_eval(y_test,preds=None,pred_probs=None):     # 원래답, 예측값(0,1), 예측 확률(0,1)
    from sklearn.metrics import confusion_matrix        # 혼돈행렬을 구하는 함수 (원래답, 예측값)
    from sklearn.metrics import accuracy_score          # 정확도 (원래답, 예측값)
    from sklearn.metrics import precision_score         # 정밀도 (원래답, 예측값)
    from sklearn.metrics import recall_score            # 재현률 (원래답, 예측값)
    from sklearn.metrics import f1_score                # 정밀도와 재현률의 조화 평균 (원래답, 예측값)
    from sklearn.metrics import roc_auc_score           # AUC 점수 : (원래답, 예측확률)
    confusion=confusion_matrix(
        y_test
        ,preds
    )
    acc=accuracy_score(
        y_test
        ,preds
    )
    precision=precision_score(
        y_test
        ,preds
    )
    recall_score_=recall_score(
        y_test
        ,preds
    )
    f1=f1_score(
        y_test
        ,preds
    )
    auc=roc_auc_score(
        y_test
        ,pred_probs
    )
    print(f'오차행렬\n{confusion}')
    
    # 평가지표 출력
    print(f'정확도 : {acc:.4f}, 정밀도 : {precision:.4f}, 재현률 : {recall_score_:.4f}, F1 : {f1:.4f}, AUC : {auc:.4f}')


# 사이킷런 기반 XGboost 사용
from xgboost import XGBClassifier

# xgb_wrapper=XGBClassifier(
#     n_estimators=1000
#     ,learning_rate=0.1
#     ,max_depth=3
#     ,eval_metric='logloss'
# )
# xgb_wrapper.fit(X_train,y_train,verbose=True)
# w_preds=xgb_wrapper.predict(X_test) # 예측값
# w_pred_proba=xgb_wrapper.predict_proba(X_test)[:,1] #이진분류 예측 확률
# get_clf_eval(y_test,w_preds,w_pred_proba)

from xgboost import XGBClassifier
xgb_wrapper=XGBClassifier(
    n_estimators=1000
    ,early_stopping_rounds=100 # 100번 동안 오차가 개선이 안되면 멈춰라
    ,learning_rate=0.1
    ,max_depth=3
    ,eval_metric='logloss'
)

# 테스트데이터 설정
evals=[
    (X_test,y_test)
]
xgb_wrapper.fit(X_train,y_train,eval_set=evals,verbose=True)
```
![image](https://github.com/user-attachments/assets/c8dc6e56-1ea3-44f1-b500-3bab8ce1c687)

```
import xgboost as xgb
from xgboost import XGBClassifier
from xgboost import plot_importance
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

dataset=load_breast_cancer()
X_features=dataset.data
y_label=dataset.target
cancer_dr=pd.DataFrame(
    data=X_features
    ,columns=dataset.feature_names
)
cancer_dr['target']=y_label

X_features=cancer_dr.iloc[:,:-1] 
y_label=cancer_dr.iloc[:,-1] 

X_train,X_test,y_train,y_test=train_test_split(
    X_features
    ,y_label
    ,train_size=0.8
    ,random_state=156
)

X_tr,X_val,y_tr,y_val=train_test_split(
    X_train
    ,y_train
    ,test_size=0.1
    ,random_state=156
)

dtr=xgb.DMatrix(data=X_tr,label=y_tr)
dval=xgb.DMatrix(data=X_val,label=y_val)
dtest=xgb.DMatrix(data=X_test,label=y_test)

params = {
    'max_depth': 3,  # 트리의 최대 깊이 (값이 클수록 모델이 복잡해지며 과적합 위험 증가)
    'eta': 0.1,  # 학습률 (값이 작을수록 학습 속도가 느려지지만 일반화 성능이 향상될 가능성이 있음)
    'objective': 'binary:logistic',  # 이진 분류 문제를 위한 로지스틱 회귀 설정
    'eval_metric': 'logloss'  # 평가 지표로 로그 손실(log loss) 사용 (값이 작을수록 성능이 좋음)
}

num_rounds = 10000  

eval_list = [(dtr, 'train'), (dval, 'eval')]

# xgb_model = xgb.train(
#     params=params,  # XGBoost 하이퍼파라미터 설정
#     dtrain=dtr,  # 학습 데이터 셋
#     num_boost_round=num_rounds,  # 부스팅 반복 횟수
#     early_stopping_rounds=500,  # 50회 연속 개선이 없으면 학습 조기 종료 (과적합 방지)
#     evals=eval_list  # 학습 과정에서 평가할 데이터 셋 목록 (학습 데이터와 검증 데이터)
# )

# pred_probs=xgb_model.predict(dtest)

# preds=[1 if x >0.5 else 0 for x in pred_probs]

def get_clf_eval(y_test,preds=None,pred_probs=None):     # 원래답, 예측값(0,1), 예측 확률(0,1)
    from sklearn.metrics import confusion_matrix        # 혼돈행렬을 구하는 함수 (원래답, 예측값)
    from sklearn.metrics import accuracy_score          # 정확도 (원래답, 예측값)
    from sklearn.metrics import precision_score         # 정밀도 (원래답, 예측값)
    from sklearn.metrics import recall_score            # 재현률 (원래답, 예측값)
    from sklearn.metrics import f1_score                # 정밀도와 재현률의 조화 평균 (원래답, 예측값)
    from sklearn.metrics import roc_auc_score           # AUC 점수 : (원래답, 예측확률)
    confusion=confusion_matrix(
        y_test
        ,preds
    )
    acc=accuracy_score(
        y_test
        ,preds
    )
    precision=precision_score(
        y_test
        ,preds
    )
    recall_score_=recall_score(
        y_test
        ,preds
    )
    f1=f1_score(
        y_test
        ,preds
    )
    auc=roc_auc_score(
        y_test
        ,pred_probs
    )
    print(f'오차행렬\n{confusion}')
    
    # 평가지표 출력
    print(f'정확도 : {acc:.4f}, 정밀도 : {precision:.4f}, 재현률 : {recall_score_:.4f}, F1 : {f1:.4f}, AUC : {auc:.4f}')


# 사이킷런 기반 XGboost 사용
from xgboost import XGBClassifier

# xgb_wrapper=XGBClassifier(
#     n_estimators=1000
#     ,learning_rate=0.1
#     ,max_depth=3
#     ,eval_metric='logloss'
# )
# xgb_wrapper.fit(X_train,y_train,verbose=True)
# w_preds=xgb_wrapper.predict(X_test) # 예측값
# w_pred_proba=xgb_wrapper.predict_proba(X_test)[:,1] #이진분류 예측 확률
# get_clf_eval(y_test,w_preds,w_pred_proba)

from xgboost import XGBClassifier
xgb_wrapper=XGBClassifier(
    n_estimators=1000
    ,early_stopping_rounds=100 # 100번 동안 오차가 개선이 안되면 멈춰라
    ,learning_rate=0.1
    ,max_depth=3
    ,eval_metric='logloss'
)

# 테스트데이터 설정
evals=[
    (X_test,y_test)
]
xgb_wrapper.fit(X_train,y_train,eval_set=evals,verbose=True)

# 예측값 추출
ws_preds=xgb_wrapper.predict(X_test)

# 예측 확률 추출
ws_prd_proba=xgb_wrapper.predict_proba(X_test)[:,-1]

# 일반화 성능 출력
get_clf_eval(y_test,ws_preds,ws_prd_proba)
```
![image](https://github.com/user-attachments/assets/d295b008-fdc9-400c-8b6b-5b769304c569)

---
## LightGBM

cpu버전
```
pip install lightgbm
```
gpu 버전
```
pip install lightgbm --install-option=--gpu
```

![image](https://github.com/user-attachments/assets/9d28364d-e89d-4766-a060-d5a627126b10)

```
pip install hyperopt
```
![image](https://github.com/user-attachments/assets/d6a2a4ad-ef77-4a44-8204-d3776e4a782d)

```
import lightgbm as lgb
import hyperopt
print(lgb.__version__)
print(hyperopt.__version__)
```
![image](https://github.com/user-attachments/assets/31edb519-9677-4769-969f-c0e834294f91)

```
from hyperopt import hp

# 검색 공간 정의
search_space = {
    # 'x' 변수: -10에서 10 사이의 값을 1 간격으로 균등하게 선택
    'x': hp.quniform('x', -10, 10, 1),
    
    # 'y' 변수: -15에서 15 사이의 값을 1 간격으로 균등하게 선택
    'y': hp.quniform('y', -15, 15, 1)
}
search_space.get('x')
```
![image](https://github.com/user-attachments/assets/bff05115-db54-4a31-b443-70745153edf6)

```
import numpy as np
from hyperopt import hp

# 검색 공간 정의
search_space = {
    # 'x' 변수: -10에서 10 사이의 값을 1 간격으로 균등하게 선택
    'x': hp.quniform('x', -10, 10, 1),
    
    # 'y' 변수: -15에서 15 사이의 값을 1 간격으로 균등하게 선택
    'y': hp.quniform('y', -15, 15, 1)
}

from hyperopt import STATUS_OK

# 목적 함수를 선언, 변수값과 변수 검색 공간을 가지는 딕셔너리를 인자로 받고, 특정 값을 반환
def objective_func(search_space):
    x=search_space['x']
    y=search_space['y']
    retval=x**2-20*y
    
    return retval

from hyperopt import fmin,tpe,Trials
#입력 결과값을 저장한 Trials 객체값 생성
trial_val = Trials()


# 목적 함수의 최솟값을 반환하는 최적 입력 변수값을 5번의 입력값 시도(max_evals=5)로 찾아냄

best_01=fmin(
    fn=objective_func
    ,space=search_space
    ,algo=tpe.suggest
    ,max_evals=5
    ,trials=trial_val
    ,rstate=np.random.default_rng(seed=0)
)

print(f'best : {best_01}')
```
![image](https://github.com/user-attachments/assets/2f3e89c7-450f-4f06-b557-b6609f211529)

```
import numpy as np
import pandas as pd
from hyperopt import hp

# 검색 공간 정의
search_space = {
    # 'x' 변수: -10에서 10 사이의 값을 1 간격으로 균등하게 선택
    'x': hp.quniform('x', -10, 10, 1),
    
    # 'y' 변수: -15에서 15 사이의 값을 1 간격으로 균등하게 선택
    'y': hp.quniform('y', -15, 15, 1)
}

from hyperopt import STATUS_OK

# 목적 함수를 선언, 변수값과 변수 검색 공간을 가지는 딕셔너리를 인자로 받고, 특정 값을 반환
def objective_func(search_space):
    x=search_space['x']
    y=search_space['y']
    retval=x**2-20*y
    
    return retval

from hyperopt import fmin,tpe,Trials
#입력 결과값을 저장한 Trials 객체값 생성
trial_val = Trials()

# 목적 함수의 최솟값을 반환하는 최적 입력 변수값을 20번의 입력값 시도(max_evals=20)로 찾아냄
best_01=fmin(
    fn=objective_func
    ,space=search_space
    ,algo=tpe.suggest
    ,max_evals=20
    ,trials=trial_val
    ,rstate=np.random.default_rng(seed=0)
)

print(f'best : {best_01}')
```
![image](https://github.com/user-attachments/assets/0ef43a06-5780-4bb8-aa5b-ae5e180bae2a)

```
import numpy as np
import pandas as pd
from hyperopt import hp

# 검색 공간 정의
search_space = {
    # 'x' 변수: -10에서 10 사이의 값을 1 간격으로 균등하게 선택
    'x': hp.quniform('x', -10, 10, 1),
    
    # 'y' 변수: -15에서 15 사이의 값을 1 간격으로 균등하게 선택
    'y': hp.quniform('y', -15, 15, 1)
}

from hyperopt import STATUS_OK

# 목적 함수를 선언, 변수값과 변수 검색 공간을 가지는 딕셔너리를 인자로 받고, 특정 값을 반환
def objective_func(search_space):
    x=search_space['x']
    y=search_space['y']
    retval=x**2-20*y
    
    return retval

from hyperopt import fmin,tpe,Trials
#입력 결과값을 저장한 Trials 객체값 생성
trial_val = Trials()

# 목적 함수의 최솟값을 반환하는 최적 입력 변수값을 600번의 입력값 시도(max_evals=600)로 찾아냄
best_01=fmin(
    fn=objective_func
    ,space=search_space
    ,algo=tpe.suggest
    ,max_evals=600
    ,trials=trial_val
    ,rstate=np.random.default_rng(seed=0)
)

print(f'best : {best_01}')
```
![image](https://github.com/user-attachments/assets/17a230e8-223b-4fca-9099-7b51b6b54f05)

```
import numpy as np
import pandas as pd
from hyperopt import hp

# 검색 공간 정의
search_space = {
    # 'x' 변수: -10에서 10 사이의 값을 1 간격으로 균등하게 선택
    'x': hp.quniform('x', -10, 10, 1),
    
    # 'y' 변수: -15에서 15 사이의 값을 1 간격으로 균등하게 선택
    'y': hp.quniform('y', -15, 15, 1)
}

from hyperopt import STATUS_OK

# 목적 함수를 선언, 변수값과 변수 검색 공간을 가지는 딕셔너리를 인자로 받고, 특정 값을 반환
def objective_func(search_space):
    x=search_space['x']
    y=search_space['y']
    retval=x**2-20*y
    
    return retval

from hyperopt import fmin,tpe,Trials
#입력 결과값을 저장한 Trials 객체값 생성
trial_val = Trials()

# 목적 함수의 최솟값을 반환하는 최적 입력 변수값을 600번의 입력값 시도(max_evals=600)로 찾아냄
best_01=fmin(
    fn=objective_func
    ,space=search_space
    ,algo=tpe.suggest
    ,max_evals=600
    ,trials=trial_val
    ,rstate=np.random.default_rng(seed=0)
)

print(f'best : {best_01}')
print(trial_val.results)
```
![image](https://github.com/user-attachments/assets/a0a0c44d-4aa0-4839-8b24-89c520e7e8d3)

```
import numpy as np
import pandas as pd
from hyperopt import hp
import matplotlib.pyplot as plt
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split

from hyperopt import STATUS_OK
from hyperopt import fmin,tpe,Trials

from sklearn.model_selection import cross_val_score

from xgboost import XGBClassifier

import warnings
warnings.filterwarnings('ignore')

dataset=load_breast_cancer()
X_features=dataset.data
y_label=dataset.target
cancer_dr=pd.DataFrame(
    data=X_features
    ,columns=dataset.feature_names
)
cancer_dr['target']=y_label

X_features=cancer_dr.iloc[:,:-1] 
y_label=cancer_dr.iloc[:,-1] 

X_train,X_test,y_train,y_test=train_test_split(
    X_features
    ,y_label
    ,train_size=0.8
    ,random_state=156
)

X_tr,X_val,y_tr,y_val=train_test_split(
    X_train
    ,y_train
    ,test_size=0.1
    ,random_state=156
)

# XGBoost 모델에 대한 하이퍼파라미터 검색 공간을 정의
xgb_search_space = {
    'max_depth': hp.quniform('max_depth', 5, 20, 1),  # 트리 깊이: 5에서 20까지 1씩 증가하는 정수 값
    'min_child_weight': hp.quniform('min_child_weight', 1, 2, 1),  # 최소 자식 노드 가중치: 1에서 2까지 1씩 증가하는 정수 값
    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2),  # 학습률: 0.01에서 0.2 사이의 연속값
    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1)  # 각 트리의 컬럼 샘플 비율: 0.5에서 1 사이의 연속값
}

# 목적 함수 정의: 주어진 하이퍼파라미터 값을 받아 XGBoost 모델의 성능을 평가
def objective_func(search_space):
    # XGBoost 분류기 객체 생성
    xgb_clf = XGBClassifier(
        n_estimators=100,  # 100개의 트리를 사용
        max_depth=int(search_space['max_depth']),  # max_depth를 정수로 변환하여 설정
        min_child_weight=int(search_space['min_child_weight']),  # min_child_weight를 정수로 변환하여 설정
        learning_rate=search_space['learning_rate'],  # learning_rate는 연속값으로 설정
        colsample_bytree=search_space['colsample_bytree'],  # colsample_bytree는 연속값으로 설정
        eval_metric='logloss'  # 평가 지표로 로그 손실 사용
    )
    
    # 교차 검증을 통해 모델 정확도 평가
    accuracy = cross_val_score(xgb_clf, X_train, y_train, scoring='accuracy', cv=3)  # 3겹 교차 검증 사용
    
    # 반환값: 평균 정확도를 음수로 변환하여 최소화하려는 함수로 반환
    # -1 * np.mean(accuracy)에서 -1을 곱하는 이유는 최적화 함수의 목적에 맞게 손실(loss) 값을 최소화하는 방향으로 Hyperopt를 사용할 수 있도록 하기 위해서입니다.
    # 정확도(accuracy)는 높을수록 좋습니다. 하지만 Hyperopt는 최소화를 목표로 하므로, 정확도가 높을수록 손실 값(loss)은 낮아져야 하므로, 음수 값으로 변환하여야 합니다.
    # cross_val_score로 얻은 accuracy는 최대화하려는 값입니다.
    # Hyperopt는 최소화 문제를 풀기 때문에, accuracy 값을 음수로 변환하여 최소화하려는 목적에 맞게 사용합니다.
    return {'loss': -1 * np.mean(accuracy), 'status': STATUS_OK}

# 하이퍼파라미터 튜닝 결과를 저장할 Trials 객체 생성
trial_val = Trials()

# Hyperopt의 fmin 함수 사용하여 하이퍼파라미터 튜닝 시작
best = fmin(
    fn=objective_func,  # 최적화할 함수
    space=xgb_search_space,  # 하이퍼파라미터 검색 공간
    algo=tpe.suggest,  # 트리 구조를 사용하는 베이지안 최적화 방법
    max_evals=50,  # 최대 평가 횟수 50번
    trials=trial_val,  # 튜닝 진행 상황을 기록할 Trials 객체
    rstate=np.random.default_rng(seed=0)  # 난수 생성기의 시드 설정
)

# 최적화 결과 출력
print(f'best : {best}')

```
![image](https://github.com/user-attachments/assets/f76d06c2-c670-488c-afa9-7bb304f5daf0)

```
import numpy as np
import pandas as pd
from hyperopt import hp
import matplotlib.pyplot as plt
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split

from hyperopt import STATUS_OK
from hyperopt import fmin,tpe,Trials

from sklearn.model_selection import cross_val_score

from xgboost import XGBClassifier

import warnings
warnings.filterwarnings('ignore')

dataset=load_breast_cancer()
X_features=dataset.data
y_label=dataset.target
cancer_dr=pd.DataFrame(
    data=X_features
    ,columns=dataset.feature_names
)
cancer_dr['target']=y_label

X_features=cancer_dr.iloc[:,:-1] 
y_label=cancer_dr.iloc[:,-1] 

X_train,X_test,y_train,y_test=train_test_split(
    X_features
    ,y_label
    ,train_size=0.8
    ,random_state=156
)

X_tr,X_val,y_tr,y_val=train_test_split(
    X_train
    ,y_train
    ,test_size=0.1
    ,random_state=156
)

# XGBoost 모델에 대한 하이퍼파라미터 검색 공간을 정의
xgb_search_space = {
    'max_depth': hp.quniform('max_depth', 5, 20, 1),  # 트리 깊이: 5에서 20까지 1씩 증가하는 정수 값
    'min_child_weight': hp.quniform('min_child_weight', 1, 2, 1),  # 최소 자식 노드 가중치: 1에서 2까지 1씩 증가하는 정수 값
    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2),  # 학습률: 0.01에서 0.2 사이의 연속값
    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1)  # 각 트리의 컬럼 샘플 비율: 0.5에서 1 사이의 연속값
}

# 목적 함수 정의: 주어진 하이퍼파라미터 값을 받아 XGBoost 모델의 성능을 평가
def objective_func(search_space):
    # XGBoost 분류기 객체 생성
    xgb_clf = XGBClassifier(
        n_estimators=1000,  # 1000개의 트리를 사용
        max_depth=int(search_space['max_depth']),  # max_depth를 정수로 변환하여 설정
        min_child_weight=int(search_space['min_child_weight']),  # min_child_weight를 정수로 변환하여 설정
        learning_rate=search_space['learning_rate'],  # learning_rate는 연속값으로 설정
        colsample_bytree=search_space['colsample_bytree'],  # colsample_bytree는 연속값으로 설정
        eval_metric='logloss'  # 평가 지표로 로그 손실 사용
    )
    
    # 교차 검증을 통해 모델 정확도 평가
    accuracy = cross_val_score(xgb_clf, X_train, y_train, scoring='accuracy', cv=3)  # 3겹 교차 검증 사용
    
    # 반환값: 평균 정확도를 음수로 변환하여 최소화하려는 함수로 반환
    # -1 * np.mean(accuracy)에서 -1을 곱하는 이유는 최적화 함수의 목적에 맞게 손실(loss) 값을 최소화하는 방향으로 Hyperopt를 사용할 수 있도록 하기 위해서입니다.
    # 정확도(accuracy)는 높을수록 좋습니다. 하지만 Hyperopt는 최소화를 목표로 하므로, 정확도가 높을수록 손실 값(loss)은 낮아져야 하므로, 음수 값으로 변환하여야 합니다.
    # cross_val_score로 얻은 accuracy는 최대화하려는 값입니다.
    # Hyperopt는 최소화 문제를 풀기 때문에, accuracy 값을 음수로 변환하여 최소화하려는 목적에 맞게 사용합니다.
    return {'loss': -1 * np.mean(accuracy), 'status': STATUS_OK}

# 하이퍼파라미터 튜닝 결과를 저장할 Trials 객체 생성
trial_val = Trials()

# Hyperopt의 fmin 함수 사용하여 하이퍼파라미터 튜닝 시작
best = fmin(
    fn=objective_func  # 최적화할 함수
    ,space=xgb_search_space  # 하이퍼파라미터 검색 공간
    ,algo=tpe.suggest  # 트리 구조를 사용하는 베이지안 최적화 방법
    ,max_evals=1000  # 최대 평가 횟수 1000번
    ,trials=trial_val  # 튜닝 진행 상황을 기록할 Trials 객체
    ,rstate=np.random.default_rng(seed=0)  # 난수 생성기의 시드 설정
)

# 최적화 결과 출력
print(f'best : {best}')

output = 'colsample_bytree:{0}, learning_rate:{1}, max_depth:{2}, min_child_weight:{3}'.format(
    round(best['colsample_bytree'], 5),
    round(best['learning_rate'], 5),
    int(best['max_depth']),
    int(best['min_child_weight'])
)

print(output)
```
![image](https://github.com/user-attachments/assets/aec4add3-e050-465f-91fe-906dcdfb4206)

```
# gpu 버전
import numpy as np
import pandas as pd
from hyperopt import hp
import matplotlib.pyplot as plt
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split

from hyperopt import STATUS_OK
from hyperopt import fmin,tpe,Trials

from sklearn.model_selection import cross_val_score

from xgboost import XGBClassifier

import warnings
warnings.filterwarnings('ignore')

dataset=load_breast_cancer()
X_features=dataset.data
y_label=dataset.target
cancer_dr=pd.DataFrame(
    data=X_features
    ,columns=dataset.feature_names
)
cancer_dr['target']=y_label

X_features=cancer_dr.iloc[:,:-1] 
y_label=cancer_dr.iloc[:,-1] 

X_train,X_test,y_train,y_test=train_test_split(
    X_features
    ,y_label
    ,train_size=0.8
    ,random_state=156
)

X_tr,X_val,y_tr,y_val=train_test_split(
    X_train
    ,y_train
    ,test_size=0.1
    ,random_state=156
)

xgb_search_space={
    'max_depth':hp.quniform('max_depth',5,20,1)
    ,'min_child_weight':hp.quniform('min_child_weight',1,2,1)
    ,'learning_rate':hp.uniform('learning_rate',0.01,0.2)
    ,'colsample_bytree':hp.uniform('colsample_bytree',0.5,1)
}

# 목적 함수를 선언, 변수값과 변수 검색 공간을 가지는 딕셔너리를 인자로 받고, 특정 값을 반환
def objective_func(search_space):
    xgb_clf = XGBClassifier(
        n_estimators=100,
        max_depth=int(search_space['max_depth']),
        min_child_weight=int(search_space['min_child_weight']),
        learning_rate=search_space['learning_rate'],
        colsample_bytree=search_space['colsample_bytree'],
        eval_metric='logloss',
        tree_method='gpu_hist',  # Specify GPU usage
        gpu_id=0  # Set to 0 if you have one GPU, or specify the appropriate GPU ID
    )
    accuracy=cross_val_score(xgb_clf,X_train,y_train,scoring='accuracy',cv=3)

    # -1 * np.mean(accuracy)에서 -1을 곱하는 이유는 최적화 함수의 목적에 맞게 손실(loss) 값을 최소화하는 방향으로 Hyperopt를 사용할 수 있도록 하기 위해서입니다.

    return {'loss':-1*np.mean(accuracy), 'status' : STATUS_OK}

#입력 결과값을 저장한 Trials 객체값 생성
trial_val = Trials()

best=fmin(
    fn=objective_func
    ,space=xgb_search_space
    ,algo=tpe.suggest
    ,max_evals=50
    ,trials=trial_val
    ,rstate=np.random.default_rng(seed=0)
)
print(f'best : {best}')
```
![image](https://github.com/user-attachments/assets/3027a68c-bf51-4f59-8db2-a8ce3543d12f)

 > GPU를 사용하면 학습이 더 빠르게 진행될 수 있습니다.
 > 
 > 이로 인해 하이퍼파라미터 튜닝의 과정에서 평가 시간이 달라질 수 있으며, 최적의 하이퍼파라미터가 약간 차이가 날 수 있습니다.

```
import numpy as np
import pandas as pd
from hyperopt import hp
import matplotlib.pyplot as plt
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split

from hyperopt import STATUS_OK
from hyperopt import fmin,tpe,Trials

from sklearn.model_selection import cross_val_score

from xgboost import XGBClassifier

import warnings
warnings.filterwarnings('ignore')

dataset=load_breast_cancer()
X_features=dataset.data
y_label=dataset.target
cancer_dr=pd.DataFrame(
    data=X_features
    ,columns=dataset.feature_names
)
cancer_dr['target']=y_label

X_features=cancer_dr.iloc[:,:-1] 
y_label=cancer_dr.iloc[:,-1] 

X_train,X_test,y_train,y_test=train_test_split(
    X_features
    ,y_label
    ,train_size=0.8
    ,random_state=156
)

X_tr,X_val,y_tr,y_val=train_test_split(
    X_train
    ,y_train
    ,test_size=0.1
    ,random_state=156
)

# XGBoost 모델에 대한 하이퍼파라미터 검색 공간을 정의
xgb_search_space = {
    'max_depth': hp.quniform('max_depth', 5, 20, 1),  # 트리 깊이: 5에서 20까지 1씩 증가하는 정수 값
    'min_child_weight': hp.quniform('min_child_weight', 1, 2, 1),  # 최소 자식 노드 가중치: 1에서 2까지 1씩 증가하는 정수 값
    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2),  # 학습률: 0.01에서 0.2 사이의 연속값
    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1)  # 각 트리의 컬럼 샘플 비율: 0.5에서 1 사이의 연속값
}

# 목적 함수 정의: 주어진 하이퍼파라미터 값을 받아 XGBoost 모델의 성능을 평가
def objective_func(search_space):
    # XGBoost 분류기 객체 생성
    xgb_clf = XGBClassifier(
        n_estimators=100,  # 100개의 트리를 사용
        max_depth=int(search_space['max_depth']),  # max_depth를 정수로 변환하여 설정
        min_child_weight=int(search_space['min_child_weight']),  # min_child_weight를 정수로 변환하여 설정
        learning_rate=search_space['learning_rate'],  # learning_rate는 연속값으로 설정
        colsample_bytree=search_space['colsample_bytree'],  # colsample_bytree는 연속값으로 설정
        eval_metric='logloss'  # 평가 지표로 로그 손실 사용
    )
    
    # 교차 검증을 통해 모델 정확도 평가
    accuracy = cross_val_score(
        xgb_clf # 점수를 구할 모델 설정
        , X_train # 학습데이터 전체를 넣어야한다.
        , y_train # 학습데이터의 값을 넣어야한다.
        , scoring='accuracy' # 평가 점수 : 정화고들 평가
        , cv=3  # 3겹 교차 검증 사용 : 정확도가 3개가 나온다
    ) 
    
    # 반환값: 평균 정확도를 음수로 변환하여 최소화하려는 함수로 반환
    # -1 * np.mean(accuracy)에서 -1을 곱하는 이유는 최적화 함수의 목적에 맞게 손실(loss) 값을 최소화하는 방향으로 Hyperopt를 사용할 수 있도록 하기 위해서입니다.
    # 정확도(accuracy)는 높을수록 좋습니다. 하지만 Hyperopt는 최소화를 목표로 하므로, 정확도가 높을수록 손실 값(loss)은 낮아져야 하므로, 음수 값으로 변환하여야 합니다.
    # cross_val_score로 얻은 accuracy는 최대화하려는 값입니다.
    # Hyperopt는 최소화 문제를 풀기 때문에, accuracy 값을 음수로 변환하여 최소화하려는 목적에 맞게 사용합니다.
    return {'loss': -1 * np.mean(accuracy), 'status': STATUS_OK}

# 하이퍼파라미터 튜닝 결과를 저장할 Trials 객체 생성
trial_val = Trials()

# Hyperopt의 fmin 함수 사용하여 하이퍼파라미터 튜닝 시작
best = fmin(
    fn=objective_func  # 최적화할 함수
    ,space=xgb_search_space  # 하이퍼파라미터 검색 공간
    ,algo=tpe.suggest  # 트리 구조를 사용하는 베이지안 최적화 방법
    ,max_evals=50  # 최대 평가 횟수 50번
    ,trials=trial_val  # 튜닝 진행 상황을 기록할 Trials 객체
    ,rstate=np.random.default_rng(seed=9)  # 난수 생성기의 시드 설정
)

# 최적화 결과 출력
print(f'best : {best}')

output = 'colsample_bytree:{0}, learning_rate:{1}, max_depth:{2}, min_child_weight:{3}'.format(
    round(best['colsample_bytree'], 5),
    round(best['learning_rate'], 5),
    int(best['max_depth']),
    int(best['min_child_weight'])
)

print(output)

# 모델 생성 성능 평가 : get_clf_eval(원래답, 예측값, 예측확률)
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.metrics import precision_score, recall_score
from sklearn.metrics import f1_score, roc_auc_score


def get_clf_eval(y_test, pred=None, pred_proba=None):
    confusion = confusion_matrix( y_test, pred)
    accuracy = accuracy_score(y_test , pred) # 정확도 점수
    precision = precision_score(y_test , pred) # 정밀도 점수
    recall = recall_score(y_test , pred) # 재현율 점수
    f1 = f1_score(y_test,pred) # 정밀도, 재현율 조화평균 값
    # ROC-AUC 추가
    roc_auc = roc_auc_score(y_test, pred_proba) # AUC 점수 : 불균형 데이터 셋에서 필요
    print('오차 행렬')
    print(confusion)
    # ROC-AUC print 추가
    print('정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f},\
    F1: {3:.4f}, AUC:{4:.4f}'.format(accuracy, precision, recall, f1, roc_auc))

xgb_wrapper = XGBClassifier(n_estimators=1000,
                            learning_rate=round(best['learning_rate'], 5),
                            max_depth=int(best['max_depth']),
                            min_child_weight=int(best['min_child_weight']),
                            colsample_bytree=round(best['colsample_bytree'], 5)
                            , early_stopping_rounds=50
                            , eval_metric='logloss'
                           )


evals = [(X_tr, y_tr), (X_val, y_val)]
xgb_wrapper.fit(
    X_tr
    , y_tr
    ,eval_set=evals
    , verbose=True
)

preds = xgb_wrapper.predict(X_test) # 예측값
pred_proba = xgb_wrapper.predict_proba(X_test)[:, 1] # 예측 확률

get_clf_eval(y_test, preds, pred_proba)
```
![image](https://github.com/user-attachments/assets/a1187c22-3ca6-43bb-bab7-cbf714b240fc)
