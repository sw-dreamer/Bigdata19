
```
import numpy as np
import pandas as pd
import yfinance as yf
import torch
from torch.utils.data import TensorDataset, DataLoader
import torch.nn as nn
import torch.optim as optim

import sklearn
from sklearn.preprocessing import StandardScaler

import psycopg2 # pip install psycopg2-binary
from zoneinfo import ZoneInfo

import warnings
warnings.filterwarnings('ignore')

print(f'numpy 버전 : {np.__version__}')
print(f'yahoo 버전 : {yf.__version__}')
print(f'torch 버전 : {torch.__version__}')
print(f'sklearn 버전 : {sklearn.__version__}')
print(f'psycopg2 버전 : {psycopg2.__version__}')
```
![image](https://github.com/user-attachments/assets/285e6491-167a-49e4-997c-df8aabd5d78c)

```
# 시계열데이터 : 10일 데이터로 학습, 11일차 데이터 레이블
data=[10,11,12,13,14,15,16]
 
def create_seq(data,seq_length):
    seqs=[]
    target=[]
    for i in range(len(data)-seq_length):
        seqs.append(data[i:i+seq_length])
        target.append(data[i+seq_length])
    return np.array(seqs), np.array(target)

seq_length=5

nd_data,target=create_seq(data,seq_length)
print(f'nd_data\n{nd_data}')
print(f'target : {target}')
```
![image](https://github.com/user-attachments/assets/d6d7772e-fc1f-4591-a3a9-482ae20ba7a2)

```
# 종목 정의 기간설정
tickers=['AAPL','MSFT','GOOGL']
start_date='2020-04-17'
end_date='2025-04-16'

# 데이터 로딩
data=yf.download(
    tickers=tickers
    ,start=start_date
    ,end=end_date
)['Close']
data
```
![image](https://github.com/user-attachments/assets/3ad78bd0-dfad-4325-af05-c8babc360b25)

```
data.info()
```
![image](https://github.com/user-attachments/assets/b8ab6459-3414-4155-818b-86d28d882203)

```
# 결측치 처리(보간법 이용)
data.interpolate(
    method='linear'
    ,inplace=True
)
```
```
# 스케일링 처리 : 각 종목별 가격 차이 심하다
scaler=StandardScaler()
scaled_data=scaler.fit_transform(data)
scaled_data[-5:]
```
![image](https://github.com/user-attachments/assets/019afc0a-3500-4d7b-8c93-eef135a48d37)

```
# 시계열데이터에서 데이터와 레이블 생성
def create_seq(data,seq_length):
    seqs=[]
    target=[]
    for i in range(len(data)-seq_length):
        seqs.append(data[i:i+seq_length])
        target.append(data[i+seq_length])
    return np.array(seqs), np.array(target)
```
```
seq_length=10 # 10일치
X,y=create_seq(data=scaled_data,seq_length=seq_length)
print('='*25,'X','='*25)
print(X[:1])
print('='*25,'y','='*25)
print(y[:1])
```
![image](https://github.com/user-attachments/assets/14cf5327-63c0-4d3f-bd77-1eb412bda778)

```
# Dataset : data + label <= tensor이어야한다
# tensor 변환
X_tensor=torch.from_numpy(X).float()
# y_tensor=torch.tensor(y)
y_tensor=torch.from_numpy(y).float()

print(f'X type : {type(X)}, X_tensor type : {type(X_tensor)}')
print(f'y type : {type(y)}, y_tensor type : {type(y_tensor)}')

# Dataset
dataset=TensorDataset(X_tensor,y_tensor)

# DataLoader : batch_size=32, shuffle=False
batch_size=32
dataLoader=DataLoader(dataset=dataset,batch_size=batch_size,shuffle=False)

print('='*100)
# 데이터로더 출력
for i, (x_batch, y_batch) in enumerate(dataLoader, start=1):
    print(f'배치 {i}번')
    print(f'입력 배치 크기 : {x_batch.shape}')
    print(f'레이블 배치 크기 : {y_batch.shape}')
```
![image](https://github.com/user-attachments/assets/230df4a8-e458-466b-a4d0-3ad4cf9e0e00)

```
class LSTMtorchModel(nn.Module):
    def __init__(self, input_size,hidden_size,output_size):
        super(LSTMtorchModel,self).__init__()
        self.lstm=nn.LSTM(input_size,hidden_size,batch_first=True)
        self.layer1=nn.Linear(hidden_size,output_size)
    def forward(self,x):
        lstm_out,_=self.lstm(x)
        out=self.layer1(lstm_out[:,-1,:])
        return out # 신경망 거쳐서 나온 예측값   

```
```
# 설정 
input_size=len(tickers) # 입력되는 피쳐수 (3개의 종목)

# LSTM : 셀
hidden_size=32 # hidden size가 unit의 개수가 아니다

output_size=len(tickers)
```
```
model=LSTMtorchModel(input_size=input_size,hidden_size=hidden_size,output_size=output_size)

loss_fn=nn.MSELoss()

optimizer=optim.Adam(
    model.parameters()
    ,lr=0.001
)

epochs=20

for epoch in range(epochs):
    epoch_loss=0
    for xb,yb in dataLoader:
        pred=model(xb)
        loss=loss_fn(pred,yb)
        optimizer.zero_grad() # 기울기 초기화
        loss.backward()
        optimizer.step()
        
        epoch_loss += loss.item()
    print(f'Epoch : {epoch+1} / {epochs}, Loss : {epoch_loss}/{len(dataLoader)}')
```
![image](https://github.com/user-attachments/assets/036c4679-ca95-4ef9-be4f-f0eb79f2d035)

```
# 추론 : 뉴 데이터입력해서 예측
# 추론하려면 모드를 변경
# 학습모드(model.train()), 추론 모드(model.eval())
model.eval()

# 추론은 포워딩해야한다. 백워드(미분)을 하면 안된다.
with torch.no_grad():
    preds=model(X_tensor).numpy() # StandScaler로 해서 원래의 값으로 돌려야한다.
    preds_original=scaler.inverse_transform(preds)

print(f'예측된 주가 : {preds_original[-1]}')
```
![image](https://github.com/user-attachments/assets/44364b30-b6ac-4df6-acbd-b8308f0d3bda)

### LSTM,GRU : 순환신경망
### 문제점 : 장기 기억이 잘 안된다. 병렬 처리가 비효율
### Transformer 계열의 신경망 활용
### 1. PatchTST
### 2. Informer
### 3. TimeNet : 푸리에변환, 시계열데이터의 특성 추출
