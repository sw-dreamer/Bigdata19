# 순환 신경망으로 순차 데이터 모델링

```
pip install torch==2.1.1+cu118 --index-url https://download.pytorch.org/whl/cu118 torchtext==0.16.0 torchdata==0.7.0 torchvision==0.16.1+cu118 --no-deps --no-cache-dir
```
```
pip install portalocker
```

---
## 영화리뷰 감성분석
```
# 데이터 로딩
from torchtext.datasets import IMDB

train_dataset=IMDB(split='train')
test_dataset=IMDB(split='test')
```
```
train_dataset
```
![image](https://github.com/user-attachments/assets/64dccf34-2105-4c36-8729-a7d13c8e5ddc)

```
# 내용 확인
for idx, item in enumerate(train_dataset):
    print(f"idx: {idx}, item: {item[1]}")
```
![image](https://github.com/user-attachments/assets/4486599e-0d78-4785-b54f-e6cc35602b03)

```
import torch
import torch.nn as nn

from torch.utils.data.dataset import random_split
import re
import numpy as np
import pandas as pd
from collections import Counter, OrderedDict
```
```
torch.manual_seed(1)
train_dataset = list(train_dataset)
total_length = len(train_dataset)
print(f"Total length of train_dataset: {total_length}")
split_lengths = [int(total_length * 0.8), total_length - int(total_length * 0.8)]
train_dataset, valid_dataset = random_split(train_dataset, split_lengths)
print(f"Training set size: {len(train_dataset)}")
print(f"Validation set size: {len(valid_dataset)}")
```
![image](https://github.com/user-attachments/assets/a7dfc7bc-80c8-4d99-a286-bb667b6c4069)
```
token_counts = Counter()

def tokenizer(text):
    text = re.sub('<[^>]*>', '', text)
    emoticons = re.findall('(?::|;|=)(?:-)?(?:\)|\(|D|P)', text.lower())
    text = re.sub('[\W]+', ' ', text.lower()) +\
        ' '.join(emoticons).replace('-', '')
    tokenized = text.split()
    return tokenized

for label, line in train_dataset:
    tokens = tokenizer(line)
    token_counts.update(tokens)

print('어휘 사전 크기:', len(token_counts))
```
![image](https://github.com/user-attachments/assets/ebaf2e6b-0358-4018-8741-ac2ca37f7b72)

```
token_counts.items()
```
![image](https://github.com/user-attachments/assets/e07763b1-5eaf-4f66-a65b-0880f8e45b56)

```
# 단어 사전 : vocab 생성
# 빈도수 높은 단어순으로 정렬
# 1. 1000 단어 : 1000단어에 포함되지 않은 단어 입력될 수 있다.
# 2. 패딩 : 500단어 입력 설정, 300단어 들어왔으면 200단어 패딩 처리 필요

from torchtext.vocab import vocab #vocav(ordered_dict)

sorted_by_freq_tuples=sorted(
    token_counts.items()
    ,key=lambda x:x[1]
    ,reverse=True
)
```
```
sorted_by_freq_tuples[0]
```
![image](https://github.com/user-attachments/assets/0ea4e12a-07c5-490c-a9ce-975e99da7a19)
