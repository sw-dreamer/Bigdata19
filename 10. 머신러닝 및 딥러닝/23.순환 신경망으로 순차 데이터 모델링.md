# 순환 신경망으로 순차 데이터 모델링

```
pip install torch==2.1.1+cu118 --index-url https://download.pytorch.org/whl/cu118 torchtext==0.16.0 torchdata==0.7.0 torchvision==0.16.1+cu118 --no-deps --no-cache-dir
```
```
pip install portalocker
```

---
## 영화리뷰 감성분석
```
# 데이터 로딩
from torchtext.datasets import IMDB

train_dataset=IMDB(split='train')
test_dataset=IMDB(split='test')
```
```
train_dataset
```
![image](https://github.com/user-attachments/assets/64dccf34-2105-4c36-8729-a7d13c8e5ddc)

```
# 내용 확인
for idx, item in enumerate(train_dataset):
    print(f"idx: {idx}, item: {item[1]}")
```
![image](https://github.com/user-attachments/assets/4486599e-0d78-4785-b54f-e6cc35602b03)

```
import torch
import torch.nn as nn

from torch.utils.data.dataset import random_split
import re
import numpy as np
import pandas as pd
from collections import Counter, OrderedDict
```
```
token_counts = Counter()

def tokenizer(text):
    text = re.sub('<[^>]*>', '', text)
    emoticons = re.findall('(?::|;|=)(?:-)?(?:\)|\(|D|P)', text.lower())
    text = re.sub('[\W]+', ' ', text.lower()) +\
        ' '.join(emoticons).replace('-', '')
    tokenized = text.split()
    return tokenized

for label, line in train_dataset:
    tokens = tokenizer(line)
    token_counts.update(tokens)

print('어휘 사전 크기:', len(token_counts))
```
![image](https://github.com/user-attachments/assets/63867525-e765-49e4-80de-07f0bb4d74c2)
