# 순환 신경망으로 순차 데이터 모델링

```
pip install torch==2.1.1+cu118 --index-url https://download.pytorch.org/whl/cu118 torchtext==0.16.0 torchdata==0.7.0 torchvision==0.16.1+cu118 --no-deps --no-cache-dir
```
```
pip install portalocker
```

---
## 영화리뷰 감성분석
```
# 데이터 로딩
from torchtext.datasets import IMDB

train_dataset=IMDB(split='train')
test_dataset=IMDB(split='test')
```
```
train_dataset
```
![image](https://github.com/user-attachments/assets/64dccf34-2105-4c36-8729-a7d13c8e5ddc)

```
# 내용 확인
for idx, item in enumerate(train_dataset):
    print(f"idx: {idx}, item: {item[1]}")
```
![image](https://github.com/user-attachments/assets/4486599e-0d78-4785-b54f-e6cc35602b03)

```
import torch
import torch.nn as nn

from torch.utils.data.dataset import random_split
import re
import numpy as np
import pandas as pd
from collections import Counter, OrderedDict
```
```
train_dataset = list(train_dataset)
total_length = len(train_dataset)
print(f"Total length of train_dataset: {total_length}")
split_lengths = [int(total_length * 0.8), total_length - int(total_length * 0.8)]
train_dataset, valid_dataset = random_split(train_dataset, split_lengths)
print(f"Training set size: {len(train_dataset)}")
print(f"Validation set size: {len(valid_dataset)}")
```
![image](https://github.com/user-attachments/assets/a7dfc7bc-80c8-4d99-a286-bb667b6c4069)
```
token_counts = Counter()

def tokenizer(text):
    text = re.sub('<[^>]*>', '', text)
    emoticons = re.findall('(?::|;|=)(?:-)?(?:\)|\(|D|P)', text.lower())
    text = re.sub('[\W]+', ' ', text.lower()) +\
        ' '.join(emoticons).replace('-', '')
    tokenized = text.split()
    return tokenized

for label, line in train_dataset:
    tokens = tokenizer(line)
    token_counts.update(tokens)

print('어휘 사전 크기:', len(token_counts))
```
![image](https://github.com/user-attachments/assets/ebaf2e6b-0358-4018-8741-ac2ca37f7b72)

