# 순환 신경망으로 순차 데이터 모델링

```
pip install torch==2.1.1+cu118 --index-url https://download.pytorch.org/whl/cu118 torchtext==0.16.0 torchdata==0.7.0 torchvision==0.16.1+cu118 --no-deps --no-cache-dir
```
```
pip install portalocker
```

---
## 영화리뷰 감성분석
```
# 데이터 로딩
from torchtext.datasets import IMDB

train_dataset=IMDB(split='train')
test_dataset=IMDB(split='test')
```
```
train_dataset
```
![image](https://github.com/user-attachments/assets/64dccf34-2105-4c36-8729-a7d13c8e5ddc)

```
# 내용 확인
for idx, item in enumerate(train_dataset):
    print(f"idx: {idx}, item: {item[1]}")
```
![image](https://github.com/user-attachments/assets/4486599e-0d78-4785-b54f-e6cc35602b03)

```
import torch
import torch.nn as nn

from torch.utils.data.dataset import random_split
import re
import numpy as np
import pandas as pd
from collections import Counter, OrderedDict
```
```
torch.manual_seed(1)
train_dataset = list(train_dataset)
total_length = len(train_dataset)
print(f"Total length of train_dataset: {total_length}")
split_lengths = [int(total_length * 0.8), total_length - int(total_length * 0.8)]
train_dataset, valid_dataset = random_split(train_dataset, split_lengths)
print(f"Training set size: {len(train_dataset)}")
print(f"Validation set size: {len(valid_dataset)}")
```
![image](https://github.com/user-attachments/assets/a7dfc7bc-80c8-4d99-a286-bb667b6c4069)
```
token_counts = Counter()

def tokenizer(text):
    text = re.sub('<[^>]*>', '', text)
    emoticons = re.findall('(?::|;|=)(?:-)?(?:\)|\(|D|P)', text.lower())
    text = re.sub('[\W]+', ' ', text.lower()) +\
        ' '.join(emoticons).replace('-', '')
    tokenized = text.split()
    return tokenized

for label, line in train_dataset:
    tokens = tokenizer(line)
    token_counts.update(tokens)

print('어휘 사전 크기:', len(token_counts))
```
![image](https://github.com/user-attachments/assets/ebaf2e6b-0358-4018-8741-ac2ca37f7b72)

```
token_counts.items()
```
![image](https://github.com/user-attachments/assets/e07763b1-5eaf-4f66-a65b-0880f8e45b56)

```
# 단어 사전 : vocab 생성
# 빈도수 높은 단어순으로 정렬
# 1. 1000 단어 : 1000단어에 포함되지 않은 단어 입력될 수 있다.
# 2. 패딩 : 500단어 입력 설정, 300단어 들어왔으면 200단어 패딩 처리 필요

from torchtext.vocab import vocab #vocav(ordered_dict)

sorted_by_freq_tuples=sorted(
    token_counts.items()
    ,key=lambda x:x[1]
    ,reverse=True
)
```
```
sorted_by_freq_tuples[0]
```
![image](https://github.com/user-attachments/assets/0ea4e12a-07c5-490c-a9ce-975e99da7a19)

```
order_dict=OrderedDict(sorted_by_freq_tuples)
vocab=vocab(order_dict)
vocab
```
![image](https://github.com/user-attachments/assets/75deafaa-eb60-4de9-8851-0e7e5be05b60)

```
# 단어 사전에 패딩 추가 : 남는 자리 채우기
vocab.insert_token('<pad>',0) # '<pad>'라는 문자열을 0번 index에 삽입
# 없는 단어를 처리하는 토큰 어휘사전에 특정값으로 삽입
vocab.insert_token('<unk>',1)
# 존재하지 않는 단어(토큰)을 조회할 때 반환할 기본 인덱스 처리
vocab.set_default_index(1)
```
```
# 어휘사전 테스트
print(
    [vocab[token] for token in ['this','is','an','example']]
)
```
![image](https://github.com/user-attachments/assets/7a4570b2-bcff-4640-98df-ce5b66a8406b)

```
# 레이블 변환
# 현재 레이블 -> 부정 : 1->0, 긍정:2->1 변환하는 함수 오브젝트 생성
# x:레이블
label_pipeline=lambda x:1. if x==2 else 0 
# 단어를 입력해서 숫자 출력 함수 오브젝트 생성
# x: 한개의 리뷰(문장=>토큰->숫자)
text_pipeline=lambda x:[vocab[token] for token in tokenizer(x)]
```
```
text_pipeline('this is an example')
```
![image](https://github.com/user-attachments/assets/4d60abec-4b92-4017-9941-a1ad39b7d8b8)

```
text_pipeline('Though New Line might just be looking at sales, they all know the only reason this one made more money than the one prior was due to its 3D ending.')
```
![image](https://github.com/user-attachments/assets/bce5640d-a6a6-44c4-89d7-0abf92c78286)

```
if not torch.cuda.is_available():
    print('gpu 처리 안됨')
else:
    print('gpu 처리 됨')
```
![image](https://github.com/user-attachments/assets/48b8edf2-4494-4049-82ef-72d82a3f0647)

```
device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(device)
```
![image](https://github.com/user-attachments/assets/0b3c755a-775a-4604-901f-60d77e219850)

```
# 배치 데이터를 처리하고 패딩을 추가하는 커스텀 collate 함수
def collate_batch(batch):
    label_list = []  # 처리된 라벨을 저장할 리스트
    text_list = []   # 처리된 텍스트(토큰화된 텍스트)를 저장할 리스트
    lengths = []     # 각 텍스트의 길이를 저장할 리스트
    
    # 배치 내의 각 샘플에 대해 반복 (각 샘플은 (라벨, 텍스트)의 튜플)
    for _label, _text in batch:
        # 라벨에 변환을 적용하고 label_list에 추가
        label_list.append(label_pipeline(_label))
        
        # 텍스트에 대해 처리 (예: 토큰화) 후, int64 타입으로 텐서 변환
        processed_text = torch.tensor(
            text_pipeline(_text),  # 텍스트 처리 파이프라인 적용 (예: 토큰화)
            dtype=torch.int64      # 데이터 타입을 int64로 설정
        )
        
        # 처리된 텍스트 텐서를 text_list에 추가
        text_list.append(processed_text)
        
        # 처리된 텍스트의 길이 (토큰 수)를 lengths 리스트에 추가
        lengths.append(processed_text.size(0))
    
    # label_list와 lengths를 텐서로 변환
    label_list = torch.tensor(label_list)
    lengths = torch.tensor(lengths)
    
    # 텍스트 시퀀스들을 패딩하여 같은 길이로 맞춤 (짧은 시퀀스는 0으로 패딩)
    padded_text_list = nn.utils.rnn.pad_sequence(
        text_list,       # 텍스트 시퀀스들의 리스트
        batch_first=True # 배치 차원을 첫 번째로 하여 시퀀스 길이를 맞춤
    )
    
    # 패딩된 텍스트 리스트, 라벨 리스트, 길이 리스트를 반환
    return padded_text_list.to(device), label_list.to(device), lengths.to(device)


# 커스텀 collate_fn을 사용하여 훈련 데이터셋에 대한 DataLoader 생성
from torch.utils.data import DataLoader

dataLoader = DataLoader(
    train_dataset,     # 로드할 데이터셋
    batch_size=4,      # 배치 당 샘플 수
    shuffle=False,     # 데이터셋을 섞을지 여부 (True로 설정하면 무작위 샘플링)
    collate_fn=collate_batch  # 위에서 정의한 커스텀 collate 함수를 사용
)

```
```
text_batch,label_batch,length_batch =next(iter(dataLoader))
print(text_batch)
```
![image](https://github.com/user-attachments/assets/454b3983-565e-4b82-a491-f0d1b350d459)

```
# 임베딩층 테스트
# 층 생성
# 임의의 데이터 입력
# 출력 결과 확인
embedding=nn.Embedding(
    num_embeddings=10 # 어휘사전의 크기. [0,1,2,3,~,8,9]
    ,embedding_dim=3 # 임베딩 벡터 차원, 축솰 차원
    ,padding_idx=0 # 패딩을 위한 특별한 인덱스 지정
)

# 임의 데이터 입력
text_encoded_input=torch.LongTensor(
    [
        [1,2,3,4], # 4개 단어가 들어있는 문자, 숫자
        [4,3,2,0] # 3개 단어가 들어있는 문자, 패딩
    ]
)
print(embedding(text_encoded_input))
```
![image](https://github.com/user-attachments/assets/bfb133b4-e568-4e83-8853-2bfa1221d7bd)
