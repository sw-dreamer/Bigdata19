# 기본적인 형태의 셀프 어텐션
```
import torch
# 입력 문자 생성 : 'can you help me to translate this sentence'
sentence=torch.tensor(
    [
        0,  # can
        7,  # you
        1,  # help
        2,  # me
        5,  # to
        6,  # translate
        4,  # this
        3   # sentence
    ]
)
sentence # vocab(딕셔너리)가 있다라고 가정 : 사이즈는 10 가정
```
![image](https://github.com/user-attachments/assets/ca956c45-8491-4abf-be24-10011acbe7af)

```
# 특정 단어가 임베딩 되어 있다라는 가정하에 만들어진 코드
torch.manual_seed(1)
embed = torch.nn.Embedding(10, 16)
embedded_sentence = embed(sentence).detach()  # detach는 계산 그래프에서 분리하여 이후 연산이 역전파에 영향을 주지 않도록 함 (gradient 계산 제외)
embedded_sentence.shape
```
![image](https://github.com/user-attachments/assets/57b5b50f-c63b-46b2-8f84-c7aa2566287f)

```
# 8x8 크기의 빈 텐서(초기값이 비어있는 텐서)를 생성합니다. omega는 유사도 행렬(또는 내적 행렬)을 저장하는 데 사용됩니다.
omega = torch.empty(8, 8)

# embedded_sentence는 8개의 벡터(예: 단어 임베딩)로 구성된 리스트(또는 텐서)라고 가정합니다.
# 각 단어 임베딩 쌍 (x_i, x_j)에 대해 내적을 계산하여 omega의 각 원소에 저장합니다.
for i, x_i in enumerate(embedded_sentence):      # i번째 단어 임베딩 x_i에 대해
    for j, x_j in enumerate(embedded_sentence):  # j번째 단어 임베딩 x_j에 대해
        omega[i, j] = torch.dot(x_i, x_j)        # x_i와 x_j의 내적을 계산하여 omega[i, j]에 저장
```
```
omega_mat = embedded_sentence.matmul(embedded_sentence.T)
```
```
torch.allclose(omega_mat, omega) 
```
![image](https://github.com/user-attachments/assets/5ccc1ff7-bc35-4273-854a-d48b8496e0a1)

```
import torch.nn.functional as F
attention_weights =F.softmax(omega,dim=1) # 어텐션 가중치
attention_weights
```
![image](https://github.com/user-attachments/assets/7280f255-6120-4208-ad43-fa7a33b065f4)

```
attention_weights.sum(dim=1)
```
![image](https://github.com/user-attachments/assets/3d581877-90f7-4d1b-8303-7628c5c30313)

```
context_vectors = torch.matmul(
        attention_weights, embedded_sentence)
context_vectors.shape
```
![image](https://github.com/user-attachments/assets/29ed96d0-e7e2-4b69-8dc4-24a2c09ec7c7)
