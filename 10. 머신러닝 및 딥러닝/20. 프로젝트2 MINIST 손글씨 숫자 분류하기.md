# 프로젝트2 MINIST 손글씨 숫자 분류하기

```
import torch
import torchvision
from torchvision import transforms
from torch.utils.data import DataLoader,TensorDataset
image_path='./'
transform= transforms.Compose(
    [transforms.ToTensor()]
)

minist_train_dataset=torchvision.datasets.MNIST(
    root=image_path
    ,train=True
    , transform=transform
    ,download=True
)
minist_test_dataset=torchvision.datasets.MNIST(
    root=image_path
    ,train=False
    , transform=transform
    ,download=True
)

batch_size=64
torch.manual_seed(1)

train_dl=DataLoader(
    minist_train_dataset
    ,batch_size=batch_size
    ,shuffle=True
)
```
![image](https://github.com/user-attachments/assets/7166818f-2f7b-401e-9600-70220610e1e4)

```
import matplotlib.pyplot as plt

images, labels = next(iter(train_dl))  # 첫 번째 배치 가져오기
images = images.squeeze()  # [64, 28, 28]

plt.figure(figsize=(10, 10))  # 전체 크기 조정

for i in range(64):
    plt.subplot(8, 8, i+1)
    plt.imshow(images[i], cmap='gray')
    plt.title(str(labels[i].item()), fontsize=8)
    plt.axis('off')  # 축 제거

plt.tight_layout()
plt.show()
```
![image](https://github.com/user-attachments/assets/b40e782d-649e-49af-aa32-345d4d73134e)

```
import torch.nn as nn
hidden_units=[32,16]
image_size=minist_train_dataset[0][0].shape
print(f'image_size : {image_size}')
input_size=image_size[0]*image_size[1]*image_size[2]
print(f'input_size : {input_size}')
all_layers=[
    nn.Flatten()
]
for hidden_unit in hidden_units:
    layer=nn.Linear(input_size,hidden_unit)
    all_layers.append(layer)
    all_layers.append(nn.ReLU())
    input_size=hidden_unit
print(f'hidden_units : {hidden_units}')
print(f'hidden_units[-1] : {hidden_units[-1]}')
all_layers.append(nn.Linear(hidden_units[-1],10))
model=nn.Sequential(*all_layers)
model
```
![image](https://github.com/user-attachments/assets/c99b0c9c-18d4-46b2-899e-91fc516d1a5f)

```
loss_fn=nn.CrossEntropyLoss()
optimizer=torch.optim.Adam(model.parameters(),lr=0.001)
torch.manual_seed(1)
num_epochs=20
for epoch in range(num_epochs):
    accuracy_hist_train=0
    for x_batch,y_batch in train_dl:
        pred=model(x_batch)
        loss=loss_fn(pred,y_batch)
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
        is_correct=(
            torch.argmax(pred,dim=1) == y_batch
        ).float()
        accuracy_hist_train += is_correct.sum()
    accuracy_hist_train /= len(train_dl.dataset)
    print(f'에포크 : {epoch} 정확도 : {accuracy_hist_train}')
```
![image](https://github.com/user-attachments/assets/2f03e56e-c57c-4d8c-8f83-0c93bca2ca39)
